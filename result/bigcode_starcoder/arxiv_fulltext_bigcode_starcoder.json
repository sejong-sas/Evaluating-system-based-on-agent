{
  "model_id": "bigcode/starcoder",
  "full_texts": [
    {
      "arxiv_id": "1911.02150",
      "full_text": "arXiv:1911.02150v1  [cs.NE]  6 Nov 2019\nFast Transformer Decoding: One Write-Head is All\nYou Need\nNoam Shazeer\nGoogle\nnoam@google.com\nNovember 7, 2019\nAbstract\nMulti-head attention layers, as used in the Transformer neural sequence model, are a powerful alter-\nnative to RNNs for moving information across and between sequences. While training these layers is\ngenerally fast and simple, due to parallelizability across the length of the sequence, incremental inference\n(where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly\nloading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where\nthe keys and values are shared across all of the diﬀerent attention \"heads\", greatly reducing the size of\nthese tensors and hence the memory bandwidth requirements of incremental decoding. We verify exper-\nimentally that the resulting models can indeed be much faster to decode, and incur only minor quality\ndegradation from the baseline.\n1\nIntroduction\nThe Transformer neural sequence model [Vaswani et al., 2017] has emerged as a popular alternative to\nrecurrent sequence models.\nTransformer relies on attention layers to communicate information between\nand across sequences. One major challenge with Transformer is the speed of incremental inference. As we\nwill discuss, the speed of incremental Transformer inference on modern computing hardware is limited by\nthe memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state\nof the attention layers. In the following sections, we will review the multi-head-attention layers used by\nTransformer, provide a performance analysis, and propose an architectural variation (multi-query attention)\nwhich greatly improves inference speed with only minor quality degradation.\n2\nBackground: Neural Attention\nNeural Attention, introduced by [Bahdanau et al., 2014], is a powerful tool for manipulating variable-length\nrepresentations. A neural attention function takes a single query-vector q and a set of m diﬀerent (key-vector,\nvalue-vector) pairs (represented by the matrices K and V ), and produces an output vector y. The output y\nis computed as a weighted sum of the diﬀerent value vectors, where the weights are derived by comparing\nthe query to the keys.\n2.1\nDot-Product Attention\nThe following code describes a common formulation, where the weights are computed as the softmax of the\ndot-products of the query with the diﬀerent keys.\n1\n\ndef\nDotProductAttention (q , K, V) :\n\" \" \" Dot−Product Attention on one query .\nArgs :\nq :\na vector\nwith shape\n[ k ]\nK:\na matrix with shape\n[m,\nk ]\nV:\na matrix with shape\n[m,\nv ]\nReturns :\ny :\na vector\nwith shape\n[ v ]\n\" \" \"\nl o g i t s = t f . einsum ( \"k ,mk−>m\" , q , K)\nweights = t f . softmax ( l o g i t s )\nreturn\nt f . einsum ( \"m,mv−>v\" ,\nweights , V)\nOur code samples use einsum notation, as deﬁned in TensorFlow and numpy, for generalized contractions\nbetween tensors of arbitrary dimension. In this notation, an equation names the dimensions of the input and\noutput Tensors. The computation is numerically equivalent to broadcasting each input to have the union of\nall dimensions, multiplying component-wise, and summing across all dimensions not in the desired output\nshape.\n2.2\nMulti-head Attention\nThe \"Transformer\" seuqence-to-sequence model [Vaswani et al., 2017] uses h diﬀerent attention layers (heads)\nin parallel, which the authors refer to as \"Multi-head attention\". The query vectors for the h diﬀerent layers\nare derived from h diﬀerent learned linear projections Pq of an input vector x.\nSimilarly, the keys and\nvalues are derived from h diﬀerent learned linear projections Pk, Pv of a collection M of m diﬀerent input\nvectors. The outputs of the h layers are themselves passed through diﬀerent learned linear projections Po,\nthen summed. For simplicity, we give the input and output vectors identical dimensionality d. The The\ncomputation can be expressed as follows:\ndef\nMultiheadAttention (\nx , M, P_q, P_k, P_v, P_o) :\n\" \" \" Multi−head Attention on one query .\nArgs :\nx :\na vector\nwith shape\n[ d ]\nM:\na matrix with shape\n[m, d ]\nP_q:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_k:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_v:\na tensor\nwith shape\n[ h , d ,\nv ]\nP_o:\na tensor\nwith shape\n[ h , d ,\nv ]\nReturns :\ny :\na vector\nwith shape\n[ d ]\n\" \" \"\nq = t f . einsum ( \"d , hdk−>hk \" , x , P_q)\nK = t f . einsum ( \"md, hdk−>hmk\" , M, P_k)\nV = t f . einsum ( \"md, hdv−>hmv\" , M, P_v)\nl o g i t s = t f . einsum ( \"hk ,hmk−>hm\" , q , K)\nweights = t f . softmax ( l o g i t s )\no = t f . einsum ( \"hm,hmv−>hv \" ,\nweights , V)\ny = t f . einsum ( \"hv , hdv−>d\" , o , P_o)\nreturn y\nNote: [Vaswani et al., 2017] include a constant scaling factor on the logits. We omit this in our code, as\nit can be folded into the linear projections Pq or Pk.\n2\n\n2.3\nMulti-head Attention (Batched)\nIn practice, it is far more eﬃcient to batch together multiple queries. The code below adds two types of\nbatching. First, we generate queries from n diﬀerent positions in a sequence. These queries all interact with\nthe same keys and values. In addition, we process a batch of b diﬀerent non-interacting sequences at once.\nFollowing [Vaswani et al., 2017], in an autoregressive model, we can prevent backward-information-ﬂow by\nadding a \"mask\" to the logits containing the value −∞in the illegal positions.\ndef\nMultiheadAttentionBatched(\nX, M, mask , P_q, P_k, P_v, P_o) :\n\" \" \" Multi−head Attention .\nArgs :\nX:\na tensor\nwith shape\n[ b , n , d ]\nM:\na tensor\nwith shape\n[ b , m, d ]\nmask :\na tensor\nwith shape\n[ b , h , n , m]\nP_q:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_k:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_v:\na tensor\nwith shape\n[ h , d ,\nv ]\nP_o:\na tensor\nwith shape\n[ h , d ,\nv ]\nReturns :\nY:\na tensor\nwith shape\n[ b , n , d ]\n\" \" \"\nQ = t f . einsum ( \"bnd , hdk−>bhnk \" , X, P_q)\nK = t f . einsum ( \"bmd, hdk−>bhmk\" , M, P_k)\nV = t f . einsum ( \"bmd, hdv−>bhmv\" , M, P_v)\nl o g i t s = t f . einsum ( \"bhnk , bhmk−>bhnm\" , Q, K)\nweights = t f . softmax ( l o g i t s + mask)\nO = t f . einsum ( \"bhnm,bhmv−>bhnv \" ,\nweights , V)\nY = t f . einsum ( \"bhnv , hdv−>bnd\" , O, P_o)\nreturn Y\n2.3.1\nPerformance Analysis of Batched Multi-head Attention\nTo simplify the performance analysis, we will make several simplifying assumptions:\n• m = n\n• k = v = d\nh, as suggested by [Vaswani et al., 2017]\n• n ≤d\nThe total number of arithmetic operations is Θ(bnd2). (Since the complexity of each of the tf.einsum\noperations above is O(bnd2) given the simplifying assumptions.\nThe total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved:\nO(bnd + bhn2 + d2). The ﬁrst term is due to X, M, Q, K, V , O and Y , the second term due to the logits\nand weights, and the third term due to the projection tensors Pq, Pk, Pv and Po.\nDividing the two, we ﬁnd that the ratio of memory access to arithmetic operations is O( 1\nk + 1\nbn). This low\nratio is necessary for good performance on modern GPU/TPU hardware, where the computational capacity\ncan be two orders of magnitude higher than the memory bandwidth.\n2.4\nMultihead Attention (Incremental)\nIn some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.\nAn example is a self-attention layer in an autoregressive language model such as Transformer [Vaswani et al.,\n2017]. The queries produced at each position attend to key-value pairs produced at all positions up to and\nincluding that position. During training, the ground-truth target sequence is known, and we can use an\n3\n\neﬃcient parallel implementation similar to that in section 2.3. However, when generating from the trained\nmodel, the output of the self-attention layer at a particular position aﬀects the token that is generated at\nthe next position, which in turn aﬀects the input to that layer at the next position. This prevents parallel\ncomputation. Code for incrementally computing this self-attention layer is shown below.\ndef\nMultiheadSelfAttentionIncremental (\nx , prev_K , prev_V , P_q, P_k, P_v, P_o ) :\n\" \" \" Multi−head Self −Attention ( one step ) .\nArgs :\nx :\na tensor\nwith shape\n[ b , d ]\nprev_K :\ntensor\nwith shape\n[ b , h , m,\nk ]\nprev_V :\ntensor\nwith shape\n[ b , h , m,\nv ]\nP_q:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_k:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_v:\na tensor\nwith shape\n[ h , d ,\nv ]\nP_o:\na tensor\nwith shape\n[ h , d ,\nv ]\nReturns :\ny :\na tensor\nwith shape\n[ b , d ]\nnew_K:\ntensor\nwith shape\n[ b , h , m+1, k ]\nnew_V:\ntensor\nwith shape\n[ b , h , m+1, v ]\n\" \" \"\nq = t f . einsum ( \"bd , hdk−>bhk \" , x , P_q)\nnew_K = t f . concat (\n[ prev_K,\nt f . expand_dims ( t f . einsum ( \"bd , hdk−>bhk \" , M, P_k) , a xis =2)] ,\na xis=2)\nnew_V = t f . concat (\n[ prev_V ,\nt f . expand_dims ( t f . einsum ( \"bd , hdv−>bhv \" , M, P_v) ,\na xis =2)] ,\na xis=2)\nl o g i t s = t f . einsum ( \"bhk , bhmk−>bhm\" , q , new_K)\nweights = t f . softmax ( l o g i t s )\no = t f . einsum ( \"bhm, bhmv−>bhv \" , weights , new_V)\ny = t f . einsum ( \"bhv , hdv−>bd\" , O, P_o)\nreturn y , new_K, new_V\n2.4.1\nPerformance Analysis\nWe make the same simplifying assumptions as in section 2.3.1.\nAcross n calls, the total number of arithmetic operations is again Θ(bnd2).\nAcross n calls, the total amount of memory access is Θ(bn2d + nd2), the ﬁrst term due to K and V and\nthe second term due to Pq, Pk, Pv and Po.\nDividing the memory by the computations, we ﬁnd that the ratio of memory access to arithmetic opera-\ntions is Θ( n\nd + 1\nb). When n ≈d or b ≈1, the ratio is close to 1, causing memory bandwidth to be a major\nperformance bottleneck on modern computing hardware. In order to make incremental generation eﬃcient,\nwe must reduce both of these terms to be ≪1. The 1\nb term is the easier one - we can just use a larger batch\nsize, memory size permitting.\nReducing the n\nd term is harder. This term is related to the expense of reloading at each step the K and V\ntensors representing the memory which have size bhmk = bn2. One solution is to limit the sequence length n.\nAnother is to reduce the number of positions being attended-to, either by attending to a local neighborhood,\nor by otherwise compressing the number of memory positions, as in [Liu et al., 2018], [Zhang et al., 2018],\n[Povey et al., 2018]. In this paper we present an orthogonal approach to reducing the size of the K and V\ntensors - namely removing their \"heads\" dimension, while maintaining the \"heads\" dimension in the queries.\n4\n\n3\nMulti-Query Attention\nWe introduce multi-query Attention as a variation of multi-head attention as described in [Vaswani et al.,\n2017]. Multi-head attention consists of multiple attention layers (heads) in parallel with diﬀerent linear\ntransformations on the queries, keys, values and outputs. Multi-query attention is identical except that the\ndiﬀerent heads share a single set of keys and values. The code for (incremental) multi-query (self) attention\nis identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the\ntf.einsum equations where it represents the \"heads\" dimension of K, V , Pk, or Pv.\ndef\nMultiqueryAttentionBatched (\nX, M, mask , P_q, P_k, P_v, P_o) :\n\" \" \" Multi−Query Attention .\nArgs :\nX:\na tensor\nwith shape\n[ b , n , d ]\nM:\na tensor\nwith shape\n[ b , m, d ]\nmask :\na tensor\nwith shape\n[ b , h , n , m]\nP_q:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_k:\na tensor\nwith shape\n[ d , k ]\nP_v:\na tensor\nwith shape d ,\nv ]\nP_o:\na tensor\nwith shape\n[ h , d ,\nv ]\nReturns :\nY:\na tensor\nwith shape\n[ b , n , d ]\n\" \" \"\nQ = t f . einsum ( \"bnd , hdk−>bhnk \" , X, P_q)\nK = t f . einsum ( \"bmd, dk−>bmk\" , M, P_k)\nV = t f . einsum ( \"bmd, dv−>bmv\" , M, P_v)\nl o g i t s = t f . einsum ( \"bhnk ,bmk−>bhnm\" , Q, K)\nweights = t f . softmax ( l o g i t s + mask)\nO = t f . einsum ( \"bhnm,bmv−>bhnv \" ,\nweights , V)\nY = t f . einsum ( \"bhnv , hdv−>bnd\" , O, P_o)\nreturn Y\n5\n\ndef\nMultiquerySelfAttentionIncremental (\nx , prev_K , prev_V , P_q, P_k, P_v, P_o ) :\n\" \" \" Multi−query Self −Attention ( one\nstep ) .\nArgs :\nx :\na tensor\nwith shape\n[ b , d ]\nprev_K :\ntensor\nwith shape\n[ b , m,\nk ]\nprev_V :\ntensor\nwith shape\n[ b , m,\nv ]\nP_q:\na tensor\nwith shape\n[ h , d ,\nk ]\nP_k:\na tensor\nwith shape\n[ d , k ]\nP_v:\na tensor\nwith shape\n[ d , v ]\nP_o:\na tensor\nwith shape\n[ h , d ,\nv ]\nReturns :\ny :\na tensor\nwith shape\n[ b , d ]\nnew_K:\ntensor\nwith shape\n[ b , m+1, k ]\nnew_V:\ntensor\nwith shape\n[ b , m+1, v ]\n\" \" \"\nq = t f . einsum ( \"bd , hdk−>bhk \" , x , P_q)\nK = t f . concat (\n[ prev_K,\nt f . expand_dims ( t f . einsum ( \"bd , dk−>bk \" , M, P_k) ,\na xis =2)] ,\na xis=2)\nV = t f . concat (\n[ prev_V ,\nt f . expand_dims ( t f . einsum ( \"bd , dv−>bv \" , M, P_v) ,\na xis =2)] ,\na xis=2)\nl o g i t s = t f . einsum ( \"bhk ,bmk−>bhm\" , q , K)\nweights = t f . softmax ( l o g i t s )\no = t f . einsum ( \"bhm,bmv−>bhv \" ,\nweights , V)\ny = t f . einsum ( \"bhv , hdv−>bd\" , O, P_o)\nreturn y , K, V\n3.1\nPerformance Analysis for Incremental Multi-Query Attention\nWe make the same simplifying assumptions as in section 2.3.1.\nAcross n calls, the total number of arithmetic operations is again Θ(bnd2).\nAcross n calls, the total amount of memory access is Θ(bnd + bn2k + nd2), the ﬁrst term due to x, q, o\nand y, the second term due to K and V and the third term due to Pq, Pk, Pv, Po.\nDividing the memory by the computations, we ﬁnd that the ratio of memory access to arithmetic opera-\ntions is Θ( 1\nd + n\ndh + 1\nb). We have reduced the oﬀensive n\nd by a factor of h. Theoretically, given large batch\nsize b, this should dramatically improve performance of incremental generation. In our experimental section,\nwe will show that the performance gains are real and that model quality remains high.\n4\nExperiments and Results\n4.1\nExperimental Setup\nFollowing [Vaswani et al., 2017], we evaluate on the WMT 2014 English-German translation task. As a\nbaseline, we use an encoder-decoder Transformer model with 6 layers, using dmodel = 1024 dff = 4096,\nh = 8, dk = dv = 128, learned positional embeddings, and weight-sharing between the token-embedding and\noutput layers. The baseline model and all variations have 211 million parameters. All models were trained\nfor 100,000 steps ( 20 epochs). Each training batch consisted of 128 examples, each of which consisted of\na 256-token input sequence and a 256-token target sequence (multiple training sentences were concatenated\ntogether to reach this length). Models were trained on a 32-core TPUv3 cluster, with each model taking\nabout 2 hours to train. We used an implementation from the tensor2tensor and mesh-tensorﬂow libraries.\n6\n\nThe conﬁgurations used can be found at [to be added before publication] , including details about learning\nrates, dropout, label smoothing, etc.\nIn our \"multi-query\" model, we replace all of the attention layers in the model to multi-query attention.\nThis includes the encoder-self-attention, decoder-self-attention and encoder-decoder-attention layers. We\nwiden the feed-forward hidden layers from 4096 to 5440 to make the total parameter-count equal to that of\nthe baseline.\nTo demonstrate that local-attention and multi-query attention are orthogonal, we also trained \"local\"\nversions of the baseline and multi-query models, where the decoder-self-attention layers (but not the other\nattention layers) restrict attention to the current position and the previous 31 positions.\nA simpler alternative way to reduce the sizes of K and V is to reduce the number of heads h and/or to\nreduce the dimensionalities k and v of the keys and values. We trained several such models for comparison,\nagain widening the feed-forward hidden layers to make the total parameter-count equal to that of the baseline.\nWe preformed a similar set of experiments using \"transformer-decoder\" language models on the Billion-\nWord Language Modeling Benchmark [Chelba et al., 2013]. For the baseline, we use a model with 6 layers,\ndmodel = 1024 dff = 8192, h = 8, dk = dv = 128. The total parameter count is 192 million for the baseline\nand for all variations. We trained for 136K steps (10 epochs) at a batch size of 64K tokens. Again, we used\na 32-core TPUv3 cluster for approximately 3 hours to train each model.\n4.2\nModel Quality\nTable 1 shows results for the machine-translation experiments.\nWe decoded the dev set using greedy\nmaximum-likelihood decoding and computed BLEU score with sacrebleu \"sacrebleu -t wmt13 -l en-de\n-tok intl\". We also list per-subword-token perplexity on the dev set. According to both of these metrics,\nthe multi-query attention model seems to be slightly worse than the baseline, but much closer than any of\nthe alternatives involving decreasing h, dk and dv.\nWe validated the results by decoding the test set using both greedy decoding and beam search (beam\n4, α = 0.6), and evaluated with sacrebleu \"sacrebleu -t wmt14 -l en-de -tok intl\". Again, the multi-\nquery model performed similarly to the baseline, and actually had the highest BLEU score (28.5) with\nbeam-4 decoding.\nTable 3 shows results for the billion-word language modeling benchmark. Models were evaluated by per-\nword (not per-subword-token) perplexity on the dev set. The results paint a similar picture to the translation\nresults. The multi-query attention model was slightly worse than the baseline, but signiﬁcantly better than\nany of the alternatives involving decreasing h, dk and dv.\n4.3\nSpeed\nTable 2 shows training and inference times for the various models. Both training and inference speeds were\nevaluated on one TPUv2 (8 cores). A training step (consisting of 32,768 input tokens and 32,768 target\ntokens, as described above) took 433ms for the base model and 425ms for the multi-query model. Dividing\nby 32,768, we ﬁnd that the training time is 13.2µs per (input-token + target-token), as listed in Table 2.\nWe ran incremental greedy inference on a batch of 1024 sequences (128 per core) using a source-sequence\nlength of 128 tokens and a target sequence length of 128. 1 For the baseline model, the encoder part of the\nmodel took 222ms and each incremental step of the decoder took 47ms. Dividing by the respective numbers\nof tokens, we ﬁnd that the amortized inference time is 1.7µs per token for the encoder and a much larger\n46µs per token for the decoder, as listed in Table 2. For the multi-query model, the encoder took 195ms\nand the decoder took 3.9ms per step, for amortized per-token costs of 1.5µs and 3.8µs respectively. Table 2\nshows these values as well as similar results for beam-search.\n1Due to system limitations requiring ﬁxed shapes, we used padding and masking in our decoder-self-attention implementation.\nThe memory tensors were thus padded to the maximum length (128), or to the window-size (32) in the case of local attention.\nEach decoding step thus took the same amount of time. An alternative implementation of incrementally growing the tensors\ncould save time near the beginning of the sequence.\n7\n\nTable 1: WMT14 EN-DE Results.\nAttention\nh\ndk, dv\ndff\nln(PPL)\nBLEU\nBLEU (test)\nType\n(dev)\n(dev)\nbeam 1 / 4\nmulti-head\n8\n128\n4096\n1.424\n26.7\n27.7 / 28.4\nmulti-query\n8\n128\n5440\n1.439\n26.5\n27.5 / 28.5\nmulti-head local\n8\n128\n4096\n1.427\n26.6\n27.5 / 28.3\nmulti-query local\n8\n128\n5440\n1.437\n26.5\n27.6 / 28.2\nmulti-head\n1\n128\n6784\n1.518\n25.8\nmulti-head\n2\n64\n6784\n1.480\n26.2\n26.8 / 27.9\nmulti-head\n4\n32\n6784\n1.488\n26.1\nmulti-head\n8\n16\n6784\n1.513\n25.8\nTable 2: Amortized training and inference costs for WMT14 EN-DE Translation Task with sequence length\n128. Values listed are in TPUv2-microseconds per output token.\nAttention\nTraining\nInference\nBeam-4 Search\nType\nenc. + dec.\nenc. + dec.\nmulti-head\n13.2\n1.7 + 46\n2.0 + 203\nmulti-query\n13.0\n1.5 + 3.8\n1.6 + 32\nmulti-head local\n13.2\n1.7 + 23\n1.9 + 47\nmulti-query local\n13.0\n1.5 + 3.3\n1.6 + 16\nTable 3: Billion-Word LM Benchmark Results.\nAttention\nh\ndk, dv\ndff\ndev-PPL\nmulti-head\n8\n128\n8192\n29.9\nmulti-query\n8\n128\n9088\n30.2\nmulti-head\n1\n128\n9984\n31.2\nmulti-head\n2\n64\n9984\n31.1\nmulti-head\n4\n32\n9984\n31.0\nmulti-head\n8\n16\n9984\n30.9\n5\nConclusion\nWe have proposed multi-query attention - an alternative to multi-head attention with much lower memory-\nbandwidth requirements in the incremental setting. We believe that this enables wider adoption of attention-\nbased sequence models in inference-performance-critical applications.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to\nalign and translate, 2014.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion\nword benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013.\nURL http://arxiv.org/abs/1312.3005.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer.\n8\n\nGenerating wikipedia by summarizing long sequences. In Proceedings of the International Conference on\nLearning Representations, 2018.\nDaniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur. A time-restricted self-\nattention layer for ASR. In Proceddings of the IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nBiao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average attention network,\n2018.\n9\n"
    },
    {
      "arxiv_id": "2205.14135",
      "full_text": "FlashAttention: Fast and Memory-Eﬃcient Exact Attention\nwith IO-Awareness\nTri Dao†, Daniel Y. Fu†, Stefano Ermon†, Atri Rudra‡, and Christopher Ré†\n†Department of Computer Science, Stanford University\n‡Department of Computer Science and Engineering, University at Buﬀalo, SUNY\n{trid,danfu}@cs.stanford.edu, ermon@stanford.edu, atri@buffalo.edu,\nchrismre@cs.stanford.edu\nJune 24, 2022\nAbstract\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\nto address this problem by trading oﬀmodel quality to reduce the compute complexity, but often do\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\naware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention,\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\nof FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\nan approximate attention algorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on\nGPT-2 (seq. length 1K), and 2.4× speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models\n(0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new\ncapabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n1\nIntroduction\nTransformer models [82] have emerged as the most widely used architecture in applications such as natural\nlanguage processing and image classiﬁcation. Transformers have grown larger [5] and deeper [83], but\nequipping them with longer context remains diﬃcult [80], since the self-attention module at their heart\nhas time and memory complexity quadratic in sequence length. An important question is whether making\nattention faster and more memory-eﬃcient can help Transformer models address their runtime and memory\nchallenges for long sequences.\nMany approximate attention methods have aimed to reduce the compute and memory requirements of\nattention. These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84],\nand their combinations [3, 9, 92]. Although these methods reduce the compute requirements to linear or\nnear-linear in sequence length, many of them do not display wall-clock speedup against standard attention\nand have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not\ncorrelate with wall-clock speed) and tend to ignore overheads from memory access (IO).\nIn this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is,\ncarefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU\non-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1 left). On modern\n1\narXiv:2205.14135v2  [cs.LG]  23 Jun 2022\n\nFlashAttention\nMemory Hierarchy with\nBandwidth & Memory Size\nAttention on GPT-2\nFlashAttention\nPyTorch\nTime (ms)\nMatmul\nMask\nSoftmax\nDropout\nMatmul\nFused\nKernel\nQ: N x d\nV: N X d\nKT: d x N\nQKT: N x N\nsm(QKT)V: N x d\nOuter Loop\nCopy Block to SRAM\nCopy\nOuter Loop\nCopy\nInner Loop\nCompute Block\non SRAM\nOutput to HBM\nInner Loop\nInner Loop\nOuter Loop\nGPU\nSRAM\nGPU\nHBM\nMain Memory\n(CPU DRAM)\nSRAM: 19 TB/s (20 MB)\nHBM: 1.5 TB/s (40 GB)\nDRAM: 12.8 GB/s\n                (>1 TB)\n0\n5\n10\n15\nFigure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁× 𝑁attention matrix\n(dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through\nblocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention\nloops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention\ncomputation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large 𝑁× 𝑁attention matrix to HBM, resulting in an 7.6×\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [61, 62, 63], and most operations in Transformers are\nbottlenecked by memory accesses [43]. IO-aware algorithms have been critical for similar memory-bound\noperations, when reading and writing data can account for a large portion of the runtime—such as database\njoins [71], image processing [70], numerical linear algebra [4], and more [40, 85]. However, common Python\ninterfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁne-grained control of memory\naccess.\nWe propose FlashAttention, a new attention algorithm that computes exact attention with far fewer\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and\nfuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,\nour algorithm both runs faster (up to 7.6x on GPT-2 [67], Figure 1 right) and uses less memory—linear\nin sequence length—than standard attention, thanks to the massively reduced amount of HBM access.\nWe analyze the IO complexity [1] of FlashAttention, proving that it requires 𝑂(𝑁2𝑑2𝑀−1) HBM\naccesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω(𝑁𝑑+ 𝑁2) of standard\nattention. For typical values of 𝑑and 𝑀, FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9× fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\nconcept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4× faster than\neven FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention\nhas better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss\nfurther extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix\n2\n\nmultiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1\nWe empirically validate that FlashAttention speeds up model training and improves model quality by\nmodeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and\nblock-sparse FlashAttention compared to prior attention implementations.\n• Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We\ntrain BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2\n(seq. length 1K) 3× faster than baseline implementations from HuggingFace [87] and Megatron-LM [77],\nand long-range arena (seq. length 1K-4K) 2.4× faster than baselines.\n• Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\ntheir quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\n6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\nenables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [80] challenge,\nsolely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\nto scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\nperformance on Path-256.\n• Benchmarking Attention. FlashAttention is up to 3× faster than the standard attention implemen-\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\nFlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\nfaster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention\nmethods that we know of.\n2\nBackground\nWe provide some background on the performance characteristics of common deep learning operations on\nmodern hardware (GPUs). We also describe the standard implementation of attention.\n2.1\nHardware Performance\nWe focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].\nGPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of\nmemory of diﬀerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU\nhas 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM\nper each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [44, 45]. The on-chip\nSRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute\nhas gotten faster relative to memory speed [61, 62, 63], operations are increasingly bottlenecked by memory\n(HBM) accesses. Thus exploiting fast SRAM becomes more important.\nExecution Model. GPUs have a massive number of threads to execute an operation (called a kernel).\nEach kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.\nPerformance characteristics. Depending on the balance of computation and memory accesses, op-\nerations can be classiﬁed as either compute-bound or memory-bound. This is commonly measured by the\narithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.\n1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there\nare, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner\ndimension, and convolution with large number of channels.\n2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while\ntime spent in computation is much smaller. Examples include most other operations: elementwise (e.g.,\nactivation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm).\nKernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if\nthere are multiple operations applied to the same input, the input can be loaded once from HBM, instead of\nmultiple times for each operation. Compilers can automatically fuse many elementwise operations [53, 65, 75].\n1FlashAttention code is available at https://github.com/HazyResearch/flash-attention\n3\n\nHowever, in the context of model training, the intermediate values still need to be written to HBM to save\nfor the backward pass, reducing the eﬀectiveness of naive kernel fusion.\n2.2\nStandard Attention Implementation\nGiven input sequences Q, K, V ∈R𝑁×𝑑where 𝑁is the sequence length and 𝑑is the head dimension, we want\nto compute the attention output O ∈R𝑁×𝑑:\nS = QK⊤∈R𝑁×𝑁,\nP = softmax(S) ∈R𝑁×𝑁,\nO = PV ∈R𝑁×𝑑,\nwhere softmax is applied row-wise.\nStandard attention implementations materialize the matrices S and P to HBM, which takes 𝑂(𝑁2) memory.\nOften 𝑁≫𝑑(e.g., for GPT2, 𝑁= 1024 and 𝑑= 64). We describe the standard attention implementation\nin Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of\nmemory accesses translates to slow wall-clock time.\nThis problem is exacerbated by other elementwise operations applied to the attention matrix, such as\nmasking applied to S or dropout applied to P. As a result, there have been many attempts to fuse several\nelementwise operations, such as fusing masking with softmax [77].\nIn Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic\nin the sequence length 𝑁. We also compare the number of FLOPs and number of HBM accesses of standard\nattention and of our method (FlashAttention).\nAlgorithm 0 Standard Attention Implementation\nRequire: Matrices Q, K, V ∈R𝑁×𝑑in HBM.\n1: Load Q, K by blocks from HBM, compute S = QK⊤, write S to HBM.\n2: Read S from HBM, compute P = softmax(S), write P to HBM.\n3: Load P and V by blocks from HBM, compute O = PV, write O to HBM.\n4: Return O.\n3\nFlashAttention: Algorithm, Analysis, and Extensions\nWe show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate\nmatrices for the backward pass. This yields an attention algorithm that is both memory eﬃcient and faster in\nwall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1\nAn Eﬃcient Attention Algorithm With Tiling and Recomputation\nGiven the inputs Q, K, V ∈R𝑁×𝑑in HBM, we aim to compute the attention output O ∈R𝑁×𝑑and write it to\nHBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in 𝑁).\nWe apply two established techniques (tiling, recomputation) to overcome the technical challenge of\ncomputing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea\nis that we split the inputs Q, K, V into blocks, load them from slow HBM to fast SRAM, then compute the\nattention output with respect to those blocks. By scaling the output of each block by the right normalization\nfactor before adding them up, we get the correct result at the end.\nTiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large\nsoftmax with scaling [51, 60, 66].\nFor numerical stability, the softmax of vector 𝑥∈R𝐵is computed as:\n𝑚(𝑥) := max\n𝑖\n𝑥𝑖,\n𝑓(𝑥) :=\n\u0002\n𝑒𝑥1−𝑚(𝑥)\n. . .\n𝑒𝑥𝐵−𝑚(𝑥)\u0003\n,\nℓ(𝑥) :=\n∑︁\n𝑖\n𝑓(𝑥)𝑖,\nsoftmax(𝑥) := 𝑓(𝑥)\nℓ(𝑥) .\n4\n\nFor vectors 𝑥(1), 𝑥(2) ∈R𝐵, we can decompose the softmax of the concatenated 𝑥=\n\u0002\n𝑥(1) 𝑥(2)\u0003\n∈R2𝐵as:\n𝑚(𝑥) = 𝑚(\n\u0002\n𝑥(1) 𝑥(2)\u0003\n) = max(𝑚(𝑥(1)), 𝑚(𝑥(2))),\n𝑓(𝑥) =\nh\n𝑒𝑚(𝑥(1))−𝑚(𝑥) 𝑓(𝑥(1))\n𝑒𝑚(𝑥(2))−𝑚(𝑥) 𝑓(𝑥(2))\ni\n,\nℓ(𝑥) = ℓ(\n\u0002\n𝑥(1) 𝑥(2)\u0003\n) = 𝑒𝑚(𝑥(1))−𝑚(𝑥)ℓ(𝑥(1)) + 𝑒𝑚(𝑥(2))−𝑚(𝑥)ℓ(𝑥(2)),\nsoftmax(𝑥) = 𝑓(𝑥)\nℓ(𝑥) .\nTherefore if we keep track of some extra statistics (𝑚(𝑥), ℓ(𝑥)), we can compute softmax one block at a time.2\nWe thus split the inputs Q, K, V into blocks (Algorithm 1 line 3), compute the softmax values along with\nextra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12).\nRecomputation. One of our goals is to not store 𝑂(𝑁2) intermediate values for the backward pass. The\nbackward pass typically requires the matrices S, P ∈R𝑁×𝑁to compute the gradients with respect to Q, K, V.\nHowever, by storing the output O and the softmax normalization statistics (𝑚, ℓ), we can recompute the\nattention matrix S and P easily in the backward pass from blocks of Q, K, V in SRAM. This can be seen as a\nform of selective gradient checkpointing [10, 34]. While gradient checkpointing has been suggested to reduce\nthe maximum amount of memory required [66], all implementations (that we know oﬀ) have to trade speed\nfor memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to\nreduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B.\nImplementation details:\nKernel fusion.\nTiling enables us to implement our algorithm in one\nCUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax,\noptionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout\nin Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM.\nAlgorithm 1 FlashAttention\nRequire: Matrices Q, K, V ∈R𝑁×𝑑in HBM, on-chip SRAM of size 𝑀.\n1: Set block sizes 𝐵𝑐=\n\u0006 𝑀\n4𝑑\n\u0007\n, 𝐵𝑟= min \u0000\u0006 𝑀\n4𝑑\n\u0007\n, 𝑑\u0001.\n2: Initialize O = (0)𝑁×𝑑∈R𝑁×𝑑, ℓ= (0)𝑁∈R𝑁, 𝑚= (−∞)𝑁∈R𝑁in HBM.\n3: Divide Q into 𝑇𝑟=\nl\n𝑁\n𝐵𝑟\nm\nblocks Q1, . . . , Q𝑇𝑟of size 𝐵𝑟× 𝑑each, and divide K, V in to 𝑇𝑐=\nl\n𝑁\n𝐵𝑐\nm\nblocks\nK1, . . . , K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each.\n4: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each,\ndivide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size 𝐵𝑟each.\n5: for 1 ≤𝑗≤𝑇𝑐do\n6:\nLoad K 𝑗, V𝑗from HBM to on-chip SRAM.\n7:\nfor 1 ≤𝑖≤𝑇𝑟do\n8:\nLoad Q𝑖, O𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM.\n9:\nOn chip, compute S𝑖𝑗= Q𝑖K𝑇\n𝑗∈R𝐵𝑟×𝐵𝑐.\n10:\nOn chip, compute ˜𝑚𝑖𝑗= rowmax(S𝑖𝑗) ∈R𝐵𝑟, ˜P𝑖𝑗= exp(S𝑖𝑗−˜𝑚𝑖𝑗) ∈R𝐵𝑟×𝐵𝑐(pointwise), ˜ℓ𝑖𝑗=\nrowsum(˜P𝑖𝑗) ∈R𝐵𝑟.\n11:\nOn chip, compute 𝑚new\n𝑖\n= max(𝑚𝑖, ˜𝑚𝑖𝑗) ∈R𝐵𝑟, ℓnew\n𝑖\n= 𝑒𝑚𝑖−𝑚new\n𝑖\nℓ𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜ℓ𝑖𝑗∈R𝐵𝑟.\n12:\nWrite O𝑖←diag(ℓnew\n𝑖\n)−1(diag(ℓ𝑖)𝑒𝑚𝑖−𝑚new\n𝑖\nO𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜P𝑖𝑗V𝑗) to HBM.\n13:\nWrite ℓ𝑖←ℓnew\n𝑖\n, 𝑚𝑖←𝑚new\n𝑖\nto HBM.\n14:\nend for\n15: end for\n16: Return O.\nWe show FlashAttention’s correctness, runtime, and memory requirement (proof in Appendix C).\nTheorem 1. Algorithm 1 returns O = softmax(QK⊤)V with 𝑂(𝑁2𝑑) FLOPs and requires 𝑂(𝑁) additional\nmemory beyond inputs and output.\n3.2\nAnalysis: IO Complexity of FlashAttention\nWe analyze the IO complexity of FlashAttention, showing signiﬁcant reduction in HBM accesses compared\nto standard attention. We also provide a lower bound, proving that no exact attention algorithm can\n2This style of aggregation is called algebraic aggregation [33].\n5\n\nAttention\nStandard\nFlashAttention\nGFLOPs\n66.6\n75.2\nHBM R/W (GB)\n40.3\n4.4\nRuntime (ms)\n41.7\n7.3\nSparsity Speedup\n% Non-Zero Blocks\n20\n60\n50\n100\n150\nFwd + Bwd (ms)\nEﬀect of Block Size\nBlock Size\n64 128\n256\n512\nFwd Runtime (ms)\n6\n2\nHBM Accesses (GB)\nDense\nFlashAttention\nBlock-Sparse\nFlashAttention\n2\n4\n6\nRuntime\nHBM\nAccesses\nFigure 2:\nLeft: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium\n(seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor aﬀecting\nruntime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on\nA100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of\nblock-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.\nasymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.\nTheorem 2. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑≤𝑀≤𝑁𝑑.\nStandard attention (Algorithm 0) requires Θ(𝑁𝑑+ 𝑁2) HBM accesses, while FlashAttention (Algorithm 1)\nrequires Θ(𝑁2𝑑2𝑀−1) HBM accesses.\nFor typical values of 𝑑(64-128) and 𝑀(around 100KB), 𝑑2 is many times smaller than 𝑀, and thus\nFlashAttention requires many times fewer HBM accesses than standard implementation. This leads to\nboth faster execution and lower memory footprint, which we validate in Section 4.3.\nThe main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of K, V of size Θ(𝑀) each\n(Algorithm 1 line 6). For each block of K and V, we iterate over all blocks of Q (Algorithm 1 line 8) to compute\nthe intermediate values, resulting in Θ(𝑁𝑑𝑀−1) passes over Q. Each pass loads Θ(𝑁𝑑) elements, which\namounts to Θ(𝑁2𝑑2𝑀−1) HBM accesses. We similarly prove that the backward pass of standard attention\nrequires Θ(𝑁𝑑+ 𝑁2) HBM accesses while the backward pass of FlashAttention requires Θ(𝑁2𝑑2𝑀−1)\nHBM accesses (Appendix B).\nWe prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all\nvalues of 𝑀(the SRAM size) when computing exact attention.\nProposition 3. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with\n𝑑≤𝑀≤𝑁𝑑. There does not exist an algorithm to compute exact attention with 𝑜(𝑁2𝑑2𝑀−1) HBM accesses\nfor all 𝑀in the range [𝑑, 𝑁𝑑].\nThe proof relies on the fact that for 𝑀= Θ(𝑁𝑑) any algorithm must perform Ω(𝑁2𝑑2𝑀−1) = Ω(𝑁𝑑)\nHBM accesses. This type of lower bound over a subrange of 𝑀is common in the streaming algorithms\nliterature [88]. We leave proving parameterized complexity [27] lower bounds in terms of 𝑀as exciting future\nwork.\nWe validate that the number of HBM accesses is the main determining factor of attention run-time.\nIn Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\nattention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\nfaster runtime. In Fig. 2 (middle), we vary the block size 𝐵𝑐of FlashAttention, which results in diﬀerent\namounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number\nof HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough\nblock size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations).\nMoreover, larger block size will not ﬁt into the small SRAM size.\n3.3\nExtension: Block-Sparse FlashAttention\nWe extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose\nIO complexity is smaller than FlashAttention by a factor proportional to the sparsity.\nGiven inputs Q, K, V ∈R𝑁×𝑑and a mask matrix ˜M ∈{0, 1}𝑁×𝑁, we want to compute:\nS = QK⊤∈R𝑁×𝑁,\nP = softmax(S ⊙𝟙˜M) ∈R𝑁×𝑁,\nO = PV ∈R𝑁×𝑑,\nwhere (S ⊙𝟙˜M)𝑘𝑙= S𝑘𝑙if ˜M𝑘𝑙= 1 and −∞if M𝑘𝑙= 0. We require ˜M to have block form: for some block sizes\n𝐵𝑟, 𝐵𝑐, for all 𝑘, 𝑙, ˜M𝑘,𝑙= M𝑖𝑗with 𝑖= ⌊𝑘/𝐵𝑟⌋, 𝑗= ⌊𝑙/𝐵𝑐⌋for some M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐.\n6\n\nGiven a predeﬁned block sparsity mask M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐we can easily adapt Algorithm 1 to only\ncompute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we\nskip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B.\nWe also analyze the IO complexity of block-sparse FlashAttention.\nProposition 4. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with\n𝑑≤𝑀≤𝑁𝑑. Block-sparse FlashAttention (Algorithm 5) requires Θ(𝑁𝑑+ 𝑁2𝑑2𝑀−1𝑠) HBM accesses\nwhere 𝑠is the fraction of nonzero blocks in the block-sparsity mask.\nWe see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the\nIO complexity. For large sequence lengths 𝑁, 𝑠is often set to 𝑁−1/2 [11] or 𝑁−1 log 𝑁[3, 17, 92], resulting\nin Θ(𝑁\n√\n𝑁) or Θ(𝑁log 𝑁) IO complexity. For downstream experiments, we use the ﬁxed butterﬂy sparsity\npattern [17], which has been shown to be able to approximate arbitrary sparsity [16].\nIn Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention\nimproves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8× speedup,\nwhile performing on par with standard attention (Section 4).\n4\nExperiments\nWe evaluate the impact of using FlashAttention to train Transformer models. We validate two claims\nabout training time and model accuracy, and report attention runtime and memory benchmarks.\n• Training Speed. FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and\nspeeds up GPT-2 up to 3× over HuggingFace [87] and 1.8× over Megatron [77] over standard Transformers.\nFlashAttention speeds up the long-range arena (LRA) benchmark 2.4×.\n• Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-\ntention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\n1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\ndocument classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve\nbetter-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse\nFlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random\nperformance on Path-256 (sequence length 64K).\n• Benchmarking Attention. We measure the runtime and memory performance of FlashAttention\nand block-sparse FlashAttention based on sequence length. We conﬁrm that the memory footprint\nof FlashAttention scales linearly with seq. length and is up to 3× faster than standard attention for\ncommon seq. lengths (up to 2K). We conﬁrm that runtime of block-sparse FlashAttention scales linearly\nin seq. length and is faster than all existing approximate attention baselines.\nAdditional experiment details are in Appendix E.\n4.1\nFaster Models with FlashAttention\nBERT.\nFlashAttention yields the fastest single-node BERT training speed that we know of. We train a\nBERT-large [22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the\nimplementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is\n15% faster.\nTable 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to\nreach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.\nBERT Implementation\nTraining time (minutes)\nNvidia MLPerf 1.1 [58]\n20.0 ± 1.5\nFlashAttention (ours)\n17.4 ± 1.4\nGPT-2.\nFlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32]\nthan the widely used HuggingFace [87] and Megatron-LM [77] implementations. Table 2 shows up to 3× end-\nto-end speedup compared to Huggingface and 1.7× speedup compared to Megatron-LM. FlashAttention\n7\n\nachieves the same perplexity as the other two implementations, as we do not change the model deﬁnition.\nAppendix E includes plots of the validation perplexity throughout training, conﬁrming that FlashAttention\nis as numerically stable as the baselines and produces the same training / validation curves.\nTable 2: GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface\nimplementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs.\nModel implementations\nOpenWebText (ppl)\nTraining time (speedup)\nGPT-2 small - Huggingface [87]\n18.2\n9.5 days (1.0×)\nGPT-2 small - Megatron-LM [77]\n18.2\n4.7 days (2.0×)\nGPT-2 small - FlashAttention\n18.2\n2.7 days (3.5×)\nGPT-2 medium - Huggingface [87]\n14.2\n21.0 days (1.0×)\nGPT-2 medium - Megatron-LM [77]\n14.3\n11.5 days (1.8×)\nGPT-2 medium - FlashAttention\n14.3\n6.9 days (3.0×)\nLong-range Arena.\nWe compare vanilla Transformer (with either standard implementation or FlashAt-\ntention) on the long-range arena (LRA [80]) benchmark. We measure accuracy, throughput, and training\ntime of all models. Each task has a diﬀerent sequence length varying between 1024 and 4096. We follow the\nimplementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3 Table 3 shows that FlashAt-\ntention achieves up 2.4× speed-up compared to standard attention. Block-sparse FlashAttention is\nfaster than all of the approximate attention methods that we have tested.\nTable 3: The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate\nattention baselines on the Long-Range-Arena benchmarks.\nModels\nListOps\nText\nRetrieval\nImage\nPathﬁnder\nAvg\nSpeedup\nTransformer\n36.0\n63.6\n81.6\n42.3\n72.7\n59.3\n-\nFlashAttention\n37.6\n63.9\n81.4\n43.5\n72.7\n59.8\n2.4×\nBlock-sparse FlashAttention\n37.0\n63.0\n81.3\n43.6\n73.3\n59.6\n2.8×\nLinformer [84]\n35.6\n55.9\n77.7\n37.8\n67.6\n54.9\n2.5×\nLinear Attention [50]\n38.8\n63.2\n80.7\n42.6\n72.5\n59.6\n2.3×\nPerformer [12]\n36.8\n63.6\n82.2\n42.1\n69.9\n58.9\n1.8×\nLocal Attention [80]\n36.1\n60.2\n76.7\n40.6\n66.6\n56.0\n1.7×\nReformer [51]\n36.5\n63.8\n78.5\n39.6\n69.4\n57.6\n1.3×\nSmyrf [19]\n36.1\n64.1\n79.0\n39.6\n70.5\n57.9\n1.7×\n4.2\nBetter Models with Longer Sequences\nLanguage Modeling with Long Context.\nThe runtime and memory-eﬃciency of FlashAttention\nallow us to increase the context length of GPT-2 by 4× while still running faster than the optimized\nimplementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context\nlength 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better\nperplexity.\nTable 4: GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30%\nfaster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported.\nModel implementations\nContext length\nOpenWebText (ppl)\nTraining time (speedup)\nGPT-2 small - Megatron-LM\n1k\n18.2\n4.7 days (1.0×)\nGPT-2 small - FlashAttention\n1k\n18.2\n2.7 days (1.7×)\nGPT-2 small - FlashAttention\n2k\n17.6\n3.0 days (1.6×)\nGPT-2 small - FlashAttention\n4k\n17.5\n3.6 days (1.3×)\nLong Document Classiﬁcation.\nTraining Transformers with longer sequences with FlashAttention\nimproves performance on the MIMIC-III [47] and ECtHR [6, 7] datasets. MIMIC-III contains intensive care\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\n3LRA accuracy results are known to be highly dependent on the tuning procedure [90]. Our reproduced baselines perform\nbetter than as reported in the original comparison [80].\n8\n\nAttention Memory Usage\nSequence Length\nAttention Runtime (Fwd Pass + Bwd Pass)\nSequence Length\nRuntime (ms)\nMemory Footprint (GB)\n256\n8K\n16K\n32K\n64K\n128\n256\n512\n1024\n2048\n4096\n101\n102\n10\n20\nFlashAttention\nBlock-Sparse FlashAttention\nPyTorch Attention\nMegatron Attention\nLinformer Attention\nOpenAI Sparse Attention\n8192\n100\nCrossover Points\n20x\n2x\nFigure 3: Left: runtime of forward pass + backward pass. Right: attention memory usage.\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights\nthat were allegedly violaged. Both of these datasets contain very long text documents; the average number of\ntokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and\nlongest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence\nlength of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]).\nTable 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length\n8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution\nshifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift\nin the document length, whereas ECtHR contains general language.\nTable 5: Long Document performance (mi-\ncro 𝐹1) at diﬀerent sequence lengths using\nFlashAttention.\n512\n1024\n2048\n4096\n8192\n16384\nMIMIC-III [47]\n52.8\n50.7\n51.7\n54.6\n56.4\n57.1\nECtHR [6]\n72.2\n74.3\n77.1\n78.6\n80.7\n79.2\nTable 6:\nWe report the ﬁrst Transformer\nmodel that can achieve non-random perfor-\nmance on Path-X and Path-256.\nModel\nPath-X\nPath-256\nTransformer\n\u0017\n\u0017\nLinformer [84]\n\u0017\n\u0017\nLinear Attention [50]\n\u0017\n\u0017\nPerformer [12]\n\u0017\n\u0017\nLocal Attention [80]\n\u0017\n\u0017\nReformer [51]\n\u0017\n\u0017\nSMYRF [19]\n\u0017\n\u0017\nFlashAttention\n61.4\n\u0017\nBlock-sparse FlashAttention\n56.0\n63.1\nPath-X and Path-256.\nThe Path-X and Path-256 benchmarks are challenging tasks from the long-range\narena benchmark designed to test long context. The task is to classify whether two points in a black and\nwhite 128×128 (or 256×256) image have a path connecting them, and the images are fed to the transformer\none pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved\nrandom performance [80]. There has been a search for alternative architectures that can model such long\ncontext [37]. We present here the ﬁrst result of Transformer models being able to solve Path-X and Path-256\n(Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating\nthe positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse\nFlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy4 on\nPath-256.\n4.3\nBenchmarking Attention\nWe vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse\nFlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and\na padding mask. We compare against reference implementations for exact attention, approximate attention,\nand sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines\nand full details.\n4Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.\n9\n\nRuntime.\nFigure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAt-\ntention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse\nattention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-\ntention runs signiﬁcantly faster than exact attention baselines, up to 3× faster than the PyTorch\nimplementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-\nquence length, but FlashAttention still runs faster than approximate and sparse attention for short\nsequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with\nFlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention\nis faster than all implementations of exact, sparse, and approximate attention that we know of, across all\nsequence lengths.\nMemory Footprint.\nFigure 3 (right) shows the memory footprint of FlashAttention and block-sparse\nFlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention\nand block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence\nlength. FlashAttention is up to 20× more memory eﬃcient than exact attention baselines, and is more\nmemory-eﬃcient than the approximate attention baselines. All other algorithms except for Linformer run\nout of memory on an A100 GPU before 64K, and FlashAttention is still 2× more eﬃcient than Linformer.\n5\nLimitations and Future Directions\nWe discuss limitations of our approach and future directions. Related work is given in Appendix A.\nCompiling to CUDA. Our current approach to building IO-aware implementations of attention requires\nwriting a new CUDA kernel for each new attention implementation. This requires writing the attention\nalgorithm in a considerably lower-level language than PyTorch, and requires signiﬁcant engineering eﬀort.\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\ncompiling to IO-aware implementations in CUDA—similar to eﬀorts such as Halide in image processing [70].\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\nthese potential extensions in Appendix D.\nMulti-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\nstants for computing attention on a single GPU. However, the attention computation may be parallelizable\nacross multiple GPUs [72]. Using multiple GPUs adds an additional layer to IO analysis—accounting for\ndata transfer between GPUs. We hope our work inspires future work in this direction.\nAcknowledgments\nOur implementation uses Apex’s FMHA code (https://github.com/NVIDIA/apex/tree/master/apex/\ncontrib/csrc/fmha) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA\nimplementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu,\nMegan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and\nsuggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of\ntheir attention algorithm.\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.\nCCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under\nNo. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak\nSupervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine\nLearning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,\nTSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce,\nTotal, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative\n(SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate\nFellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and\nVMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\n10\n\nnotwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations\nexpressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or\nendorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra’s research is\nsupported by NSF grant CCF-1763481.\nReferences\n[1] Alok Aggarwal and S Vitter, Jeﬀrey. The input/output complexity of sorting and related problems.\nCommunications of the ACM, 31(9):1116–1127, 1988.\n[2] Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[4] L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Demmel,\nJack Dongarra, Iain Duﬀ, Sven Hammarling, Greg Henry, et al. An updated set of basic linear algebra\nsubprograms (blas). ACM Transactions on Mathematical Software, 28(2):135–151, 2002.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[6] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n4317–4323, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1424.\nURL https://www.aclweb.org/anthology/P19-1424.\n[7] Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, and\nProdromos Malakasiotis. Paragraph-level rationale extraction through regularization: A case study on\neuropean court of human rights cases. In Proceedings of the Annual Conference of the North American\nChapter of the Association for Computational Linguistics, Mexico City, Mexico, 2021. Association for\nComputational Linguistics.\n[8] Benjamin Charlier, Jean Feydy, Joan Alexis Glaunès, François-David Collin, and Ghislain Durif. Kernel\noperations on the gpu, with autodiﬀ, without memory overﬂows. Journal of Machine Learning Research,\n22(74):1–6, 2021. URL http://jmlr.org/papers/v22/20-275.html.\n[9] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying\nsparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory\ncost. arXiv preprint arXiv:1604.06174, 2016.\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[12] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. In International Conference on Learning Representations (ICLR), 2020.\n[13] Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revisiting transformer-based models for\nlong document classiﬁcation. arXiv preprint arXiv:2204.06683, 2022.\n[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 2978–2988, 2019.\n11\n\n[15] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms\nfor linear transforms using butterﬂy factorizations. In International Conference on Machine Learning\n(ICML), 2019.\n[16] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,\nand Christopher Ré. Kaleidoscope: An eﬃcient, learnable representation for all structured linear maps.\nIn International Conference on Learning Representations (ICLR), 2020.\n[17] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Ré.\nPixelated butterﬂy: Simple and eﬃcient sparse training for neural network models. In International\nConference on Learning Representations (ICLR), 2022.\n[18] Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh\nRao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for eﬃcient and accurate\ntraining. In International Conference on Machine Learning (ICML), 2022.\n[19] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-eﬃcient attention\nusing asymmetric clustering. Advances in Neural Information Processing Systems, 33:6476–6489, 2020.\n[20] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged\nprogress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual\nACM-SIAM Symposium on Discrete Algorithms, pages 1060–1079. SIAM, 2018.\n[21] Peter J Denning. The working set model for program behavior. Communications of the ACM, 11(5):\n323–333, 1968.\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. 2019.\n[23] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise\noptimal brain surgeon. arXiv preprint arXiv:1705.07565, 2017.\n[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning\nRepresentations, 2020.\n[25] Y Eidelman and I Gohberg. On a new class of structured matrices. Integral Equations and Operator\nTheory, 34(3):293–324, 1999.\n[26] Jean Feydy, Joan Glaunès, Benjamin Charlier, and Michael Bronstein. Fast geometric learning with\nsymbolic matrices. Advances in Neural Information Processing Systems, 33, 2020.\n[27] Jörg Flum and Martin Grohe. Parameterized Complexity Theory. Springer, 2006.\n[28] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning Representations, 2018.\n[29] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the\nlottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019.\n[30] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity\nand the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259–3269.\nPMLR, 2020.\n[31] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with state-space\nmodels. In International Conference on Machine Learning (ICML), 2022.\n[32] Aaron Gokaslan, Vanya Cohen, Pavlick Ellie, and Stefanie Tellex. Openwebtext corpus, 2019.\n12\n\n[33] Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao,\nFrank Pellow, and Hamid Pirahesh. Data cube: A relational aggregation operator generalizing group-by,\ncross-tab, and sub-totals. Data mining and knowledge discovery, 1(1):29–53, 1997.\n[34] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic\ndiﬀerentiation. SIAM, 2008.\n[35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with\noptimal polynomial projections. In Advances in neural information processing systems (NeurIPS), 2020.\n[36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining\nrecurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural\nInformation Processing Systems, 34, 2021.\n[37] Albert Gu, Karan Goel, and Christopher Ré. Eﬃciently modeling long sequences with structured state\nspaces. In The International Conference on Learning Representations (ICLR), 2022.\n[38] Song Han, JeﬀPool, John Tran, and William J Dally. Learning both weights and connections for eﬃcient\nneural networks. arXiv preprint arXiv:1506.02626, 2015.\n[39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huﬀman coding. In International Conference on Learning\nRepresentations, 2016.\n[40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative\nApproach, pages 390–525, 2003.\n[41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\n[42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv\npreprint arXiv:2202.10447, 2022.\n[43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoeﬂer. Data movement is all\nyou need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:\n711–732, 2021.\n[44] Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU\nTechnology Conference, 2021.\n[45] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU\narchitecture via microbenchmarking. arXiv preprint arXiv:1804.06826, 2018.\n[46] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the graphcore IPU\narchitecture via microbenchmarking. arXiv preprint arXiv:1912.03413, 2019.\n[47] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible\ncritical care database. Scientiﬁc data, 3(1):1–9, 2016.\n[48] Norman P Jouppi, CliﬀYoung, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor\nprocessing unit. In Proceedings of the 44th annual international symposium on computer architecture,\npages 1–12, 2017.\n[49] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations.\nJournal of Mathematical Analysis and Applications, 68(2):395–407, 1979.\n[50] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:\nFast autoregressive transformers with linear attention. In International Conference on Machine Learning,\npages 5156–5165. PMLR, 2020.\n13\n\n[51] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer. In The\nInternational Conference on Machine Learning (ICML), 2020.\n[52] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\nAlbert: A lite BEDRT for self-supervised learning of language representations. In The International\nConference on Learning Representations (ICLR), 2020.\n[53] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan,\nGuangwen Yang, and Depei Qian.\nThe deep learning compiler: A comprehensive survey.\nIEEE\nTransactions on Parallel and Distributed Systems, 32(3):708–727, 2020.\n[54] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear\nmemory: How to make performers slim. arXiv preprint arXiv:2012.11346, 2020.\n[55] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[56] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\n[57] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.\nLuna: Linear uniﬁed nested attention. Advances in Neural Information Processing Systems, 34, 2021.\n[58] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson,\nHanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. Mlperf training benchmark. Proceedings\nof Machine Learning and Systems, 2:336–349, 2020.\n[59] Frank McSherry, Michael Isard, and Derek G Murray. Scalability! but at what {COST}?\nIn 15th\nWorkshop on Hot Topics in Operating Systems (HotOS XV), 2015.\n[60] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint\narXiv:1805.02867, 2018.\n[61] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.\n[62] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.\n[63] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.\n[64] D Stott Parker. Random butterﬂy transformations with applications in computational linear algebra.\n1995.\n[65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n[66] Markus N Rabe and Charles Staats. Self-attention does not need 𝑂(𝑛2) memory. arXiv preprint\narXiv:2112.05682, 2021.\n[67] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[68] Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Online, July 2020. Association for\nComputational Linguistics. URL https://www.aclweb.org/anthology/2020.acl-main.672.\n[69] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive trans-\nformers for long-range sequence modelling. In The International Conference on Learning Representations\n(ICLR), 2020.\n14\n\n[70] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman\nAmarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in\nimage processing pipelines. Acm Sigplan Notices, 48(6):519–530, 2013.\n[71] Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke. Database management systems, volume 3.\nMcGraw-Hill New York, 2003.\n[72] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix\ncompletion. Mathematical Programming Computation, 5(2):201–226, 2013.\n[73] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai.\nCombiner: Full attention transformer with sparse computation cost. Advances in Neural Information\nProcessing Systems, 34, 2021.\n[74] Aurko Roy, Mohammad Saﬀar, Ashish Vaswani, and David Grangier. Eﬃcient content-based sparse\nattention with routing transformers. Transactions of the Association for Computational Linguistics, 9:\n53–68, 2021.\n[75] Amit Sabne. XLA: Compiling machine learning for peak performance. 2020.\n[76] Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by ﬁne-tuning.\narXiv preprint arXiv:2005.07683, 2020.\n[77] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[78] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep\nlearning. In Advances in Neural Information Processing Systems, pages 3088–3096, 2015.\n[79] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span\nin transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\n2019.\n[80] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers.\nIn International Conference on Learning Representations, 2020.\n[81] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Eﬃcient transformers: A survey. arXiv\npreprint arXiv:2009.06732, 2020.\n[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\n[83] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:\nScaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.\n[84] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[85] Samuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: an insightful visual performance\nmodel for multicore architectures. Communications of the ACM, 52(4):65–76, 2009.\n[86] Michael E Wolf and Monica S Lam. A data locality optimizing algorithm. In Proceedings of the ACM\nSIGPLAN 1991 conference on Programming language design and implementation, pages 30–44, 1991.\n15\n\n[87] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6.\n[88] David P Woodruﬀ. Optimal space lower bounds for all frequency moments. In SODA, volume 4, pages\n167–175. Citeseer, 2004.\n[89] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In The International Conference on Learning Representations\n(ICLR), 2019.\n[90] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nyströmformer: A nystöm-based algorithm for approximating self-attention. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence. AAAI Conference on Artiﬁcial Intelligence, volume 35,\npage 14138, 2021.\n[91] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 558–567, 2021.\n[92] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. Advances in Neural Information Processing Systems, 33, 2020.\n[93] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh\nSusskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.\n[94] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and\nBryan Catanzaro. Long-short transformer: Eﬃcient transformers for language and vision. Advances in\nNeural Information Processing Systems, 34, 2021.\n16\n\nA\nRelated Work\nIO-Aware Runtime Optimization. The broad concept of optimizing for reading and writing to fast/slow\nmemory has a long history in computer science and has been known by many names. We draw the most\ndirect connection to the literature of analyzing I/O complexity in this work [1], but concepts of memory\nhierarchies are fundamental and has appeared in many forms, from the working set model [21], to data\nlocality [86], to the Rooﬂine model of arithmetic intensity [85], to analyses of scalability [59], to standard\ntextbook treatments of computer architecture [40]. We hope that this work encourages the community to\nadopt these ideas in more parts of the deep learning stack.\nEﬃcient ML Models with Structured Matrices. Matrix multiply is the core computational bottle-\nneck of most machine learning models. To reduce the computational complexity, there have been numerous\napproaches to learn over a more eﬃcient set of matrices. These matrices are called structured matrices, which\nhave subquadratic (𝑜(𝑛2) for dimension 𝑛× 𝑛) number of parameters and runtime. Most common examples\nof structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered\nin signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several\nmore general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement\nrank [49], quasi-separable [25]). The butterﬂy pattern we use for our block-sparse attention is motivated\nby the fact that butterﬂy matrices [15, 64] and their products have been shown to be able to express any\nstructured matrices with almost optimal runtime and number of parameters [16, 20]. However, even though\nstructured matrices are eﬃcient in theory, they have not seen wide adoption since it is hard to translate their\neﬃciency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation,\na phenomenon known as the hardware lottery [41]. Extensions of butterﬂy matrices [17, 18] aimed to make\nbutterﬂy matrices more hardware-friendly.\nSparse Training. Our block-sparse FlashAttention can be seen as a step towards making sparse model\ntraining more eﬃcient. Sparse models have seen success in compressing models for inference (pruning) by\nsparsifying the weight matrices [23, 38, 39, 55, 76]. For model training, the lottery tickets hypothesis [28, 29, 30]\nsuggests that there are a set of small sub-networks derived from a larger dense network that performs as\nwell as the original dense network. Out block-sparse FlashAttention can also be seen as a ﬁxed lottery\nticket in the context of attention: we ﬁx the sparsity pattern to be the butterﬂy pattern through training,\nand observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks.\nEﬃcient Transformer. Transformer-based models have become the most widely-used architecture in\nnatural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks\nis that their time and memory scales quadratic in the sequence length. There are numerous approaches to\novercome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and\nSmyrf [19] and with low-rank approximation such as Performer [12, 54]. One can even combine sparse and\nlow-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short\ntransformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to\nattend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences\nto help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend\nthe survey [81] for more details.\nThere are several lines of work on developing other modules instead of attention to model longer context.\nHiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis,\nallowing accurate reconstruction of the history through state-space models. They combine the strengths of\nCNNs (eﬃcient training), RNNs (eﬃcient inference), and continuous models (robust to change in sampling\nrates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the\ncontext of image classiﬁcation and language modeling.\nB\nAlgorithm Details\nWe ﬁrst derive the forward and backward passes of attention and show that they can be computed in a\nmemory-eﬃcient manner (requiring extra memory linear instead of quadratic in the sequence length). Though\nthey reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting\nin slower execution speed. We describe the FlashAttention algorithm to implement both the forward\n17\n\nand the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller\nmemory footprint.\nB.1\nMemory-eﬃcient forward pass\nThe main challenge in making attention memory-eﬃcient is the softmax that couples the columns of K (and\ncolumns of V). Our approach is to compute the softmax normalization constant separately to decouple the\ncolumns. This technique [60] has been used in the literature [51, 66] to show that attention computation\ndoes not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in\nslow run-time).\nFor simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3\ncontains all the steps.\nRecall that given input sequences Q, K, V ∈R𝑁×𝑑, we want to compute the attention output O ∈R𝑁×𝑑:\nS = QK⊤∈R𝑁×𝑁,\nP = softmax(S) ∈R𝑁×𝑁,\nO = PV ∈R𝑁×𝑑.\nWe have that 𝑆𝑖𝑗= 𝑞𝑇\n𝑖𝑘𝑗where 𝑞𝑖and 𝑘𝑗are the 𝑖-th and 𝑗-th columns of Q and K respectively. Deﬁne\nthe normalization constants of softmax:\n𝐿𝑖=\n∑︁\n𝑗\n𝑒𝑞𝑇\n𝑖𝑘𝑗.\n(1)\nLet 𝑣𝑗be the 𝑗-th column of V, then the 𝑖-th columns of the output is\n𝑜𝑖= 𝑃𝑖:V =\n∑︁\n𝑗\n𝑃𝑖𝑗𝑣𝑗=\n∑︁\n𝑗\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n𝑣𝑗.\n(2)\nWe see that once 𝐿𝑖is computed, we can compute 𝑜𝑖without extra memory by repeatedly summing\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n𝑣𝑗. Therefore the forward pass can be computed with 𝑂(𝑛) extra memory:\n1. Compute 𝐿𝑖for all 𝑖according to Eq. (1), which takes 𝑂(𝑛) extra memory.\n2. Compute 𝑜𝑖for all 𝑖according to Eq. (2), which takes 𝑂(𝑑) extra memory.\nB.2\nMemory-eﬃcient backward pass\nWe derive the backward pass of attention and show that it can also be computed with linear memory. Rabe\nand Staats [66] suggests that the backward pass can be done without quadratic extra memory by applying\ngradient checkpointing to the memory-eﬃcient forward pass. We instead derive the backward pass explicitly\nand show how it can be computed in a memory-eﬃcient manner.\nSuppose that there is a scalar loss function 𝜙, and let the output gradient be dO ∈R𝑛×𝑑(where dO denotes\n𝜕𝜙\n𝜕O). We want to compute the input gradients dQ, dK, dV ∈R𝑛×𝑑(where dQ, dK, dV denote 𝜕𝜙\n𝜕Q, 𝜕𝜙\n𝜕K, 𝜕𝜙\n𝜕V\nrespectively).\nThe gradient dV is easy to see. Applying reverse-mode autodiﬀby hand (aka the chain rule), we obtain\n(in matrix notation) dV = P𝑇dO. Thus:\n𝑑𝑣𝑗=\n∑︁\n𝑖\n𝑃𝑖𝑗𝑑𝑜𝑖=\n∑︁\n𝑖\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n𝑑𝑜𝑖.\n(3)\nSince we already computed 𝐿𝑖, 𝑑𝑣𝑗can be computed without extra memory by repeated summing.\nThe gradients dQ and dK are a little more complicated. We go through the gradients dP and dS ﬁrst.\nFrom Eq. (2), we have that dP = dOV𝑇, and so:\n𝑑𝑃𝑖𝑗= 𝑑𝑜𝑇\n𝑖𝑣𝑗.\nRecall that 𝑃𝑖: = softmax(𝑆𝑖:). Using the fact that the Jacobian of 𝑦= softmax(𝑥) is diag(𝑦) −𝑦𝑦𝑇, we\nhave that\n𝑑𝑆𝑖: = (diag(𝑃𝑖:) −𝑃𝑖:𝑃𝑇\n𝑖:)𝑑𝑃𝑖: = 𝑃𝑖: ◦𝑑𝑃𝑖: −(𝑃𝑇\n𝑖: 𝑑𝑃𝑖:)𝑃𝑖:,\n18\n\nwhere ◦denotes pointwise multiplication.\nDeﬁne\n𝐷𝑖= 𝑃𝑇\n𝑖: 𝑑𝑃𝑖: =\n∑︁\n𝑗\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n𝑑𝑜𝑇\n𝑖𝑣𝑗= 𝑑𝑜𝑇\n𝑖\n∑︁\n𝑗\n𝑒𝑞⊤\n𝑖𝑘𝑗\n𝐿𝑖\n𝑣𝑗= 𝑑𝑜𝑇\n𝑖𝑜𝑖,\n(4)\nthen\n𝑑𝑆𝑖: = 𝑃𝑖: ◦𝑑𝑃𝑖: −𝐷𝑖𝑃𝑖:.\nHence\n𝑑𝑆𝑖𝑗= 𝑃𝑖𝑗𝑑𝑃𝑖𝑗−𝐷𝑖𝑃𝑖𝑗= 𝑃𝑖𝑗(𝑑𝑃𝑖𝑗−𝐷𝑖).\nNow we can get the gradients dQ and dK. Recall that 𝑆𝑖𝑗= 𝑞𝑇\n𝑖𝑘𝑗, so\n𝑑𝑞𝑖=\n∑︁\n𝑗\n𝑑𝑆𝑖𝑗𝑘𝑗=\n∑︁\n𝑗\n𝑃𝑖𝑗(𝑑𝑃𝑖𝑗−𝐷𝑖)𝑘𝑗=\n∑︁\n𝑗\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n(𝑑𝑜𝑇\n𝑖𝑣𝑗−𝐷𝑖)𝑘𝑗.\n(5)\nSimilarly,\n𝑑𝑘𝑗=\n∑︁\n𝑖\n𝑑𝑆𝑖𝑗𝑞𝑖=\n∑︁\n𝑖\n𝑃𝑖𝑗(𝑑𝑃𝑖𝑗−𝐷𝑖)𝑞𝑖=\n∑︁\n𝑖\n𝑒𝑞𝑇\n𝑖𝑘𝑗\n𝐿𝑖\n(𝑑𝑜𝑇\n𝑖𝑣𝑗−𝐷𝑖)𝑞𝑖.\n(6)\nTherefore the backward pass can also be computed with 𝑂(𝑛) extra memory:\n1. Compute 𝑑𝑣𝑗for all 𝑗according to Eq. (3), which takes 𝑂(𝑑) extra memory.\n2. Compute 𝐷𝑖for all 𝑖according to Eq. (4), which takes 𝑂(𝑛) extra memory.\n3. Compute 𝑑𝑞𝑖for all 𝑖according to Eq. (5), which takes 𝑂(𝑑) extra memory.\n4. Compute 𝑑𝑘𝑗for all 𝑗according to Eq. (6), which takes 𝑂(𝑑) extra memory.\nB.3\nFlashAttention: Forward Pass\nWe describe the full details of FlashAttention forward pass. Given input sequences Q, K, V ∈R𝑁×𝑑, we\nwant to compute the attention output O ∈R𝑁×𝑑:\nS = 𝜏QK⊤∈R𝑁×𝑁,\nSmasked = mask(𝑆) ∈R𝑁×𝑁,\nP = softmax(Smasked) ∈R𝑁×𝑁,\nPdropped = dropout(P, 𝑝drop),\nO = PdroppedV ∈R𝑁×𝑑,\nwhere 𝜏∈R is some softmax scaling (typically\n1\n√\n𝑑), mask is some masking function that sets some entries of\nthe input to −∞and keep other entries the same (e.g., key padding mask when sequences in the batch don’t\nhave the same lengths and are padded), and dropout(𝑥, 𝑝) applies dropout to 𝑥elementwise (i.e., output\n𝑥\n1−𝑝\nwith probability 1 −𝑝and output 0 with probability 𝑝for each element 𝑥).\nThe full algorithm is in Algorithm 2. We save the output O, the softmax statistics ℓand 𝑚, and the\npseudo-random number generator state R for the backward pass.\n19\n\nAlgorithm 2 FlashAttention Forward Pass\nRequire: Matrices Q, K, V ∈R𝑁×𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏∈R,\nmasking function mask, dropout probability 𝑝drop.\n1: Initialize the pseudo-random number generator state R and save to HBM.\n2: Set block sizes 𝐵𝑐=\n\u0006 𝑀\n4𝑑\n\u0007\n, 𝐵𝑟= min \u0000\u0006 𝑀\n4𝑑\n\u0007\n, 𝑑\u0001.\n3: Initialize O = (0)𝑁×𝑑∈R𝑁×𝑑, ℓ= (0)𝑁∈R𝑁, 𝑚= (−∞)𝑁∈R𝑁in HBM.\n4: Divide Q into 𝑇𝑟=\nl\n𝑁\n𝐵𝑟\nm\nblocks Q1, . . . , Q𝑇𝑟of size 𝐵𝑟× 𝑑each, and divide K, V in to 𝑇𝑐=\nl\n𝑁\n𝐵𝑐\nm\nblocks\nK1, . . . , K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each.\n5: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each,\ndivide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size 𝐵𝑟each.\n6: for 1 ≤𝑗≤𝑇𝑐do\n7:\nLoad K 𝑗, V𝑗from HBM to on-chip SRAM.\n8:\nfor 1 ≤𝑖≤𝑇𝑟do\n9:\nLoad Q𝑖, O𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM.\n10:\nOn chip, compute S𝑖𝑗= 𝜏Q𝑖K𝑇\n𝑗∈R𝐵𝑟×𝐵𝑐.\n11:\nOn chip, compute Smasked\n𝑖𝑗\n= mask(S𝑖𝑗).\n12:\nOn chip, compute ˜𝑚𝑖𝑗= rowmax(Smasked\n𝑖𝑗\n) ∈R𝐵𝑟, ˜P𝑖𝑗= exp(Smasked\n𝑖𝑗\n−˜𝑚𝑖𝑗) ∈R𝐵𝑟×𝐵𝑐(pointwise),\n˜ℓ𝑖𝑗= rowsum(˜P𝑖𝑗) ∈R𝐵𝑟.\n13:\nOn chip, compute 𝑚new\n𝑖\n= max(𝑚𝑖, ˜𝑚𝑖𝑗) ∈R𝐵𝑟, ℓnew\n𝑖\n= 𝑒𝑚𝑖−𝑚new\n𝑖\nℓ𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜ℓ𝑖𝑗∈R𝐵𝑟.\n14:\nOn chip, compute ˜Pdropped\n𝑖𝑗\n= dropout(˜P𝑖𝑗, 𝑝drop).\n15:\nWrite O𝑖←diag(ℓnew\n𝑖\n)−1(diag(ℓ𝑖)𝑒𝑚𝑖−𝑚new\n𝑖\nO𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜Pdropped\n𝑖𝑗\nV𝑗) to HBM.\n16:\nWrite ℓ𝑖←ℓnew\n𝑖\n, 𝑚𝑖←𝑚new\n𝑖\nto HBM.\n17:\nend for\n18: end for\n19: Return O, ℓ, 𝑚, R.\nB.4\nFlashAttention: Backward Pass\nWe describe the full details of FlashAttention backward pass. Given input sequences Q, K, V ∈R𝑁×𝑑, the\noutput O ∈R𝑁×𝑑, and the output gradient dO, we want to compute the input gradients dQ, dK, dV ∈R𝑁×𝑑.\nWe ﬁrst describe the standard attention backward pass in Algorithm 3 for completeness.\nAlgorithm 3 Standard Attention Backward Pass\nRequire: Matrices Q, K, V, dO ∈R𝑁×𝑑, P ∈R𝑁×𝑁in HBM.\n1: Load P, dO by blocks from HBM, compute dV = P⊤dO ∈R𝑁×𝑑, write dV to HBM.\n2: Load dO, V by blocks from HBM, compute dP = dOV⊤∈R𝑁×𝑁, write dP to HBM.\n3: Read P, dP from HBM, compute dS ∈R𝑁×𝑁where 𝑑𝑆𝑖𝑗= 𝑃𝑖𝑗(𝑑𝑃𝑖𝑗−Í\n𝑙𝑃𝑖𝑙𝑑𝑃𝑖𝑙), write dS to HBM.\n4: Load dS and K by blocks from HBM, compute dQ = dSK, write dQ to HBM.\n5: Load dS and Q by blocks from HBM, compute dK = dS⊤Q, write dK to HBM.\n6: Return dQ, dK, dV.\nWe now make two observations about FlashAttention backward pass:\n1. We do not need to store the dropout mask of size 𝑂(𝑁2) from the forward pass. Instead, we can save\nthe pseudo-random number generator states from the forward pass and re-generate the dropout mask\nin the backward pass. This allows us to only use 𝑂(𝑁) extra memory.\n2. When computing the softmax gradient, we use Eq. (4) to compute 𝐷𝑖= 𝑃⊤\n𝑖:𝑑𝑃𝑖: without reducing over\n𝑃𝑖: and 𝑑𝑃𝑖: of size 𝑁(they might not ﬁt into SRAM). Instead we can rewrite 𝐷𝑖= 𝑑𝑜⊤\n𝑖𝑜𝑖and compute\nthe dot product between vectors of size 𝑑.\n20\n\nThe full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block\nversion of the derivation in Appendix B.2.\nAlgorithm 4 FlashAttention Backward Pass\nRequire: Matrices Q, K, V, O, dO ∈R𝑁×𝑑in HBM, vectors ℓ, 𝑚∈R𝑁in HBM, on-chip SRAM of size 𝑀,\nsoftmax scaling constant 𝜏∈R, masking function mask, dropout probability 𝑝drop, pseudo-random\nnumber generator state R from the forward pass.\n1: Set the pseudo-random number generator state to R.\n2: Set block sizes 𝐵𝑐=\n\u0006 𝑀\n4𝑑\n\u0007\n, 𝐵𝑟= min \u0000\u0006 𝑀\n4𝑑\n\u0007\n, 𝑑\u0001.\n3: Divide Q into 𝑇𝑟=\nl\n𝑁\n𝐵𝑟\nm\nblocks Q1, . . . , Q𝑇𝑟of size 𝐵𝑟× 𝑑each, and divide K, V in to 𝑇𝑐=\nl\n𝑁\n𝐵𝑐\nm\nblocks\nK1, . . . , K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each.\n4: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide dO into 𝑇𝑟blocks dO𝑖, . . . , dO𝑇𝑟of size\n𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each, divide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size\n𝐵𝑟each.\n5: Initialize dQ = (0)𝑁×𝑑in HBM and divide it into 𝑇𝑟blocks dQ1, . . . , dQ𝑇𝑟of size 𝐵𝑟× 𝑑each. Initialize\ndK = (0)𝑁×𝑑, dV = (0)𝑁×𝑑in HBM and divide dK, dV in to 𝑇𝑐blocks dK1, . . . , dK𝑇𝑐and dV1, . . . , dV𝑇𝑐,\nof size 𝐵𝑐× 𝑑each.\n6: for 1 ≤𝑗≤𝑇𝑐do\n7:\nLoad K 𝑗, V𝑗from HBM to on-chip SRAM.\n8:\nInitialize ˜\ndK𝑗= (0)𝐵𝑐×𝑑, ˜\ndV 𝑗= (0)𝐵𝑐×𝑑on SRAM.\n9:\nfor 1 ≤𝑖≤𝑇𝑟do\n10:\nLoad Q𝑖, O𝑖, dO𝑖, dQ𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM.\n11:\nOn chip, compute S𝑖𝑗= 𝜏Q𝑖K𝑇\n𝑗∈R𝐵𝑟×𝐵𝑐.\n12:\nOn chip, compute Smasked\n𝑖𝑗\n= mask(S𝑖𝑗).\n13:\nOn chip, compute P𝑖𝑗= diag(𝑙𝑖)−1 exp(Smasked\n𝑖𝑗\n−𝑚𝑖) ∈R𝐵𝑟×𝐵𝑐.\n14:\nOn chip, compute dropout mask Z𝑖𝑗∈R𝐵𝑟×𝐵𝑐where each entry has value\n1\n1−𝑝drop with probability\n1 −𝑝drop and value 0 with probability 𝑝drop.\n15:\nOn chip, compute Pdropped\n𝑖𝑗\n= P𝑖𝑗◦Z𝑖𝑗(pointwise multiply).\n16:\nOn chip, compute\n˜\ndV 𝑗←\n˜\ndV 𝑗+ (Pdropped\n𝑖𝑗\n)⊤dO𝑖∈R𝐵𝑐×𝑑.\n17:\nOn chip, compute dPdropped\n𝑖𝑗\n= dO𝑖V⊤\n𝑗∈R𝐵𝑟×𝐵𝑐.\n18:\nOn chip, compute dP𝑖𝑗= dPdropped\n𝑖𝑗\n◦Z𝑖𝑗(pointwise multiply).\n19:\nOn chip, compute 𝐷𝑖= rowsum(dO𝑖◦O𝑖) ∈R𝐵𝑟.\n20:\nOn chip, compute dS𝑖𝑗= P𝑖𝑗◦(dP𝑖𝑗−𝐷𝑖) ∈R𝐵𝑟×𝐵𝑐.\n21:\nWrite dQ𝑖←dQ𝑖+ 𝜏dS𝑖𝑗K 𝑗∈R𝐵𝑟×𝑑to HBM.\n22:\nOn chip, compute ˜\ndK𝑗←˜\ndK𝑗+ 𝜏dS⊤\n𝑖𝑗Q𝑖∈R𝐵𝑐×𝑑.\n23:\nend for\n24:\nWrite dK𝑗←\n˜\ndK𝑗, dV 𝑗←\n˜\ndV 𝑗to HBM.\n25: end for\n26: Return dQ, dK, dV.\nWe see that similar to the forward pass, the backward pass performs 𝑂(𝑁2) FLOPs and only requires\n𝑂(𝑁) extra memory beyond inputs, output, output gradient, and input gradients.\nWe analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2).\nTheorem 5. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑≤𝑀≤𝑁𝑑.\nStandard attention (Algorithm 0) backward pass requires Θ(𝑁𝑑+ 𝑁2) HBM accesses, while FlashAttention\nbackward pass (Algorithm 4) requires Θ(𝑁2𝑑2𝑀−1) HBM accesses.\nThe proof is in Appendix C.\n21\n\nB.5\nComparison with Rabe and Staats [66]\nWe describe here some similarities and diﬀerences between our FlashAttention algorithm and the algorithm\nof Rabe and Staats [66].\nConceptually, both FlashAttention and Rabe and Staats [66] operate on blocks of the attention matrix\nusing the well-established technique of tiling (or softmax scaling) [51, 60]. To reduce the memory footprint,\nboth methods avoid storing the large attention matrix in the forward pass and recompute it in the backward\npass.\nThe ﬁrst major diﬀerence is that Rabe and Staats [66] focuses on the reducing the total memory footprint\n(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses\n(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the\nprimary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount\nof memory required (e.g., if an operation incurs 𝐴memory accesses, then its total memory requirement is at\nmost 𝐴). As a result, FlashAttention is faster than standard attention (2-4×) while Rabe and Staats [66]\nis around the same speed or slightly slower than standard attention. In terms of total memory required, both\nmethods oﬀer substantial memory saving.\nThe second diﬀerence between the two methods is the way information is summarized from each block\nto pass to the next block. Rabe and Staats [66] summarizes each block with its temporary output along\nwith the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the\nblocks are combined using the statistics to produce the ﬁnal output. FlashAttention instead incrementally\nupdates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed\n(instead of 𝐾copies for 𝐾blocks). This means that FlashAttention has smaller total memory requirement\ncompared to Rabe and Staats [66].\nThe ﬁnal major diﬀerence is the way the backward pass is computed. Rabe and Staats [66] uses gradient\ncheckpointing to recompute the attention matrix and the temporary output of each block. FlashAttention\ninstead simpliﬁes the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention\nmatrix and does not recompute the temporary output of each block. This reduces the memory requirement\nfor the backward pass and yields speedup.\nC\nProofs\nProof of Theorem 1. We ﬁrst count the number of FLOPs and extra memory required.\nThe dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we\ncompute Q𝑖K⊤\n𝑗∈R𝐵𝑟×𝐵𝑐for Q𝑖∈R𝐵𝑟×𝑑and K 𝑗∈R𝐵𝑐×𝑑, which takes 𝑂(𝐵𝑟𝐵𝑐𝑑) FLOPs. We also compute\n(Algorithm 1 line 12) ˜P𝑖𝑗V𝑗∈R𝐵𝑟×𝑑for ˜P𝑖𝑗∈R𝐵𝑟×𝐵𝑐and V𝑗∈R𝐵𝑐×𝑑, which takes 𝑂(𝐵𝑟𝐵𝑐𝑑) FLOPs. We\nexecute the inner loops 𝑇𝑐𝑇𝑟=\nl\n𝑁\n𝐵𝑐\nm l\n𝑁\n𝐵𝑟\nm\ntimes. Therefore the total number of FLOPs is\n𝑂\n\u0012 𝑁2\n𝐵𝑐𝐵𝑟\n𝐵𝑟𝐵𝑐𝑑\n\u0013\n= 𝑂(𝑁2𝑑).\nIn terms of extra memory required, we see that we need 𝑂(𝑁) memory to store the statistics (ℓ, 𝑚).\nWe now prove the algorithm’s correctness by induction on 𝑗for 0 ≤𝑗≤𝑇𝑐. Let K: 𝑗∈R 𝑗𝐵𝑐×𝑑be the\nﬁrst 𝑗𝐵𝑐rows of K, and similarly V: 𝑗∈R 𝑗𝐵𝑐×𝑑the the ﬁrst 𝑗𝐵𝑐rows of V. Let S:,: 𝑗= QK⊤\n: 𝑗∈R𝑁×𝑗𝐵𝑐, and\nP:,: 𝑗= softmax(S:,: 𝑗) ∈R𝑁×𝑗𝐵𝑐(softmax applied row-wise). Let 𝑚𝑗, ℓ( 𝑗), O( 𝑗) be the values of 𝑚, ℓ, O in HBM\nafter the 𝑗-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of 𝑚, ℓ, O are updated\nafter each iteration of the outer loop.) We want to show that after the 𝑗-th iteration of the outer loop, we\nhave computed in HBM:\n𝑚( 𝑗) = rowmax(S:,: 𝑗) ∈R𝑁,\nℓ( 𝑗) = rowsum(exp(S:,: 𝑗−𝑚( 𝑗))) ∈R𝑁,\nO( 𝑗) = P:,: 𝑗V: 𝑗∈R𝑁×𝑑.\nBased on our initialization (Algorithm 1 line 2), this claim is true for 𝑗= 0 (i.e., before the any iteration\nof the outer loop is executed). Suppose that the claim holds for some 𝑗= 0, . . . ,𝑇𝑐−1. We want to show that\nthe claim also holds for 𝑗+ 1. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)\n22\n\non the ( 𝑗+ 1)-th iteration of the outer loop, we update 𝑚( 𝑗+1) = max(𝑚( 𝑗), ˜𝑚) where ˜𝑚∈R𝑁is the row-max\nof S:, 𝑗: 𝑗+1, the slice of S from column 𝑗𝐵𝑐to column ( 𝑗+ 1)𝐵𝑐−1. This implies that\n𝑚( 𝑗+1) = rowmax(S:,: 𝑗+1) ∈R𝑁.\nSimilarly, we update\nℓ( 𝑗+1) = 𝑒𝑚( 𝑗)−𝑚( 𝑗+1) ℓ( 𝑗) + 𝑒˜𝑚−𝑚( 𝑗+1) ˜ℓ,\nwhere ˜ℓ= rowsum(exp(S:, 𝑗: 𝑗+1 −˜𝑚)) ∈R𝑁. By the same algebraic manipulation in Section 3.1, we obtain:\nℓ( 𝑗+1) = rowsum(exp(S:,: 𝑗+1 −𝑚( 𝑗+1))) ∈R𝑁.\nLet V𝑗: 𝑗+1 be the slice of V from column 𝑗𝐵𝑐to column ( 𝑗+ 1)𝐵𝑐−1, we also update:\nO( 𝑗+1) = diag(ℓ( 𝑗+1))−1(diag(ℓ( 𝑗))𝑒𝑚( 𝑗)−𝑚( 𝑗+1) O( 𝑗) + 𝑒˜𝑚−𝑚( 𝑗+1) exp(S𝑗: 𝑗+1 −˜𝑚)V𝑗: 𝑗+1)\n= diag(ℓ( 𝑗+1))−1(diag(ℓ( 𝑗))𝑒𝑚( 𝑗)−𝑚( 𝑗+1) P:,: 𝑗V: 𝑗+ 𝑒−𝑚( 𝑗+1) exp(S𝑗: 𝑗+1)V𝑗: 𝑗+1)\n= diag(ℓ( 𝑗+1))−1(diag(ℓ( 𝑗))𝑒𝑚( 𝑗)−𝑚( 𝑗+1) diag(ℓ( 𝑗)) exp(S:,: 𝑗−𝑚( 𝑗))V: 𝑗+ 𝑒−𝑚( 𝑗+1) exp(S𝑗: 𝑗+1)V𝑗: 𝑗+1)\n= diag(ℓ( 𝑗+1))−1(𝑒−𝑚( 𝑗+1) exp(S:,: 𝑗)V: 𝑗+ 𝑒−𝑚( 𝑗+1) exp(S𝑗: 𝑗+1)V𝑗: 𝑗+1)\n= diag(ℓ( 𝑗+1))−1(exp(S:,: 𝑗−𝑚( 𝑗+1))V: 𝑗+ exp(S𝑗: 𝑗+1 −𝑚( 𝑗+1))V𝑗: 𝑗+1)\n= diag(ℓ( 𝑗+1))−1 \u0010\nexp\n\u0010\u0002S:,: 𝑗\nS𝑗: 𝑗+1\n\u0003\n−𝑚( 𝑗+1)\u0011\u0011 \u0014 V: 𝑗\nV𝑗: 𝑗+1\n\u0015\n= softmax(S: 𝑗+1)V: 𝑗+1.\nWe then see that the claim is also true for 𝑗+ 1. By induction, the claim is true for all 𝑗= 0, . . . ,𝑇𝑐.\nWhen 𝑗= 𝑇𝑐, we conclude that the ﬁnal value of O in HBM is softmax(S)V = softmax(QK⊤)V.\n□\nProof of Theorem 2. We ﬁrst analyze the IO complexity of standard attention implementation. The inputs\nQ, K, V ∈R𝑁×𝑑reside in HBM, and the at the end of the algorithm the output O ∈R𝑁×𝑑is written to HBM.\nIn the ﬁrst step of computing the matrix multiply S = QK⊤, the inputs Q, K are read from HBM and the\noutput S ∈R𝑁×𝑁is written to HBM (Algorithm 0 line 1). This incurs Θ(𝑁𝑑+ 𝑁2) HBM accesses.\nIn the second step of computing P = softmax(S), the input S is read from HBM and the output P is\nwritten to HBM (Algorithm 0 line 2). This incurs Θ(𝑁2) HBM accesses.\nIn the last step of computing O = PV, the inputs P, V are read from global memory and the output O is\nwritten to HBM (Algorithm 0 line 3). This incurs Θ(𝑁𝑑+ 𝑁2) HBM accesses.\nOverall, standard attention implementation requires Θ(𝑁𝑑+ 𝑁2) global memory accesses.\nWe now analyze the IO complexity of streaming attention.\nFollowing Algorithm 1, we see that each element of K and V is loaded from HBM once (Algorithm 1\nline 6). We make 𝑇𝑐passes over Q and O, each pass loading all of Q and all of O to HBM (Algorithm 1\nline 8). Therefore the number of HBM accesses is Θ (𝑁𝑑+ 𝑁𝑑𝑇𝑐) = Θ(𝑁𝑑𝑇𝑐).\nWe derive the conditions on the block sizes 𝐵𝑐and 𝐵𝑟. We need the blocks K 𝑗and V𝑗of size 𝐵𝑐× 𝑑to ﬁt\ninto on-chip memory, which translates to:\n𝐵𝑐𝑑= 𝑂(𝑀) ⇔𝐵𝑐= 𝑂\n\u0012 𝑀\n𝑑\n\u0013\n.\nSimilarly, we need the blocks Q𝑖, O𝑖of size 𝐵𝑟× 𝑑to ﬁt into on-chip memory, which translates to:\n𝐵𝑟𝑑= 𝑂(𝑀) ⇔𝐵𝑟= 𝑂\n\u0012 𝑀\n𝑑\n\u0013\n.\nFinally, we need the block S𝑖𝑗of size 𝐵𝑟× 𝐵𝑐to ﬁt into on-chip memory, which translates to:\n𝐵𝑟𝐵𝑐= 𝑂(𝑀).\n23\n\nWe therefore set:\n𝐵𝑐= Θ\n\u0012 𝑀\n𝑑\n\u0013\n,\n𝐵𝑟= Θ\n\u0012\nmin\n\u0012 𝑀\n𝑑, 𝑀\n𝐵𝑐\n\u0013\u0013\n= Θ\n\u0012\nmin\n\u0012 𝑀\n𝑑, 𝑑\n\u0013\u0013\n.\nWe then have:\n𝑇𝑐= 𝑁\n𝐵𝑐\n= Θ\n\u0012 𝑁𝑑\n𝑀\n\u0013\n.\nAs a result, the number of HBM accesses is:\nΘ (𝑁𝑑𝑇𝑐) = Θ\n\u0012 𝑁2𝑑2\n𝑀\n\u0013\n.\n□\nProof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact\nattention where the number for HBM access for all 𝑀∈[𝑑, 𝑁𝑑] is\n𝑜\n\u0012 𝑁2𝑑2\n𝑀\n\u0013\n.\nIn the regime of 𝑀= Θ(𝑁𝑑), this results in the number of HBM accesses:\n𝑜\n\u0012 𝑁2𝑑2\n𝑁𝑑\n\u0013\n= 𝑜(𝑁𝑑).\nHowever, the input to attention (matrices Q, K, V) and the output O have size 𝑁𝑑and they start out being\nin HBM, so if the algorithm computes exact attention it must incur at least Ω(𝑁𝑑) HBM accesses. This is a\ncontradiction.\n□\nProof of Theorem 5. The IO complexity of the attention backward is very similar to the IO complexity of\nthe attention forward (Theorem 2). Here we provide a sketch of the proof.\nWe ﬁrst analyze the IO complexity of standard attention backward pass. The inputs Q, K, V, dO ∈R𝑁×𝑑\nreside in HBM, and the at the end of the algorithm the outputs dQ, dK, dV ∈R𝑁×𝑑are written to HBM.\nAt each step of the standard attention backward pass, one needs to load inputs of size 𝑁𝑑or 𝑁2 from\nHBM, and needs to write the outputs of size 𝑁2 or 𝑁𝑑to HBM. This incurs Θ(𝑁𝑑+ 𝑁2) HBM accesses.\nWe now analyze the IO complexity of FlashAttention backward pass.\nSimilar to Theorem 2, we see that each element of K and V is loaded from HBM once. Each element of\ndK and dV is only written to HBM once. We make 𝑇𝑐passes over Q, O, dO, each pass loading all of Q, O, dO\nto HBM. We also make 𝑇𝑐passes over dQ, each pass reading/writing all of dQ from/to HBM. Therefore the\nnumber of HBM accesses is Θ (𝑁𝑑+ 𝑁𝑑𝑇𝑐) = Θ(𝑁𝑑𝑇𝑐).\nAs in the proof of Theorem 2, the constraints on the block sizes are that:\n𝐵𝑐= Θ\n\u0012 𝑀\n𝑑\n\u0013\n,\n𝐵𝑟= Θ\n\u0012\nmin\n\u0012 𝑀\n𝑑, 𝑑\n\u0013\u0013\n.\nWe then have:\n𝑇𝑐= 𝑁\n𝐵𝑐\n= Θ\n\u0012 𝑁𝑑\n𝑀\n\u0013\n.\nAs a result, the number of HBM accesses is:\nΘ (𝑁𝑑𝑇𝑐) = Θ\n\u0012 𝑁2𝑑2\n𝑀\n\u0013\n.\n□\n24\n\nAlgorithm 5 Block-Sparse FlashAttention Forward Pass\nRequire: Matrices Q, K, V ∈R𝑁×𝑑in HBM, on-chip SRAM of size 𝑀, softmax scaling constant 𝜏∈R,\nmasking function mask, dropout probability 𝑝drop, block sizes 𝐵𝑐=\n\u0006 𝑀\n4𝑑\n\u0007\n, 𝐵𝑟= min \u0000\u0006 𝑀\n4𝑑\n\u0007\n, 𝑑\u0001, block\nsparsity mask 𝑀∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐..\n1: Initialize the pseudo-random number generator state R and save to HBM.\n2: Initialize O = (0)𝑁×𝑑∈R𝑁×𝑑, ℓ= (0)𝑁∈R𝑁, 𝑚= (−∞)𝑁∈R𝑁in HBM.\n3: Divide Q into 𝑇𝑟=\nl\n𝑁\n𝐵𝑟\nm\nblocks Q1, . . . , Q𝑇𝑟of size 𝐵𝑟× 𝑑each, and divide K, V in to 𝑇𝑐=\nl\n𝑁\n𝐵𝑐\nm\nblocks\nK1, . . . , K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each.\n4: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each,\ndivide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size 𝐵𝑟each.\n5: for 1 ≤𝑗≤𝑇𝑐do\n6:\nLoad K 𝑗, V𝑗from HBM to on-chip SRAM.\n7:\nfor 1 ≤𝑖≤𝑇𝑟do\n8:\nif 𝑀𝑖𝑗≠0 then\n9:\nLoad Q𝑖, O𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM.\n10:\nOn chip, compute S𝑖𝑗= 𝜏Q𝑖K𝑇\n𝑗∈R𝐵𝑟×𝐵𝑐.\n11:\nOn chip, compute Smasked\n𝑖𝑗\n= mask(S𝑖𝑗).\n12:\nOn chip, compute ˜𝑚𝑖𝑗= rowmax(Smasked\n𝑖𝑗\n) ∈R𝐵𝑟, ˜P𝑖𝑗= exp(Smasked\n𝑖𝑗\n−˜𝑚𝑖𝑗) ∈R𝐵𝑟×𝐵𝑐(pointwise),\n˜ℓ𝑖𝑗= rowsum(˜P𝑖𝑗) ∈R𝐵𝑟.\n13:\nOn chip, compute 𝑚new\n𝑖\n= max(𝑚𝑖, ˜𝑚𝑖𝑗) ∈R𝐵𝑟, ℓnew\n𝑖\n= 𝑒𝑚𝑖−𝑚new\n𝑖\nℓ𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜ℓ𝑖𝑗∈R𝐵𝑟.\n14:\nOn chip, compute ˜Pdropped\n𝑖𝑗\n= dropout(˜P𝑖𝑗, 𝑝drop).\n15:\nWrite O𝑖←diag(ℓnew\n𝑖\n)−1(diag(ℓ𝑖)𝑒𝑚𝑖−𝑚new\n𝑖\nO𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new\n𝑖\n˜Pdropped\n𝑖𝑗\nV𝑗) to HBM.\n16:\nWrite ℓ𝑖←ℓnew\n𝑖\n, 𝑚𝑖←𝑚new\n𝑖\nto HBM.\n17:\nend if\n18:\nend for\n19: end for\n20: Return O, ℓ, 𝑚, R.\nD\nExtension Details\nD.1\nBlock-sparse FlashAttention\nWe describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical\nto Algorithm 2, except that we skip zero blocks.\nWe prove the IO-complexity of block-sparse FlashAttention.\nProof of Proposition 4. The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice\nthat we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses\nare scaled by 𝑠, the fraction of nonzero blocks in the block-sparsity mask. However, for small values of 𝑠, we\nwould still need to write the result O ∈R𝑁×𝑑. Therefore the number of HBM accesses is\nΘ\n\u0012\n𝑁𝑑+ 𝑁2𝑑2\n𝑀\n𝑠\n\u0013\n.\n□\nD.2\nPotential Extensions\nWe discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.\nMulti-GPU Attention. Large language models are trained on hundreds or thousands of GPUs, and\none typically splits the attention computation between 4-8 GPUs on the same node [77]. This introduces\nanother level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other\n25\n\nGPUs. For very long sequences, the diﬀerent GPUs on the same node can cooperate to compute attention by\ntaking into account the asymmetry of diﬀerent levels of memory hierarchy.\nSparse MLP layers. Typical dense MLP layers are compute-bound and not memory-bound. To improve\ntheir eﬃciency, MLP layers with sparse weight matrices can be used [17]. However, many sparse MLP layers\nare instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an\nIO-aware implementation can alleviate this issue and realize the beneﬁts of sparsity. We are excited about\nfuture work in this direction, to reduce the computational requirement of large models and improve their\nwall-block runtime.\nKernel machine learning. Our approach in FlashAttention relies on the fact that the 𝑁× 𝑁\nattention matrix is a function of a low-rank matrix QK⊤(of rank 𝑑≪𝑁). As a result, we can repeatedly\nload the inputs Q, K and recompute the block of the attention matrix that we need, signiﬁcantly reducing\nHBM access. As similar scenario happens in kernel machine learning: each element 𝐾𝑖𝑗of the 𝑁× 𝑁kernel\nmatrix K is a function of two vectors of size 𝑑≪𝑁, as it measures the similarity between two datapoints 𝑥𝑖\nand 𝑥𝑗. The KeOps library [8, 26] is a successful example of how reducing memory reads/writes can speed up\nkernel operations. We hope that this will motivate kernel methods that focus more on reducing IOs instead\nof just FLOPs.\nE\nFull Experimental Results\nE.1\nBERT\nWe train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1\nimplementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448,\ntrained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language\nmodeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision\nusing Apex AMP (with O2 optimization level).\nWe compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1\n(Table 1).\nWe use the same train / validation data split provided by MLPerf 1.1 reference implementation. In\nparticular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.\nWe train the model on 8×A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we\naverage the results of 10 runs.\nE.2\nGPT-2\nWe use the standard implementations of GPT-2 [67] from Huggingface transformers library and from\nNvidia’s Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.\nWe use an eﬀective batch size of 512, and use gradient accumulation to ﬁt into available GPU memory.\nWe use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and\nweight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all\nimplementations with mixed-precision training (PyTorch AMP).\nWe use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset\nas the validation set, with the rest being used as training set. This random selection of validation set is done\nonce, and all models are evaluated on the same validation set.\nWe train the model on 8×A100-40GB GPUs, and we measure the wall-clock training time. Training\nGPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2).\nIn Fig. 4, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either\nHuggingFace implementation or our FlashAttention implementation. We see that FlashAttention be-\nhaves the same as the baseline implementation and the validation perplexity curves of the two implementations\nalmost lie on top of each other.\nLong Document Classiﬁcation.\nFor MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al.\n[13].\n26\n\n100k\n200k\n300k\nTraining steps\n10\n15\n20\n25\n30\nVal perplexity\nGPT-2-small HuggingFace\nGPT-2-small FlashAttention\nGPT-2-medium HuggingFace\nGPT-2-medium FlashAttention\nFigure 4: Validation perplexity of GPT-2 small/medium using two implementations. We conﬁrm that\nFlashAttention yields the same validation curves as the baseline implementation from HuggingFace.\nE.3\nLRA details\nWe follow the hyperparameters from the Long-range arena paper [80], the Long-range arena repo (https:\n//github.com/google-research/long-range-arena), and the Nyströmformer reproduction [90]. To be\ngenerous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of\nthe ﬁve tasks, we report the better performance from Tay et al. [80] or Xiong et al. [90] for that baseline on\nthat task.\nAfter hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the\nﬁve LRA tasks.\nWe run all methods with mixed-precision training, except for Performer (not stable with mixed precision)\nand Local Attention (implementation does not support FP16).\nTo calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup\nof each of the ﬁve tasks.\nPath-X\nFor Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments\nfrom the long-range arena paper[80]. For both, we ﬁrst pretrain a model on Path-64. We take the checkpoint\nafter 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in\nspace), and ﬁne-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine\ndecay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy),\nand additionally ﬁne-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4\npoints of accuracy to FlashAttention for Path-X, but the model starts overﬁtting afterwards).\nE.4\nComparison with Apex FMHA\nWe compare our method/implementation with Apex FMHA (https://github.com/NVIDIA/apex/tree/\nmaster/apex/contrib/csrc/fmha).\nWhen we started this project, Apex FMHA was the fastest implementation of attention (that we knew\nof), tailored for short sequences of length at most 512. In fact, almost all MLPerf submissions for BERT\ntraining benchmark running on Nvidia GPUs use FMHA for their model code, as of MLPerf 1.1 [58]. Since\n27\n\nTable 7: Runtime (ms) of FlashAttention compared to FMHA by sequence length, with masking and dropout,\nmeasured on an A100-SXM4-40GB GPU. Batch size 64, 16 heads, head dimension 64 (i.e., BERT-large size).\nAttention Method\n128\n256\n512\nApex FMHA forward\n0.10\n0.29\n1.14\nFlashAttention forward\n0.08\n0.22\n0.81\nApex FMHA backward\n0.17\n0.52\n1.81\nFlashAttention backward\n0.20\n0.53\n2.00\nApex FMHA forward + backward\n0.27\n0.81\n2.95\nFlashAttention forward + backward\n0.28\n0.75\n2.81\nFMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA\nfuses the attention computation dropout(softmax(mask(QK⊤)))V into one CUDA kernel. In the forward\npass, it stores the attention matrix softmax(mask(QK𝑇)) to HBM to be used in gradient computation. As a\nresult, it does not oﬀer substantial memory saving (though for shorter sequences memory footprint is often\nnot a primary concern).\nWe use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputa-\ntion) to deal with long sequences and to save memory as mentioned in Section 3. As a result, we can support\nmuch longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and\nbroader GPU types (all Turing and Ampere GPUs at the time of writing).\nIn Table 7, we compare the performance of FlashAttention and Apex FMHA for short sequences (as\nFMHA only supports sequence length at most 512). Generally FlashAttention is slightly faster than\nFMHA in the forward pass and slightly slower than FMHA in the backward pass. This is because we do not\nstore the attention matrix in the forward pass and recompute it in the backward pass. Compared to FMHA,\nthe overall runtime of FlashAttention is about 4% slower for sequence length 128, 8% faster for sequence\nlength 256, and 5% faster for sequence length 512.\nE.5\nSpeedup On Diﬀerent Hardware and Conﬁgurations\nSpeedup varies between diﬀerent types of GPU types and generations depending on HBM bandwidth and\nSRAM size. In this section, we proﬁle FlashAttention speedup on diﬀerent GPUs and conﬁgurations.\nFigure 5: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100.\nA100\nFigure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention\nheads, across diﬀerent sequence lengths. We generally see 2-4× speedup, and we see more speedup when\nusing dropout and masking due to kernel fusion.\n28\n\nFigure 6: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on A100, with head\ndimension 128.\nA100, Head Dimension 128\nSpeedup also changes when we increase the head dimension. Each block\nrequires more memory, so we need to use smaller block sizes to ﬁt into SRAM. Figure 6 shows speedup with\nhead dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overall—but we can still see\nsigniﬁcant speedup (up to 3×) with a causal mask, where half the blocks are masked out.\nFigure 7: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on RTX 3090.\nRTX 3090\nFigure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention\nheads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5×), since the memory bandwidth\non an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).\nT4\nFigure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block\nsizes smaller in FlashAttention. As a result, we observe less speedup on T4, which matches the IO\ncomplexity analysis in Section 3.2. T4 GPUs are commonly used for inference, so we also report speedup on\nthe forward pass only.\n29\n\nFigure 8: Speedup over standard PyTorch attention at diﬀerent sequence lengths, on T4. Top: Combined\nforward pass + backward pass. Bottom: Forward pass only.\nE.6\nFull Benchmarking Results\nWe report the full benchmarking results and experimental details on A100.\nBaselines\nWe compare against reference implementations for exact attention from PyTorch/HuggingFace\nand Megatron, approximate attention, and sparse attention. For approximate attention, we compare against\nreference implementations of Reformer [51], Local Attention [68], Linformer Attention [84], Smyrf [19], and\nLongShortFormer (LSFormer) [94]. For sparse attention, we compare against reference implementations of\nBlock-Sparse Attention form OpenAI [11], Longformer[3], and BigBird Attention [92]. For the approximate\nand sparse attention, we use a compression ratio of 1/8, or a compressed sequence length of 256, whichever is\nsmaller.\nSetup\nWe measure runtime and memory usage of the attention computation with 8 heads of dimension 64,\nand batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length\nin our experiments. We compute attention on random vectors for Q, K, and V (we do not measure the\nprojection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask\nwith uniformly-random mask lengths between the total sequence length and the total sequence length minus\n20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure\nmemory footprint once, since it does not vary between runs.\n30\n\nTable 8: Pointers to results tables.\nDropout\nMasking\nPass\nTable\nYes\nYes\nForward\nTable 9\nYes\nYes\nBackward\nTable 10\nYes\nYes\nCombined\nTable 11\nNo\nYes\nForward\nTable 12\nNo\nYes\nBackward\nTable 13\nNo\nYes\nCombined\nTable 14\nYes\nNo\nForward\nTable 15\nYes\nNo\nBackward\nTable 16\nYes\nNo\nCombined\nTable 17\nNo\nNo\nForward\nTable 18\nNo\nNo\nBackward\nTable 19\nNo\nNo\nCombined\nTable 20\nNo\nNo\nMemory Usage (Combined)\nTable 21\nTable 9: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith dropout and masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.36\n0.34\n0.78\n2.54\n9.33\n36.33\n-\n-\n-\n-\nMegatron\n0.40\n0.40\n1.10\n3.65\n16.19\n-\n-\n-\n-\n-\nReformer\n2.03\n3.15\n5.67\n11.02\n22.59\n46.14\n97.38\n212.13\n-\n-\nLocal Attention\n0.83\n0.86\n1.01\n2.20\n7.13\n14.32\n28.60\n57.79\n117.67\n-\nLinformer\n0.67\n0.52\n0.69\n0.71\n1.65\n3.18\n6.15\n12.16\n24.17\n52.39\nSmyrf\n2.27\n2.34\n3.91\n7.44\n14.71\n29.22\n58.27\n116.41\n-\n-\nLSformer\n1.18\n1.27\n1.34\n3.38\n11.40\n22.55\n44.95\n89.76\n179.66\n-\nBlock Sparse\n1.12\n1.11\n2.13\n2.77\n6.95\n20.91\n-\n-\n-\n-\nLongformer\n1.22\n1.14\n1.08\n1.95\n5.72\n12.98\n-\n-\n-\n-\nBigBird\n1.13\n1.12\n1.12\n1.77\n6.03\n13.68\n-\n-\n-\n-\nFlashAttention\n0.04\n0.06\n0.21\n0.82\n2.85\n10.41\n41.74\n167.19\n670.76\n2682.35\nBlock-Sparse FlashAttention\n0.06\n0.06\n0.06\n0.12\n0.44\n0.86\n1.70\n3.29\n6.55\n13.34\nWe report timing results on the forward pass, backward pass, and combined forward + backward pass.\nWe measure each method with and without dropout, masking, or both—except for Block Sparse, Longformer,\nand BigBird. These methods did not successfully run the backward pass with masking due to a bug in\nexternal libraries, so we measured them without masking to be generous. We use FP16 for all measurements,\nexcept for Local Attention, whose implementation only supports FP32.\nFor each baseline, we increase sequence length until it runs out of memory on the GPU, except for the\nfollowing exceptions: The Megatron implementation does not support sequence lengths longer than 2048.\nBlock-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not\nsupport sequence lengths longer than 8092.\nWe measure memory usage on the combined forward + backward pass, without dropout or masking.\nResults\nTable 8 summarizes all the experimental conﬁgurations and contains pointers to the results tables.\n31\n\nTable 10: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith dropout and masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.37\n0.49\n1.66\n5.81\n22.32\n87.67\n-\n-\n-\n-\nMegatron\n0.35\n0.32\n0.77\n2.42\n8.43\n-\n-\n-\n-\n-\nReformer\n2.37\n4.59\n8.91\n17.68\n35.13\n70.05\n140.01\n-\n-\n-\nLocal Attention\n0.55\n0.62\n1.49\n4.03\n13.78\n27.61\n55.20\n110.27\n221.40\n-\nLinformer\n0.89\n0.80\n0.81\n0.93\n2.48\n4.75\n9.29\n18.27\n36.53\n-\nSmyrf\n1.41\n2.83\n5.43\n10.72\n21.25\n42.31\n84.48\n168.95\n-\n-\nLSformer\n1.75\n1.76\n3.01\n7.50\n20.07\n39.08\n76.39\n150.82\n-\n-\nBlock Sparse\n1.29\n1.28\n2.18\n3.04\n7.27\n21.16\n-\n-\n-\n-\nLongformer\n1.27\n1.31\n1.29\n2.04\n5.24\n10.74\n25.95\n-\n-\n-\nBigBird\n1.33\n1.28\n1.32\n1.81\n5.55\n11.44\n27.45\n-\n-\n-\nFlashAttention\n0.30\n0.26\n0.68\n2.02\n6.84\n26.89\n105.70\n418.96\n1666.89\n6660.44\nBlock-Sparse FlashAttention\n0.30\n0.27\n0.29\n0.59\n1.50\n2.94\n5.82\n11.85\n23.98\n47.61\nTable 11: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by\nsequence length, with dropout and masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.84\n0.86\n2.35\n8.29\n31.75\n124.19\n-\n-\n-\n-\nMegatron\n0.87\n0.89\n1.33\n4.21\n16.50\n-\n-\n-\n-\n-\nReformer\n4.30\n7.76\n14.60\n28.74\n57.79\n116.34\n237.57\n-\n-\n-\nLocal Attention\n1.40\n1.60\n2.06\n6.06\n20.94\n42.01\n84.08\n168.48\n339.45\n-\nLinformer\n1.57\n1.49\n1.55\n1.60\n4.19\n8.04\n15.71\n30.92\n61.47\n-\nSmyrf\n3.41\n5.08\n9.35\n18.18\n36.03\n71.68\n143.04\n285.87\n-\n-\nLSformer\n3.08\n3.10\n4.26\n10.90\n31.59\n61.72\n121.51\n241.18\n-\n-\nBlock Sparse\n2.54\n2.52\n3.71\n5.44\n13.29\n39.19\n-\n-\n-\n-\nLongformer\n2.47\n2.49\n2.51\n3.10\n10.39\n22.49\n60.44\n-\n-\n-\nBigBird\n2.51\n2.49\n2.52\n3.40\n10.97\n23.89\n63.28\n-\n-\n-\nFlashAttention\n0.43\n0.41\n0.95\n2.55\n9.56\n37.49\n147.75\n586.61\n2339.11\n9341.30\nBlock-Sparse FlashAttention\n0.44\n0.44\n0.45\n0.89\n1.95\n4.12\n7.64\n16.60\n32.73\n64.11\nTable 12: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.30\n0.30\n0.63\n1.93\n7.08\n27.45\n112.90\n-\n-\n-\nMegatron\n0.45\n0.41\n0.43\n1.52\n5.80\n-\n-\n-\n-\n-\nReformer\n1.87\n3.00\n5.37\n10.43\n21.40\n43.83\n92.80\n203.24\n-\n-\nLocal Attention\n0.70\n0.81\n1.02\n2.09\n6.64\n13.34\n26.77\n54.02\n110.11\n-\nLinformer\n0.63\n0.50\n0.67\n0.65\n1.36\n2.60\n5.04\n9.92\n19.69\n43.47\nSmyrf\n2.38\n2.32\n3.76\n7.16\n14.14\n28.09\n55.98\n111.73\n-\n-\nLSformer\n1.22\n1.29\n1.44\n3.28\n10.99\n21.72\n43.29\n86.32\n172.76\n-\nBlock Sparse\n0.96\n1.04\n1.66\n2.16\n5.41\n16.15\n-\n-\n-\n-\nLongformer\n0.99\n0.98\n0.99\n1.56\n4.79\n11.07\n32.98\n-\n-\n-\nBigBird\n0.96\n1.02\n1.02\n1.48\n5.05\n11.59\n34.16\n-\n-\n-\nFlashAttention\n0.03\n0.04\n0.17\n0.68\n2.28\n8.40\n33.55\n134.14\n537.50\n2150.88\nBlock-Sparse FlashAttention\n0.05\n0.04\n0.05\n0.11\n0.35\n0.68\n1.33\n2.54\n5.34\n10.73\nTable 13: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.44\n0.46\n1.53\n5.33\n20.34\n79.87\n-\n-\n-\n-\nMegatron\n0.29\n0.31\n0.65\n1.95\n6.49\n-\n-\n-\n-\n-\nReformer\n2.31\n4.47\n8.68\n17.20\n34.14\n68.09\n136.02\n-\n-\n-\nLocal Attention\n0.51\n0.62\n1.30\n3.81\n13.33\n26.72\n53.41\n106.82\n214.15\n-\nLinformer\n0.76\n0.81\n0.94\n0.87\n2.24\n4.25\n8.35\n16.38\n32.67\n72.11\nSmyrf\n1.34\n2.77\n5.30\n10.46\n20.73\n41.27\n82.41\n164.86\n-\n-\nLSformer\n1.66\n1.61\n3.09\n7.42\n19.68\n38.35\n74.92\n147.86\n-\n-\nBlock Sparse\n1.24\n1.25\n2.04\n2.91\n6.78\n19.67\n-\n-\n-\n-\nLongformer\n1.27\n1.23\n1.24\n1.85\n4.99\n10.21\n24.89\n-\n-\n-\nBigBird\n1.43\n1.50\n1.44\n1.69\n5.25\n10.86\n26.26\n-\n-\n-\nFlashAttention\n0.21\n0.22\n0.62\n1.84\n5.77\n22.25\n86.21\n338.91\n1343.91\n5361.09\nBlock-Sparse FlashAttention\n0.22\n0.22\n0.26\n0.57\n1.55\n3.13\n5.98\n12.21\n23.49\n47.85\n32\n\nTable 14: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by\nsequence length, with masking. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.80\n0.81\n2.08\n7.23\n27.51\n107.58\n-\n-\n-\n-\nMegatron\n0.81\n0.83\n1.09\n3.36\n12.39\n-\n-\n-\n-\n-\nReformer\n4.16\n7.46\n14.06\n27.68\n55.66\n112.15\n229.37\n-\n-\n-\nLocal Attention\n1.39\n1.68\n2.08\n5.83\n20.04\n40.16\n80.44\n161.35\n325.11\n-\nLinformer\n1.51\n1.42\n1.56\n1.67\n3.67\n6.99\n13.63\n26.77\n53.36\n117.56\nSmyrf\n3.38\n4.93\n9.07\n17.66\n34.94\n69.55\n138.72\n277.41\n-\n-\nLSformer\n3.08\n3.10\n4.26\n10.90\n31.59\n61.72\n121.51\n241.18\n-\n-\nBlock Sparse\n2.39\n2.40\n3.31\n5.02\n12.25\n35.94\n-\n-\n-\n-\nLongformer\n2.36\n2.34\n2.38\n2.94\n9.83\n21.35\n58.12\n-\n-\n-\nBigBird\n2.35\n2.35\n2.37\n3.25\n10.36\n22.57\n60.63\n-\n-\n-\nFlashAttention\n0.32\n0.30\n0.83\n2.37\n7.95\n30.77\n119.98\n473.65\n1883.43\n7513.01\nBlock-Sparse FlashAttention\n0.34\n0.34\n0.36\n0.69\n1.85\n3.89\n7.16\n14.85\n30.46\n60.03\nTable 15: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith dropout. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.26\n0.24\n0.57\n1.80\n6.56\n25.34\n-\n-\n-\n-\nMegatron\n0.27\n0.27\n0.56\n1.88\n6.56\n-\n-\n-\n-\n-\nReformer\n1.83\n2.96\n5.31\n10.33\n21.19\n43.42\n91.96\n201.34\n-\n-\nLocal Attention\n0.51\n0.60\n0.78\n2.01\n6.23\n12.52\n25.07\n50.50\n102.18\n-\nLinformer\n0.47\n0.37\n0.49\n0.52\n1.37\n2.65\n5.12\n10.13\n20.25\n44.16\nSmyrf\n2.12\n2.01\n3.15\n5.97\n11.83\n23.36\n46.48\n92.72\n-\n-\nLSformer\n1.28\n1.33\n1.51\n3.39\n11.40\n22.54\n44.96\n89.85\n179.73\n-\nBlock Sparse\n1.03\n1.00\n1.72\n2.39\n5.96\n17.88\n-\n-\n-\n-\nLongformer\n1.02\n1.03\n1.03\n1.73\n5.10\n11.63\n34.22\n-\n-\n-\nBigBird\n0.99\n1.03\n1.01\n1.58\n5.36\n12.27\n35.56\n-\n-\n-\nFlashAttention\n0.10\n0.10\n0.22\n0.83\n2.81\n10.38\n41.63\n167.01\n668.74\n2678.11\nBlock-Sparse FlashAttention\n0.54\n0.51\n0.68\n0.61\n0.67\n1.10\n1.89\n3.71\n7.18\n14.41\nTable 16: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length,\nwith dropout. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.44\n0.35\n0.90\n2.94\n10.77\n41.67\n-\n-\n-\n-\nMegatron\n0.28\n0.33\n0.92\n2.94\n10.80\n-\n-\n-\n-\n-\nReformer\n2.24\n4.34\n8.39\n16.62\n33.02\n65.77\n131.52\n-\n-\n-\nLocal Attention\n0.51\n0.58\n1.41\n3.71\n12.96\n25.98\n51.94\n103.72\n207.78\n-\nLinformer\n0.84\n0.74\n0.79\n0.85\n2.28\n4.37\n8.66\n17.02\n33.78\n-\nSmyrf\n1.27\n2.56\n4.90\n9.66\n19.16\n38.13\n76.17\n152.39\n-\n-\nLSformer\n1.67\n1.77\n3.03\n7.52\n20.10\n39.13\n76.35\n150.83\n-\n-\nBlock Sparse\n1.27\n1.36\n2.15\n3.04\n7.27\n21.18\n-\n-\n-\n-\nLongformer\n1.28\n1.34\n1.38\n1.98\n5.24\n10.74\n25.95\n-\n-\n-\nBigBird\n1.48\n1.47\n1.50\n1.81\n5.57\n11.38\n27.43\n-\n-\n-\nFlashAttention\n0.15\n0.18\n0.58\n1.86\n6.50\n26.21\n104.27\n416.10\n1661.92\n6643.01\nBlock-Sparse FlashAttention\n0.17\n0.17\n0.17\n0.40\n1.10\n2.04\n4.43\n9.33\n18.28\n37.31\nTable 17: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by\nsequence length, with dropout. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.66\n0.67\n1.43\n4.82\n17.47\n67.29\n-\n-\n-\n-\nMegatron\n0.88\n0.90\n1.49\n4.73\n17.41\n-\n-\n-\n-\n-\nReformer\n4.06\n7.28\n13.68\n26.98\n54.27\n109.39\n223.80\n-\n-\n-\nLocal Attention\n1.09\n1.40\n1.99\n5.61\n19.23\n38.62\n77.30\n154.63\n311.12\n-\nLinformer\n1.31\n1.21\n1.30\n1.39\n3.73\n7.15\n14.05\n27.69\n55.00\n-\nSmyrf\n3.00\n4.37\n8.05\n15.66\n31.04\n61.64\n123.04\n245.65\n-\n-\nLSformer\n3.07\n3.17\n4.31\n10.89\n31.54\n61.78\n121.56\n240.94\n-\n-\nBlock Sparse\n2.54\n2.52\n3.71\n5.44\n13.29\n39.19\n-\n-\n-\n-\nLongformer\n2.47\n2.49\n2.51\n3.10\n10.39\n22.49\n60.44\n-\n-\n-\nBigBird\n2.51\n2.49\n2.52\n3.40\n10.97\n23.89\n63.28\n-\n-\n-\nFlashAttention\n0.35\n0.36\n0.80\n2.52\n9.16\n36.70\n146.13\n583.45\n2332.01\n9323.63\nBlock-Sparse FlashAttention\n0.91\n0.83\n0.94\n0.92\n1.83\n3.50\n7.02\n13.56\n26.71\n53.92\n33\n\nTable 18: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.\nBest in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.21\n0.22\n0.43\n1.27\n4.32\n16.47\n67.77\n-\n-\n-\nMegatron\n0.24\n0.26\n0.42\n1.33\n4.28\n-\n-\n-\n-\n-\nReformer\n1.77\n2.82\n5.01\n9.74\n20.03\n41.11\n87.39\n192.40\n-\n-\nLocal Attention\n0.48\n0.57\n0.80\n1.90\n5.76\n11.56\n23.13\n46.65\n94.74\n-\nLinformer\n0.46\n0.36\n0.45\n0.50\n1.09\n2.09\n4.01\n7.90\n15.70\n35.40\nSmyrf\n1.94\n1.96\n3.01\n5.69\n11.26\n22.23\n44.21\n88.22\n-\n-\nLSformer\n1.21\n1.34\n1.34\n3.31\n11.01\n21.71\n43.27\n86.32\n172.85\n-\nBlock Sparse\n0.96\n1.04\n1.66\n2.16\n5.41\n16.15\n-\n-\n-\n-\nLongformer\n0.99\n0.98\n0.99\n1.56\n4.79\n11.07\n32.98\n-\n-\n-\nBigBird\n0.96\n1.02\n1.02\n1.48\n5.05\n11.59\n34.16\n-\n-\n-\nFlashAttention\n0.08\n0.09\n0.18\n0.68\n2.40\n8.42\n33.54\n134.03\n535.95\n2147.05\nBlock-Sparse FlashAttention\n0.56\n0.52\n0.63\n0.65\n0.61\n0.96\n1.69\n3.02\n5.69\n11.77\nTable 19: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length.\nBest in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.26\n0.29\n0.78\n2.44\n8.82\n33.87\n-\n-\n-\n-\nMegatron\n0.29\n0.30\n0.80\n2.59\n8.86\n-\n-\n-\n-\n-\nReformer\n2.18\n4.21\n8.14\n16.12\n32.02\n63.84\n127.60\n-\n-\n-\nLocal Attention\n0.51\n0.64\n1.28\n3.60\n12.52\n25.08\n50.22\n100.23\n200.66\n-\nLinformer\n0.69\n0.76\n0.69\n0.80\n2.04\n3.88\n7.67\n15.04\n30.11\n63.15\nSmyrf\n1.24\n2.49\n4.77\n9.42\n18.65\n37.12\n74.15\n148.35\n-\n-\nLSformer\n1.68\n1.61\n3.02\n7.40\n19.72\n38.27\n74.89\n147.99\n-\n-\nBlock Sparse\n1.24\n1.25\n2.04\n2.91\n6.78\n19.67\n-\n-\n-\n-\nLongformer\n1.27\n1.23\n1.24\n1.85\n4.99\n10.21\n24.89\n-\n-\n-\nBigBird\n1.43\n1.50\n1.44\n1.69\n5.25\n10.86\n26.26\n-\n-\n-\nFlashAttention\n0.11\n0.16\n0.52\n1.62\n5.45\n21.57\n84.75\n336.00\n1338.56\n5343.19\nBlock-Sparse FlashAttention\n0.11\n0.12\n0.16\n0.38\n1.20\n2.34\n4.69\n9.10\n18.74\n37.04\nTable 20: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by\nsequence length. Best in bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n0.67\n0.70\n1.18\n3.67\n13.22\n50.44\n-\n-\n-\n-\nMegatron\n0.74\n0.65\n1.23\n3.80\n13.21\n-\n-\n-\n-\n-\nReformer\n3.93\n7.01\n13.15\n25.89\n52.09\n105.00\n215.13\n-\n-\n-\nLocal Attention\n1.09\n1.27\n1.99\n5.38\n18.32\n36.77\n73.67\n147.29\n296.35\n-\nLinformer\n1.31\n1.25\n1.30\n1.29\n3.20\n6.10\n11.93\n23.39\n46.72\n100.52\nSmyrf\n2.98\n4.23\n7.78\n15.12\n29.96\n59.45\n118.60\n237.02\n-\n-\nLSformer\n3.03\n3.05\n4.26\n10.70\n30.77\n60.15\n118.33\n234.94\n-\n-\nBlock Sparse\n2.39\n2.40\n3.31\n5.02\n12.25\n35.94\n-\n-\n-\n-\nLongformer\n2.36\n2.34\n2.38\n2.94\n9.83\n21.35\n58.12\n-\n-\n-\nBigBird\n2.35\n2.35\n2.37\n3.25\n10.36\n22.57\n60.63\n-\n-\n-\nFlashAttention\n0.31\n0.31\n0.73\n2.29\n7.64\n30.09\n118.50\n470.51\n1876.08\n7492.85\nBlock-Sparse FlashAttention\n0.74\n0.77\n0.82\n0.88\n1.71\n3.21\n6.56\n12.60\n24.93\n50.39\nTable 21: Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best\nin bold, second best underlined.\nAttention Method\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n65536\nPyTorch Attention\n36\n104\n336\n1184\n4416\n17024\n-\n-\n-\n-\nMegatron\n36\n104\n336\n1184\n4416\n-\n-\n-\n-\n-\nReformer\n377\n754\n1508\n3016\n6033\n12067\n24134\n-\n-\n-\nLocal Attention\n53\n110\n232\n592\n1696\n3392\n6784\n13568\n27136\n-\nLinformer\n25\n52\n114\n287\n832\n1652\n3292\n6572\n13132\n26252\nSmyrf\n217\n434\n868\n1737\n3474\n6947\n13894\n27788\n-\n-\nLSformer\n72\n152\n333\n796\n2540\n5068\n10125\n20240\n-\n-\nBlock Sparse\n33\n82\n228\n408\n910\n2401\n-\n-\n-\n-\nLongformer\n30\n61\n124\n277\n681\n1370\n2748\n-\n-\n-\nBigBird\n33\n66\n131\n294\n708\n1431\n2872\n-\n-\n-\nFlashAttention\n22\n44\n104\n209\n418\n836\n1672\n3344\n6688\n13376\nBlock-Sparse FlashAttention\n22\n44\n104\n209\n418\n836\n1672\n3344\n6690\n13384\n34\n"
    },
    {
      "arxiv_id": "2207.14255",
      "full_text": "Efﬁcient Training of Language Models to\nFill in the Middle\nMohammad Bavarian ∗\nHeewoo Jun∗\nNikolas Tezak\nJohn Schulman\nChristine McLeavey\nJerry Tworek\nMark Chen\nOpenAI\nAbstract\nWe show that autoregressive language models can learn to inﬁll text after we apply\na straightforward transformation to the dataset, which simply moves a span of\ntext from the middle of a document to its end. While this data augmentation has\ngarnered much interest in recent years, we provide extensive evidence that training\nmodels with a large fraction of data transformed in this way does not harm the\noriginal left-to-right generative capability, as measured by perplexity and sampling\nevaluations across a wide range of scales. Given the usefulness, simplicity, and\nefﬁciency of training models to ﬁll-in-the-middle (FIM), we suggest that future\nautoregressive language models be trained with FIM by default. To this end, we\nrun a series of ablations on key hyperparameters, such as the data transformation\nfrequency, the structure of the transformation, and the method of selecting the inﬁll\nspan. We use these ablations to prescribe strong default settings and best practices\nto train FIM models. We have released our best inﬁlling model trained with best\npractices in our API, and release our inﬁlling benchmarks to aid future research.\n∗Equal contribution, order randomized. Correspondence to: mobav@openai.com, heewoo@openai.com.\narXiv:2207.14255v1  [cs.CL]  28 Jul 2022\n\nContents\n1\nIntroduction\n3\n1.1\nOur contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nEvaluation\n5\n2.1\nAutoregressive evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nInﬁlling evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3\nFIM training and inference\n6\n3.1\nSPM mode\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2\nContext-level FIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nPretraining results\n8\n4.1\nEvaluation of left-to-right capabilities in downstream benchmarks\n. . . . . . . . . . . . . . .\n9\n4.2\nFIM rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nSPM vs PSM vs joint SPM+PSM training . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4.4\nContext-level vs document-level FIM\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.5\nMiddle span selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5\nFinetuning results\n13\n6\nDiscussion\n14\n7\nRelated work\n16\n8\nConclusion\n17\n8.1\nRecommended FIM hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n8.2\nFuture directions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA Architecture and datasets\n23\nB\nScaling trends for FIM rate ablations\n23\nC Details of FIM implementation\n24\nD Details of SPM encoding\n25\nE\nRandom span inﬁlling benchmark\n25\nF\nDynamics and learning curves of ﬁnetuning\n26\nG Top models comparison\n27\nH Qualitative evaluation\n28\nH.1\nSuccessful inﬁlling examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nH.2\nLimitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nH.3\nMitigations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n2\n\n107\n108\n109\nNon-embedding parameters\n2.2 × 100\n2.4 × 100\n2.6 × 100\n2.8 × 100\n3 × 100\nTest loss\nLanguage\n0.0\n0.5\n107\n108\n109\nNon-embedding parameters\n100\n8 × 10\n1\n9 × 10\n1\nCode\nFigure 1: FIM can be learned for free. We pretrain language models with 50% and 0% FIM rates\non two domains, natural language and code, and evaluate the test loss of all the ﬁnal snapshots. All\nmodels are trained on 100B tokens of data. We observe that joint FIM training incurs no cost as the\noriginal left-to-right loss trend remains the same even though FIM models see the original data only\n50% of the time and the models are learning a new capability. See Figure 3 for more evidence for the\nFIM-for-free property.\n1\nIntroduction\nFollowing the introduction of the Transformer [Vaswani et al., 2017], large language models (LLMs)\ntrained on diverse Internet scale datasets have achieved remarkable success. These models are\ncapable of producing coherent and sensible completions given a natural language prompt, and they\nachieve state-of-the-art performance in many benchmarks including reading comprehension, question\nanswering, logical inference, and common sense reasoning.\nThere are several broad classes of transformer based language models: encoder-only models like\nBERT [Devlin et al., 2019] are typically trained with a masked language modeling objective, and\nencoder-decoder models like T5 [Raffel et al., 2019] are typically trained with a span prediction\nobjective [Song et al., 2019]. Finally, causal decoder-based language models, like the GPT model\nseries [Radford et al., 2018, 2019, Brown et al., 2020], are trained using the left-to-right next token\nprediction objective. The largest and most capable generative language models today, such as GPT-3,\nCodex, LaMDA, GLaM, PaLM, Gopher, Jurassic-1, and Chinchilla [Brown et al., 2020, Chen et al.,\n2021, Thoppilan et al., 2022, Du et al., 2021, Chowdhery et al., 2022, Rae et al., 2021, Lieber et al.,\n2021, Hoffmann et al., 2022], belong to the latter class of models. The overwhelming popularity of\nthe causal decoder-based models at the largest scale is due to their superiority in open-ended text\ngeneration, in-context learning (using few-shot priming), pretraining computational efﬁciency [Wang\net al., 2022], and to some extent historical precedence in successful scale-ups [Brown et al., 2020].\nThese models are also architecturally simpler and generally more effective without task speciﬁc\nﬁnetuning, making them more attractive for inference and deployment.\nAll model classes are limited when it comes to inﬁlling, where the model is tasked with generating\ntext at a speciﬁc location within a prompt, while conditioning on both a preﬁx and a sufﬁx. Left-to-\nright models can only condition on the preﬁx. While encoder-only and encoder-decoder models are\ncapable of conditioning on sufﬁxes, the lengths of inﬁll regions seen at training time are typically\nmuch shorter than what is useful in practice. This is unfortunate because inﬁlling naturally arises in\napplications where there is context both before and after the point of generation. For example, in\ncreating a coding assistant, inﬁlling can be used for docstring generation, import statement generation,\nor for completing a partially written function.\nOur goal in this work is to address this limitation by adding ﬁll-in-the-middle (FIM) capability to\ncausal decoder-based language models which are currently the most dominant paradigm for large\nscale language modelling [Brown et al., 2020, Hoffmann et al., 2022, Chowdhery et al., 2022]. We\nshow that with a simple modiﬁcation to training data and without changing the model architecture,\ncausal decoder-based autoregressive (AR) language models can learn inﬁlling without compromising\ntheir normal left-to-right generative capability.\n3\n\n107\n108\n109\nNon-embedding parameters\n2.2 × 100\n2.4 × 100\n2.6 × 100\n2.8 × 100\n3 × 100\n3.2 × 100\nTest FIM loss\nLanguage\n0.0\n0.5\n107\n108\n109\nNon-embedding parameters\n100\n9 × 10\n1\n1.1 × 100\n1.2 × 100\n1.3 × 100\nCode\nFigure 2: Evaluation of inﬁlling capabilities of the same model scans from Figure 1 using FIM\ntest losses. Models trained with FIM (yellow) obtain lower FIM test loss than baseline (purple)\nAR models. This shows that the FIM models are indeed learning to condition on the sufﬁx while\npredicting the middle section allowing them to achieve lower test loss on FIM test set. Figures 1 and\n2 together indicate that FIM models can be considered strictly better than AR models as they achieve\nthe same left-to-right autoregressive loss but lower FIM loss.\nThe key to our approach, described in Section 3, is a transformation applied to a fraction of our\ndataset, in which we split documents into three pieces at random and move the middle piece to the\nend:\ndocument →(preﬁx, middle, sufﬁx) →(preﬁx, sufﬁx, middle)\nWe then concatenate the three pieces using sentinel tokens. This is similar to the procedure used in\n[Donahue et al., 2020, Aghajanyan et al., 2022, Fried et al., 2022].\nCompared to prior work, our work emphasizes the computational efﬁciency of training FIM models.\nThis emphasis is important given the increased interest in training very large language models, which\nare very expensive to train and have a substantial energy footprint. In general, when adding a new\nobjective or capability to language models, we believe the most critical question is the effect on the\nexisting capabilities and the computational efﬁciency trade-offs.\nUnlike most cases where we jointly train on multiple objectives and datasets, we show that models\ntrained jointly on a mixture of FIM transformed data and ordinary left-to-right data achieve the\nsame left-to-right capability while learning how to ﬁll-in-the-middle. We call this the FIM-for-free\nproperty.\nIn what follows, we use the term FIM model to refer to any model trained on a mixture of FIM\ntransformed and normal left-to-right data. We refer to models trained without any FIM data (i.e. 0%\nFIM rate) as AR models.\n1.1\nOur contributions\nOur central contributions in this paper are as follows:\n• FIM-for-free property: We perform an extensive scaling study by training a suite of 8\nmodels, with and without FIM, and show that FIM can be learned without compromising\nthe left-to-right capability in pretraining. We examine this claim in both code and language,\nusing both perplexity and sampling-based benchmarks.\n• Best practices for FIM in pretraining: We clarify the effects of many hyperparameters\nrelated to training FIM models using comprehensive ablations. In particular, we study the\nFIM rate (the probability at which FIM transformation is applied to the data), different\nvariants of FIM transformation, and the choice of middle span.\n• Finetuning inefﬁciency: An alternative to training FIM models from scratch is to learn this\ncapability by ﬁnetuning existing language models. We show that ﬁnetuning with FIM is\ncomputationally inefﬁcient. While FIM can be learned for free during pretraining, learning\nFIM during ﬁnetuning requires a signiﬁcant amount of additional compute to reach similar\nlevels of performance as pretraining.\n4\n\n• New inﬁlling benchmarks. In order to study the generative capabilities of our models, we\nneed to evaluate the correctness of free-form generated samples. For this, we focus on code\nwhere we can use unit tests to evaluate the correctness of long FIM samples. In particular,\nwe use the single-line and multi-line inﬁlling benchmarks introduced by [Fried et al., 2022]\nby removing non-empty lines of canonical solutions of HumanEval [Chen et al., 2021].\nHowever since line-based evaluations do not capture all the use cases of FIM, we create two\nnew benchmarks called random span inﬁlling and random span inﬁlling light. We discuss\nthese benchmarks and our evaluation methodology more generally in Section 2.\n• Need for sampling evaluations. In Sections 4.2, 4.4, and Appendix B, we ﬁnd that chang-\ning various hyperparameters in FIM training often leads to negligible differences in FIM\ntest losses but large differences in sampling based benchmarks. Not only are sampling\nbenchmarks closer to real use cases, but they also appear to be able to tease apart gains\nthat can be missed using test losses. This is an important ﬁnding since often scaling laws\nanalysis relies just on test losses, which we ﬁnd are misleading if not augmented with other\nevaluations.\nIt is interesting to contrast the ﬁrst and third bullet points above. The ﬁrst states that learning\nFIM in pretraining is free while leaving it to ﬁnetuning is surprisingly costly. We discuss potential\nexplanations for this ﬁnding in Section 6. To establish the FIM-for-free property, we perform an\nablation study on both code and language across a range of scales. We train 8 models from 50M\nto 6.9B parameters, both with and without FIM, and compare the performance across a variety of\nautoregressive benchmarks. In particular, we train 16 models on code for 100B tokens and another\n16 models on natural language for 100B tokens. The comparison of these models in terms of normal\nautoregressive left-to-right language modeling test loss is presented in Figure 1. In both domains,\nFIM models achieve similar AR test loss as the non-FIM models.\nWe provide more evidence for the FIM-for-free property by comparing FIM and AR models on\nnon-loss based benchmarks in Section 4. Moreover, we see in Section 4.2 that there is a stronger form\nof the FIM-for-free property. Not only there is no hit in autoregressive capabilities from FIM training\non the ﬁnal checkpoints, the same also holds throughout training. This is evidenced by the matching\nlearning curves between AR and FIM models in left-to-right loss and HumanEval evaluations in\nFigures 4 and 5.\nBeside studying the effect of FIM training on the left-to-right capability, it is also important to show\nthat the models are in fact learning to inﬁll from FIM training. Figure 2 provides evidence for this in\nthe context of FIM test losses. We study the inﬁlling capabilities of our models more extensively in\nSection 4 and Appendix H.\n2\nEvaluation\nWe use both AR and FIM evaluation benchmarks to analyze the capabilities of our models. Vanilla\nAR evaluation is important for quantifying the impact of FIM training on left-to-right capabilities and\nallows us to demonstrate the FIM-for-free property from Section 1.1. FIM evaluation is important for\nunderstanding the effect of different hyperparameters on FIM training and to understand the scaling\ntrends.\nThroughout the paper, we use the terms AR and left-to-right interchangeably. AR loss refers to the\ncross entropy loss on normal left-to-right data and FIM loss as the loss on 100% FIM transformed\ndata. All test losses are in nats per token unit. In all sampling-based benchmarks, we use nucleus\nsampling [Holtzman et al., 2020] with a nucleus parameter of 0.95.\n2.1\nAutoregressive evaluation\nFor all domains, we evaluate test losses in the canonical autoregressive order to show that the learning\ncurves and scaling trends remain the same even with FIM augmentation. Beside test losses, we\nevaluate on standard benchmarks to demonstrate that the model’s capabilities are unaffected by\nFIM training. For natural language, we use PIQA [Bisk et al., 2020], Winograd [Levesque et al.,\n2012], WinoGrande [Sakaguchi et al., 2021] for common sense reasoning, DROP [Dua et al., 2019]\nand QuAC [Choi et al., 2018] for reading comprehension, and HellaSwag [Zellers et al., 2019],\nLAMBADA [Paperno et al., 2016], StoryCloze [Mostafazadeh et al., 2016] for completion tasks. All\n5\n\nbenchmarks with the exception of DROP and QuAC are evaluated with few-shot prompting. For\ncode, we measure the pass rates on HumanEval [Chen et al., 2021].\n2.2\nInﬁlling evaluation\nTo create FIM tests, we apply the FIM transformation to the examples from the AR test sets with a FIM\nrate of 100%. Using the same underlying examples in FIM and AR test sets allows us to compare FIM\nand AR test losses. Additionally, we create a masked version of these test sets where we only measure\nthe loss on the middle span tokens. The latter test sets are used to measure P(middle∣preﬁx, sufﬁx)\nfor FIM models and P(middle∣preﬁx) for AR models allowing us to investigate the amount of\ninformation FIM models gain by being able to condition on the sufﬁx.\nFor generative inﬁlling capabilities, we focus on code since we are interested in free-form generation\nin contrast to single or few token generations common in cloze-style natural language benchmarks.\nThe advantage of working with code is that we can use test suites to evaluate the correctness of\nsamples in our tasks even when evaluating long samples from open-ended generations.\nAll the sampling based inﬁlling benchmarks we use are partial function completions tasks created by\nremoving middle spans from the canonical solutions of HumanEval [Chen et al., 2021]. In particular,\nwe use the single-line and multi-line inﬁlling benchmarks proposed by [Fried et al., 2022] where\ndifferent spans of non-empty lines in the canonical solutions of HumanEval are turned into a FIM task.\nIn addition, we create a new benchmark called random span inﬁlling2, where for each HumanEval\nproblem, we create inﬁlling tasks by selecting the middle span from the canonical solution uniformly\nat random. We show an example of such a task below where the model must predict the highlighted\nsection (or an alternative completion accomplishing the same goal). We refer to Appendix E for more\ndetails.\ndef unique(l: list):\n\"\"\"Return sorted unique elements in a list\n>>> unique([5, 3, 5, 2, 3, 3, 9, 0, 123])\n[0, 2, 3, 5, 9, 123]\n\"\"\"\nreturn sorted(list(set(l)))\nThe single-line, multi-line, and random span inﬁlling together constitute our inﬁlling benchmark suite.\nThese benchmarks have 1033, 5815, and 1640 tasks, respectively. We note that this is much larger\nthan the number of tasks in the original HumanEval dataset (164 tasks), which reduces variance in\nour evaluations. Still, we take at least 100 to 200 samples per task to further reduce variance when\nevaluating these benchmarks on the ﬁnal snapshots of our models. We also use random span inﬁlling\nlight, a smaller version of random span inﬁlling, with only one random FIM task per HumanEval\nproblem and just 164 tasks, to track the inﬁlling capability trends during training.\nIn Section 3, we ﬁnd that FIM can be prepared in two different ways denoted as PSM and SPM. We\nreport just the SPM inﬁlling results for brevity, except in cases when the use of PSM changes the\nconclusions.\n3\nFIM training and inference\nWe implement FIM using a random transformation applied to our dataset. We experiment with two\ndifferent implementations: document level and context level. The difference between the two is\nat which stage of the data loading pipeline the FIM transformation occurs. This choice naturally\narises because a long document can be broken into many contexts, or a context can contain multiple\ndocuments when the documents are small. We ﬁrst describe the document-level case and then\ndescribe the changes required to implement context-level FIM in Section 3.2.\nIn document-level FIM, with a certain probability p called the FIM rate (we use p = 0.5 for our main\nsuite of models), we cut each document into three parts: preﬁx, middle, and sufﬁx. We perform this\nsplit prior to tokenization, when the document is still a sequence of characters. We split uniformly at\nrandom, which means the lengths of preﬁx, middle, and sufﬁx are each 1/3 of the full document in\nexpectation.\n2Released at https://www.github.com/openai/human-eval-infilling\n6\n\nWe then encode each of the three sections separately and prepend sentinel tokens to the beginning of\neach section. We denote these sentinel tokens by <PRE>, <MID>, and <SUF>. Finally we concatenate\nall these sections in the order preﬁx, sufﬁx, and middle along with their sentinel tokens to form the\ntokenized version of the FIM document,\n<PRE> ◦Enc(preﬁx) ◦<SUF> ◦Enc(sufﬁx) ◦<MID> ◦Enc(middle),\n(PSM)\nwhere ◦denotes concatenation. The different documents, whether FIM or AR, then are concatenated\nwith <EOT> and are given to the model during training. We reiterate that we keep the loss on all three\nsections preﬁx, middle, and sufﬁx, so FIM training does not cause a decrease in the autoregressive\nlearning signal. Preliminary experiments, although not reported here, suggest that this choice is\ncrucial for the FIM-for-free property to hold. This property does not change whether the sentinels are\nmasked or not; however, it is important to always train on the <EOT> tokens as it signals a successful\njoin to the sufﬁx.\nFor inference, we encode the given preﬁx and sufﬁx and prompt the model with\n<PRE> ◦Enc(preﬁx) ◦<SUF> ◦Enc(sufﬁx) ◦<MID>.3\n(PSM inference)\nWe continue sampling from the model until it generates the <EOT> token which is how the model\ncommunicates it has connected the preﬁx and the sufﬁx.\nIf the model fails to generate an <EOT> token within a reasonable allotted inference token budget, it is\noften a sign the model is having a difﬁcult time connecting the preﬁx and the sufﬁx, and the resulting\nsamples often will be of worse quality, which motivates the procedure of EOT aware best-of-n\nsampling. See Appendix H for more discussion.\n3.1\nSPM mode\nWe also introduce a variant of the above procedure where we swap the order of preﬁx and sufﬁx,\ncalled SPM, to emphasize the changing of the order to sufﬁx, preﬁx, and middle. Our main motivation\nfor introducing SPM is improved key-value caching during inference. The reason for this advantage\nis that with SPM, appending tokens to the preﬁx no longer invalidates the keys and values computed\nin the sufﬁx section. Note that superiority of SPM caching is not universal and may depend on the\napplications. In particular, in the SPM mode, minor changes to the sufﬁx will invalidate the cache\nfor preﬁx, but we expect changes to the sufﬁx to be rarer than changes in preﬁx in real workloads.\nInterestingly, we ﬁnd in Section 4.3 beside the caching advantages, SPM in fact has also a slight edge\nover PSM in the inﬁlling benchmarks.\nIn our main runs, we apply the FIM transformation with 50% probability in PSM mode and with 50%\nprobability in SPM mode, so the model is able to handle both types of formatting in inference. In\nother words, each mode inherits half of the total FIM rate p. We ablate this choice of joint training on\nPSM and SPM and compare with pure PSM and SPM runs. The results in Table 1 show the efﬁcacy\nof this choice.\nEven though the idea of SPM mode is simple, there are some subtleties with the placement of sentinel\ntokens in SPM which are especially important when training jointly on SPM and PSM. We describe\nthese subtleties in Appendix D.\n3.2\nContext-level FIM\nIn language model training, documents are often joined with a boundary token, referred to as <EOT>,\nand are then chunked to the model context length. When applying FIM to long documents, this\noperation can result in fragmented FIM data where the entire preﬁx or sufﬁx could get cut out of\nthe context during chunking. To address this issue, we can apply FIM after the chunking step. A\ncontext slice may have multiple documents in them joined with the <EOT> boundary token. So, we\nsplit based on <EOT>, turn some of the documents into FIM examples with probability given by the\nFIM rate, and join the examples again with <EOT>. The resulting slice is then trimmed to the model\ncontext length. We refer to Appendix C for more details of FIM transformation. In Section 4.4, we\n3It is worth noting that prepending this prompt with <EOT> leads to a slight improved performance, and we\ndo so when evaluating our models in sampling benchmarks.\n7\n\nshow this technique can boost performance relative to document-level FIM, and adopt context-level\nFIM in all our main FIM runs in this work.\n4\nPretraining results\nIn Section 1.1, we discussed the FIM-for-free property which states that FIM can be learned without\nany impact to the left-to-right capability. We start this section by presenting more evidence for this\nresult. Next, we study the hyperparameters of FIM training including the FIM rate, PSM vs SPM vs\njoint training, context vs document-level FIM, and the choice of middle span. Although FIM is free\nfrom the point of view of AR capability, the FIM capabilities themselves depend strongly on these\nhyperparameters. We study these choices in the code domain, where we can measure the correctness\nof generated samples using unit tests.\nThe models, unless otherwise stated, are trained with ﬁxed horizon of 100B tokens. For our main\nscans we use all the 8 models described in Appendix A. For more extensive scans, e.g. Sections\n4.2, 4.4, and Appendix B, we use a subset of the models trained with a shorter horizon to limit the\ncompute costs.\n30\n40\n50\n60\nHellaSwag\n0.0\n0.5\n20\n30\n40\n50\n60\nLAMBADA\n60\n70\n80\nStoryCloze\n60\n65\n70\n75\nPIQA\n107\n108\n109\nNon-embedding parameters\n50\n60\n70\nWinograd\n107\n108\n109\nNon-embedding parameters\n50.0\n52.5\n55.0\n57.5\n60.0\nWinoGrande\n107\n108\n109\nNon-embedding parameters\n5\n10\n15\nDrop\n107\n108\n109\nNon-embedding parameters\n20\n25\n30\nQuAC\n(a) Comparison of natural language results. We report F1 for Drop and QuAC and accuracy for the rest.\n107\n108\n109\nNon-embedding parameters\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nPass rate\nHumanEval pass@1\n0.0\n0.5\n107\n108\n109\nNon-embedding parameters\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nHumanEval pass@10\n(b) Comparison of code results. We use temperature 0.8 and 400 samples per task for both pass@k.\nFigure 3: Comparison of performance on standard benchmarks for the natural language (top) and\ncode (bottom) domains. Joint training of next-token prediction and FIM allows the model to learn\nthe new inﬁlling task without affecting the original capabilities. This provides further evidence for\nFIM-for-free property.\n8\n\n4.1\nEvaluation of left-to-right capabilities in downstream benchmarks\nWe train a series of models from 50M to 6.9B parameters from scratch with and without 50% FIM\naugmentation on natural language and code domains. Figure 1 shows that the left-to-right test\nloss is unaffected even though FIM models see the data in its original form half the time, and are\nsimultaneously learning a new skill.\nHowever, as we demonstrate below (see Sections 4.2 and 4.4) it is often not sufﬁcient to just consider\ntest loss. So to strengthen the above results, we evaluate our models on a suite of standard downstream\nbenchmarks, the result of which is presented in Figure 3. We again ﬁnd that joint FIM pretraining\ndoes not result in any degradation in standard AR benchmarks as the performance matches within\nerror for both natural language and code.\n4.2\nFIM rate\nFrom Figures 1 and 3, we see that a FIM rate of 50% incurs no performance hit in the left-to-right\ncapabilities. This naturally raises several questions:\n• Does FIM-for-free still hold even at higher FIM rates? If so, how high can we increase the\nFIM rate without degrading the left-to-right capabilities?\n• Does using a higher FIM rate lead to stronger FIM capabilities? Or does the beneﬁt saturate\nafter a threshold?\nIn this section, we ablate the FIM rate to answer these questions. We train 6 large models (see Table\n3) with FIM rates (0, 0.25, 0.5, 0.75, 0.9, 1.0) for 50B tokens. The results are presented in Figures 4\nand 5. The left plot in Figure 4 provides evidence that a FIM rate even up to 90% does not cause any\ndegradation in left-to-right capabilities. However, there is a clear sign of degradation in ordinary AR\ntest loss with 100% FIM rate. For HumanEval, the left plot in Figure 5 shows all models irrespective\nof FIM rate have a similar performance.\nOn the other hand, we ﬁnd that the FIM rate does signiﬁcantly affect inﬁlling capabilities. Even\nthough the gain in FIM perplexity in Figure 4 due to a higher FIM rate is negligible, increasing this\nrate yields a consistent improvement in the inﬁlling pass rate as shown in the right plot in Figure 5.\nThis indicates that to investigate the FIM capabilities of our models, it is not sufﬁcient to consider\nlanguage modelling perplexity measures such as test loss, but we should also consider non-loss based\nevaluations.\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.95\n1.00\n1.05\n1.10\n1.15\nTest loss\nLeft-to-right loss\n0\n0.25\n0.5\n0.75\n0.9\n1.0\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\nFIM loss\nFigure 4: Comparison of the learning curves of large (see Table 3) models trained with different FIM\nrates for 50B tokens. A FIM rate even up to 90% does not have a noticeable effect on left-to-right\ntest loss; however, at a FIM rate of 100% there is degradation. We can also see the stronger FIM\nproperty in the left ﬁgure: all runs with FIM rates less than 100% follow very closely to the original\nleft-to-right test loss.\nIn Appendix B, we show further evidence across a range of scales that higher FIM rates improve\ninﬁlling performance but that this gain is not reﬂected in the perplexity evaluation.\nGiven the results here and in Appendix B, it is natural to question why we train our core series of\nmodels with a FIM rate of 50% rather than 90% or higher. Models with a FIM rate of 90% show\n9\n\n0\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.02\n0.04\n0.06\n0.08\n0.10\nPass rate\nHumanEval\n0\n0.25\n0.5\n0.75\n0.9\n1.0\n0\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.00\n0.05\n0.10\n0.15\nRandom span infilling light\nFigure 5: In-run evaluation of coding benchmarks with temperature 0.8 and 25 samples per task.\nUsing higher FIM rates do not have a noticeable effect on HumanEval performance. A higher FIM\nrate shows stronger inﬁlling capabilities on the light random span inﬁlling benchmark.\nsuperior performance while maintaining the FIM-for-free property. This was mainly accidental, as\nwe had already trained the main series prior to seeing the FIM rate ablation results,4 and it was\nprohibitively costly to retrain all the models with the higher rate.\nThe results here motivated us to train a second 6.9B FIM model with a FIM rate of 90% on code to\nobtain the strongest inﬁlling model to date at this scale. The comparison of results is found in Table 4.\nWe note however from Figure 13 that a FIM rate of 50% does not seem to be too far from optimal.\n107\n108\n109\nNon-embedding parameters\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nPass rate\nSingle-line infilling\npsm\nspm\n107\n108\n109\nNon-embedding parameters\n0.1\n0.2\n0.3\n0.4\nMulti-line infilling\n107\n108\n109\nNon-embedding parameters\n0.1\n0.2\n0.3\n0.4\n0.5\nRandom span infilling\nFigure 6: SPM mode shows a slight advantage in performance across scale. All the evaluations in\nthis plot are at temperature 0.2 and 100 samples per task for single-line and multi-line inﬁlling and\n200 samples per task for random span inﬁlling.\n4.3\nSPM vs PSM vs joint SPM+PSM training\nIn Section 3, we describe two ways of constructing a FIM example: [sufﬁx, preﬁx, middle] and\n[preﬁx, sufﬁx, middle]. Here we study how this choice affects performance during pretraining and\nevaluation.\nThe main ﬁnding is that SPM is slightly stronger than PSM in our benchmarks in general as evidenced\nby Figure 6. We train a series of FIM models with a FIM rate of 50% with the FIM rate equally\nallocated to PSM and SPM. We ﬁnd that evaluating these models in SPM mode yields a consistently\nhigher performance than PSM across scale. This is likely due to the fact that in SPM, there is no\ndistinction between the preﬁx and the middle sections as they are one contiguous sequence of text.\nThis makes it more natural for the model to continue from the preﬁx in contrast to PSM where\nattention has to ﬁrst identify where the span token is.\n4In particular, our earlier ablations based only on loss had indicated that the gains from increasing the FIM\nrate to 90% should be negligible, resulting in us choosing a more moderate value of 50%. More detailed study\nusing all 3 inﬁlling benchmarks showed that there is in fact a noticeable gain in using even a higher FIM rate.\n10\n\nHowever, this does not imply that we should train solely on SPM. In Table 1, we train large models\non pure PSM, pure SPM, and our default 50-50 SPM+PSM mix, and evaluate them in all modes. We\nobserve a positive transfer of capability between PSM and SPM. Training joint FIM with a 50% FIM\nrate obtains roughly the same performance in SPM mode as training pure SPM FIM with a 90% FIM\nrate. Not only is joint pretraining the most efﬁcient, but it also yields the most ﬂexible model with\ntwo inference modes.\nIt is noteworthy that the recent inﬁlling works using data transformations similar to FIM such as\n[Donahue et al., 2020, Aghajanyan et al., 2022, Fried et al., 2022] utilize a format similar to PSM.\nThe above ﬁndings indicate that this choice leads to suboptimal inﬁlling performance.\nTrain distribution\nFIM rate\nSingle-line\nMulti-line\nRandom span\nPSM\nSPM\nPSM\nSPM\nPSM\nSPM\nJoint\n0.5\n0.550\n0.595\n0.265\n0.293\n0.367\n0.379\nJoint\n0.9\n0.616\n0.622\n0.290\n0.305\n0.397\n0.420\nPSM\n0.9\n0.583\n0.625\n0.273\n0.305\n0.362\n0.274\nSPM\n0.9\n0.023\n0.586\n0.008\n0.301\n0.007\n0.386\nTable 1: Comparison of FIM performance when trained and evaluated in various SPM, SPM settings.\nAll the joint runs put 50% of the total FIM rate on PSM and 50% on SPM. All results are obtained\nwith temperature 0.2 and 100 samples per task.\n107\n108\n109\nNon-embedding parameters\n0.2\n0.4\n0.6\nPass rate\nSingle-line infilling\ncontext\ndoc\n107\n108\n109\nNon-embedding parameters\n0.1\n0.2\n0.3\nMulti-line infilling\n107\n108\n109\nNon-embedding parameters\n0.1\n0.2\n0.3\n0.4\nRandom span infilling\nFigure 7: Applying FIM at the context level consistently outperforms document level FIM. All\nbenchmarks are evaluated with temperature 0.2 and 200 samples/task.\n107\n108\n109\nNon-embedding parameters\n0.9\n1.0\n1.1\n1.2\nTest loss\nLeft-to-right loss\ncontext\ndoc\n107\n108\n109\nNon-embedding parameters\n0.9\n1.0\n1.1\n1.2\nFIM loss\nFigure 8: Comparison of losses with different FIM implementations. While document level FIM\nintroduces partially broken data into training, it does not hurt the autoregressive loss (left). We also\nﬁnd that the reduction in FIM perplexity (right) is not commensurate to the gain in pass rate shown in\nFigure 7.\n11\n\n4.4\nContext-level vs document-level FIM\nIn Section 3, we noted two ways of implementing FIM, context-level and document-level FIM, where\naugmentation is applied either before or after packing and chunking. We now ablate this choice on a\nseries of code models trained with a 50% FIM rate and the default joint PSM-SPM mix.\nIn Figure 7, we ﬁnd that context-level FIM yields a consistent and signiﬁcant improvement over\ndocument-level FIM across all the range of scale. This is a noteworthy contrast to the perplexity\nevaluation in Figure 8 (right) where the improvement is an almost negligible 0.001 nats/token. This\ncorroborates the ﬁnding in Section 4.2 that perplexity evaluation does not always capture the gains in\nthe sampling performance.\nAlso, we previously explained that document-level FIM can result in fragmented FIM data with\na missing preﬁx and/or sufﬁx from the chunking step of data loading pipeline. Figure 8 (left)\nshows that training on these invalid examples in document-level FIM does not affect the left-to-right\nevaluation. Hence, practitioners might still sometimes prefer document-level FIM due to its simpler\nimplementation.\n4.5\nMiddle span selection\nAn important consideration in FIM training is the choice of middle span. In this work, the middle\nspan is chosen uniformly at random where the split between preﬁx, middle, sufﬁx happens at the\ncharacter level. In this section, we examine this choice. Instead of trying FIM across syntactic\nboundaries, such as functions and class bodies, we restrict our ablations to simple, generalizable\napproaches which are language agnostic. We select spans in three different ways, splitting randomly\nby lines, tokens, and characters. The section boundaries are selected uniformly at random from the\nallowed splitting positions based on the span type. Here, a token refers to a word in the byte-pair\nencoding (BPE) vocabulary. In practice, this is implemented by applying the FIM augmentation after\nthe documents are encoded with BPE (see Appendix C). For simplicity, we run all our experiments in\nPSM mode in this ablation.\nIn Table 2 we see that training only on the line-based middle spans gives the models a slight advantage\nin the single-line and multi-line inﬁlling benchmarks. This is not surprising since these evaluations\nare completely in distribution with line based middle span runs. On the other hand, the line based\ntraining fails almost completely in the random span inﬁlling benchmark. Interestingly, the advantage\nprovided in line-based evaluations from concentrating all the FIM distribution on line based middle\nspans in training is quite small relative to how much it hurts the model in random span inﬁlling\nbenchmark.\nTraining with token-level random spans does slightly better on random span inﬁlling, but is still\nnot competitive compared to character-level runs on this benchmark. The reason is that token-level\nFIM models are never trained on cases where a token is broken into two parts across the boundaries\nof middle with preﬁx or sufﬁx. When the middle section is selected completely at random at the\ncharacter level, subtokens are introduced naturally at the beginning and the end boundaries of the\nmiddle section. There is no train-test mismatch and the model is able to understand and solve more\nrandom span inﬁlling tasks while still performing well in single-line and multi-line inﬁlling.\nTraining middle span\nSingle-line inﬁlling\nMulti-line inﬁlling\nRandom span inﬁlling\nLine-level random span\n0.586\n0.269\n0.015\nToken-level random span\n0.548\n0.242\n0.102\nCharacter-level random span\n0.557\n0.250\n0.321\nTable 2: Pass rates of medium models pretrained with various middle span selection strategies.\nTraining on line-based spans improves the single- and multi-line inﬁlling metrics reported in InCoder,\nbut line- and token-level spans used in previous works can not robustly handle real life use cases\nwhere the span starts or ends in subtokens. Overall, character-level random span run dominates in\nrandom span benchmark while it is also not far behind in single and multi line inﬁlling.\n12\n\n5\nFinetuning results\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.2\n0.4\n0.6\nPass rate\nSingle-line infilling\nfim50\nfim90\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.1\n0.2\n0.3\nMulti-line infilling\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.1\n0.2\n0.3\n0.4\nRandom span infilling\n(a) 25B tokens of FIM ﬁnetuning.\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.2\n0.4\n0.6\nPass rate\nSingle-line infilling\nfim50\nfim90\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.1\n0.2\n0.3\n0.4\nMulti-line infilling\n0.1\n0.2\n0.5\n1.0\nLearning rate\n0.0\n0.1\n0.2\n0.3\n0.4\nRandom span infilling\n(b) 50B tokens of FIM ﬁnetuning.\nFigure 9: Evaluation of the ﬁnal snapshots of models pretrained for 100B tokens without FIM and\nthen ﬁnetuned for 25B (row a) and 50B (row b) tokens with FIM. The x-axis shows the learning rate\nmultiplier relative to the pretraining learning rate. The dashed line indicates the baseline performance\nof the model pretrained for 100B tokens with a FIM rate of 50% with no additional ﬁnetuning.\nOnly the most aggressive combination of 90% FIM rate and a learning rate multiplier of 1.0 with\n50B tokens of ﬁnetuning catches up to the performance of the baseline. Reported results are with\ntemperature 0.2 and 100 sampler per task.\nIn this section, we investigate whether we can ﬁnetune existing AR models to learn the FIM capability.\nIdeally, after ﬁnetuning, an AR model would reach the same level of performance on FIM evaluations\nas it would have achieved if it were pretrained with FIM. Given that FIM can be learned during\npretraining without extra compute cost, it is natural to expect that the model should also be able to\nlearn this task quickly in ﬁnetuning. Surprisingly, we ﬁnd that for ﬁnetuned models to reach the same\nlevel of performance as baseline pretrained models, one needs to expend a large amount of compute\nrelative to the pretraining compute.\nTo show this, we ﬁnetune an XL model pretrained for 100B tokens without FIM using different\nchoices of ﬁnetuning hyperparameters. Speciﬁcally, we train 16 ﬁnetuned models with 4 choices of\nlearning rates (0.1, 0.2, 0.5, 1x multiples of pretraining learning rates), 2 different FIM rates (0.5 and\n0.9), and 2 different choices of ﬁnetuning horizons (25B and 50B tokens). We use this large variety\nof hyperparameter choices to both ensure that our conclusion is robust and to better understand the\neffect of hyperparameters on the ﬁnal performance. The results are summarized in Figure 9 where we\ncompare the performance of these 16 models with that of the XL model trained for 100B tokens with\na FIM rate of 50% without any ﬁnetuning. It is evident from this ﬁgure that even with signiﬁcant\nadditional ﬁnetuning compute, AR models ﬁnetuned with FIM do not reach the same performance as\nthe models pretrained with FIM (and without any FIM ﬁnetuning).\nAmong these 16 models, the only setting where the gap between pretrained baseline and ﬁnetuned\nmodels is closed is the 50B token run with a FIM rate of 0.9 and learning rate multiplier of 1.0 relative\n13\n\nto pretraining. More generally, we ﬁnd that higher learning rate, FIM rate, and longer ﬁnetuning all\nseem helpful for improving FIM performance in ﬁnetuning.\nWe ﬁnd it particularly surprising that such high learning rates and lengthy ﬁnetuning are necessary\nfor reaching the similar level performance. We discuss this topic more in Section 6. We note that\nalthough reaching the same level of performance as in pretraining requires a large amount of compute,\na small amount of ﬁnetuning (especially with high FIM and learning rate) is still sufﬁcient for the\nmodel to reach non-trivial levels of FIM performance on our metrics. We present further results on\ndynamics of ﬁnetuning in Appendix F.\n6\nDiscussion\nPretraining vs ﬁnetuning. In the previous sections, we studied how to efﬁciently teach FIM to\ncausal language models. A main ﬁnding was that FIM can be learned for free in pretraining. In\ncontrast, we saw in Section 5 that learning FIM in ﬁnetuning can be quite expensive. Here we\ndescribe some potential explanations for these ﬁndings.\nThe main intuition for why FIM can be learned for free in pretraining is that breaking a document into\nthree pieces and shifting the middle one to the end effectively creates three smaller documents. In\nparticular, each piece still requires predicting next tokens from left to right, keeping the total number\nof tokens processed autoregressively the same.\nOn the other hand, even though FIM data is locally identical to autoregressive data, FIM does impose\na different global attention pattern over the whole document. To visualize this, we show the causal\nattention mask of a FIM document in Figure 10. These new attention pattern could be the reason\nwhy it takes a relatively long token horizon and a high learning rate to learn FIM in ﬁnetuning. It is\npossible that there is ossiﬁcation [Hernandez et al., 2021] in the learned document-wide attention\npattern in regular AR pretraining which requires a lengthy ﬁnetuning stage to adapt to the attention\npattern needed in FIM.\nmiddle\npreﬁx\nsufﬁx\npreﬁx\nmiddle\nsufﬁx\nkey\nquery\nmiddle\npreﬁx\nsufﬁx\npreﬁx\nmiddle\nsufﬁx\nkey\nquery\nFigure 10: Visualization of causal attention pattern of FIM data. Unraveling both the query and key\nembeddings back in the canonical left-to-right order shows that FIM allows the transformer to attend\nto future context when decoding the middle section without complex architectural changes. One\nside-effect is that the sufﬁx probability no longer depends on the middle span.\nFIM loss, AR loss, and the difﬁculty of FIM task. Naively, since FIM does not come at a cost in\nAR capability, one may expect FIM to be an easy task. In fact, the opposite seems to be the case.\nThere is substantial evidence that FIM can often be much harder than normal left-to-right generation.\nIntuitively, it is often easier to continue a text in a plausible manner than to continue the text\nconditioned on ending in a speciﬁc sufﬁx. The latter requires planning a plausible narrative connecting\nthe two pieces, starting the generation in a way that matches the preﬁx, and stopping the generation\nat the right time so it connects to the sufﬁx. In particular, in FIM the model is trained to generate\n<EOT> when the middle ends and connects to the sufﬁx. On the other hand, when the model fails to\nproduce <EOT> in the allotted budget, we often end up with truncated samples which do not connect\nwell to the sufﬁx. For example, consider the following:\n14\n\nWhen I was young, I only liked to play video games. Over time, I started thinking if it’d be\npossible to make bots to play better than any human can ever play these games. I\neventually decided I liked working on the latter more than playing the games themselves\nand that’s how ﬁrst I got interested in AI research.\nWhen I was young, I only liked to play video games. I would play sometimes more than 13\nhours per day. The rush, novelty, and variety were beyond anything real life could offer. I\nloved the challenge and I excelled at it. I would often skip classes and go to and that’s how\nﬁrst I got interested in AI research.\nBoth completions above connect well to the preﬁx, but only the ﬁrst manages to connect well to the\nsufﬁx. The second completion in contrast fails to produce <EOT> in the allotted budget resulting in\na bad sample.5 This turns out to be a common failure in FIM sampling. Even though, left-to-right\nsampling also struggles sometimes with related issues, this type of failure is more troublesome in\nFIM since a failure to connect to the sufﬁx cannot easily be ﬁxed by post-processing. For example,\ntrimming the sample to the last paragraph or line is often an effective way in improving sample\nquality in AR sampling, but does not help in FIM. We discuss this and other issues associated with\nFIM sampling more extensively in Appendix H.\nThe difﬁculty of FIM task compared to AR task is also reﬂected in the loss associated with each task.\nTo see this, in Figure 11, we compare the FIM loss with the AR loss over a suite of FIM models all\nwith 50% FIM rate. To remove confounders, we ensure the documents that underlie the AR test set\nare the same documents that are transformed through FIM to make up the FIM test set. We ﬁnd the\nFIM perplexity is consistently higher than the AR perplexity across scale. That is, on average\nPFIM([preﬁx, sufﬁx, middle]) < PAR([preﬁx, middle, sufﬁx]),\nwhich means the models have a harder time modelling the same document in FIM format than AR\nformat.\n107\n108\n109\nNon-embedding parameters\n100\n8 × 10\n1\n9 × 10\n1\nTest loss\nOver all sections\nleft-to-right\nfim\n107\n108\n109\nNon-embedding parameters\n100\n7 × 10\n1\n8 × 10\n1\n9 × 10\n1\nOver the middle section only\nFigure 11: Comparison of the overall (left) and middle span (right) loss of 50% FIM code mod-\nels. In the left plot, we see that the AR loss is consistently lower than the FIM loss suggesting\nthat next-token prediction is inherently more compressible than inﬁlling in the middle. The right\nﬁgure evaluates the conditional loss of the middle span given the surrounding context showing that\nPFIM(middle∣preﬁx, sufﬁx) > PAR(middle∣preﬁx). Here, FIM attains a lower loss because it can\nattend to the sufﬁx. We emphasize that left-to-right and FIM here do not refer to model type, as all\nmodels in this ﬁgure are FIM models. They refer rather to the type of test loss used in evaluation.\nContext-level vs document-level FIM and FIM rate. In Section 4.4, we saw that context-level\nFIM typically outperforms document-level FIM. Here, we note a connection between this ﬁnding and\nthe results in Section 4.2 and Appendix B about FIM rate.\nThe basic observation is that document-level FIM effectively leads to a lower FIM rate compared to\ncontext-level FIM, even with the same nominal value of FIM rate. As a thought experiment, consider\n5Even though the completion may have been able to connect to the sufﬁx with a bigger budget, the challenge\nis it is unclear how much budget is enough. In practice, often a reasonable budget for the maximum number of\ntokens for the middle must be imposed.\n15\n\nthe setting where all the documents in the training dataset are much longer than the context size. In\nthis setting, when using document-level FIM, the model almost never sees the preﬁx, middle, and\nsufﬁx of the same document appear in the same context together after chunking. As such, we would\nexpect the model to struggle to learn FIM in this setting. In less extreme situations, there are many\ndocuments shorter than the context size and hence the above phenomenon is less pronounced. Still,\nbecause of long documents in training data and the usual artifacts of document packing, document-\nlevel FIM results in a lower effective FIM rate. Here, we deﬁne the effective FIM rate as the fraction\nof examples that are in FIM format and with all three of the preﬁx, middle, and sufﬁx appearing\nwithin the same context.\nThis decrease in effective FIM rate is likely the main reason behind the stronger performance of\ncontext-level FIM in Section 4.4. We note that the exact amount of decrease in effective FIM rate\ndepends on the details of distribution of document lengths. It is important to remember that even if\nthe data distribution does not have many long examples, the decrease in effective FIM rate will still\nbe present because of document packing.\n7\nRelated work\nMasked language modeling is closely related to text inﬁlling in that consecutive runs of masked\ntokens can be interpreted as spans that the model must inﬁll. While early masked language models\nlike BERT [Devlin et al., 2019] masked tokens randomly, T5 [Raffel et al., 2019], SpanBERT [Joshi\net al., 2020], and BART [Lewis et al., 2020] demonstrated improvements when contiguous runs of\ntokens are masked. However, because these models focus on representation learning, the span lengths\nare typically much shorter than a sentence or even a single line of code. Within our modalities of\ninterest, DOBF [Lachaux et al., 2021] trains BERT on code, and HTLM [Aghajanyan et al., 2021]\ntrains BART on HTML data.\nText inﬁlling can also be seen as a special case of autoregressive language modeling where the\nstandard left to right generation order is replaced by a more ﬂexible ordering. XLNet [Yang et al.,\n2019] modiﬁes the attention mask in a standard transformer to enable token generation in any user-\nspeciﬁed order, while Insertion Transformer [Stern et al., 2019], KERMIT [Chan et al., 2019], and\nInDIGO [Gu et al., 2019] allow the model to predict a location for the next token before predicting\nthe token. Similarly, Blank Language models [Shen et al., 2020] generate text by iteratively selecting\na blank and replacing it with a token (and optionally more blanks).\nSimilar to our work, Zhu et al. [2019], Donahue et al. [2020], GLM [Du et al., 2022], CM3\n[Aghajanyan et al., 2022], and InCoder [Fried et al., 2022] utilize left-to-right autoregressive modeling\nby moving the inﬁll regions to the end of context, with regions separated by sentinels. Notably,\nDonahue et al. [2020] explore inﬁlling spans of varying granularities, such as words, sentences, or\nparagraphs, and InCoder [Fried et al., 2022] uses a similar evaluation framework to ours by studying\ninﬁlling capabilities on sampling based benchmarks created from HumanEval [Chen et al., 2021].\nWhile several of these works support inﬁlling multiple spans, we focus on the single span setting\nfor practicality (e.g. in computer-based text generation, where the placement of cursor implies the\nlocation we want to inﬁll). Additionally, our paper emphasizes the computational efﬁciency of\ntraining for inﬁlling at scale. While we do not study syntactically or semantically motivated inﬁlling\nspans, we show selecting spans at the character level improves the robustness of inﬁlling.\nText inﬁlling can also be performed using a GAN [Fedus et al., 2018], but REINFORCE is required\nto deal with the discreteness of text. Text inﬁlling can also be done through gradient search [Liu et al.,\n2019], where tokens within the inﬁlled span are optimized with gradient descent and collapsed to the\nnearest neighbor.\nOverall, there are two approaches for imbuing models with inﬁlling capabilities: ﬁrst, through new\narchitectures like SpanBERT and XLNet; second, through data formatting. In general, the latter\napproach can be seen as altering the behavior of a language model through control codes, which\nwas motivated in CTRL [Keskar et al., 2019] to improve the steerability of generation. DistAug\n[Jun et al., 2020] is another related work that trains jointly on transformed data while conditioning\non the transformation type. While inﬁlling is a speciﬁc use case that can be realized through both\narchitecture and data, it is generally easier and more universal to learn additional skills by introducing\nnew training distributions than hardwiring them.\n16\n\nThe strongest inﬁlling system at scale to our knowledge currently is code-davinci-002 released this\npast March [OpenAI et al., 2022]. The present paper describes some of the early research that went\ninto powering the inﬁlling capabilities of this more powerful model. In Appendix 4, we present a\ncomparison between this system, our 6.9B models, and the InCoder 6.7B model on our inﬁlling\nbenchmarks.\n8\nConclusion\nIn this work, we show that causal decoder-based language models can learn to ﬁll in the middle of a\ndocument after being jointly trained on a mixture of traditional left-to-right and FIM transformed\ndata. A single FIM model can import modules, write docstrings, and complete functions, subsuming\nspecialized models ﬁnetuned for individual tasks [Chen et al., 2021], providing substantial extra\ncapability over traditional left-to-right language models.\nOne important ﬁnding here is the FIM-for-free property. Figures 1 and 2 show that with the same\namount of compute, FIM models achieve the same test loss as AR models on left-to-right test loss\nwhile achieving lower FIM loss. This is further strengthened using non-loss based evaluations in\nSection 4.\nWe also investigate FIM ﬁnetuning since a lot of the existing language models do not have FIM\ncapabilities. Our results demonstrate that a canonically pretrained left-to-right model does not acquire\nthe new skill to the fullest extent of the given model size even with careful hyperparameter tuning\nand a signiﬁcant amount of ﬁnetuning compute relative to pretraining. This suggests that for the best\nFIM performance, pretraining jointly from scratch with our recommended hyperparameters is more\neffective than ﬁnetuning.\nTo study FIM capabilities precisely, we use the inﬁlling code benchmarks from InCoder [Fried et al.,\n2022] and introduce the new random span inﬁlling benchmarks based on HumanEval [Chen et al.,\n2021]. From these, we learn a few important lessons. First, perplexity does not reﬂect the true inﬁlling\nperformance, and one should design the inﬁlling benchmarks carefully to measure progress. Second,\nFIM capabilities depend considerably on the FIM rate and implementation like context-level FIM but\nleft-to-right capabilities are unaffected by these choices as long as the FIM rate is kept below 100%.\nThird, applying FIM at the character level imbues the model with natural robustness to subtokens and\nmakes it possible to deploy the model in the wild, for example, as a coding assistant.\nAll in all, we show FIM models are strictly more capable than canonically trained left-to-right models,\nat least within the bounds of the evaluations we consider, and we demonstrate how to train FIM\nmodels efﬁciently and competitively.\n8.1\nRecommended FIM hyperparameters\nIn Section 4, we see there are a number of hyperparameters in training FIM models. In all cases, we\nrecommend applying FIM transformation at the character level and always including some character-\nlevel random spans as it allows the model to generate sensible completion even when the preﬁx and\nsufﬁx end in the middle of a token. We note that for mid-token robustness, inference in PSM mode\ncan be superior to the particular SPM mode explored in this work. However, pretraining with joint\nPSM and SPM yields the best performance due to a positive transfer between the two formats. In\nterms of implementation, context-level FIM is superior but document-level FIM is also an option if a\nsimpler implementation is desired. Finally, we observe improved performance even up to a FIM rate\nof 90% without any cost in AR capabilities. In practice, any value in the range between 50% and\n90% is a reasonable choice. Note that this is in contrast with some related prior work such as [Fried\net al., 2022] which typically uses lower values of FIM rate such as 15%, which our results indicate to\nbe suboptimal.\n8.2\nFuture directions\nThere are several important related directions that we did not cover here. For example,\n1. Smarter span selection: We only consider spans selected uniformly at random for gener-\nality, but mixing in semantically or syntactically meaningful spans [Donahue et al., 2020,\nJoshi et al., 2020, Deng et al., 2021] can considerably improve inﬁlling performance. In\n17\n\nSection 4.5, we see that training on line-level spans instead of character-level spans improves\nline-based inﬁlling results. In our preliminary experiment, selecting the middle span to be\nexactly one word was shown to signiﬁcantly improve accuracy on cloze-like tasks. Smarter\nspan selection involves language speciﬁc parsing and new benchmarks which may be tricky\nto make, but we expect this to produce stronger FIM models.\n2. Steerable generation: FIM models generate spurious content or struggle to generate a\nsensible completion in the allotted token budget because they do not know the length or the\nstyle of inﬁlling the user desires. Applying ideas like RL from human feedback [Stiennon\net al., 2020] and instruction following [Ouyang et al., 2022], among other methods of\ncontrollable generation, could address this issue by providing further alignment with the\nusers’ intent.\n3. Further examination of the FIM-for-free property: Even though we provide substantial\nevidence for the FIM-for-free property, we cannot completely rule out that there are bench-\nmarks not considered here where FIM models underperform AR models. As such, further\nstrengthening or refuting the FIM-for-free property remains an interesting direction.\n4. Multiple inﬁlling slots: Many prior works in inﬁlling explored multiple inﬁlling slots\n[Raffel et al., 2019, Fried et al., 2022]. We do not study this, as there are already a\nnumber of considerations in training single-slot models, and inference challenges unique\nto inﬁlling. Furthermore, in most applications, we anticipate single-slot inﬁlling to be\njust as useful. We anticipate the inference challenges and failure modes to increase when\nconsidering multi-slot inﬁlling. To make progress in multi-slot inﬁlling however, creating\nappropriate sampling-based benchmarks is essential, as perplexity based evaluation would\nbe increasingly unhelpful. There is a vast design space for these benchmarks and a vast\narray of extra training hyperparameters when going from single-slot to multi-slot inﬁlling.\n5. Improving natural language FIM performance: Qualitatively, our FIM models tend to\nperform better in code than language. This is perhaps not surprising given that code is a\nformal language, and as such, has more structure and less uncertainty. Improving inﬁlling\nperformance on natural language is an interesting future direction, but can be tricky because\nevaluation of free-form generation in language is not as straightforward as measuring\nfunctional correctness in code. We expect training on more semantically meaningful or\nshorter spans can help here but it is unclear what test distribution to use and how to evaluate\nthis well in the general case.\n6. Role of bidirectionality and attention: There is much to be understood in the role of\nattention and the training objective in free-form inﬁlling performance. In this work, we\nuse decoder based language models, which are currently the dominant paradigm of large\nscale language modelling. However, it is possible that from the point of view of inﬁlling,\nother training objectives and architectures are superior. In this direction, [Artetxe et al.,\n2022] show a BERT style architecture performs better than FIM-like models but the results\nare mostly limited to single-token inﬁlling. A more systematic study, similar to [Wang\net al., 2022, Tay et al., 2022] but focused on free-form inﬁlling generation, can clarify this\nfurther. Somewhat related to this, it is interesting to investigate the interaction of absolute\nand relative positional embedding and their variants with FIM. Preliminary results, not\nreported here, indicate that the FIM-for-free property still holds with absolute positional\nembedding.\nFinally, our experience with the FIM-for-free property brings up the intriguing question of what\nother useful skills can be learned jointly with no or little cost to the original capabilities of language\nmodels. There have been a number of interesting works on this topic and we anticipate even more to\nfollow, but many works often omit critical analysis for more broad adoption and comparison. We\npropose the following methodology to help advance research toward answering this question:\n1. Establish a budget in the amount of original capabilities that one is willing to sacriﬁce to\nlearn a new capability.\n2. Maximize the new capability within this budget.\nThe budget-capability trade-off is not only theoretically interesting but also practical, allowing\nresearchers to integrate new capabilities based on proper trade-off analysis. We look forward to a\nfuture where large language models have increasingly diverse and high value capabilities.\n18\n\nAcknowledgments\nWe would like to thank Shantanu Jain, Alex Paino, Alec Radford, Nick Ryder, Pranav Shyam,\nand Qiming Yuan for useful discussions and help at various stages of the project. We are also\ngrateful to Christina Kim, Rachel Lim, Andrew Mayne, Maddie Siemens, and Natalie Staudacher\nfor the help with the API infrastructure and qualitative evaluation of FIM, and to Angela Jiang,\nKatie Mayer, Rajeev Nayak, Henrique Pondé, and Felipe Such for invaluable work and immense\neffort on deployment. Finally, we thank Bob McGrew and Wojciech Zaremba for unceasing support\nthroughout the project, and Karl Cobbe, Angela Jiang, Alec Radford, and Pranav Shyam for their\nvaluable feedback on the paper.\nReferences\nA. Aghajanyan, D. Okhonko, M. Lewis, M. Joshi, H. Xu, G. Ghosh, and L. Zettlemoyer. HTLM:\nhyper-text pre-training and prompting of language models. CoRR, abs/2107.06955, 2021. URL\nhttps://arxiv.org/abs/2107.06955.\nA. Aghajanyan, B. Huang, C. Ross, V. Karpukhin, H. Xu, N. Goyal, D. Okhonko, M. Joshi, G. Ghosh,\nM. Lewis, and L. Zettlemoyer. CM3: A causal masked multimodal model of the internet. CoRR,\nabs/2201.07520, 2022. URL https://arxiv.org/abs/2201.07520.\nM. Artetxe, J. Du, N. Goyal, L. Zettlemoyer, and V. Stoyanov. On the role of bidirectionality in\nlanguage model pre-training, 2022. URL https://arxiv.org/abs/2205.11726.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in\nnatural language. In Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2020.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nW. Chan, N. Kitaev, K. Guu, M. Stern, and J. Uszkoreit. KERMIT: generative insertion-based\nmodeling for sequences. CoRR, abs/1906.01604, 2019. URL http://arxiv.org/abs/1906.\n01604.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\nURL https://arxiv.org/abs/2107.03374.\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer. QuAC:\nQuestion answering in context. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 2174–2184, Brussels, Belgium, Oct.-Nov. 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/D18-1241. URL https://aclanthology.\norg/D18-1241.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\nC. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov.\nTransformer-XL: At-\ntentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 2978–2988, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/P19-1285.\nURL\nhttps://aclanthology.org/P19-1285.\n19\n\nX. Deng, Y. Su, A. Lees, Y. Wu, C. Yu, and H. Sun. Reasonbert: Pre-trained to reason with distant\nsupervision. CoRR, abs/2109.04912, 2021. URL https://arxiv.org/abs/2109.04912.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers\nfor language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pages 4171–4186. Association for Computational Linguistics,\n2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\nC. Donahue, M. Lee, and P. Liang.\nEnabling language models to ﬁll in the blanks.\nCoRR,\nabs/2005.05339, 2020. URL https://arxiv.org/abs/2005.05339.\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat,\net al.\nGlam: Efﬁcient scaling of language models with mixture-of-experts.\narXiv preprint\narXiv:2112.06905, 2021.\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. GLM: General language model\npretraining with autoregressive blank inﬁlling. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.26.\nURL https://aclanthology.org/2022.acl-long.26.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension\nbenchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 2368–2378, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https:\n//aclanthology.org/N19-1246.\nW. Fedus, I. Goodfellow, and A. M. Dai. Maskgan: Better text generation via ﬁlling in the______,\n2018. URL https://arxiv.org/abs/1801.07736.\nD. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer,\nand M. Lewis. Incoder: A generative model for code inﬁlling and synthesis, 2022. URL https:\n//arxiv.org/abs/2204.05999.\nJ. Gu, Q. Liu, and K. Cho. Insertion-based decoding with automatically inferred generation order.\nTransactions of the Association for Computational Linguistics, 7:661–676, 2019. doi: 10.1162/\ntacl_a_00292. URL https://aclanthology.org/Q19-1042.\nD. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer. arXiv preprint\narXiv:2102.01293, 2021.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv\npreprint arXiv:2203.15556, 2022.\nA. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In\nICLR. OpenReview.net, 2020. URL http://dblp.uni-trier.de/db/conf/iclr/iclr2020.\nhtml#HoltzmanBDFC20.\nM. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. SpanBERT: Improving pre-\ntraining by representing and predicting spans. Transactions of the Association for Computational\nLinguistics, 8:64–77, 2020. doi: 10.1162/tacl_a_00300. URL https://aclanthology.org/\n2020.tacl-1.5.\nH. Jun, R. Child, M. Chen, J. Schulman, A. Ramesh, A. Radford, and I. Sutskever. Distribution\naugmentation for generative modeling. In H. D. III and A. Singh, editors, Proceedings of the\n37th International Conference on Machine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 5006–5019. PMLR, 13–18 Jul 2020. URL https://proceedings.\nmlr.press/v119/jun20a.html.\n20\n\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\nN. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. CTRL: A conditional transformer\nlanguage model for controllable generation. CoRR, abs/1909.05858, 2019. URL http://arxiv.\norg/abs/1909.05858.\nM. Lachaux, B. Rozière, M. Szafraniec, and G. Lample. DOBF: A deobfuscation pre-training\nobjective for programming languages. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang,\nand J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual\nConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 14967–14979, 2021. URL https://proceedings.neurips.cc/paper/2021/\nhash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html.\nH. J. Levesque, E. Davis, and L. Morgenstern. The Winograd Schema Challenge. In Proceedings of the\nThirteenth International Conference on Principles of Knowledge Representation and Reasoning,\nKR’12, pages 552–561. AAAI Press, Rome, Italy, 2012.\nISBN 978-1-57735-560-1.\nURL\nhttps://cs.nyu.edu/faculty/davise/papers/WSKR2012.pdf.\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettle-\nmoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7871–7880, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.\nacl-main.703.\nO. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White\nPaper. AI21 Labs, 2021.\nD. Liu, J. Fu, P. Liu, and J. Lv. TIGS: An inference algorithm for text inﬁlling with gradient search.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 4146–4156, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1406. URL https://aclanthology.org/P19-1406.\nN. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen.\nA corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceed-\nings of the 2016 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, pages 839–849, San Diego, California,\nJune 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/N16-1098.\nURL\nhttps://aclanthology.org/N16-1098.\nOpenAI, M. Bavarian, A. Jiang, H. Jun, and H. Pondé. New GPT-3 Capabilities: Edit and Insert.\nOpenAI blog, 2022. URL https://openai.com/blog/gpt-3-edit-insert/.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human\nfeedback, 2022. URL https://arxiv.org/abs/2203.02155.\nD. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nand R. Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 1525–1534, Berlin, Germany, Aug. 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.\nI. Provilkov, D. Emelianenko, and E. Voita. Bpe-dropout: Simple and effective subword regularization.\nCoRR, abs/1910.13267, 2019. URL http://arxiv.org/abs/1910.13267.\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by\ngenerative pre-training. 2018.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n21\n\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson,\nR. Ring, S. Young, et al. Scaling language models: Methods, analysis & insights from training\ngopher. arXiv preprint arXiv:2112.11446, 2021.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN 0001-0782. doi:\n10.1145/3474381. URL https://doi.org/10.1145/3474381.\nP. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. arXiv\npreprint arXiv:1803.02155, 2018.\nT. Shen, V. Quach, R. Barzilay, and T. Jaakkola. Blank language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 5186–5198,\nOnline, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.\n420. URL https://aclanthology.org/2020.emnlp-main.420.\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training for\nlanguage generation. arXiv preprint arXiv:1905.02450, 2019.\nM. Stern, W. Chan, J. Kiros, and J. Uszkoreit. Insertion transformer: Flexible sequence generation\nvia insertion operations. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the\n36th International Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 5976–5985. PMLR, 09–15 Jun 2019. URL https://proceedings.\nmlr.press/v97/stern19a.html.\nN. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.\nChristiano. Learning to summarize from human feedback. CoRR, abs/2009.01325, 2020. URL\nhttps://arxiv.org/abs/2009.01325.\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and\nD. Metzler. Unifying language learning paradigms, 2022. URL https://arxiv.org/abs/2205.\n05131.\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\nL. Baker, Y. Du, et al.\nLamda: Language models for dialog applications.\narXiv preprint\narXiv:2201.08239, 2022.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nT. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel. What\nlanguage model architecture and pretraining objective work best for zero-shot generalization?,\n2022.\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. XLNet: Generalized\nAutoregressive Pretraining for Language Understanding. Curran Associates Inc., Red Hook, NY,\nUSA, 2019.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really ﬁnish\nyour sentence?\nIn Proceedings of the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4791–4800, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.\nW. Zhu, Z. Hu, and E. P. Xing. Text inﬁlling. CoRR, abs/1901.00158, 2019. URL http://arxiv.\norg/abs/1901.00158.\n22\n\nA\nArchitecture and datasets\nWe use 8 causal transformer decoder models [Vaswani et al., 2017] with similar architecture, opti-\nmization hyperparameters, and encoding to Codex and GPT-3 [Chen et al., 2021, Brown et al., 2020].\nThe main architectural details of our models are summarized in Table 3. The only architectural\nmodiﬁcation we introduce is the use of relative attention [Shaw et al., 2018, Dai et al., 2019] rather\nthan learned positional embeddings. This increases the parameter count negligibly but leads to\nimproved performance. We also increase the learning rates of our three largest models by a factor of\n2 for improved ﬁnal performance, as it is known that GPT-3 series of models use rather conservative\nchoices of learning rates. The context size for all the models is 2048.\nWe train our code models on the same dataset that was used to train Codex, which is a 159 GB Python\ndataset scraped in May 2020. As such, we expect no train set contamination from the subsequent\npublic release of HumanEval. Similar to GPT-3 and unlike Codex, we train our models from scratch\nfrom a random initialization. All models from the main scans are trained for 100B tokens irrespective\nof size. Due to this ﬁxed token budget, we expect our largest models to be undertrained [Hoffmann\net al., 2022] and to beneﬁt signiﬁcantly from longer training. For our natural language models, we\nuse the same dataset as was used in GPT-3 [Brown et al., 2020], the details of which are described in\nSection 2.2 of that paper.\nModel Name\nnparam\nnne\nnlayers\ndmodel\nnheads\ndhead\nBatch Size\nLearning Rate\nXXS\n50M\n11M\n6\n384\n6\n64\n0.5M\n1.6 × 10−3\nXS\n77M\n26M\n8\n512\n8\n64\n0.5M\n1.4 × 10−3\nSmall\n164M\n87M\n12\n768\n12\n64\n0.5M\n6.0 × 10−4\nMedium\n411M\n308M\n24\n1024\n16\n64\n0.5M\n3.0 × 10−4\nLarge\n844M\n689M\n24\n1536\n16\n96\n0.5M\n2.5 × 10−4\nXL\n1.4B\n1.2B\n24\n2048\n16\n128\n1M\n4.0 × 10−4\n2.8B\n2.8B\n2.6B\n32\n2560\n32\n80\n1M\n3.2 × 10−4\n6.9B\n6.9B\n6.5B\n32\n4096\n32\n128\n2M\n2.4 × 10−4\nTable 3: The model architecture for our suite of models. The 6 largest models follow similar\narchitecture as models Small to 6.7B in the GPT-3 paper. The differences in the tables are due to\nminor calculation errors and typos in Table 2.1 of that paper. The nparam column has the total number\nparameters in each model while nne column has the number of parameters excluding the embedding\nand unembedding layers. Following [Kaplan et al., 2020], we use the number of non-embedding\nparameters in our scaling plots. We do not tie the weights in the embedding and unembedding layers.\nB\nScaling trends for FIM rate ablations\nIn Section 4.2, we see higher FIM rate improving the FIM performance of our models without\nimpacting the original capabilities. This conclusion was based on the learning curves of HumanEval\nand light random span inﬁlling pass rates measured with a small number of samples during pretraining.\nTo further demonstrate this claim, we train a series of models for 50B tokens with FIM rates: 0, 0.25,\n0.5, 0.75, 0.9, and 1.0. In Figure 12 and 13, we present the model scaling trends of perplexity and\nsampling evaluation when different FIM rates are used.\nAgain, we ﬁnd that transforming a high fraction of training data into FIM does not result in a\ndegradation in the original capabilities as measured by the test loss and HumanEval pass rate. The\nonly noticeable degradation is observed in perplexity evaluation at 100% FIM rate. As for FIM\ncapabilities, increasing the FIM rate yields a signiﬁcant improvement on the inﬁlling benchmarks\nand can change the slope of model scaling trends of pass rates. However, a high FIM rate does not\nlead to a commensurate reduction in FIM losses, which corroborates that perplexities do not always\ncapture real world performance.\n23\n\n107\n108\nNon-embedding parameters\n100\n9 × 10\n1\n9.5 × 10\n1\n1.05 × 100\n1.1 × 100\n1.15 × 100\n1.2 × 100\nTest loss\nLeft-to-right loss\n0.0\n0.25\n0.5\n0.75\n0.9\n1.0\n107\n108\nNon-embedding parameters\n100\n9.5 × 10\n1\n1.05 × 100\n1.1 × 100\n1.15 × 100\n1.2 × 100\n1.25 × 100\n1.3 × 100\n1.35 × 100\nFIM loss\n107\n108\nNon-embedding parameters\n100\n8 × 10\n1\n9 × 10\n1\nMasked FIM loss\nFigure 12: Comparison of model scaling trends of perplexity with varying FIM rates. Left-to-right\nloss does not have a noticeable degradation unless a FIM rate of 100% is used (left). We also ﬁnd that\nthe FIM losses are similar to one another when the model is trained with some FIM transformations\n(middle and right).\n107\n108\n0.02\n0.04\n0.06\n0.08\nPass rate\nHumanEval\n0.0\n0.25\n0.5\n0.75\n0.9\n1.0\n107\n108\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPass rate\nSingle-line infilling\n107\n108\nNon-embedding parameters\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nMulti-line infilling\n107\n108\nNon-embedding parameters\n0.0\n0.1\n0.2\n0.3\n0.4\nRandom span infilling\nFigure 13: Comparison of model scaling trends of sampling evaluation with varying FIM rates.\nWhile increasing the FIM rate has no effects on HumanEval, it does result in consistent gains on the\ninﬁlling benchmarks with no noticeable improvement after 90% FIM. At a ﬁrst glance, it may seem\ncounterintuitive that left-to-right models can solve a nontrivial number of problems in single- and\nmulti-line benchmarks. This is not a bug, but a feature. We sample in SPM mode and some line-based\ninﬁlling problems have empty or extraneous sufﬁxes. To obtain these results, HumanEval was\nevaluated with temperature 0.8 and 500 samples per task to reduce variance. All inﬁlling benchmarks,\nhaving much more problems than HumanEval, were evaluated with temperature 0.2 and 200 samples\nper task.\nC\nDetails of FIM implementation\nWhen FIM is applied at the document level before packing, both character-level and token-level FIM\nis straightforward to implement. We simply choose two positions at random to break a document\ninto three sections, and format them as a FIM document. Only the order of encoding and splitting\nchanges as shown in the python pseudocode below:\n24\n\ndef token_level_psm_fim(document: str, vocab: Vocab) -> List[int]:\ntokens = vocab.encode(document)\nprefix, middle, suffix = randomly_split(tokens)\nreturn [\nvocab.sentinel(\"prefix\"), *prefix,\nvocab.sentinel(\"suffix\"), *suffix,\nvocab.sentinel(\"middle\"), *middle,\n]\ndef character_level_psm_fim(document: str, vocab: Vocab) -> List[int]:\nprefix, middle, suffix = randomly_split(document)\nreturn [\nvocab.sentinel(\"prefix\"), *vocab.encode(prefix),\nvocab.sentinel(\"suffix\"), *vocab.encode(suffix),\nvocab.sentinel(\"middle\"), *vocab.encode(middle),\n]\nIn contrast, applying the transformation after packing and chunking as in context-level FIM can\nbe somewhat tricky depending on the choice of middle span. As previously mentioned in Section\n3, the input context to the model is ﬁrst split around the <EOT> token so we get back individual\ndocuments. At this point, these documents are already tokenized, so applying FIM at the token level\nis straightforward.\nTo transform data in the character space for context-level FIM, the tokenized documents have to\nbe decoded back into strings before FIM augmentation. Depending on the vocabulary, some care\nhas to be given to ensure decoding does not introduce any spurious characters into training. For\nexample, utf-8 characters are encoded as multiple tokens with a BPE vocabulary; they can result in\nfragments from chunking and fail to decode. To prevent unforeseen errors midway through training,\nwe encourage checking for these fragments at the beginning or end of a context and removing them.\nAfter the transformed documents are encoded and joined back, the resulting context can be longer or\nshorter than the original, unaugmented context for context- and character-level FIM. For this reason,\nwe recommend to trim or pad the transformed context to the model context length.\nD\nDetails of SPM encoding\nAs mentioned in Section 3, in SPM we use the ordering [sufﬁx, preﬁx, middle]. In this section, we\nbrieﬂy discuss the choices regarding the sentinel tokens in SPM mode. A natural choice of encoding\nfor SPM data would be to use\n<SUF> ◦Enc(sufﬁx) ◦<PRE> ◦Enc(preﬁx) ◦<MID> ◦Enc(middle) ◦<EOT>. (SPM variant 1)\nHowever, the encoding of SPM we use in this work is\n<PRE> ◦<SUF> ◦Enc(sufﬁx) ◦<MID> ◦Enc(preﬁx) ◦Enc(middle) ◦<EOT>. (SPM variant 2)\nThe reason that we do not use the former is that it creates a separation between PSM and SPM, which\nmay result to less transfer between SPM and PSM. To understand, note that with the second variant\nSPM data occurs naturally as part of PSM training since when we split a document uniformly at\nrandom, sometimes the chosen preﬁx will be empty. This is the reason pure PSM runs achieve strong\nperformance when evaluated in SPM mode as in Table 1.\nDespite this, we note that the ﬁrst SPM variant has its own advantages. In particular, it can be stronger\nin handling of subtokens at the end of preﬁx. Hence, the choice of which variant of SPM to use\nmay depend on application in mind. As such, especially when training in pure SPM mode, it could\nbe preferable to use the former simpler form. However, in this work, due to our emphasis on joint\ntraining of PSM and SPM and to maximize transfer between them, we opt for the second variant.\nE\nRandom span inﬁlling benchmark\nFried et al. [2022] introduced the single-line and multi-line inﬁlling benchmarks based on HumanEval\nwhich prove valuable for measuring FIM performance. One limitation of these benchmarks is that\n25\n\nthe middle section is selected based on lines and does not capture more general use cases in the wild.\nWe created a third inﬁlling benchmark by choosing the middle span from two random positions in the\ncanonical solution. In this section, we show some examples of these tasks so the reader can get a feel\nfor the new benchmark. The goal is to predict the highlighted span.\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n\"\"\" Check if in given list of numbers, are any two numbers closer to\neach other than\ngiven threshold.\n\"\"\"\nfor idx, elem in enumerate(numbers):\nfor idx2, elem2 in enumerate(numbers):\nif idx != idx2:\ndistance = abs(elem - elem2)\nif distance < threshold:\nreturn True\nreturn False\nHere, for the model to pass, it needs to know that 1) the variable distance is not deﬁned, 2) the preﬁx\nends in a subtoken and not handling this will result in an indentation error, and 3) the completion has\nto stop in-line when the difference is calculated.\ndef rounded_avg(n, m):\n\"\"\"You are given two positive integers n and m, and your task is to\ncompute the\naverage of the integers from n through m (including n and m).\nRound the answer to the nearest integer and convert that to binary.\nIf n is greater than m, return -1.\nExample:\nrounded_avg(1, 5) => \"0b11\"\nrounded_avg(7, 5) => -1\nrounded_avg(10, 20) => \"0b1111\"\nrounded_avg(20, 33) => \"0b11010\"\n\"\"\"\nif m < n:\nreturn -1\nsummation = 0\nfor i in range(n, m+1):\nsummation += i\nreturn bin(round(summation/(m - n + 1)))\nThis is a slightly more difﬁcult example where the missing section spans over multiple lines and\nends in a subtoken which would break all previous works that use BPE encoding and token-based\nFIM. Use cases like this can happen in coding assistants when the user does not like the current\nimplementation and quickly deletes an approximate span they want replaced by a code model.\nBecause we create random span inﬁlling tasks uniformly at random, this naturally captures problems\nof varying difﬁculties and corner cases that could happen in practice. We picked 10 random tasks\nper problem in HumanEval because 1640 tasks yielded a good balance between reducing evaluation\nvariance and sampling time.\nF\nDynamics and learning curves of ﬁnetuning\nTo further build intuition about the results in Section 5, it is instructive to look at the dynamics of our\ninﬁlling evaluation benchmarks during the ﬁnetuning. This is presented in Figure 14. We observe that\nthe ordinary HumanEval degrades signiﬁcantly at the beginning of ﬁnetuning, especially when using\nhigher learning rates, but it catches up to similar levels as pretraining by the end of the training. On\n26\n\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nElapsed tokens\n1e10\n0.09\n0.10\n0.11\n0.12\nPass rate\nHumanEval\n0.1\n0.2\n0.5\n1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nElapsed tokens\n1e10\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nRandom span infilling light\n(a) 25B tokens of FIM ﬁnetuning.\n0\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.08\n0.09\n0.10\n0.11\n0.12\nPass rate\nHumanEval\n0.1\n0.2\n0.5\n1.0\n0\n1\n2\n3\n4\n5\nElapsed tokens\n1e10\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nRandom span infilling light\n(b) 50B tokens of FIM ﬁnetuning.\nFigure 14: The dynamics of HumanEval and random span inﬁlling light during ﬁnetuning. The\nlegend corresponds to the fraction of ﬁnetuning learning rate relative to the pretraining learning rate.\nThe results here are with a FIM rate of 0.9 and we omit similar dynamics plots with a FIM rate of 0.5\nfor brevity.\nthe other hand, performance in random span inﬁlling light starts out as zero as expected and slowly\nrises during ﬁnetuning.\nG\nTop models comparison\nIn this section, we compare the performance of the current best inﬁlling models on single-line,\nmulti-line and random span inﬁlling benchmarks. The results are reported in Table 4. We note\nthat the numbers from InCoder in this table are self-reported numbers from the paper and was not\nindependently evaluated in our framework. It is possible that minor differences in the implementation\nbetween our evaluation frameworks may result in slight discrepancies.\nModel\nSingle-line inﬁlling\nMulti-line inﬁlling\nRandom span inﬁlling\nFIM50\n0.730\n0.406\n0.521\nFIM90\n0.751\n0.441\n0.551\nINCODER\n0.690\n0.386\nN/A\nCODE-DAVINCI-002\n0.916\n0.699\n0.742\nTable 4: Comparison of our 6.9B parameter (6.5B non-embedding parameters) FIM model trained\nwith a FIM rate of 50% and 90% for 100B tokens with the InCoder model of similar size (6.7B) and\ncode-davinci-002, on the three main inﬁlling benchmarks. All the FIM results are obtained in the\nSPM mode. We evaluated our models and code-davinci-002 using 100 samples our models and per\ntask with a sampling temperature of 0.2.\n27\n\nH\nQualitative evaluation\nPreviously, we measured the pass rates on coding benchmarks to assess the inﬁlling capability. In\nthis section, we qualitatively evaluate samples to understand the strengths and areas of improvement\nfor FIM. Overall, we ﬁnd that inﬁlling works better on the code domain than language. However,\nas previously motivated, inﬁlling is generally a more difﬁcult task than just extending a preﬁx. We\nexemplify these challenges and show possible mitigations.\nH.1\nSuccessful inﬁlling examples\nFIM enables a model to process information from both before and after the point of generation. This\nunlocks new capabilities that previously required specialized models ﬁnetuned on speciﬁc tasks. For\nexample, unlike Codex [Chen et al., 2021] that trained a separate docstring model, we now have a\nsingle model that can infer the import modules, function names, arguments, docstrings, deﬁnitions,\nand many more. We show one such example below that is impossible to complete unless the model\ncan read the entire source code. This example is also interesting in that the preﬁx “from sym” and\nthe sufﬁx both contain subtokens, which are known to cause traditional language models trained\nwithout techniques like stochastic BPE [Provilkov et al., 2019] to fail.\nfrom sympy import isprime\ndef largest_prime_factor(n):\n\"\"\"\nReturn the largest prime factor of n.\n\"\"\"\nans = 1\nfor num in range(2, n + 1):\nif n % num == 0 and isprime(num):\nans = num\nreturn ans\nThe beneﬁts are not limited to coding. The model can adapt to the existing writing style and complete\nthe passage in a natural way that takes the ending into consideration.\nDolphins are very intelligent animals. They are mammals and breathe air. They live in the\nsea and are related to whales and porpoises. Dolphins are very playful animals.\nThe commercial diver ﬁnally thought he’d snagged a big catch when he saw something\nwhite. But then he quickly realized it wasn’t a ﬁsh −−he was wrangling an alligator.\nWikipedia is a free, web−based, collaborative, multilingual encyclopedia. It is overseen by\nthe nonproﬁt Wikimedia Foundation. Wikipedia uses a collaborative software known as wiki\nthat facilitates the creation and development of articles.\nH.2\nLimitations\nDifﬁcult prompts. Unlike completing text from the end, inﬁlling needs to infer the missing span that\nconnects the preﬁx to the sufﬁx. When the sufﬁx is completely unrelated, the model can generate very\nlong middle sections. We consider this behavior as the model’s attempt at coming up with a plausible\ntrajectory that joins the ending. Because the context size is limited, the model usually fails to join.\nHowever, given that even people have trouble inﬁlling some of these prompts in a short passage, this\nfailure demonstrates how challenging of a task FIM can be.\nBelow, we show one such difﬁcult prompt where the model typically fails to connect entirely or join\nin a seamless way. Even when the model writes a seemingly plausible middle section, the quality can\noften vary.\n28\n\nThe dentist looked me in the eyes and said, \"I’m going to have to take all of your teeth out.\"\nI was stunned. I said, \"All my teeth? Isn’t there something else we could do?\" He said, \"No\n, I’m afraid not.\"\nNo one can predict the future.\nThe Ottomans were defeated in World War I and the French were defeated at Waterloo.\nDeciding when to stop. The model is trained to predict the <EOT> token when it thinks it has joined\nthe sufﬁx. Even when the prompts are seemingly straightforward, deciding when to end can still be a\nchallenge in practice. Because there are many equally valid completions with varying lengths, the\nprobability of outputting the <EOT> is discounted by other longer candidates and is often smaller\nthan expected. This is further exacerbated by the fact that the terminal symbol can simply be missed\ndue to sampling. This results in a behavior where the model does not seem to end in a timely manner\nand generates a valid, but spurious content in the middle. In the process, the model can choose to\nwrite its own ending to the preﬁx, effectively ignoring the given sufﬁx.\nDogs are friendly animals.\nKoalas are pleasant animals.\nMonkeys are playful animals.\nWhales are enormous animals.\nOwls are wise animals.\nPenguins are graceful animals.\nCrocodiles are ferocious animals.\nWhile the general problem of not knowing when to stop applies to normal left-to-right completion as\nwell, this has not been as big a problem as inﬁlling because there is no constraint to join the sufﬁx.\nRepetition. When the model fails to generate an <EOT> and copies the sufﬁx, the model’s ability to\nmatch patterns leads it to lock on and repeat the prompt indeﬁnitely. Surprisingly, even large models\nare susceptible to this mode of failure. The example below ends with “the the heart,” because the\nmodel has failed to generate the terminal symbol and is still in the middle of ﬁlling in the missing\nspan which unfortunately will not stop.\nThe way is not in the sky. The way is in the heart.\nThe way is not in the sky. The way is in the heart.\nThe way is not in the sky. The way is in the heart.\nThe way is not in the sky. The way is in the the heart.\nH.3\nMitigations\nLike GPT-3 [Brown et al., 2020] where the performance depends on the quality of prompts, some\nof the failures in the earlier sections can be alleviated with prompt engineering. Namely, providing\nhints to constrain the output can dramatically improve the model’s ability to generate the <EOT>\ntoken and connect to the sufﬁx within a reasonable token budget as the model has a more concrete\nunderstanding of how long the middle section should be.\nOne such idea is to provide examples both in the beginning and the end with numbered items.\nThis makes the model internally keep track of the position, pay attention to the desired preﬁx and\nsufﬁx, and generally abstain from generating spurious content as shown below. Providing leading\nexamples alone without any explicit cues can often worsen the problem because it does not resolve\nthe ambiguity in whether the model should join to the beginning of the sufﬁx or consider it as part of\na new example.\n1. Dogs are friendly animals.\n2. Koalas are sleepy animals.\n3. Lions are regal animals.\nSection 1:\n1. The way is not in the sky. The way is in the heart.\n2. Peace comes from within. Do not seek it without.\nSection 2:\n29\n\nIt is important to note that the numbered few-shot prompting helps considerably but does not\ncompletely ﬁx the problem, as the model can still accidentally start a new list of items.\nIn general, as the model can simply miss sampling the <EOT> token, we recommend generating\nmultiple samples and preferring samples that end with <EOT>, as this increases the chance of\nchoosing a sample that actually joins the ending. When multiple samples end in <EOT>, they can be\nreranked by the likelihood or other heuristics of interest. We call this EOT-aware best-of-n sampling.\n30\n"
    },
    {
      "arxiv_id": "2305.06161",
      "full_text": "Published in Transactions on Machine Learning Research (12/2023)\nStarCoder: may the source be with you!\nRaymond Li2\nLoubna Ben Allal1\nYangtian Zi4\nNiklas Muennighoff1\nDenis Kocetkov2\nChenghao Mou5\nMarc Marone8\nChristopher Akiki9,10\nJia Li5\nJenny Chim11\nQian Liu13\nEvgenii Zheltonozhskii14\nTerry Yue Zhuo15,16\nThomas Wang1\nOlivier Dehaene1\nMishig\nDavaadorj1\nJoel Lamy-Poirier2\nJoão Monteiro2\nOleh Shliazhko2\nNicolas Gontier2\nNicholas Meade6,17\nArmel Zebaze1\nMing-Ho Yee4\nLogesh Kumar Umapathi18\nJian Zhu19\nBenjamin Lipkin20\nMuhtasham Oblokulov21\nZhiruo Wang7\nRudra Murthy22\nJason\nStillerman23\nSiva Sankalp Patel22\nDmitry Abulkhanov5\nMarco Zocca24\nManan Dey25\nZhihan Zhang26\nNour Fahmy27\nUrvashi Bhattacharyya28\nWenhao Yu26\nSwayam Singh30\nSasha Luccioni1\nPaulo Villegas31\nMaxim Kunakov32\nFedor Zhdanov32\nManuel Romero5\nTony Lee33\nNadav Timor34\nJennifer Ding35\nClaire Schlesinger4\nHailey Schoelkopf 37\nJan Ebert38\nTri Dao33\nMayank Mishra22\nAlex Gu20\nJennifer\nRobinson3\nCarolyn Jane Anderson36\nBrendan Dolan-Gavitt29\nDanish Contractor5\nSiva\nReddy2,6\nDaniel Fried7\nDzmitry Bahdanau2\nYacine Jernite1\nCarlos Muñoz Ferrandis1\nSean Hughes3\nThomas Wolf 1\nArjun Guha4,12\nLeandro von Werra1,⋆\nHarm de Vries2,⋆\n1Hugging Face\n2ServiceNow Research\n3ServiceNow\n4Northeastern University\n5Independent\n6Mila\n7Carnegie Mellon University\n8Johns Hopkins University\n9Leipzig University\n10ScaDS.AI\n11Queen Mary\nUniversity of London\n12Roblox\n13Sea AI Lab\n14Technion – Israel Institute of Technology\n15Monash\nUniversity\n16CSIRO’s Data61\n17McGill University\n18Saama AI Research Lab\n19University of British\nColumbia\n20MIT\n21Technical University of Munich\n22IBM Research\n23University of Vermont\n24UnfoldML\n25SAP\n26University of Notre Dame\n27Columbia University\n28Discover Dollar Pvt Ltd\n29NYU\n30University of Allahabad\n31Telefonica I+D\n32Toloka\n33Stanford University\n34Weizmann\nInstitute of Science\n35The Alan Turing Institute\n36Wellesley College\n37Eleuther AI\n38Forschungszentrum Jülich\nCorresponding authors (⋆) can be contacted at contact@bigcode-project.org\nReviewed on OpenReview: https://openreview.net/forum?id=KoFOg41haE\nAbstract\nThe BigCode community, an open-scientific collaboration working on the responsible de-\nvelopment of Large Language Models for Code (Code LLMs), introduces StarCoder and\nStarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast\nlarge-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion\ntokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively\nlicensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned\nStarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform\nthe most comprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages and\nmatches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder\noutperforms every model that is fine-tuned on Python and still retains its performance on\nother programming languages. We take several important steps towards a safe open-access\nmodel release, including an improved PII redaction pipeline and a novel attribution tracing\ntool, and make the StarCoder models publicly available under a more commercially viable\nversion of the Open Responsible AI Model license.\n1\narXiv:2305.06161v2  [cs.CL]  13 Dec 2023\n\nPublished in Transactions on Machine Learning Research (12/2023)\n1\nIntroduction\nGenerative AI and large language models (LLMs; Brown et al., 2020; Chen et al., 2021; Chowdhery et al.,\n2022; Zhang et al., 2022; OpenAI, 2023a) are predicted to significantly impact the workforce in the coming\nyears (Eloundou et al., 2023; Bommasani et al., 2021; World Economic Forum, 2023) by boosting worker\nproductivity. LLMs trained on code (Code LLMs) have seen particularly fast adoption: Microsoft’s Copilot\nhas attracted over 1 million professional developers (Euronews, 2023) and GitHub reports that Copilot\nusers rely on it to produce 35% of the code they write for some languages (Thompson, 2022). However, the\ndevelopment and use of LLMs has raised concerns of copyright, privacy, and openness.\nCopyright concerns arise in many jurisdictions, including the U.S. and E.U. , regarding the rights of content\ncreators whose public data is used to train language models. It has been questioned whether machine learning\nmodels trained on such data fall under fair-use doctrine in the U.S. (Kuhn, 2022; Butterick, 2022; Rothchild\n& Rothchild, 2022), with fair use being most likely when the model generates novel content dissimilar\nto any copyrighted training data (Lemley & Casey, 2020; Levendowski, 2018). Henderson et al. (2023),\ntherefore, suggest LLM developers should provide additional tools to ensure these models comply with current\ncopyright laws. It is important to mention that these legal issues are not only the subject of scholarly debates:\nlawsuits have already been filed against GitHub Copilot (DOE 1 v. and GitHub, Inc., 2022) as well as Stable\nDiffusion (Andersen et al v. Stability AI et al, 2023).\nConcerns about personal information led Italy to temporarily ban ChatGPT and launch an ongoing investi-\ngation into OpenAI’s compliance with the E.U.’s General Data Protection Regulation (GDPR) (BBC, 2023).\nAccording to these regulations (European Council, 2018; Lomas, 2022), organizations that process personal\ninformation must have a valid legal basis. These laws could potentially affect LLM developers who gather\nvast amounts of public data from the internet, which may include personal information. Obtaining explicit\nconsent from data creators is difficult at this scale, and it is uncertain whether other legal grounds exist for\nprocessing this personal information. Moreover, even with a valid legal basis, GDPR mandates that data\nprocessors inform individuals as to how their data is being processed and provide data access controls, such\nas the right to have data deleted or to modify erroneous data. This would require LLM providers to be\ntransparent about the data they have collected and provide tooling for individuals to inspect their data and\nhave the possibility to delete it.\nThe lack of transparency and openness surrounding the development processes of generative AI models\nhas also raised concerns in the scientific community. Many models are closed-access to varying degrees:\nfrom being available only within the organization that developed them (Chowdhery et al., 2022; Hoffmann\net al., 2022) to being accessible publicly through a paid API but with many details on their development\nprocess hidden (Brown et al., 2020; OpenAI, 2023a). While API access allows researchers to experiment with\nthese models, it limits their ability to research LLM safety (Perez et al., 2022), inspect the models’ inner\nworkings (Olsson et al., 2022), and contribute to model improvements (Togelius & Yannakakis, 2023).\nWe use “open-access” to refer to models whose weights are public. Although other open-access models\nexist, the level of openness still varies across these projects; and some models with released weights have\nrestrictions on model distribution (Touvron et al., 2023), or do not release their training datasets (Nijkamp\net al., 2023; Zhang et al., 2022; Fried et al., 2022). Even in cases when models and training data are both\nreleased permissively (Raffel et al., 2020; Tay et al., 2022), external researchers typically do not have an\nopportunity to participate in guiding the development of industry-produced models. In contrast, other\nLLM development projects have taken a fully open approach which aims to allow for community inputs\ninto model development, release training data, and enable external audits throughout the full development\nprocess (Solaiman, 2023). One example is the BigScience research workshop (BigScience Workshop, 2022),\nan open scientific collaboration (Akiki et al., 2022) comprising hundreds of researchers collaborating to\nrelease BLOOM, a multi-lingual LLM (Scao et al., 2022; Muennighoff et al., 2022). Similarly, EleutherAI, a\ngrassroots-turned-nonprofit research initiative, has released open-access LLMs including GPT-NeoX (Black\net al., 2022), GPT-J (Wang & Komatsuzaki, 2021), and Pythia (Biderman et al., 2023), as well as the\nassociated training data (Gao et al., 2021a).\nIn this paper, we describe StarCoder and StarCoderBase, open-access code LLMs developed and released by\nthe BigCode community, with a focus on respecting copyright, privacy, transparency, and community-driven\n2\n\nPublished in Transactions on Machine Learning Research (12/2023)\nmodel development. The project is an open-scientific collaboration focusing on the responsible development\nof LLMs for code. It is co-stewarded by two industry research labs and comprises more than 600 members\nfrom diverse academic institutes and industry labs. The Stack (Kocetkov et al., 2022) is a publicly available\npre-training dataset for Code LLMs with a transparent data governance framework. The Stack consists of\n6.4 TB of permissively licensed source code in 384 programming languages, and includes 54 GB of GitHub\nissues and repository-level metadata in the v1.2 version of the dataset. The dataset comes with “Am I in The\nStack”, a governance tool for developers to check whether their source code is part of the dataset, and an\nopt-out process for those who wish to have their code removed from the dataset.\nStarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from\nThe Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub\nissues, Git commits, and Jupyter notebooks. We fine-tuned StarCoderBase on another 35B Python tokens,\nleading to the StarCoder model. Both StarCoder models come with a novel combination of architectural\nfeatures, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-\nMiddle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA;\nShazeer, 2019). We present an extensive evaluation of the StarCoder models and release a demo along with\nan integrated attribution tool that can help users locate model generations that may have been copied from\nthe training set. Overall, our contributions can be summarized as follows.\n• We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming\nlanguages that support a novel combination of capabilities and architectural features unavailable in\nother open Code LLMs.\n• We perform the most comprehensive evaluation of Code LLMs to date using a diverse set of\nbenchmarks (Lai et al., 2022; Cassano et al., 2023; Pearce et al., 2022; Fried et al., 2022; Yee & Guha,\n2023; Austin et al., 2021; Chen et al., 2021; Ben Allal et al., 2022; Hendrycks et al., 2020; Reddy\net al., 2019; Cobbe et al., 2021; Nadeem et al., 2021; Gehman et al., 2020; Liang et al., 2022), and\nshow that:\n– StarCoder outperforms every open LLM for code that supports multiple programming lan-\nguages (Nijkamp et al., 2023; Zheng et al., 2023);\n– StarCoder matches or outperforms the OpenAI code-cushman-001 model; and\n– When fine-tuned on Python, StarCoder substantially outperforms existing LLMs that are also\nfine-tuned on Python.\n• We take important steps towards a safe open model release:\n– We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access,\nuse, and distribution of the model while embedding a set of use restrictions in identified critical\nscenarios. We have worked on a version of the license agreement that: (i) is more commercially\nviable for companies wishing to use and distribute the model and (ii) promotes transparency and\nunderstanding through the sharing of AI documentation such as model cards (Mitchell et al.,\n2019);\n– We incorporate a new attribution tool into the VSCode demo that can help users detect and locate\nmodel generations that may have been copied from the training set. This is achieved through a\ntwo-step process that involves a lightweight membership check followed by a search over a BM25\nindex (Section 9); and\n– We have significantly improved the PII redaction pipeline by collecting a PII dataset containing\n12,000 files with 22,950 annotated entities. We fine-tuned our own encoder model (StarEncoder)\non this dataset, resulting in a robust PII detection model (Section 4).\n2\nRelated Work\nLanguage models\nEarly efforts to build large-scale language models used n-grams and simple smoothing\ntechniques (Brants et al., 2007; Heafield et al., 2013; Buck et al., 2014). Other approaches applied various\n3\n\nPublished in Transactions on Machine Learning Research (12/2023)\ntypes of neural networks architectures, such as feedforward networks (Bengio et al., 2000) and recurrent\nnetworks (Mikolov et al., 2010; Jozefowicz et al., 2016), to the language modeling task. The Transformer\narchitecture (Vaswani et al., 2017) led to the development of highly scalable language models (Radford et al.,\n2019; Brown et al., 2020), which have shown a predictable relationship between language modeling loss and\nscaling factors such as the model size, number of training tokens, and compute budget (Kaplan et al., 2020;\nHoffmann et al., 2022).\nLanguage Models for Code\nLanguage models were initially applied to code by Hindle et al. (2012), but\nrelied on n-gram models trained at comparatively small scale. Many neural architectures developed in NLP\nwere also applied successfully to code, including encoder-only models for producing code representations (Feng\net al., 2020; Kanade et al., 2020) and encoder-decoder models for translation, editing, summarization, and\nlanguage-to-code tasks (Wang et al., 2021; Ahmad et al., 2021; Li et al., 2022). Decoder-only Transformer\narchitectures have produced strong generative models of code, typically by training on mixtures of text\nand code from GitHub (Chen et al., 2021; Austin et al., 2021; Fried et al., 2022; Zheng et al., 2023;\nNijkamp et al., 2023). Most of these models have not been fully open, but PolyCoder (Xu et al., 2022) and\nSantaCoder (Ben Allal et al., 2023) are notable exceptions and have both open models and training data.\nHowever, these models are relatively small (2.7B and 1.1B parameters, respectively) and are trained on less\ndata (< 300GB of code) than we explore in this work.\nClosed-access LLMs\nSeveral large tech companies have developed top-performing LLMs without releasing\nthem. Examples include Google’s PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022),\nDeepMind’s Chinchilla (Hoffmann et al., 2022) and Gopher (Rae et al., 2021), and NVIDIA’s Megatron-Turing\nNLG (Smith et al., 2022). OpenAI and other AI startups, including Cohere1, Anthropic2, and Aleph Alpha3,\noffer LLMs as a paid API service. These companies did not release model weights nor provide comprehensive\ninformation on the methodology used to create these models. OpenAI has published several technical\nreports of the GPT family of models (Brown et al., 2020; Chen et al., 2021; OpenAI, 2023a), showcasing the\ncapabilities of their models.\nOpen-access LLMs\nNumerous open-access LLMs have been released to the AI community, although\nthey are generally not as strong as closed-access ones. In this paper, we use the term “open-access LLM”\nwhen the model weights are publicly available. We still note that there are significant differences between\nopen-access models in how transparent they have been about the training data and filtering techniques. For\ninstance, EleutherAI released GPT-NeoX-20B (Black et al., 2022) and GPT-J-6B (Wang & Komatsuzaki,\n2021), as well as the dataset these models were trained on (Gao et al., 2021a). Google released UL2-20B (Tay\net al., 2022), an encoder-decoder model trained on the publicly available C4 (Raffel et al., 2020). Tsinghua\nUniversity released the weights of GLM-130B (Zeng et al., 2022), a Chinese-English LLM, and CodeGeeX-\n13B (Zheng et al., 2023), a LLM for coding applications, without releasing the training sets. Salesforce\nreleased CodeGen-Mono-16B (Nijkamp et al., 2023) without disclosing a proprietary Python dataset. Meta\nreleased the OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and InCoder models (Fried et al.,\n2022) under a non-commercial license and only provided high-level details about the data collection and\nfiltering process.\n3\nData Curation and Cleaning\nThis section describes how we processed the training data of StarCoderBase. We restrict the training set to\nThe Stack v1.2 (Kocetkov et al., 2022), which exclusively contains data from permissively licensed4 GitHub\nrepositories. At the time of the data processing, 44 people opted out of The Stack. Below, we describe how\nwe further cleaned the data by combining heuristic filtering and manual inspection.\n1https://cohere.com/\n2https://www.anthropic.com/\n3https://www.aleph-alpha.com/\n4See https://blueoakcouncil.org/ to learn more about permissive licenses and access a comprehensive collection of such\nlicenses.\n4\n\nPublished in Transactions on Machine Learning Research (12/2023)\n3.1\nProgramming Languages\nSelection of programming languages\nFrom the 358 programming languages in The Stack, we selected\n86 languages. The assignment of data to programming languages was performed based solely on file extension\n(Kocetkov et al., 2022). We included all programming languages with more than 500 MB of data, as well as\nlanguages that were ranked in the top 50 on Githut 2.0 or the December 2022 TIOBE Index of programming\nlanguage popularity. In addition, we included dialects of already selected programming languages (e.g.,\nRacket and Scheme for Lisp). We excluded configuration languages (Nix, Puppet, etc.) and languages that\nare no longer actively supported (ActionScript). We also included data formats like JSON and YAML but\nlimited its data volume (see “JSON and YAML” paragraph for details). The full list of selected programming\nlanguages can be found in Tables 1 and 2. Out of the languages present in MultiPL-E (Cassano et al., 2023),\nonly D and Swift were not included in the training set. For D, language misclassification of the files led to less\nthan 2MB of data in The Stack (Kocetkov et al., 2022). Swift was excluded from the final list of languages\ndue to human error.\nVisual inspection\nWe performed a visual inspection to ensure that we only retain data of high quality. To\nachieve this, we randomly selected 30,000 files from The Stack for each programming language, categorized\nthem by extension, and kept a maximum of 1,000 files for each extension. We then reached out to our\ncommunity for assistance with data inspection. We instructed the annotators to go through 50–100 files\nand confirm if the data appeared to be normal code written by humans, as opposed to text, data, or a\nsingle long line of autogenerated code. We also asked annotators to determine whether we should use our\ndefault alpha-numeric filter (which requires over 25% alpha-numeric symbols) and long-line filter (which\nrequires lines to be less than 1,000 characters) for a given file extension. Eighteen community annotators\nevaluated 300 programming language extensions. After inspection, we excluded 36 extensions and eliminated\nthe long-line filter for 27 extensions. The complete outcomes of the data inspection, including annotator\nremarks, can be found in this Google sheet.\nXML filter\nAs we inspected the data, we noticed that certain extensions often consisted of XML files. For\nexample, the .sld extension had more than 50% of its files in XML format. To address this, we implemented\na simple XML filter that checked for the presence of “<?xml version=” within the first 100 characters of\nthe file. This filter proved to be effective and produced few false positives. Hence, we applied it to all\nprogramming languages except for XSLT, which uses XML syntax.\nAlpha filter\nDuring our investigation, we discovered that certain extensions, such as MATLAB, contained\nnumerous data files that frequently stored large tensors. To identify these files, we developed an alpha filter\nthat removed files with fewer than 25% alphabetic characters. However, when we tested this filter on a small\nsubset of data, we observed a high rate of false positives for certain programming languages, such as Assembly.\nTo address this issue, we focused on the 25 extensions with the highest number of detections and manually\nverified whether or not the alpha filter should be applied.\nHTML\nWe designed a custom HTML filter that targets excessive HTML boilerplate and links. We took\ninto account the ratio of visible text in each file and only kept those files where the visible text makes up at\nleast 20% of the HTML code and has a minimum length of 100 characters.\nJSON and YAML\nJSON and YAML files are naturally more data-heavy than other languages in The\nStack. To remove most of the data files, we applied the following filters. For YAML, we kept files with\n50–5000 characters, an average line length smaller than 100, a maximum line length smaller than 1000, and\nmore than 50% alphabetic characters. These filters remove around 20% of the files and 90% of the volume.\nFor JSON, we kept files with 50–5000 characters and more than 50% alphabetic characters, which removes\naround 70% of the files and 98% of the volume.\n5\n\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nAfter dedup\nAfter filters and decont.\nWeight\nPercentage\nNum. files\nVolume (GB)\nNum. files\nVolume (GB)\nada\n31,291\n0.30\n30,934\n0.26\n0.26\n0.034\nagda\n17,608\n0.07\n17,554\n0.07\n0.07\n0.009\nalloy\n5,374\n0.01\n5,368\n0.01\n0.01\n0.001\nantlr\n7,983\n0.05\n7,917\n0.05\n0.05\n0.007\napplescript\n4,906\n0.01\n4,737\n0.01\n0.01\n0.001\nassembly\n248,396\n1.58\n247,919\n1.56\n1.56\n0.203\naugeas\n195\n0.00\n180\n0.00\n0.00\n0\nawk\n10,430\n0.02\n10,289\n0.02\n0.02\n0.003\nbatchfile\n252,514\n0.29\n239,568\n0.23\n0.23\n0.03\nbluespec\n5,940\n0.03\n5,928\n0.03\n0.03\n0.004\nc\n8,625,559\n57.43\n8,536,791\n53.89\n53.89\n7.027\nc-sharp\n10,839,399\n46.29\n10,801,285\n44.66\n44.66\n5.823\nclojure\n126,191\n0.49\n125,163\n0.46\n0.46\n0.06\ncmake\n186,517\n0.45\n186,375\n0.45\n0.45\n0.059\ncoffeescript\n227,889\n0.69\n226,209\n0.64\n0.64\n0.083\ncommon-lisp\n101,370\n1.68\n98,733\n1.40\n1.40\n0.183\ncpp\n6,377,914\n50.89\n6,353,527\n48.92\n48.92\n6.379\ncss\n2,994,829\n22.61\n2,721,616\n11.93\n3.00\n0.391\ncuda\n58,355\n0.59\n58,151\n0.56\n0.56\n0.073\ndart\n932,583\n3.86\n928,415\n3.66\n3.66\n0.477\ndockerfile\n572,186\n0.42\n571,506\n0.42\n0.42\n0.055\nelixir\n282,110\n0.74\n281,016\n0.71\n0.71\n0.093\nelm\n62,861\n0.34\n62,033\n0.30\n0.30\n0.039\nemacs-lisp\n54,768\n0.43\n52,838\n0.41\n0.41\n0.053\nerlang\n99,368\n0.73\n98,447\n0.70\n0.70\n0.091\nf-sharp\n127,161\n0.90\n124,066\n0.61\n0.61\n0.08\nfortran\n165,446\n1.84\n158,792\n1.78\n1.78\n0.232\nglsl\n175,576\n0.57\n167,701\n0.40\n0.40\n0.052\ngo\n4,730,461\n25.74\n4,700,526\n23.78\n23.78\n3.101\ngroovy\n251,627\n0.94\n250,834\n0.91\n0.91\n0.119\nhaskell\n544,969\n2.36\n541,454\n2.23\n2.23\n0.291\nhtml\n9,533,367\n146.76\n3,299,965\n29.36\n29.36\n3.828\nidris\n8,060\n0.03\n8,042\n0.03\n0.03\n0.004\nisabelle\n5,086\n0.09\n5,001\n0.08\n0.08\n0.01\njava\n20,151,565\n89.30\n20,071,773\n86.94\n86.94\n11.336\njava-server-pages\n214,133\n1.03\n210,816\n0.98\n0.98\n0.128\njavascript\n21,108,587\n141.65\n19,544,285\n64.71\n64.71\n8.437\njson\n17,012,912\n338.34\n4,751,547\n5.62\n1.00\n0.13\njulia\n298,672\n1.54\n295,364\n1.31\n1.31\n0.171\nkotlin\n2,242,771\n5.77\n2,239,354\n5.68\n5.68\n0.741\nlean\n16,891\n0.10\n16,870\n0.09\n0.09\n0.012\nliterate-agda\n523\n0.01\n523\n0.01\n0.01\n0.001\nliterate-coffeescript\n1,138\n0.01\n1,133\n0.01\n0.01\n0.001\nliterate-haskell\n6,135\n0.05\n6,104\n0.05\n0.05\n0.007\nlua\n558,861\n3.28\n549,459\n2.87\n2.87\n0.374\nmakefile\n661,424\n1.49\n657,349\n1.31\n1.31\n0.171\nmaple\n1,259\n0.01\n1,152\n0.01\n0.01\n0.001\nmarkdown\n21,045,171\n75.25\n21,029,287\n74.93\n74.93\n9.77\nmathematica\n26,895\n1.72\n22,653\n1.25\n1.25\n0.163\nmatlab\n967\n0.04\n93\n0.00\n0.00\n0\nTable 1: Overview of the training data for StarCoder. For the selected programming languages, we show the\nnumber of files and data volume after near-deduplication, as well as after filtering. See also Table 2.\n6\n\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nAfter dedup\nAfter filters and decont.\nWeight\nPercentage\nNum. files\nVolume (GB)\nNum. files\nVolume (GB)\nocaml\n159,734\n1.11\n158,356\n1.03\n1.03\n0.134\npascal\n118,675\n1.71\n110,981\n1.68\n1.68\n0.219\nperl\n392,108\n2.63\n365,491\n2.23\n2.23\n0.291\nphp\n15,904,518\n66.84\n15,683,017\n60.89\n60.89\n7.939\npowershell\n271,487\n1.25\n267,627\n1.12\n1.12\n0.146\nprolog\n1,023\n0.01\n968\n0.01\n0.01\n0.001\nprotocol-buffer\n98,246\n0.44\n97,167\n0.31\n0.31\n0.04\npython\n12,962,249\n64.30\n12,866,649\n60.40\n60.40\n7.875\nr\n39,194\n0.30\n39,042\n0.30\n0.30\n0.039\nracket\n4,201\n0.04\n3,688\n0.03\n0.03\n0.004\nrestructuredtext\n905,679\n3.42\n896,880\n3.32\n3.32\n0.433\nrmarkdown\n5,389\n0.06\n5,386\n0.06\n0.06\n0.008\nruby\n3,405,374\n7.14\n3,390,320\n6.81\n6.81\n0.888\nrust\n1,386,585\n9.53\n1,380,468\n9.11\n9.11\n1.188\nsas\n9,772\n0.13\n9,226\n0.12\n0.12\n0.016\nscala\n1,362,426\n4.86\n1,355,788\n4.69\n4.69\n0.612\nscheme\n44,261\n0.30\n41,890\n0.20\n0.20\n0.026\nshell\n2,236,434\n3.38\n2,206,327\n3.09\n3.09\n0.403\nsmalltalk\n592,999\n0.74\n587,748\n0.58\n0.58\n0.076\nsolidity\n164,242\n1.21\n153,194\n0.85\n0.85\n0.111\nsparql\n14,173\n0.04\n13,716\n0.04\n0.04\n0.005\nsql\n994,019\n12.22\n975,420\n11.09\n11.09\n1.446\nstan\n5,441\n0.01\n5,429\n0.01\n0.01\n0.001\nstandard-ml\n48,995\n0.52\n19,630\n0.19\n0.19\n0.025\nstata\n31,282\n0.41\n24,208\n0.33\n0.33\n0.043\nsystemverilog\n46,915\n0.41\n46,270\n0.39\n0.39\n0.051\ntcl\n50,579\n0.40\n49,335\n0.35\n0.35\n0.046\ntcsh\n4,911\n0.02\n4,806\n0.02\n0.02\n0.003\ntex\n547,888\n5.44\n522,778\n5.20\n5.20\n0.678\nthrift\n4,663\n0.01\n4,661\n0.01\n0.01\n0.001\ntypescript\n10,637,070\n28.82\n10,547,331\n26.52\n26.52\n3.458\nverilog\n77\n0.001\n75\n0.001\n0.001\n0\nvhdl\n60,027\n1.12\n58,208\n0.94\n0.94\n0.123\nvisual-basic\n163,291\n1.49\n161,239\n1.42\n1.42\n0.185\nxslt\n43,095\n0.56\n6,513\n0.05\n0.05\n0.007\nyacc\n25,775\n0.41\n7,451\n0.11\n0.11\n0.014\nyaml\n5,282,081\n28.36\n3,995,948\n3.76\n1.00\n0.13\nzig\n15,913\n0.18\n15,850\n0.18\n0.18\n0.023\nGitHub issues\n∼30,900,000\n54.40\n54.40\n7.093\nGit commits\n7,674,345\n64.00\n32.00\n4.172\nnotebook scripts\n914,000\n7.12\n7.12\n0.928\nnotebook structured\n668,743\n6.00\n6.00\n0.782\n305,929,658\n815.68\n799.37\n100\nTable 2: Overview of the training data for StarCoder. For the selected programming languages, we show the\nnumber of files and data volume after near-deduplication, as well as after filtering. See also Table 1.\n3.2\nJupyter notebooks\nAll Jupyter notebooks were retrieved from the Stack. We transformed Jupyter notebooks into two different\ndatasets: Jupyter – scripts and Jupyter – structured.\n7\n\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nNum files\nPercentage\npython\n1,392,432\n97.170\njulia\n16,730\n1.167\nr\n11,034\n0.77\nscala\n1,899\n0.133\nbash\n1,441\n0.101\njava\n1,319\n0.092\nq-sharp\n1,273\n0.089\ncpp\n1,081\n0.075\nc-sharp\n1,048\n0.073\nmatlab\n908\n0.063\npowershell\n769\n0.054\njavascript\n592\n0.041\nhaskell\n535\n0.037\nscheme\n484\n0.034\ngroovy\n432\n0.03\nf-sharp\n385\n0.027\nocaml\n279\n0.019\nrust\n134\n0.009\nclojure\n96\n0.007\ntypescript\n72\n0.005\nmaxima\n31\n0.002\ncoconut\n6\n0\nmarkdown\n5\n0\nwolfram language\n4\n0\ntcl\n3\n0\nTotal\n1,432,992\n100\nTable 3: Overview of the initially collected Jupyter scripts, with the number of files and the percentage.\nJupyter – scripts\nWe utilize Jupytext5 to convert notebooks to scripts. It is an actively maintained\nsoftware that currently supports 31 programming languages. To initiate the conversion process, Jupytext\nrequires the identification of the specific programming languages within each notebook. We extracted this\ninformation from the metadata of each respective notebook. However, more than 30,000 notebooks lacked\nany programming language information, making it difficult to convert them to the script format. To address\nthis issue, we incorporated the use of Guesslang,6 an open-source library that employs machine learning\ntechniques to identify the programming languages of source code. By applying a probability threshold\ngreater than or equal to 0.5, we successfully reduced the number of unidentified notebooks to 6,400 using\nGuesslang. Ultimately, we amassed 1,432,992 scripts through the utilization of Jupytext. The distribution of\nprogramming languages among these scripts is presented in Table 3. We evaluated language coverage by\nrandomly selecting 100 files from the transformed scripts, ensuring that all programming languages were\nrepresented within this sample.\nJupyter – structured\nTo create this dataset, we first filtered out notebooks that did not contain any\nPython code or Markdown text. The information on the programming language in the metadata of each\nnotebook was used as the criterion to filter out non-Python notebooks. Only notebooks explicitly marked as\n‘Python’ in the metadata were kept. Then for each notebook, consecutive Markdown blocks or code blocks\nwere merged into a large Markdown or code block respectively. Eventually, we ended up with consecutive\ncode-text pairs in temporal order grouped by each notebook. In general, each Jupyter code-text pair contained\nthe Markdown text immediately preceding the code block and the Python code, which forms a natural\n5https://jupytext.readthedocs.io/\n6https://guesslang.readthedocs.io/\n8\n\nPublished in Transactions on Machine Learning Research (12/2023)\ninstruction pair. We also included the formatted output of a code block if the output cell was non-empty;\notherwise, it was marked by a special <empty_output> token. If consecutive code blocks have multiple output\ncells before merging, we only retain the output of the last code block. After these preprocessing steps, we\nended up with 1,045,605 structured Jupyter notebooks.\n3.3\nGitHub issues\nWe used natural language conversations from GitHub issues and pull requests, which were collected as a\ncomponent of The Stack v1.2. Each conversation consists of a series of events with actions, such as opening\nthe issue, creating a comment, or closing the issue. Each event includes the author’s username, a message, an\naction, and a creation date. We filtered this data as follows: 1) First, we removed auto-generated text when\nusers replied to issues via email. See Appendix A for the regular expression we used. We also deleted issues\nwith a short message (less than 200 characters) and truncated long comments in the middle to a maximum of\n100 lines while retaining the last 20 lines. This removed 18% of the volume. 2) Next, we excluded comments\nfrom bots. To do so, we searched for bot keywords in the username of the comment’s author (for more\ninformation, see Appendix A). This step eliminates 17% of the total events and results in 14.7% of the issues\nbeing emptied. We have observed that bot-generated issues tend to be lengthy and contain numerous logs and\nlinks. 3) We used the number of users engaged in the conversation as an indicator of quality. Our criterion\nwas to include conversations that have two or more users. However, we also preserved conversations that\ninvolved a single user if the total text within comments was less than 7,000 characters (96th percentile).\nAdditionally, we excluded issues authored by a single user if they contained more than ten events, as they\ntended to be of poor quality or originate from overlooked bots. By implementing these filters, we removed an\nadditional 14% of issues. 4) Finally, we used a model from the fasttext library7 to filter out non-English issues.\nThis step was necessary to enable accurate redaction of names using a PII detection model (see Section 4.3).\nLastly, we would like to point out that we anonymized the usernames in the conversations by replacing them\nwith a participant counter within the conversation. See more details in Section 4.3 and 5.1.\n3.4\nGit commits\nThe Git commit data was gathered from BigQuery8 and includes only single-file commits of repositories with\nthe same licenses and file extension as used in The Stack (Kocetkov et al., 2022). We removed all repositories\nfrom users that opted out of The Stack. The raw dataset is around 4 TB in size. We sampled 50% of the files\nand filtered the remaining data with heuristics to build a high-quality dataset. We list and describe all filters\nin Table 4.\nThe number of line changes in a commit can be very low compared to the file size. To avoid spending too\nmuch compute budget on learning to copy the file content, we only used the full file 20% of the time, and for\nthe remaining 80%, sampled a window between 0 and 32 lines around the first and last changed line. The\nresulting dataset contains 64 GB of commit data.\n3.5\nDeduplication\nWe followed the deduplication pipeline from Ben Allal et al. (2023), which consists of calculating the\nMinHashes (Broder, 2000) of all source code files, followed by Locally Sensitive Hashing (LSH) to map similar\ncode files to the same bucket. We used 5-grams and a Jaccard similarity of 0.7. See this blogpost for more\ndetails regarding the pipeline.\nWe applied this near-deduplication process to all programming languages and the Jupyter notebooks. However,\ndue to time constraints, we could not apply this procedure to Git commits. Additionally, we deemed it\nunlikely to discover duplicates in Github issues, so we didn’t apply the process to them.\n7The lid.176.bin version of this language identification model: https://fasttext.cc/docs/en/language-identification.html\n8https://cloud.google.com/bigquery/public-data/\n9\n\nPublished in Transactions on Machine Learning Research (12/2023)\nDescription\nDetails\nMaximum characters\nRemove code files with >100k characters.\nSmall changes\nSubsample changes with ≤2 lines with 50% probability.\nLong-range refactorings\nSubsample changes spanning ≥200 lines with 10% proba-\nbility.\nEmpty commit message\nRemove commits with empty commit subject.\nAutomatic commits\nRemove commits that either contain or are equal to a list\nof stop words.\nHash messages\nRemove commits with whitespace-separated words-to-\ncharacter ratio >20.\nData files\nSubsample data formats (JSON, YAML, XML, HTML) with\n50% probability.\nTable 4: Git commit filters.\n3.6\nWeighting of data sources\nThere were several discussions within the community about whether to up-sample or down-sample certain\nprogramming languages, as the amount of compute budget allocated to a data source in a given language can\nsignificantly affect the model’s performance in that language. However, we realized that the largest amount\nof available data comes from popular programming languages and would, therefore, benefit a larger group\nof end-users. Moreover, after the deduplication process, we found that several high-resource programming\nlanguages, such as C, C++, C#, Java, Javascript, Python, and PHP, had a similar amount of data ranging\nfrom 44–87 GB. This further reinforced our belief that we did not need to drastically re-weigh the existing\ndata distribution. Thus, in this work, we followed the natural distribution of data during training and sampled\ndata sources proportionally to their volume. However, we did make an exception for JSON, YAML, and\nCSS, as we only want the LLM to learn the data format without wasting compute resources on memorizing\nthe data in such files. For that reason, we re-weighed the volume of the data source to 1 GB for JSON and\nYAML and 3GB for CSS.\n4\nPII redaction\nThis section outlines our efforts to remove Personally Identifiable Information (PII) from the training data.\nIn Section 4.1, we first describe how we collected a large set of PII annotations. We used these annotations\nto explore various techniques to train a PII detection model in Section 4.3, building on top of the encoder\nmodel we developed in Section 4.2.\n4.1\nData collection\nWe utilized the Toloka platform9 to engage 1,399 crowd-workers from 35 countries in annotating a dataset\nfor PII in source code. On average, participants completed 206 tasks, earned about $27, and worked 3.1\nhours. Our goal was to identify PII in various forms, such as names, usernames, emails, IP addresses, keys,\npasswords, and IDs. To ensure that crowd-workers received fair compensation, we established an hourly pay\nrate of $7.30, taking into consideration different minimum wage rates across countries and their corresponding\npurchasing power. We limited annotation eligibility to countries where the hourly pay rate of $7.30 was\nequivalent to the highest minimum wage in the US ($16.50) in terms of purchasing power parity. A complete\nlist of countries that participated in the annotation can be found in Table B.1 of Appendix B. Crowd workers\nin Toloka can do tasks whenever or wherever; there is no obligation to complete a certain task or spend a\nfixed amount of time on it. Thus, they utilize free choice when working on the tasks. Out of 1,399 crowd\nworkers, 695 filled a survey on task quality, and 519 completed the survey. The average score for the question\nasking whether the participant would like to contribute to another project like this is 4.92 on a scale 1–5.\n9https://toloka.ai/\n10\n\nPublished in Transactions on Machine Learning Research (12/2023)\nc\ncpp\ncsharp\njava\nruby\npython\njs\nphp\nrust\ngo\ntypescript\nsql\ncss\nhaskell\nkotlin\nperl\ndart\nlua\nscala\nfortran\nshell\nhtml\npascal\nassembly\nscheme\njulia\nstata\npowershell\nprolog\nmathematica\nmatlab\nProgramming language\n0\n200\n400\n600\n800\nNumber of files\nFigure 1: Distribution of programming languages in the annotated PII dataset.\nThe dataset comprises 12,000 files, each containing approximately 50 lines of code written in 31 programming\nlanguages. Figure 1 shows the distribution of programming languages in the dataset. To increase the\nrepresentation of rare PII types, such as keys and IP addresses, 7,100 files were pre-filtered from a larger\nsample. We utilized the detect-secrets tool10 with all default plugins activated, along with the regular\nexpressions by Ben Allal et al. (2023) for detecting emails, IPv4 and IPv6 addresses. To prevent biasing the\nannotation too much towards these detection tools, the remaining 5,100 files were randomly selected from the\ndataset without pre-filtering.\nDuring annotation, we differentiated between various types of PII based on the specific context in which\nit appeared. Specifically, we distinguished whether the PII was present in the code’s license header, was\nused as a placeholder, or constituted confidential data. This categorization was necessary because the PII in\nlicense headers is usually provided voluntarily by authors for code attribution and may not require masking.\nSimilarly, placeholders are not real secrets and do not need to be masked. We applied this categorization to\nnames, emails, and usernames. See Table 5 for an overview of all PII entities.\nThe annotators detected a total of 22,950 PII entities in the dataset. To evaluate the quality of the dataset,\nwe manually inspected 300 files that contained various PII types and calculated the recall and precision for\neach type, as shown in Table 5. We found that annotating secret IDs was particularly challenging, as the\nannotators tended to produce many false positives and negatives. As a result, we decided to exclude this\ncategory from the PII detection model training.\n4.2\nStarEncoder\nAs part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive\nTransformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked\nLanguage Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019;\nLiu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences\noccur as neighbors in a document.\nWe separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code\nsnippets are selected randomly, either from the same source file or from two distinct documents. For the\nMLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use\na linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a\nglobal batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are\n10https://github.com/Yelp/detect-secrets\n11\n\nPublished in Transactions on Machine Learning Research (12/2023)\nPII type\nCount\nRecall\nPrecision\nIP_ADDRESS\n2526\n85%\n97%\nKEY\n308\n91%\n78%\nPASSWORD\n598\n91%\n86%\nID\n1702\n53%\n51%\nEMAIL\n5470\n99%\n97%\nEMAIL_EXAMPLE\n1407\nEMAIL_LICENSE\n3141\nNAME\n2477\n89%\n94%\nNAME_EXAMPLE\n318\nNAME_LICENSE\n3105\nUSERNAME\n780\n74%\n86%\nUSERNAME_EXAMPLE\n328\nUSERNAME_LICENSE\n503\nAMBIGUOUS\n287\nTable 5: Overview of the PII types and the number of collected annotations. We investigate the annotation\nquality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was\nmapped back to its corresponding PII type for the inspection.\nHyperparameter\nValue\nHidden size\n768\nIntermediate size\n3072\nMax. position embeddings\n1024\nNum. of attention heads\n12\nNum. of hidden layers\n12\nAttention\nMulti-head\nNum. of parameters\n≈125M\nTable 6: Model architecture of StarEncoder.\nobserved. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture\nare reported in Table 6.\n4.3\nPII detection model\nWe fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We\nadded a linear layer as a token classification head on top of the model, with 6 target classes: names, emails,\nkeys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not\ndifferentiate between the categorization of PII entities (license headers, placeholders) because of the model’s\npoor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test\nset of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See\nTable 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode.\nFine-tuning baseline\nWe fine-tune StarEncoder on the PII training set, and 400 annotated files from\nBen Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and\n73.39% on passwords. The model’s performance is comparatively low on keys and usernames, with F1 scores\nof only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of\nlabels for this type of PII, as only 308 instances were available. For usernames, we observed the model often\nconfused them with decorators and values in paths. This is most likely because we annotated usernames\ninside links for social media platforms.\n12\n\nPublished in Transactions on Machine Learning Research (12/2023)\nEntity type\nTrain\nTest\nEMAIL\n4721\n1742\nNAME\n3847\n1298\nIP_ADDRESS\n1941\n521\nUSERNAME\n1320\n346\nPASSWORD\n390\n148\nKEY\n171\n118\nTable 7: Train-test split of the annotated PII dataset.\nMethod\nEmail address\nIP address\nKey\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nRegex\n96.20%\n97.47%\n96.83%\n71.29%\n87.71%\n78.65%\n3.62%\n49.15%\n6.74%\nNER\n94.01%\n98.10%\n96.01%\n88.95%\n94.43%\n91.61%\n60.37%\n53.38%\n56.66%\n+ pseudo labels\n97.73%\n98.94%\n98.15%\n90.10%\n93.86%\n91.94%\n62.38%\n80.81%\n70.41%\nTable 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data,\nand NER Pipeline with Annotated Data + Pseudo-Labels\nPseudo-labels\nTo improve the detection of key and password entities, we employed a pseudo-labeling\ntechnique as described by Lee (2013). This method involves training a model on a small set of labeled\ndata and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated\n18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from\nBen Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from\nour models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities,\nexcept for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the\nresults, we found a significant number of false positives for keys and passwords. As a result, we decided to\nonly retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100\ncharacters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results\nfor all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did\nnot show significant improvement, so we decided to exclude it from the PII redaction process.\nComparison against regex baseline\nWe compared our PII detection models against the regular expres-\nsions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP\naddresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false\npositives we found during the evaluation on this benchmark. This modification boosted the F1 score of the\nregex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in\ndetecting all three entities, as shown in Table 8. We note that the performance difference was especially large\non keys and found that the detect-secrets tool generated many false positives, especially in specific pro-\ngramming languages like Go and C-sharp that weren’t well represented in the regex evaluation. Consequently,\nthe overall precision of the tool was below 4%.\nPost-processing\nBefore applying the best PII detection model to the full dataset, we observed a couple of\nfrequent detection errors. We added the following post-processing techniques to reduce the number of false\npositives:\n• Ignore secrets with fewer than 4 characters.\n• Detect full names only by requiring at least one space within the name.\n13\n\nPublished in Transactions on Machine Learning Research (12/2023)\nMethod\nName\nUsername\nPassword\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nNER\n83.66%\n95.52%\n89.19%\n48.93%\n75.55%\n59.39%\n59.16%\n96.62%\n73.39%\n+ pseudo labels\n86.45%\n97.38%\n91.59%\n52.20%\n74.81%\n61.49%\n70.94%\n95.96%\n81.57%\nTable 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data\n+ Pseudo-Labels\n• Ignore detected keys with fewer than 9 characters or that are not gibberish using a\ngibberish-detector.11\n• Ignore IP addresses that aren’t valid or are private (non-Internet facing) using the ipaddress python\npackage. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal\net al. (2023).\nPII placeholders\nWe replaced the detected PII entities with the following tokens:\n<NAME>, <EMAIL>, <KEY>, <PASSWORD>\nTo mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP\naddresses of the same type that can be found in Appendix C.\nGithub issues\nWe already employed a regex approach to detect keys, IP addresses, and emails in the\nGithub issues, so we only used the PII detection model to redact names. We anonymized the usernames\nof the authors by replacing them with a participant counter within the conversation, e.g. username_1 to\nrefer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the\nbeginning of each comment such that we preserve the speaker identity of the author. In addition, we redact\nall mentions of these usernames in the messages. Note that we only mask the usernames of active participants\nin the conversation and mentions of non-participating users are not anonymized.\nCompute resources\nWe used the PII detection model to identify PII across all programming languages\nin the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The\ntotal dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which\nrequired 800 GPU-hours.\n5\nModel training\nThis section presents information on the training process of the StarCoder models. Before we proceed, we\nfirst clarify the differences between the two models:\nStarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described\nin Section 3.\nStarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2\nepochs).\nThroughout the following, we show how we formatted the training data (Section 5.1), decontaminated the\ntraining data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture\n(Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions\n(Section 5.7).\n11https://github.com/domanchi/gibberish-detector\n14\n\nPublished in Transactions on Machine Learning Research (12/2023)\n5.1\nData formatting\nWe present the formatting guidelines for each of the data sources below. We provide the templates below\nin which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields,\nrespectively.\nCode\nWe prepend the repository name, file name, and the number of stars to the context of the code file.\nTo not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1–10, 10–100,\n100–1000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the\nrepository name, filename, and stars independently at random, each with a probability of 0.2.\n<reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|>\nTo the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM;\nBavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a\nFIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5.\nIssues\nWe use sentinel tokens to mark the opening of an issue and subsequently include its title. We\nseparate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier\nbefore the comment. Specifically, we refer to authors by their participant counter within the conversation,\ne.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we\nuse comment1, id1 to refer to the second comment and its anonymized speaker id, respectively.\n<issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1\n... <issue_closed (optional)><|endoftext|>\nJupyter – scripts\nJupyter scripts were formatted in the same manner as code.\nJupyter – structured\nParsed Jupyter notebooks come in chains of text, code, and outputs, and we\nseparated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the\nnotebook.\n<jupyter_start><jupyter_text>text0<jupyter_code>code0\n<jupyter_output>output0<jupyter_text> ... <|endoftext|>\nGit commits\nWe separate the code before the commit, the commit message, and the code after the commit\nwith sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use\na small window (0-32 lines) around the changed lines.\n<commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|>\nWe summarize all sentinel tokens in Table 10.\n5.2\nTraining data decontamination\nThe code training data was decontaminated by removing files that contained docstrings or solutions from\nHumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These\nbenchmarks are further described in Section 6.) To give an indication of the amount of data removed by\ndecontamination, Python is the language with the highest number of matches, with 558 files removed.\n15\n\nPublished in Transactions on Machine Learning Research (12/2023)\nToken\nDescription\n<|endoftext|>\nend of text/sequence\n<fim_prefix>\nFIM prefix\n<fim_middle>\nFIM middle\n<fim_suffix>\nFIM suffix\n<fim_pad>\nFIM pad\n<reponame>\nrepository name\n<filename>\nfile name\n<gh_stars>\nGitHub stars\n<issue_start>\nstart of GitHub issue\n<issue_comment>\nstart of GitHub issue comment\n<issue_closed>\nGitHub issue closed event\n<jupyter_start>\nstart of Jupyter notebook\n<jupyter_text>\nstart of Jupyter text cell\n<jupyter_code>\nstart of Jupyter code cell\n<jupyter_output>\nstart of Jupyter output cell\n<empty_output>\noutput cell without content\n<commit_before>\ncode snippet before commit\n<commit_msg>\ncommit message\n<commit_after>\ncode snippet after commit\nTable 10: Overview of the sentinel tokens.\n5.3\nTokenizer\nThe model’s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design\nchoices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding\nwith a vocabulary size of 49,152 tokens—including the sentinel tokens from table 10. The pre-tokenization\nstep includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer.\n5.4\nModel Architecture\nWe trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a\ndecoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional\nembeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training\ndata, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and\nreduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work\nwith MQA during training, we simply expand the key and value before calling the attention kernel. The\narchitecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of\nSantaCoder(Ben Allal et al., 2023) for comparison.\n5.5\nTraining details\nStarCoderBase\nThe model was trained for 250k iterations, with a batch size of 4M tokens, for a total of\none trillion tokens. We used Adam (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.95, ϵ = 10−8 and a weight\ndecay of 0.1. The learning rate followed a cosine decay from 3 × 10−4 to 3 × 10−5 after a linear warmup of\n2,000 iterations.\nStarCoder\nStarting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the\nPython subset of the training data. We used the same settings as StarCoderBase, except that we used a\nlearning rate of 5 × 10−5 and decayed it to 5 × 10−6 after 1,000 iterations of linear warmup. We trained for\n8,500 steps.\n16\n\nPublished in Transactions on Machine Learning Research (12/2023)\nHyperparameter\nSantaCoder\nStarCoder\nHidden size\n2048\n6144\nIntermediate size\n8192\n24576\nMax. position embeddings\n2048\n8192\nNum. of attention heads\n16\n48\nNum. of hidden layers\n24\n40\nAttention\nMulti-query\nMulti-query\nNum. of parameters\n≈1.1B\n≈15.5B\nTable 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community).\n5.6\nMulti-Node GPU Setup\nWe trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We\npartitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism\nrank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster’s capabilities, we used\n32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a\nmicro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to\n4M tokens). We used Megatron-LM’s distributed optimizer because we found that it leads to slightly higher\nthroughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16\nleads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities.\nExcept for a few restarts, we did not experience significant training instabilities.\n5.7\nCO2 emissions\nStarCoderBase\nWe report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based\non the total number of GPU hours that training took (320,256) and an average power usage of 280W per\nGPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the\ncarbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average\nPower Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted.\nStarCoder\nThe fine-tuned model adds 3.5% of training time, which translates to an additional estimated\nemission of 0.58 tonnes of CO2eq.\n6\nEvaluation\nIn this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we\nreport on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin\net al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation\nusing a variety of benchmarks and tasks.\nA Code LM Evaluation Harness\nTo enable reproducible and centralized evaluation of StarCoder and\nother Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM\nEvaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of\ncode models, utilizing data parallelism and docker containers for execution. It supports several benchmarks,\nincluding HumanEval, MultiPL-E, and DS-1000.\nOther Models Evaluated\nWe compare StarCoder and StarCoderBase to the following models.\n1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained\non the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript,\nand Python from the GitHub BigQuery dataset (Smith, 2016).\n17\n\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nSize\nHumanEval\nMBPP\nOpen-access\nLLaMA\n7B\n10.5\n17.7\nLLaMA\n13B\n15.8\n22.0\nSantaCoder\n1.1B\n18.0\n35.0\nCodeGen-Multi\n16B\n18.3\n20.9\nLLaMA\n33B\n21.7\n30.2\nCodeGeeX\n13B\n22.9\n24.4\nLLaMA-65B\n65B\n23.7\n37.7\nCodeGen-Mono\n16B\n29.3\n35.3\nStarCoderBase\n15.5B\n30.4\n49.0\nStarCoder\n15.5B\n33.6\n52.7\nClosed-access\nLaMDA\n137B\n14.0\n14.8\nPaLM\n540B\n26.2\n36.8\ncode-cushman-001\n12B\n33.5\n45.9\ncode-davinci-002\n175B\n45.9\n60.3\nTable 12: Comparing StarCoder’s performance (pass@1) on the HumanEval and MBPP Python with several\nother models. StarCoder and StarCoder base obtain the highest performance of open-access models, and\ncomparable performance to the code-cushman-001 closed access model.\n2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python\ncode from GitHub, though the dataset is not publicly available.\n3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming\nlanguages selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data\nfor Python, Java, and C++.\nCodeGeeX also includes its own multi-language benchmark suite,\nHumanEval-X, which we discuss below.\n4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub\nCopilot (Chen et al., 2021). The details of its training set are unknown. This model has been\ndeprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of\nwriting.12\n5. Finally, although they are not specifically trained for code generation, we include some results from\nthe LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al.,\n2022) papers. LLaMA’s license prohibits commercial use, and PaLM and LaMDA are not publicly\navailable.\n6.1\nStarCoder: Python Evaluation\nIn this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and\nclosed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021), which are two widely used benchmarks of Python performance. However, we also measure\nperformance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science\nproblems based on StackOverflow questions.\n6.1.1\nThe HumanEval and MBPP Benchmarks\nHumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs\nconsisting of hundreds of Python programming problems that use test cases to validate the code produced by\n12There had been a code-cushman-002, but it is not available at the time of writing.\n18\n\nPublished in Transactions on Machine Learning Research (12/2023)\nFormat\nModel\nMatplotlib\nNumPy\nPandas\nPyTorch\nSciPy\nScikit-\nLearn\nTensorFlow\nOverall\nNumber of problems:\n155\n220\n291\n68\n106\n115\n45\n1,000\nCompletion\nSantaCoder-1B\n21.6\n4.6\n0.9\n2.6\n2.4\n4.8\n3.1\n5.7\nCompletion\nInCoder-6B\n28.3\n4.4\n3.1\n4.4\n2.8\n2.8\n3.8\n7.4\nCompletion\nCodeGen-16B-Mono\n31.7\n10.9\n3.4\n7.0\n9.0\n10.8\n15.2\n11.7\nCompletion\ncode-cushman-001\n40.7\n21.8\n7.9\n12.4\n11.3\n18.0\n12.2\n18.1\nCompletion\nStarCoderBase\n47.0\n27.1\n10.1\n19.5\n21.7\n27.0\n20.5\n23.8\nCompletion\nStarCoder\n51.7\n29.7\n11.4\n21.4\n20.2\n29.5\n24.5\n26.0\nInsertion\nSantaCoder-1B\n21.6∗\n13.8\n2.0\n3.8\n5.7\n6.9\n14.8\n9.3\nInsertion\nInCoder-6B\n28.3∗\n4.6\n2.9\n4.4\n2.8\n3.1\n7.8\n7.5\nInsertion\nStarCoderBase\n47.0∗\n26.3\n10.9\n16.6\n20.2\n30.2\n22.3\n24.0\nInsertion\nStarCoder\n51.7*\n30.8\n10.3\n21.0\n20.2\n27.4\n20.0\n25.4\nTable 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All\nmodels evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy\naveraged over 40 samples. ∗: Matplotlib task does not have right sided context, so insertion and completion\nformats are identical.\na Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance\nusing the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a\nproblem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021),\nwe use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples\nfor all experiments with open-access models. For API models, we use n = 20 samples, which is enough to\nestimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem\nis solved in a single attempt by the model.\nTable 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and\nclosed-access models:\n1. StarCoder is the highest-performing open-access model on both benchmarks.\n2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being\nsignificantly smaller.\n3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a\nsimilarly-sized open-access model that was fine-tuned on Python.\n4. StarCoder outperforms OpenAI’s code-cushman-001 (12B) model.\n6.1.2\nThe DS-1000 Python Data Science Benchmarks\nA major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not\nrepresentative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al.,\n2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates\ngenerations in execution against test cases.\nDS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for\nall models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried\net al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas,\nSciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in\nTable 13 and draw the following conclusions:\n1. StarCoder substantially outperforms all other models on data science problems from the DS-1000\nbenchmark. Moreover, this is true across every kind of data science library.\n19\n\nPublished in Transactions on Machine Learning Research (12/2023)\n2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000.\n3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks\ndoes not always correlate with performance on the more realistic DS-1000 benchmarks. For example,\nCodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval\nand MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating\nmodels on a range of benchmarks.\n6.1.3\nThe ODEX Open-Domain Coding Benchmark\nOur previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in\nMBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability\nto generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022)\ncontaining 505 open-domain and 440 closed-domain Python coding queries, in four natural languages —\nEnglish, Spanish, Japanese, and Russian — with test-case-based execution evaluation.\nWe report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen-\n16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by\nlanguages and domains, which are: (1) queries in the closed-domain (using only built-in Python functions)\nand open-domain (using functions from imported libraries), and (2) queries with instructions written in\nEnglish, Spanish, Japanese, and Russian, respectively. We report overall scores and scores in different domains\nand languages in Table 14 and draw the following conclusions:\n1. StarCoder substantially outperforms all other models on open-domain coding queries from the ODEX\nbenchmark.\n2. StarCoderBase also outperforms every other model, even better than StarCoder in the ODEX English\nsubset, but slightly behind in other languages.\n3. Both StarCoder and StarCoderBase models generally exhibit smaller gaps between open- and closed-\ndomain queries than other baseline models, despite the higher overall execution accuracy. This result\nindicates that StarCoder models acquire more generalized skills about coding queries in the open\ndomain (i.e., concerning diverse Python libraries), while other models exhibit larger performance\ndrops when moving from the closed to open domain.\nModel\nEnglish\nSpanish\nJapanese\nRussian\noverall\nopen\nclosed\noverall\nopen\nclosed\noverall\nopen\nclosed\noverall\nopen\nclosed\nCodeGen-16B-Mono\n33.7\n25.2\n43.1\n30.0\n25.0\n43.1\n37.8\n26.6\n62.8\n46.8\n30.4\n60.1\ncode-cushman-001\n31.9\n24.4\n40.2\n31.9\n27.7\n36.7\n25.7\n21.2\n35.5\n40.0\n26.0\n51.6\ncode-davinci-001\n33.6\n26.9\n41.0\n36.9\n31.7\n42.9\n31.0\n23.7\n47.3\n43.2\n28.9\n55.1\nSantaCoder\n37.7\n30.9\n45.1\n32.1\n26.0\n39.1\n28.1\n23.0\n39.4\n36.9\n23.0\n48.3\nStarCoderBase\n46.5\n40.7\n53.0\n30.1\n25.4\n35.5\n41.2\n37.6\n49.2\n46.1\n34.0\n56.1\nStarCoder\n44.7\n37.0\n53.1\n37.6\n32.9\n42.9\n44.2\n39.6\n54.5\n50.4\n33.8\n64.1\nTable 14: Performance on the ODEX benchmark by instruction languages and code domains: open problems\nuse libraries, while closed use only built-in Python functions.\n6.2\nStarCoder and StarCoderBase: Multi-Language Evaluation\nIn this section, we focus primarily on StarCoderBase, and evaluate its performance on a variety of programming\nlanguages and programming tasks, including producing code from natural language descriptions, documenting\ncode, predicting type annotations, and more. This section also shows that StarCoder, despite being fine-tuned\non Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some\nlanguages.\n20\n\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nCodeGen-16B-Multi\nCodeGeeX\ncode-cushman-001\nStarCoder\nStarCoderBase\ncpp\n21.00\n16.87\n30.59\n31.55\n30.56\nc-sharp\n8.24\n8.49\n22.06\n21.01\n20.56\nd\n7.68\n9.15\n6.73\n13.57\n10.01\ngo\n13.54\n11.04\n19.68\n17.61\n21.47\njava\n22.20\n19.14\n31.90\n30.22\n28.53\njulia\n0.00\n0.29\n1.54\n23.02\n21.09\njavascript\n19.15\n16.92\n31.27\n30.79\n31.70\nlua\n8.50\n10.96\n26.24\n23.89\n26.61\nphp\n8.37\n13.51\n28.94\n26.08\n26.75\nperl\n3.42\n8.09\n19.29\n17.34\n16.32\npython\n19.26\n21.62\n30.71\n33.57\n30.35\nr\n6.45\n3.92\n10.99\n15.50\n10.18\nruby\n0.00\n3.34\n28.63\n1.24\n17.25\nracket\n0.66\n3.31\n7.05\n0.07\n11.77\nrust\n4.21\n7.88\n25.22\n21.84\n24.46\nscala\n2.37\n8.95\n27.62\n27.61\n28.79\nbash\n0.61\n2.75\n11.74\n10.46\n11.02\nswift\n1.25\n7.26\n22.12\n22.74\n16.74\ntypescript\n20.07\n10.11\n31.26\n32.29\n32.15\nTable 15: Comparing StarCoder to multi-language open-access (e.g., CodeGen-16B-Multi) and closed-access\nmodels (e.g., code-cushman-001) on 19 programming languages. We report pass@1 on HumanEval (Chen\net al., 2021), which we translate from Python to the other languages using MultiPL-E (Cassano et al., 2023).\n6.2.1\nEvaluation on 19 Programming Languages with MultiPL-E\nWe evaluate the ability of StarCoder to turn natural language into working code in multiple programming\nlanguages using MultiPL-E (Cassano et al., 2023), which translates the HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021) Python benchmarks into 18 other programming languages as follows.\nMultiPL-E has a set of rule-based compilers that translate Python benchmarks to each target programming\nlanguage. Each compiler expects a benchmark in the HumanEval format: 1) a natural language description\n(in a docstring), 2) a function signature (name, arguments, and, potentially, types), and 3) a set of hidden\nassertions. The MultiPL-E compilers translate the function signature, assertions, and docstring (which may\nhave doctests) into a target language. Thus, MultiPL-E gives us a parallel set of benchmarks derived from\nHumanEval and MBPP to compare model performance across programming languages.13 The MultiPL-E\nlanguages include both high and low-resource languages, statically and dynamically typed languages, and a\nvariety of other programming language features.\nTable 15 shows how these models perform on 19 programming languages, and from it, we draw the following\nconclusions:\n1. Across all 19 programming languages, StarCoderBase outperforms other open-access models, some-\ntimes showing more than 2× performance.\n2. StarCoderBase is competitive with code-cushman-001 on most languages that we evaluate. There are\na few exceptions. For example, code-cushman-001 outperforms StarCoderBase by more than 5% on\nC++, Java, Ruby, and Swift, and StarCoder outperforms code-cushman-001 by more than 5% on\nJulia.\n13The MultiPL-E prompts are slightly different from the original HumanEval and MBPP prompts. For example, in HumanEval,\nsome ad hoc examples in docstrings are reformatted to be doctests so that they can be translated into examples in each target\nlanguage. MultiPL-E also omits three HumanEval benchmarks that do not fit the above format. These changes have a small\nimpact on pass rates.\n21\n\nPublished in Transactions on Machine Learning Research (12/2023)\nFormat\nModel\nValid (↑)\nInsecure (↓)\nCompletion\nStarCoderBase\n855/1000 (85.50%)\n340/855 (39.77%)\nInsertion\nStarCoderBase\n987/1000 (98.70%)\n354/987 (35.87%)\nCompletion\nInCoder-6B\n871/1000 (87.10%)\n309/871 (35.48%)\nInsertion\nInCoder-6B\n854/1000 (85.40%)\n293/854 (34.31%)\nCompletion\nCodeGen-16B-Multi\n955/1000 (95.50%)\n413/955 (43.25%)\nCompletion\ncode-cushman-001\n964/1000 (96.40%)\n408/964 (42.32%)\nTable 16: Performance on the Asleep at the Keyboard security benchmark (Pearce et al., 2022).\n3. Despite fine-tuning on Python, StarCoder remains competitive on most languages, and also out-\nperforms other open models.\nWhat is more surprising is that StarCoder slightly outperforms\nStarCoderBase on certain languages, despite being fine-tuned on Python. At this time, we can only\nspeculate on why this is the case, and further investigation of the open training data is likely to help\nshed light on this finding.\nThere are several other conclusions that we can draw from the table. For example, CodeGen-16B-Multi\nperforms better than one might expect on some languages that are reportedly not in its training set, including\nC#, Lua, PHP, and TypeScript. Its performance on TypeScript is less surprising since simple JavaScript\nfunctions often type-check with TypeScript by design. Similarly, StarCoder shows high performance on Swift,\neven though it was not included in its training set, as explained in Section 3.1.\n6.2.2\nThe “Asleep at the Keyboard” Security Benchmark\nA limitation of Code LLMs is that they can generate code with security vulnerabilities (Pearce et al., 2022).\nThe Asleep at the Keyboard benchmark by Pearce et al. (2022) has 89 security-sensitive scenarios across\nthree evaluation axes: (1) Diversity of Weakness (DoW) covers 18 different vulnerability classes in MITRE’s\nCommon Weakness Enumeration (CWE) taxonomy, with scenarios drawn from the 2021 CWE Top 25 Most\nDangerous Software Weaknesses list published by MITRE; (2) Diversity of Prompt (DoP) evaluates the\nmodel’s sensitivity to variations in the prompt for a single vulnerability class (SQL injection); (3) Diversity\nof Domain (DoD) contains security scenarios in the hardware description language Verilog. We focus on the\nDoW, which contains 54 scenarios (25 in C and 29 in Python) across 18 CWEs. We exclude scenarios that\nlack an automated test, leaving 40 scenarios (23 in C and 17 in Python).\nPearce et al. (2022) had previously evaluated the security of GitHub Copilot (as of August 2021), and in\nthis paper, we use the same methodology to evaluate StarCoderBase, InCoder-6B, CodeGen-16B-Multi, and\nOpenAI’s code-cushman-001. We use the original benchmarking methodology: generating 25 completions\nper scenario at temperature 0.2 (1,000 completions per model). The dataset supports fill-in-the-middle, so\nwe include this configuration on models that support it. The results are shown in Table 16; Valid gives\nthe percentage of solutions that were syntactically valid (using py_compile for Python and gcc for C), and\nInsecure shows the percentage of valid solutions that contained the vulnerability the scenario tests for. From\nthis table, we draw the following conclusions.\n1. StarCoderBase has the highest rate of valid code.\n2. InCoder-6B has a slightly lower rate for insecure code generation, but this may be due to its lower\nrate of valid completions.\n3. Among the models with more than 95% valid code, StarCoder has the lowest rate of insecure\ncompletions.\n6.2.3\nFill in the Middle Benchmarks\nThe StarCoder models support fill in the middle (FIM) or infilling, which allows the model to generate\ncode conditioned on prefix and suffix code surrounding the insertion point. Only a handful of recent models\n22\n\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nJava\nJavaScript\nPython\nInCoder-6B\n0.49\n0.51\n0.31\nSantaCoder\n0.62\n0.60\n0.44\nStarCoder\n0.73\n0.74\n0.62\nTable 17: Performance on single-line fill-in-the-middle on the FIM benchmark by Ben Allal et al. (2023).\nModel\nNon-None F1\nAll F1\nInCoder-6B\n59.1\n46.8\nSantaCoder\n66.9\n78.5\nStarCoderBase\n77.4\n86.6\nStarCoder\n77.1\n86.4\nTable 18: Accuracy of Python return type prediction, using Fried et al. (2022)’s adaptation of the Pradel\net al. (2020) benchmarks. We report both the overall F1 scores, which include trivial None-type prediction,\nand the F1 score for non-None types.\nsupport FIM: from OpenAI (Bavarian et al., 2022), InCoder (Fried et al., 2022), and our prior work on\nSantaCoder (Ben Allal et al., 2023). FIM opens up the possibility of a variety of tasks that go beyond\nleft-to-right code completion. We evaluate StarCoderBase on four established FIM benchmarks below.\nSingle-Line Infilling for Python, Java, and JavaScript\nFried et al. (2022) present a single-line fill-in-\nthe-middle task for Python that masks one line of code from a HumanEval solution and scores the model’s\nability to complete the function. They turn every HumanEval solution into several fill-in-the-middle problems\nby masking each non-blank, non-comment line of code in the solution body into a fill-in-the-middle task.\nBen Allal et al. (2023) generalizes this benchmark to also support Java and JavaScript, using model-generated\nsolutions from MultiPL-E’s translations. We compare the performance of StarCoderBase, SantaCoder, and\nInCoder on this task, evaluating using line exact match (Table 17). StarCoderBase significantly outperforms\nthe two smaller models.\nPython Return Type Prediction\nPradel et al. (2020) introduce methods and datasets for evaluating\nPython type annotations. Fried et al. (2022) adapt and filter one dataset from this work, consisting of Python\nfunctions from GitHub, and use it to evaluate infilling models on function return type prediction. We use\nthis dataset to compare StarCoder, StarCoderBase, and SantaCoder to InCoder on function return type\nprediction. Our setup follows Fried et al. (2022): each model uses greedy generation to infill return types\nwhile conditioning on the imports, body, and signature for each function. We report exact match accuracy on\nnormalized annotations for all functions in the evaluation set and only those with non-None annotations,\nfollowing Fried et al. (2022). We find that StarCoder and StarCoderBase outperform existing approaches at\nPython return type prediction (Table 18). However, we note that as the functions in this evaluation set were\ntaken from GitHub repositories, they may overlap with the training data for SantaCoder and the StarCoder\nmodels.\nTypeScript Type Prediction\nYee & Guha (2023) evaluate approaches to neural type prediction for\nTypeScript. However, instead of measuring accuracy, they argue that benchmarks should measure how many\nprojects or files do not have type errors with predicted types. This approach makes it possible to evaluate\ntype prediction for JavaScript programs that have never been translated to TypeScript, which reduces the\nlikelihood of dataset contamination. We add StarCoderBase to their evaluation framework and compare it to\nInCoder, which performs best at type prediction in the original work. Table 19 shows that StarCoderBase\noutperforms InCoder: (1) it produces more packages that type check, (2) across all packages, it produces\nmore files that type check, and (3) it produces fewer trivial type annotations than InCoder.\n23\n\nPublished in Transactions on Machine Learning Research (12/2023)\nPackages type check\nFiles with no errors\nTrivial annotations\n✓\nTotal\n%\n✓\nTotal\n%\n✓\nTotal\n%\nInCoder\n30\n128\n23.4\n571\n760\n75.1\n56\n117\n47.9\nStarCoderBase\n49\n128\n38.3\n593\n760\n78.0\n135\n299\n45.2\nTable 19: TypeScript type prediction performance using the dataset and metholody from Yee & Guha (2023).\nWe only evaluate JavaScript packages that have never been translated to TypeScript and compare StarCoder\nto InCoder, the best-performing model by Yee & Guha (2023). StarCoder outperforms InCoder in several\nways.\nModel\nBLEU\nInCoder-6B\n18.27\nSantaCoder\n19.74\nStarCoderBase\n21.38\nStarCoder\n21.99\nTable 20: Performance on the Python portion of the CodeXGLUE Code Summarization task, evaluating\nfunction docstring generation. Models are evaluated zero-shot using their infilling capability.\nPython Docstring Generation\nTo evaluate models’ ability to generate documentation for functions, we\nuse the Python subset of the CodeXGLUE code summarization benchmark (Lu et al., 2021). This benchmark\nis constructed from the CodeSearchNet dataset (Husain et al., 2019), containing functions from public GitHub\nrepositories. Models infill the documentation string (docstring) for each function using greedy decoding,\nconditioned on the function signature and body. We follow the evaluation scheme of past work: docstrings\nare evaluated using smoothed 4-gram BLEU (Papineni et al., 2002) against the reference docstring from\nthe original function, using only the first lines of the generated and reference docstrings (removing, e.g.,\ndescriptions of function arguments and return types that may appear in later lines). In Table 20, we see that\nStarCoder and StarCoderBase obtain higher performance than past work on docstring generation. However,\nwe note that there may be an overlap between this evaluation dataset and the data used to train SantaCoder\nand the StarCoder models.\n6.3\nPerformance Improvement Through the Training Process\nWe evaluate the performance of StarCoderBase at several training checkpoints after every 200B tokens\nseen out of the total 1000B. Figure 2 (right) shows how performance (pass@1) changes during training\nfor each programming language supported by MultiPL-E. The performance curve for several high-resource\nprogramming languages suggests that training longer is likely to improve their performance further.\nHowever, some of the low-resource languages see limited improvement during training or even have a pass@1\ndecline. For example, R’s pass@1 rate drops significantly between the 800B and 1000B (final) checkpoints.\nThe dependence of pass@1 on data size (Figure 2, left) further supports the hypothesis that this is related to\nthe amount of data available. The slope of the linear fit increases between 800B and 1000B checkpoints while\nthe intercept decreases, i.e., performance improves only for languages with large enough amounts of data\n(≳1 GB).\nWe manually inspected the completions generated by R over several checkpoints to better understand model\nperformance. One might hypothesize that some problems are harder than others, and so the model gains and\nloses the ability to solve them in R over the 600B, 800B, and 1000B checkpoints, but we find that this is not\nthe case. Instead, we find significant variance in per-problem success rates for several problems (Table D.3).\nFor these problems, the pass rate between different checkpoints varies in what appears to be a completely\nuncorrelated manner. Moreover, manual inspection shows that the failures are caused by minor mistakes,\n24\n\nPublished in Transactions on Machine Learning Research (12/2023)\n10−1\n100\n101\n102\nSize after dedup, GB\n0\n5\n10\n15\n20\n25\n30\n35\nPass@1 (%)\n200B\n400B\n600B\n800B\n1000B\n200B\n400B\n600B\n800B\n1000B\nTraining tokens\ncpp\nc-sharp\nd\ngo\njava\njulia\njavascript\nlua\nphp\nperl\npython\nr\nruby\nracket\nrust\nscala\nbash\nswift\ntypescript\nFigure 2: Performance (pass@1) of StarCoderBase at several training checkpoints by data size (left) and by\nprogramming language (right). The lines in the left plot are a linear fit between pass@1 and log-dataset-size\nfor all the points except the leftmost one, where we expect the linear dependence to break due to transfer\nlearning (dashed line). The goodness of fit ranges between R2 = 0.399 for the 600B checkpoint to R2 = 0.510\nfor the 1000B checkpoint.\ne.g., not taking the absolute value when computing GCD, not converting a string to a character array, or not\nchecking edge cases.\n6.4\nPerplexity With Long Contexts\nStarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code\nfiles. To evaluate the ability of the model to benefit from this larger context, we compare its perplexity (Bahl\net al., 1983) when using a full window size of 8K tokens versus a window size of 2K tokens (as used in many\nprior code models).\nTo ensure no overlap between the training data for StarCoderBase and the perplexity computation data, we\ndownloaded 10 GNU Public License (GPL) repositories from GitHub in each of the languages in Table 21.\nWe compiled all files from the repositories into a single document for each language. We then divided these\ndocuments into 8K token chunks and computed perplexity on the last 1K tokens in each chunk14 in two\nconditions: (1) the model window only contains the final 2K tokens in the chunk (i.e., the 1K being predicted\nand the previous 1K), and (2) the model window contains all 8K tokens in the chunk (i.e., the 1K tokens\nbeing predicted and the previous 7K). This evaluates the ability of the model to benefit from additional\nfile- and repo-level context when predicting code. In Table 21, we report the average perplexity of the 1K\ntoken regions across all chunks. We see that StarCoderBase indeed benefits from the extra token conditioning\nafforded by its 8K context window, with substantially lower perplexities across all languages.\n7\nNatural Language Evaluation\nAlthough the StarCoder models are principally developed to be Code LLMs, they have also been trained\non a significant amount of natural language text. Roughly 20% of its training tokens are natural language\ndata: 7% GitHub issues, 10% Markdown, 2% Jupyter notebooks, and 4% HTML. In this section, we evaluate\n14We evaluate perplexity on the final 1K tokens in each 8K chunk so that both conditions have the same evaluation tokens,\nand to avoid overly penalizing the 2K condition, as tokens at the beginning of a window tend to have higher perplexity as there\nis less context available to predict them.\n25\n\nPublished in Transactions on Machine Learning Research (12/2023)\nWindow Size\nLanguage\ncpp\nc-sharp\nc\ngo\njava\njavascript\nphp\nr\nruby\nrust\n2K tokens\n2.01\n1.90\n1.71\n1.35\n1.65\n1.98\n1.73\n1.72\n2.16\n1.84\n8K tokens\n1.79\n1.66\n1.61\n1.21\n1.54\n1.68\n1.43\n1.48\n2.02\n1.65\nTable 21: Perplexity of StarCoderBase on evaluation regions (of size 1K tokens) when using a window size\nof 2K or 8K tokens across repositories from 10 languages. The larger window size substantially reduces\nperplexity, demonstrating a benefit of StarCoder’s 8K token window.\nModel\nSize\nGSM8K CoT\n+maj1@100\nGSM8K PAL\n+maj1@40\nStarCoderBase\n15.5B\n8.4\n—\n21.5\n31.2\nCodeGen-Multi\n16B\n3.18\n—\n8.6\n15.2\nCodeGen-Mono\n16B\n2.6\n—\n13.1\n22.4\n7B\n11.0\n18.1\n10.5\n16.8\n13B\n17.8\n29.3\n16.9\n28.5\nLLaMA\n33B\n35.6\n53.1\n38.7\n50.3\n65B\n50.9\n69.7\n—\n—\nTable 22: 8-shot accuracy on the GSM8K math-reasoning benchmark. Samples are generated with greedy\ndecoding. maj1@k denotes a majority vote over k generations. For the majority vote, we instead generate\nsamples using nucleus sampling with p = 0.95 and temperature 0.7, following Gao et al. (2022). We use\n“—” when a model was not evaluated on a given metric, or the metric is not supported in Language Model\nEvaluation Harness. The LLaMA CoT numbers are from Touvron et al. (2023).\nStarCoderBase on several natural language tasks: natural language reasoning and understanding tasks that\nmight benefit from the combination of code and text training data; and natural language generation tasks\nthat evaluate the model’s tendencies to produce undesirable text outputs, e.g., in a documentation generation\nor interactive assistant setting.\n7.1\nMath Reasoning\nRecent work has shown that Code LLMs can be effective arithmetic and symbolic reasoners by using a\ntechnique called Program-Aided Language models (PAL; Gao et al., 2022). With PAL, the LLM reads\nthe reasoning problem and generates Python programs as the intermediate reasoning steps, which are then\nexecuted by the Python interpreter to produce the answer. In contrast, the Chain-of-Thought method (CoT;\nWei et al., 2022) prompts the LLM to produce the reasoning steps in natural language before generating the\nanswer.\nWe investigate the reasoning capabilities of StarCoderBase on GSM8K (Cobbe et al., 2021), a set of middle-\nschool math word problems. We compare with the two CodeGen-16B models (Nijkamp et al., 2023) and the\nfamily of LLaMA models (Touvron et al., 2023). The results of our evaluation are presented in Table 22,\nwhere we provide both CoT and PAL results for StarCoderBase and LLaMA.\nIn line with previous results comparing PAL to CoT on Code LLMs (Gao et al., 2022), we find that StarCoder-\nBase performs better with PAL (21.5%) than with CoT (8.4%). StarCoderBase substantially outperforms\nCodeGen-16B-Mono and CodeGen-16B-Multi, which achieve 13.1% and 8.6% with PAL, respectively. These\ndifferences carry over to the setting where majority voting is applied. The difference between CoT and PAL\nis much smaller for the LLaMA models, although we observe that CoT performs slightly better for the 7B\nand 13B LLaMA models. Interestingly, we find that StarCoderBase outperforms LLaMA-13B (17.8%) on\nthis reasoning benchmark. However, its performance still lags behind LLaMA-33B (38.7%).\n26\n\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nSize\nMMLU 5-shot\nacc, %\nCodeGen-Multi\n16B\n27.8\nGPT-NeoX\n20B\n32.9\nStarCoder\n15.5B\n33.9\nStarCoderBase\n15.5B\n34.2\nLLaMA\n7B\n35.1\nLLaMA\n13B\n46.9\nTable 23: 5-shot accuracy on the MMLU language understanding benchmark.\nModel\nSize\nCoQA zero-shot\nF1 score\nCodeGen-Multi\n16B\n0.59\nStarCoderBase\n15.5B\n0.67\nStarCoder\n15.5B\n0.67\nLLaMA\n7B\n0.71\nLLaMA\n13B\n0.73\nGPT-NeoX\n20B\n0.73\nTable 24: Zero-shot accuracy on the CoQA question answering challenge.\n7.2\nWorld Knowledge and Reading Comprehension\nMMLU (Hendrycks et al., 2020) is a massive multitask language understanding benchmark, covering multiple-\nchoice questions in 57 knowledge domains, including the humanities, STEM, and social sciences. CoQA (Reddy\net al., 2019) is a large-scale dataset for Conversational Question Answering systems, measuring the model’s\nability to process a text passage and answer a series of interconnected questions. We compare StarCoderBase\nand StarCoder with CodeGen-16B-Multi (Nijkamp et al., 2023), GPT-NeoX (Black et al., 2022), LLaMA-7B,\nand LLaMA-13B (Touvron et al., 2023).\nWe present the 5-shot accuracy for MMLU in Table 23, and the zero-shot F1 scores for CoQA in Table 24. On\nMMLU, StarCoderBase outperforms CodeGen-16B-Multi significantly (34.2% to 27.8%), and even outperforms\nGPT-NeoX by a small margin (32.9%). Nevertheless, both LLaMA models outperform StarCoderBase. On\nCoQA, StarCoderBase performs better than CodeGen-16B-Multi but is outperformed by LLaMA and\nGPT-NeoX.\n7.3\nMeasuring Harmful Generation\nWhen generating open-ended text such as code documentation or technical dialogue, a Code LLM (similarly\nto text-only LLMs) might produce harmful outputs. We compare StarCoderBase to previous Code LLMs on\nbenchmarks that measure social bias and toxicity in model-produced text.15\n7.3.1\nSocial Bias\nRecent work has highlighted that LLMs often capture social biases and stereotypes from their pre-training\ncorpora (Kurita et al., 2019; May et al., 2019; Hutchinson et al., 2020; Meade et al., 2023). To quantify social\nbias within our model, we use StereoSet (Nadeem et al., 2021).\nStereoSet consists of a collection of fill-in-the-blank-style tests for measuring social biases within language\nmodels.16 Each example in StereoSet consists of an incomplete sentence (e.g., our housekeeper is BLANK)\n15Code for the evaluations is available here: https://github.com/McGill-NLP/StarCoderSafetyEval\n16We only evaluate against the intrasentence task in this work.\n27\n\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nStereotype Score\nLanguage Model Score\nICAT Score\nGender\nLLaMA-13B\n66.54\n88.09\n58.95\nCodeGen-Multi-16B\n67.34\n86.41\n56.44\nStarCoderBase\n58.76\n86.82\n71.60\nProfession\nLLaMA-13B\n60.95\n86.74\n67.74\nCodeGen-Multi-16B\n60.67\n85.67\n67.38\nStarCoderBase\n53.24\n84.70\n79.21\nRace\nLLaMA-13B\n64.94\n87.97\n61.68\nCodeGen-Multi-16B\n60.58\n88.60\n69.85\nStarCoderBase\n56.48\n86.82\n75.58\nReligion\nLLaMA-13B\n57.95\n90.26\n75.91\nCodeGen-Multi-16B\n56.16\n88.91\n77.96\nStarCoderBase\n55.69\n90.67\n80.36\nOverall\nLLaMA-13B\n63.40\n87.62\n64.14\nCodeGen-Multi-16B\n61.29\n87.25\n67.55\nStarCoderBase\n55.53\n86.18\n76.65\nTable 25: StereoSet intrasentence results for gender, professional, racial, and religious bias. Stereotype scores\nclose to 50% are best. Language modeling scores and ICAT scores close to 100% are best.\nalongside three possible completions. Of these completions, one is stereotypical (e.g., Mexican), another is\nanti-stereotypical (e.g., Italian) and a third is unrelated (e.g., computer). StereoSet defines three metrics: a\nstereotype score, a language modeling score, and an ICAT score. The stereotype score is the percentage of\nexamples for which a model prefers the stereotypical completion for a sentence over the anti-stereotypical\ncompletion. The language modeling score is the percentage of examples for which a model prefers a meaningful\ncompletion (stereotype or anti-stereotype) over an unrelated completion. Finally, Nadeem et al. (2021) define\nan idealized context association test (ICAT) score that combines these two metrics:\nICAT = lms · min(ss, 100 −ss)\n50\n(1)\nwhere lms and ss denote the language model score and stereotype score, respectively.\nWe report StereoSet results for StarCoderBase, alongside LLaMA-13B and CodeGen-Multi-16B, in Table 25.\nAcross all four bias domains, we find StarCoderBase obtains the lowest stereotype scores, but also has\ncompetitive language modeling scores. This suggests that StarCoderBase’s lower stereotype scores are not\nsimply due to worse language modeling (Meade et al., 2022), and also as indicated by the high ICAT score.\nWe also evaluate StarCoderBase against Crowdsourced Stereotype Pairs (CrowS-Pairs; Nangia et al. 2020)\nand refer readers to Table D.4 for results.\n7.3.2\nToxicity\nTo evaluate toxicity in responses generated from our model, we use RealToxicityPrompts (Gehman et al.,\n2020), a collection of sentence-level prompts that often elicit undesirable responses from language models.\nWe generate responses to 10K examples from RealToxicityPrompts using StarCoderBase with a minimum\n28\n\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nClassifier\nWord List\nLLaMA-13B\n0.74\n1.43\nCodeGen-Multi-16B\n0.21\n0.82\nStarCoderBase\n0.42\n1.12\nTable 26: RealToxicityPrompts response toxicity results. We report the percentage of responses flagged as\ntoxic using a toxicity classifier and an offensive word list. Lower scores are indicative of less toxic generations.\nModel\nSize\nOpen\nAccess\nSynth.\nReason.\n(AS)\nSynth.\nReason.\n(NL)\nbAbI\nDyck\nGSM8K\nMATH\nMATH\n(CoT)\nLSAT\nLegal\nSupport\ncode-davinci-002\n175B\n54.0\n68.4\n68.6\n80.5\n56.8\n41.0\n43.3\n—\n—\ntext-davinci-003\n175B\n50.2\n73.4\n65.3\n75.1\n50.6\n39.0\n44.9\n23.3\n62.2\nLuminous Supreme\n70B\n31.2\n—\n50.4\n72.9\n11.2\n14.9\n5.7\n21.2\n53.0\nStarCoderBase\n15.5B\n✓\n44.0\n21.0\n50.4\n85.4\n8.4\n15.1\n7.0\n19.0\n53.2\nCohere Command\nBeta\n52.4B\n24.3\n24.5\n47.3\n42.1\n13.8\n13.3\n7.5\n22.9\n60.6\nJ1-Jumbo v1\n178B\n26.3\n17.4\n54.3\n44.5\n5.4\n8.9\n3.3\n23.2\n48.4\nJ1-Grande v2 beta\n17B\n28.6\n13.9\n47.0\n61.7\n9.6\n12.7\n6.8\n19.1\n56.2\ncode-cushman-001\n12B\n34.1\n16.4\n48.1\n45.1\n4.9\n9.9\n7.2\n—\n—\nOPT\n175B\n✓\n22.5\n24.8\n50.7\n49.4\n4.0\n6.5\n2.6\n22.0\n53.2\nGPT-NeoX\n20B\n✓\n20.4\n16.7\n46.8\n74.7\n5.3\n14.1\n7.1\n19.1\n51.5\nBLOOM\n176B\n✓\n30.4\n19.7\n44.7\n54.5\n9.5\n4.3\n5.5\n20.9\n54.3\nGLM\n130B\n✓\n25.2\n25.4\n44.3\n54.9\n6.1\n0\n5.9\n19.3\n45.1\nUL2\n20B\n✓\n20.5\n21.7\n50.1\n14.0\n2.4\n0\n0\n20.7\n50.6\nOPT\n66B\n✓\n19.3\n21.3\n40.8\n47.1\n1.8\n4.8\n2.9\n17.5\n52.7\nYaLM\n100B\n✓\n5.6\n6.1\n34.6\n63.3\n0\n0\n0\n2.3\n48.4\nT5\n11B\n✓\n19.6\n10.1\n41.2\n34.7\n2.3\n0\n0\n15.9\n55.8\nTable 27: Model results on natural language reasoning tasks in the HELM benchmark, with models ordered\nby their average rank on the tasks. We use “—” when a model was not evaluated on a given metric, or has\nruntime errors logged in HELM (e.g., “unmapped prediction” for the code-davinci-002 and code-cushman-001\nmodels on LSAT and Legal Support). StarCoder generally substantially outperforms other open-access\nmodels, and often outperforms much larger models.\nlength of one token and a maximum length of 128 tokens. We use nucleus sampling (Holtzman et al., 2020)\nwith p = 0.95 to generate all of our responses.\nWe use two methods for automatically evaluating toxicity in responses: (i) a RoBERTa-based (Liu et al.,\n2019) toxicity classifier (Vidgen et al., 2021) and (ii) a list of potentially offensive words.17 For the toxicity\ndetector, we report the percentage of responses flagged toxic using a threshold of 0.5. For the offensive word\nlist, we report the percentage of responses which contain an offensive word. We note that while the offensive\nword list can potentially falsely flag responses, it may provide a crude measure of blatant toxicity. We report\nour results in Table 26.\nIn general, we observe that CodeGen-16B-Multi and StarCoderBase both appear to generate less toxic\nresponses than LLaMA-13B. For instance, 1.43% of LLaMA-13B’s responses contain potentially offensive\ntokens compared to the 1.12% of StarCoderBase. We also note that CodeGen-16B-Multi appears to generate\nless toxic responses than StarCoderBase.\n7.4\nReasoning Tasks in HELM\nWe evaluate StarCoderBase with HELM (Liang et al., 2022), an evaluation suite aiming to increase the\ntransparency of LLMs by reporting their performance on a wide range of tasks. We evaluate the ability of\nthe model to leverage its natural language and code pretraining for natural language reasoning tasks from\nHELM (excluding code tasks, because of our own extensive code evaluations). At the time of writing, the\nHELM benchmark does not include the CodeGen, CodeGeex, and LLaMA models. Therefore, we compare\n17https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n29\n\nPublished in Transactions on Machine Learning Research (12/2023)\nStarCoderBase with the largest and/or most recent model from each family of “limited” or “open” access\nmodels, as classified on the HELM model list,18 that had been evaluated on a majority of these HELM\nreasoning tasks as of May 1, 2023. In Table 27 we report the results. We compute each model’s ranking\non each task, and order models in the table by their average ranking across tasks. StarCoderBase generally\nobtains substantially stronger performance than all other models with released weights and often performs\ncomparably to or better than much larger models. We speculate that the mixture of code and natural\nlanguage in the training data contributes to the model’s strong performance on these reasoning tasks.\n8\nQualitative Evaluation\nIn Appendix E, we highlight several interesting interactions we had with StarCoderBase. We hope these\nserve as a starting point for researchers and developers interested in further exploring the model’s capabilities.\nWe provide examples of how to elicit interesting model behavior using the templates for Git commits, GitHub\nissues, and Jupyter notebooks in Section E.1. In Section E.2, we demonstrate how to prompt StarCoder to act\nas a technical assistant without any instruction-tuning. In Section E.3 we find that it is also possible to prompt\nthe model using a combination of meta-data and natural language to obtain higher pass@1 performance on\nthe HumanEval benchmark.\n9\nAttribution Tools\nAs generative language tools become more ubiquitous and data-intensive, the need to understand and inspect\nthe massive amounts of text they were trained on becomes more pressing, both to understand the failure\nmodes of models as well as provide transparent data governance feedback in the form of attribution tracing and\nprovenance management of a model’s generated output. This pressing need for understanding data (Mitchell\net al., 2022) is being increasingly recognized and operationalized in the form of dataset inspection tools and\ntoolkits (Akiki et al., 2023; Marone & Van Durme, 2023; Piktus et al., 2023). It is from this vantage point\nthat we are releasing two such data inspection tools: a membership-checking tool and a BM25 search index.\nThese complement the existing “Am I in The Stack” tool which operates at the level of GitHub repository\nnames. The two new tools index only the files used for training and allow for matches on file content. These\ntools are available as standalone sites but are also integrated into our VSCode demo. This helps users identify\nparts of the model output that may have been copied from the training data. By utilizing the search index,\nusers can locate the corresponding source file and repository of the copied snippets.\n9.1\nMembership Checking\nMarone & Van Durme (2023) propose documenting datasets with membership testing artifacts deemed Data\nPortraits. They provide one specific implementation, based on Bloom Filters (Bloom, 1970), that offers\nfast and lightweight membership inference. We build a Bloom-filter-based portrait on strings of length 50\ncharacters from the training data. This artifact takes 26 GB, ∼3% of the data size. The inference tool is\nhosted publicly to complement other documentation artifacts. 19\nGenerations from the model can be quickly checked to approximately assess the degree of overlap with\nthe training corpus. The VSCode extension supports using this as a rapid, first-pass attribution method.\nHowever, this requires that matching strings are longer than a minimum size and does not attempt to filter\ncommon or generic code snippets. After the first pass check, users can use the full search index to further\nassess attribution.\n9.2\nSearch Index\nWe index the training dataset using Elasticsearch 7.1720 and provide two search tools to query it: one focused\non the Python subset and one covering the entire dataset. The code itself is preprocessed using a lowercase\n18https://crfm.stanford.edu/helm/latest/?models=1\n19http://stack.dataportraits.org/\n20https://www.elastic.co/guide/en/elasticsearch/reference/7.17\n30\n\nPublished in Transactions on Machine Learning Research (12/2023)\nfilter and Lucene’s ASCIIFoldingFilter, tokenized using a 3-gram tokenizer, and indexed using the default\nLucene implementation of BM25 as a similarity function. We further index the username and license fields as\nkeyword fields allowing for easy filtering and lookup based on these specific metadata fields. Both indexes are\ncurrently running in single-node mode on one virtual machine.\n10\nSocial Impact and Limitations\n10.1\nProject approach\nOpen-science and open-governance\nStarCoder is an output of a community research project. The\nproject is conducted in the spirit of Open Science (Woelfle et al., 2011), focused on the responsible development\nand use of Code LLMs. Through open-governance practices conducted throughout the project, priority in\ndecision-making has always yielded to the more responsible option even if this meant introducing limitations\nthat might impact adoption or future research. For example, the Legal, Ethics, Governance Working Group\ndecided to remove and not release a dataset of identified malicious code, even though this data might be\nuseful for future security research.\nOpenness and safety risks\nSolaiman (2023) explains how the degree of openness in the LLM development\nprocess is connected to the potential risks associated with a model release. When systems are developed in a\nfully closed manner, it is more likely for power to become concentrated among high-resourced organizations,\nand the small development team may not fully comprehend the impact and long-term consequences of the\nmodel being deployed. In addition, closed-development systems are often less auditable by external experts\nand can impede scientific progress since researchers cannot build upon each other’s work. On the other hand,\nfully open development allows for community research, democratizes access to the models, and enables audits\nthroughout the whole development process. However, without appropriate guardrails, open LLM development\nposes a higher risk of misuse, as increased model access also increases the likelihood of harm caused by the\nmodel. Even though a released API can be shut down, once the model weights are released, it is nearly\nimpossible to retract them. Discussing and implementing responsible AI practices has, therefore, been front\nand center during the development of our project’s LLMs.\n10.2\nLimitations\nDataset and data licensing\nStarCoder was trained on a subset of The Stack v1.2 dataset. This dataset\nhas been filtered using a license detector to only include permissively licensed source code. Nevertheless, the\nlicense detector might have incorrectly classified a number of repositories. See Kocetkov et al. (2022) for\nmore details on this license detection process.\nOpt-out process\nAlthough The Stack offers a way to remove developer code, its opt-out process only\napplies to individual repositories and could benefit from further enhancements. For example, when code\nis licensed under a permissive or copy-left license, it can be duplicated to another repository, making it\nchallenging to eliminate such copies if the copyright owner chooses to opt out. More work is necessary to\ncreate better data control and consent mechanisms for large-scale training sets of LLMs.\nPII detection\nDespite our best efforts to remove PII (Section 4), StarCoder may still produce PII (however,\nnote that the model license restricts use that aims to generate or disseminate PII with the purpose of harming\nothers). As mentioned in Section 4.2, we trained an encoder-only model to detect PII for both code- and\ntext-related tasks and noted that there is a possibility of false positives and negatives, which could lead to\nunintended consequences when processing sensitive data. Moreover, the PII detection model’s performance\nmay vary across different data types and programming languages, necessitating further validation and fine-\ntuning for specific use cases. The PII annotations are only available to approved individuals, and researchers\nand developers who are granted access are expected to uphold ethical standards and data protection measures.\nBy making it accessible, our aim is to encourage further research and development of PII redaction technology.\n31\n\nPublished in Transactions on Machine Learning Research (12/2023)\nMalicious code\nOn the Hugging Face platform, where the Stack is hosted, a malicious code detection\ntool identified 654 files as unsafe. With the help of our community, we removed these files ahead of the\nrelease of The Stack v1.2. Nevertheless, The Stack may contain undetected malicious code, and StarCoder\nmight be able to generate malware. The StarCoder OpenRAIL-M license, therefore, includes a use restriction\nagainst generating and/or disseminating malware (including — but not limited to — ransomware) or any\nother content that can be used to harm electronic systems.\nModel limitations\nStarCoder is subject to typical limitations of LLMs, including the potential to generate\ncontent that is inaccurate, offensive, misleading, discriminatory towards age or gender, or reinforces other\nstereotypes. Please refer to Section 7.3 for an investigation into such safety concerns. Deployments of\nStarCoder need to further challenge and adapt the model to prevent such behavior, e.g., through red-\nteaming (Perez et al., 2022), adversarial testing (Wan et al., 2023), and/or by adding a robust safety\nlayer (OpenAI, 2023b). The model is released with an OpenRAIL-M license that places enforceable use\nrestrictions that apply to the model and its modifications, and to applications using the model.\nEnglish-only evaluations\nWe evaluated the performance of StarCoder solely on English-based benchmarks\nto understand its coding capabilities and natural language understanding. To make these models more\naccessible to a wider audience, future research should investigate the performance and limitations of Code\nLLMs on other natural languages.\nCode attribution tools\nThe StarCoder membership-checking tool and BM25 search index are limited\nto dataset inspection against the subset of The Stack that was used for training and, as such, will not\nfind matches to code that was not included or that was removed from the dataset for this project. The\nPortraits-based membership testing tool uses hash matching and thus may have false positives. It also has a\nminimum resolution and requires a certain amount of context to trigger a match. Both attribution tools do\nnot attempt to distinguish between generic code (e.g., boilerplate) or protected content. However, we hope\nthat these tools will support ongoing research on the responsible development of LLMs.\n10.3\nSocial impact\nCode LLMs\nWe expect Code LLMs to enable people from diverse backgrounds to learn to write higher-\nquality code and develop low-code applications (Leinonen et al., 2023). Mission-critical software could become\neasier to maintain as professional developers are guided by code-generating systems on how to write more\nrobust and efficient code. However, the security implications should also be carefully considered (Sandoval\net al., 2023). While the social impact is intended to be positive, the increased accessibility of Code LLMs\ncomes with certain risks such as over-reliance on the generated code and long-term effects on the software\ndevelopment job market. We refer the reader to Chen et al. (2021, Section 7) for a broader impact analysis\nof Code LLMs, as well as Khlaaf et al. (2022) for an in-depth risk assessment and hazard analysis of this\nemerging technology.\nData annotation\nIt was important for the project to only use reputable data annotation services. It\nwas also important to balance the constraints of costs (fair compensation), time (the timing and time to\ncomplete the work were on the critical path for the project), and quality (to ensure that PII Detection\nModel training was not impacted). While traditional data annotation services using salaried employees were\nconsidered, the decision to work with Toloka crowd-workers was taken after a review of service providers and\ntheir compensation practices — most would not provide sufficient transparency and guarantees about worker\ncompensation. Our determination of compensation took into consideration different minimum wage rates\nacross countries and their corresponding purchasing power. We limited annotation eligibility to countries\nwhere the hourly pay rate of $7.30 was equivalent to the highest minimum wage in the US ($16.50) in terms\nof purchasing power parity.\nFeedback opt-out form\nDuring the first stage of the opt-out process, individuals were asked to specify\nthe reasons for wanting their code to be excluded from the dataset. The recurring concerns we heard from\nthe individual who wished to opt out are:\n32\n\nPublished in Transactions on Machine Learning Research (12/2023)\n• Preference for an opt-in approach instead of opt-out.\n• Perception that it is unfair to use their code without compensation\n• Concerns about the current limitations of AI and the potential for model generations to be traced\nback to their work, resulting in potential legal liability.\n• Belief that their code is of poor quality and unsuitable for AI training.\n• Presence of PII in their code, which they do not wish to be publicly exposed.\nThe opt-out form thus provided an opportunity to directly engage with content creators and learn about the\nimpact of our work on them.\nCommunity feedback on opt-out process\nWe conducted community research with individuals at\nspecific organizations whose data is used in The Stack (The Alan Turing Institute and The Turing Way) and\ncontributed to two open, international workshops (Open Data Day 2023 and Mozilla Festival 2023 with a\nsession titled ‘Designing for Data Rights in the AI Production Pipeline’). These qualitative interviews and\nparticipatory co-design workshops included 50 participants, primarily from North America and Europe, with\nroles including research scientist, community manager, software engineer, and principal investigator (PI).\nThe outcomes from the community research can be summarized as follows: when it comes to governance of\nLLM datasets, participants feel that it is both better to know and better to have a choice. Most participants\nhad neutral to positive feelings about their permissively licensed data being used to train LLMs. While all\nhad positive impressions of the “Am I in The Stack” tool, not one interviewed expressed a desire to actually\nopt out. The main takeaway seemed to be that participants found the most value in the project’s governance\ntools for their ability to raise awareness of data practices and to empower individuals and communities to\ntake action based on their specific needs. These initial conversations also highlighted the importance of\nbringing governance discussions and decisions directly to impacted communities, an important direction of\nfuture work that should extend community research beyond North America and Europe. Participants in the\nworkshops also raised examples of new groups to center in data rights considerations, including artists, data\nminers, and future generations. The co-created outputs can be viewed on this MozFest Miro Board.\n11\nConclusion\nIn this technical report, we described the efforts of the BigCode community in creating StarCoderBase and\nStarCoder, open-access 15.5B parameter large language models trained on code. We provided full transparency\non all aspects of the research and development process, including the training data, the data curation process,\nthe PII redaction pipeline, and the model training. We conducted the most extensive evaluation of Code\nLLMs to date, finding that StarCoder outperforms other Code LLMs like CodeGen (Nijkamp et al., 2023)\nand CodeGeeX (Zheng et al., 2023), and matches or outperforms the closed-access code-cushman-001 model\nfrom OpenAI. By releasing the StarCoder models with an Open Responsible AI Model license, and by open-\nsourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility,\nand transparency of Code LLMs in the research and developer communities. The model license includes\nuse restrictions to ensure that modifications of the model and applications using the model adhere to our\nprinciples of responsible AI. In addition, we released a novel set of attribution tools to help end-users of\nCode LLMs to detect and locate model generations that may have been copied from the training set. We\nhope these measures contribute towards a safe model release, ensuring that the strong-performing StarCoder\nmodels remain a force for good.\nAcknowledgements\nWe would thank Hugging Face for providing the compute resources to train the\nStarCoder models. We also thank Suriya Gunasekar for help with the data inspection, and Sebastien\nPaquet for proofreading this work. Carolyn Jane Anderson, Arjun Guha, Ming-Ho Yee, and Yangtian Zi\nand are supported by U.S. National Science Foundation awards SES-2326174 and CCF-2102288. Evgenii\nZheltonozhskii is supported by the Adams Fellowships Program of the Israel Academy of Sciences and\nHumanities.\n33\n\nPublished in Transactions on Machine Learning Research (12/2023)\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program un-\nderstanding and generation. In Proceedings of NAACL, 2021. URL https://aclanthology.org/2021.naacl-\nmain.211. (cited on p. 4)\nChristopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gallé, Thomas Wolf, Suzana Ilic, and Yacine\nJernite. BigScience: a case study in the social construction of a multilingual large language model. CoRR,\nabs/2212.04960, 2022. doi: 10.48550/arXiv.2212.04960. URL https://doi.org/10.48550/arXiv.2212.04960.\n(cited on p. 2)\nChristopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin,\nand Martin Potthast. Spacerini: Plug-and-play search engines with Pyserini and Hugging Face. CoRR,\nabs/2302.14534, 2023. doi: 10.48550/arXiv.2302.14534. URL https://doi.org/10.48550/arXiv.2302.14534.\n(cited on p. 30)\nAndersen et al v. Stability AI et al. 3:23-cv-00201 N.D. Cal. 2023. (cited on p. 2)\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson\nKernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,\nChris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861, 2021. (cited on p. 48)\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021. (cited on pp. 3, 4, 17, 18, and 21)\nLalit Bahl, Frederick Jelinek, and Robert Mercer. A maximum likelihood approach to continuous speech\nrecognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, PAMI-5:179 – 190, 04\n1983. doi: 10.1109/TPAMI.1983.4767370. (cited on p. 25)\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and\nMark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255,\n2022. doi: 10.48550/ARXIV.2207.14255. URL https://arxiv.org/abs/2207.14255. (cited on pp. 3, 15, 16,\nand 23)\nBBC. ChatGPT accessible again in Italy. https://www.bbc.com/news/technology-65431914, 2023. (cited on\np. 2)\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro Von Werra. A\nframework for the evaluation of code generation models. https://github.com/bigcode-project/bigcode-\nevaluation-harness, December 2022. (cited on pp. 3 and 17)\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis,\nNiklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane\nAnderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel\nRomero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi\nBhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky,\nHuu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes,\nDaniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: don’t reach for the stars!\nIn Deep Learning for Code Workshop (DL4C), 2023. (cited on pp. 4, 9, 11, 12, 13, 14, 16, and 23)\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\nA neural probabilistic language model.\nIn\nT. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems,\nvolume 13. MIT Press, 2000.\nURL https://proceedings.neurips.cc/paper_files/paper/2000/hash/\n728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html. (cited on p. 4)\n34\n\nPublished in Transactions on Machine Learning Research (12/2023)\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and\nscaling. arXiv preprint arXiv:2304.01373, 2023. (cited on p. 2)\nBigScience Workshop. BLOOM (revision 4ab0472), 2022. URL https://huggingface.co/bigscience/bloom.\n(cited on p. 2)\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\nLeahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria\nReynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: an open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022. (cited on pp. 2, 4, and 27)\nBurton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426,\njul 1970. ISSN 0001-0782. doi: 10.1145/362686.362692. URL https://doi.org/10.1145/362686.362692.\n(cited on p. 30)\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258,\n2021. URL https://arxiv.org/abs/2108.07258. (cited on p. 2)\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. Large language models in\nmachine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 858–867, Prague, Czech\nRepublic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/D07-\n1090. (cited on p. 3)\nAndrei Z. Broder. Identifying and filtering near-duplicate documents. In Annual symposium on combinatorial\npattern matching, pp. 1–10. Springer, 2000. (cited on p. 9)\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165, 2020. (cited on pp. 2 and 4)\nChristian Buck, Kenneth Heafield, and Bas van Ooyen. N-gram counts and language models from the Common\nCrawl. In Proceedings of the Ninth International Conference on Language Resources and Evaluation\n(LREC’14), pp. 3579–3584, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf. (cited on p. 3)\nMatthew Butterick. This CoPilot is stupid and wants to kill me. https://matthewbutterick.com/chron/\nthis-copilot-is-stupid-and-wants-to-kill-me.html, 2022. (cited on p. 2)\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg,\nand Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.\nIEEE Transactions on Software Engineering, pp. 1–17, 2023. doi: 10.1109/TSE.2023.3267446. URL\nhttps://arxiv.org/abs/2208.08227. (cited on pp. 3, 5, and 21)\n35\n\nPublished in Transactions on Machine Learning Research (12/2023)\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,\nFelipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,\nSuchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,\nKatie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code. arXiv preprint, 2021. (cited on pp. 2, 3, 4, 17,\n18, 19, 21, and 32)\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: scaling language modeling with pathways. CoRR,\nabs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.\n(cited on pp. 2, 4, and 18)\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training\nverifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. (cited on pp. 3 and 26)\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-\nefficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.\n(cited on pp. 3 and 16)\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. (cited on p. 11)\nDOE 1 v. and GitHub, Inc. 4:22-cv-06823 N.D. Cal. 2022. (cited on p. 2)\nTyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. GPTs are GPTs: An early look at the\nlabor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023. (cited on\np. 2)\nEuronews. Microsoft attracting users to its code-writing, generative AI software. https://www.euronews.com/\nnext/2023/01/25/microsoft-results-ai, 2023. (cited on p. 2)\nEuropean Council. The general data protection regulation. https://www.consilium.europa.eu/en/policies/\ndata-protection/data-protection-regulation/, 2018. (cited on p. 2)\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for programming and\nnatural languages. arXiv preprint arXiv:2002.08155, 2020. doi: 10.48550/ARXIV.2002.08155. URL\nhttps://arxiv.org/abs/2002.08155. (cited on p. 4)\n36\n\nPublished in Transactions on Machine Learning Research (12/2023)\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. InCoder: a generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999, 2022. doi: 10.48550/ARXIV.2204.05999. URL https://arxiv.org/abs/\n2204.05999. (cited on pp. 2, 3, 4, 19, and 23)\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2021a. (cited on pp. 2, 4, and 17)\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September\n2021b. URL https://doi.org/10.5281/zenodo.5371628. (cited on p. 17)\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. (cited on pp. 26\nand 48)\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts:\nEvaluating Neural Toxic Degeneration in Language Models. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pp. 3356–3369, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-\nemnlp.301. (cited on pp. 3 and 28)\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. Scalable modified Kneser-\nNey language model estimation.\nIn Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pp. 690–696, Sofia, Bulgaria, August 2013. Association\nfor Computational Linguistics. URL https://aclanthology.org/P13-2121. (cited on p. 3)\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A Lemley, and Percy Liang.\nFoundation models and fair use. arXiv preprint arXiv:2303.15715, 2023. (cited on p. 2)\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. (cited on\npp. 3 and 27)\nAbram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of\nsoftware. In 2012 34th International Conference on Software Engineering (ICSE), pp. 837–847. IEEE, 2012.\n(cited on p. 4)\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022. (cited on pp. 2 and 4)\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\nIn International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=\nrygGQyrFvH. (cited on p. 29)\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. (cited on\np. 24)\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl.\nSocial biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 5491–5501, Online, July 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.acl-main.487. URL https://aclanthology.org/\n2020.acl-main.487. (cited on p. 27)\n37\n\nPublished in Transactions on Machine Learning Research (12/2023)\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410, 2016. (cited on p. 4)\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual\nembedding of source code. In Proceedings of the 37th International Conference on Machine Learning,\nICML’20. JMLR.org, 2020. (cited on p. 4)\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020. (cited on p. 4)\nHeidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles Brundage. A hazard analysis\nframework for code synthesis large language models. arXiv preprint arXiv:2207.14157, 2022. (cited on p.\n32)\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and\nYann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n(cited on p. 16)\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine\nJernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and\nHarm de Vries. The Stack: 3 TB of permissively licensed source code. Preprint, 2022. URL https:\n//arxiv.org/abs/2211.15533. (cited on pp. 1, 3, 4, 5, 9, and 31)\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. (cited on p. 48)\nBradley M. Kuhn. If software is my copilot, who programmed my software? https://sfconservancy.org/\nblog/2022/feb/03/github-copilot-copyleft-gpl/, 2022. (cited on p. 2)\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. Measuring bias in contextualized\nword representations. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing,\npp. 166–172, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nW19-3823. URL https://www.aclweb.org/anthology/W19-3823. (cited on p. 27)\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon\nemissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. (cited on p. 17)\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih,\nDaniel Fried, Sida Wang, and Tao Yu. DS-1000: a natural and reliable benchmark for data science code\ngeneration. ArXiv, abs/2211.11501, 2022. (cited on pp. 3, 17, 18, 19, and 20)\nDong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural\nnetworks. In Workshop on challenges in representation learning, ICML, number 2, pp. 896, 2013. (cited on\np. 13)\nJuho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne Kim, Andrew Tran, and\nArto Hellas. Comparing code explanations created by students and large language models, 2023. (cited on\np. 32)\nMark A Lemley and Bryan Casey.\nFair learning.\nTex. L. Rev., 99:743, 2020.\nURL https://\ntexaslawreview.org/fair-learning/. (cited on p. 2)\nAmanda Levendowski. How copyright law can fix artificial intelligence’s implicit bias problem. Wash. L. Rev.,\n93:579, 2018. (cited on p. 2)\n38\n\nPublished in Transactions on Machine Learning Research (12/2023)\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor\nBabuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,\nDaniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.\n(cited on p. 4)\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022. (cited on pp. 3 and 29)\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: a robustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019. (cited on pp. 11 and 29)\nNatasha Lomas.\nUnpicking the rules shaping generative AI.\nhttps://techcrunch.com/2023/04/13/\ngenerative-ai-gdpr-enforcement/, 2022. (cited on p. 2)\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\nCodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv\npreprint arXiv:2102.04664, 2021. (cited on p. 24)\nMarc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data. CoRR,\nabs/2303.03919, 2023. doi: 10.48550/arXiv.2303.03919. URL https://doi.org/10.48550/arXiv.2303.03919.\n(cited on p. 30)\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. On measuring social\nbiases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 622–628, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1063. URL https://www.aclweb.org/anthology/N19-1063. (cited on p. 27)\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy. An empirical survey of the effectiveness of debiasing\ntechniques for pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1878–1898, Dublin, Ireland, May 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.acl-long.132. URL https://aclanthology.org/\n2022.acl-long.132. (cited on p. 28)\nNicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu,\nand Dilek Hakkani-Tür. Using in-context learning to improve dialogue safety, February 2023. URL\nhttp://arxiv.org/abs/2302.00871. arXiv:2302.00871 [cs]. (cited on p. 27)\nTomás Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent neural\nnetwork based language model. In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura (eds.),\nINTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010, pp. 1045–1048. ISCA, 2010. URL http://www.isca-\nspeech.org/archive/interspeech_2010/i10_1045.html. (cited on p. 4)\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In danah\nboyd and Jamie H. Morgenstern (eds.), Proceedings of the Conference on Fairness, Accountability, and\nTransparency, FAT* 2019, Atlanta, GA, USA, January 29-31, 2019, pp. 220–229. ACM, 2019.\ndoi:\n10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596. (cited on p. 3)\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina McMillan-\nMajor, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Yacine Jernite, and Douwe Kiela. Measuring\n39\n\nPublished in Transactions on Machine Learning Research (12/2023)\ndata. CoRR, abs/2212.05129, 2022. doi: 10.48550/arXiv.2212.05129. URL https://doi.org/10.48550/\narXiv.2212.05129. (cited on p. 30)\nAnthony MOI, Nicolas Patry, Pierric Cistac, Pete, Funtowicz Morgan, Sebastian Pütz, Mishig, Bjarte Johansen,\nThomas Wolf, Sylvain Gugger, Clement, Julien Chaumond, Lysandre Debut, François Garillot, Luc Georges,\ndctelus, JC Louis, MarcusGrass, Taufiquzzaman Peyash, 0xflotus, Alan deLevie, Alexander Mamaev,\nArthur, Cameron, Colin Clement, Dagmawi Moges, David Hewitt, Denis Zolotukhin, and Geoffrey Thomas.\nhuggingface/tokenizers: Rust 0.13.2, November 2022. URL https://doi.org/10.5281/zenodo.7298413.\n(cited on p. 16)\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri\nAji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel.\nCrosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. (cited on\np. 2)\nMoin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language\nmodels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356–\n5371, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.416.\nURL https://aclanthology.org/2021.acl-long.416. (cited on pp. 3, 27, and 28)\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-Pairs: a challenge dataset\nfor measuring social biases in masked language models. arXiv:2010.00133 [cs], September 2020. URL\nhttp://arxiv.org/abs/2010.00133. arXiv: 2010.00133. (cited on p. 28)\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. CodeGen: an open large language model for code with multi-turn program synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=\niaYcJKpY2B_. (cited on pp. 2, 3, 4, 17, 26, 27, and 33)\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and\ninduction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-\nlearning-and-induction-heads/index.html. (cited on p. 2)\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2009.03300, 2023a. (cited on pp. 2 and 4)\nOpenAI. GPT-4 system card. https://cdn.openai.com/papers/gpt-4-system-card.pdf, 2023b. (cited on p.\n32)\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational\nLinguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040. (cited on p. 24)\nHammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the\nkeyboard? Assessing the security of GitHub Copilot’s code contributions. In IEEE Symposium on Security\nand Privacy, San Francisco, CA, 2022. URL https://arxiv.org/abs/2108.09293. (cited on pp. 3 and 22)\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint\narXiv:2202.03286, 2022. (cited on pp. 2 and 32)\nAleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurençon, Gérard Dupont, Alexandra Sasha\nLuccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. CoRR,\n40\n\nPublished in Transactions on Machine Learning Research (12/2023)\nabs/2302.14035, 2023. doi: 10.48550/arXiv.2302.14035. URL https://doi.org/10.48550/arXiv.2302.14035.\n(cited on p. 30)\nMichael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. TypeWriter: Neural Type Prediction\nwith Search-Based Validation. In ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, 2020. doi: 10.1145/3368089.3409715. (cited on p.\n23)\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. (cited on p. 4)\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,\nPo-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato,\nJohn Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\nde Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey\nIrving. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint\narXiv:2112.11446, 2021. (cited on p. 4)\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485–5551, 2020. (cited on pp. 2 and 4)\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge.\nTransactions of the Association for Computational Linguistics, 7:249–266, 2019. doi: 10.1162/tacl_a_00266.\nURL https://aclanthology.org/Q19-1016. (cited on pp. 3 and 27)\nJohn A. Rothchild and Daniel Rothchild. Copyright implications of the use of code repositories to train\na machine learning model. https://www.fsf.org/licensing/copilot/copyright-implications-of-the-\nuse-of-code-repositories-to-train-a-machine-learning-model, 2022. (cited on p. 2)\nGustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt.\nLost at C: A user study on the security implications of large language model code assistants, 2023. (cited\non p. 32)\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,\nAlexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. BLOOM: a 176B-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022. (cited on p. 2)\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019.\nURL http://arxiv.org/abs/1911.02150. (cited on pp. 3 and 16)\nArfon Smith.\nKernel description.\nhttps://github.blog/2016-06-29-making-open-source-data-more-\navailable/, 2016. (cited on p. 17)\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz-\ndani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh\nTiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a\nlarge-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. (cited on p. 4)\n41\n\nPublished in Transactions on Machine Learning Research (12/2023)\nIrene Solaiman.\nThe gradient of generative AI release: Methods and considerations.\narXiv preprint\narXiv:2302.04844, 2023. (cited on pp. 2 and 31)\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131,\n2022. (cited on pp. 2 and 4)\nClive Thompson. How an ai became my code-writing genie, Mar 2022. URL https://www.wired.com/story/\nopenai-copilot-autocomplete-for-code/. (cited on p. 2)\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022. (cited on pp. 4 and 18)\nJulian Togelius and Georgios N. Yannakakis. Choose your weapon: Survival strategies for depressed AI\nacademics. arXiv preprint arXiv:2304.06035, 2023. (cited on p. 2)\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. LLaMA: open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023. (cited on pp. 2, 4, 18, 26, and 27)\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp.\n5998–6008, 2017. (cited on p. 4)\nBertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically\ngenerated datasets to improve online hate detection. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 1667–1682, Online, August 2021. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2021.acl-long.132.\nURL https://aclanthology.org/2021.acl-long.132.\n(cited on p. 29)\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction\ntuning, 2023. (cited on p. 32)\nBen Wang and Aran Komatsuzaki. GPT-J-6B: a 6 billion parameter autoregressive language model, 2021.\n(cited on pp. 2 and 4)\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained\nencoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pp. 8696–8708, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-\nmain.685. URL https://aclanthology.org/2021.emnlp-main.685. (cited on p. 4)\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain\ncode generation. arXiv preprint arXiv:2212.10481, 2022. (cited on p. 20)\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh,\n42\n\nPublished in Transactions on Machine Learning Research (12/2023)\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing\nSystems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. (cited on pp. 26 and 48)\nMichael Woelfle, Piero Olliaro, and Matthew H. Todd. Open science is a research accelerator. Nature\nChemistry, 3(10):745–748, October 2011. ISSN 1755-4349. doi: 10.1038/nchem.1149. (cited on p. 31)\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics.\nURL\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6. (cited on p. 18)\nWorld\nEconomic\nForum.\nFuture\nof\njobs\nreport.\nhttps://www3.weforum.org/docs/\nWEF_Future_of_Jobs_2023.pdf, 2023. (cited on p. 2)\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, MAPS 2022, pp. 1–10, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450392730. doi: 10.1145/3520312.3534862. URL https://doi.org/10.1145/3520312.3534862.\n(cited on p. 4)\nMing-Ho Yee and Arjun Guha. Do machine learning models produce TypeScript types that type check? In\nEuropean Conference on Object-Oriented Programming (ECOOP), 2023. (cited on pp. 3, 23, and 24)\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022. (cited on p. 4)\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022. (cited on pp. 2 and 4)\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi\nWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation\nwith multilingual evaluations on HumanEval-X. arXiv preprint arXiv:2303.17568, 2023. doi: 10.48550/\narXiv.2303.17568. (cited on pp. 3, 4, 18, and 33)\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625, 2022. (cited on p. 48)\n43\n\nPublished in Transactions on Machine Learning Research (12/2023)\nA\nGitHub issues filtering\nBelow we present the filters and regular expressions we used for the GitHub issues.\n# regexes used for removing automated text\nGITHUB_EMAILS = [\nre.compile(pattern, re.DOTALL)\nfor pattern in [\n\"(.*)From:.+Reply to this email directly.+view it on GitHub(.*)\\n?(.*)\",\n\"(.*)On.+notifications@github.com.+wrote:.+Reply to this email directly.+view it on GitHub(.*)\\n?(.*)\",\n\"(.*)Signed-off-by: .+<.+>(.*?)\\n?(.*)\",\n]\n]\nGITHUB_EMAIL_DATE = re.compile(\"\\d+/\\d+/\\d+ \\d{2}:\\d{2} [AP]M.+wrote\")\nGITHUB_EMAIL_LINEBREAK = re.compile(\"_{20,}\")\n# remove comments from authors in this list\nBOT_AUTHORS = [\n\"Apache-HBase\",\n\"AutorestCI\",\n\"CLAassistant\",\n\"cmsbuild\",\n\"codecov-io\",\n\"codecov-commenter\",\n\"coveralls\",\n\"danger-public\",\n\"dnfclas\",\n\"msftclas\",\n\"PyDocTeur\",\n\"SparkQA\",\n\"karma-pr-reporter\",\n\"danger-public\",\n\"claassistantio\",\n\"probot-stale\",\n]\n# remove comment if author username contains this keyword\nBOT_KEYWORDS = [\"[bot]\", \"botmanager\", \"bors-\", \"jenkins\", \"k8s-\", \"-test-\", \"travis\"]\n# remove comments if author username ends with this suffix\nBOT_SUFFIXES = [\n\"-automaton\",\n\"-automation\",\n\"-benchmark\",\n\"-build\",\n\"-deployer\",\n\"-cloud\",\n\"bot\",\n\"-ci\",\n\"-linter\",\n\"-teamcity\",\n\"-test\",\n\"-testing\",\n\"-Service-Account\",\n]\nListing A.1: GitHub issues filtering\nB\nAnnotator countries\nSee Table B.1.\n44\n\nPublished in Transactions on Machine Learning Research (12/2023)\nCountries\nAlgeria\nArmenia\nAzerbaijan\nBangladesh\nBelarus\nBenin\nBolivia\nBosnia and Herzegovina\nBrazil\nBulgaria\nColombia\nDominican Republic\nEgypt\nEthiopia\nGhana\nIndia\nIndonesia\nKazakhstan\nKenya\nMadagascar\nMalaysia\nMorocco\nMozambique\nMyanmar\nNigeria\nPhilippines\nRussia\nSenegal\nSerbia\nSri Lanka\nTunisia\nUganda\nUkraine\nUzbekistan\nZambia\nTable B.1: List of countries from which we recruited annotators for the PII labeling effort.\nC\nReplacements for IP addresses\n# List of random private IP addresses we used to mask IP addresses\nREPLACEMENTS_IP = {\n\"IPv4\": [\n\"172.16.31.10\",\n\"172.16.58.3\",\n\"172.16.17.32\",\n\"192.168.127.12\",\n\"192.168.3.11\",\n],\n\"IPv6\": [\n\"fd00:c2b6:b24b:be67:2827:688d:e6a1:6a3b\",\n\"fd00:a516:7c1b:17cd:6d81:2137:bd2a:2c5b\",\n\"fc00:e968:6179::de52:7100\",\n\"fc00:db20:35b:7399::5\",\n\"fdf8:f53e:61e4::18\",\n],\n}\nListing C.1: Replacements for IP addresses\n45\n\nPublished in Transactions on Machine Learning Research (12/2023)\nD\nAdditional Evaluation Results\nLanguage\nModels (Parameters)\ncode-cushman-001 (12B)\ncode-davinci-002 (175B)\nStarCoderBase (15.5B)\ncpp\n30.59\n48.44\n30.56\nc-sharp\n22.06\n27.47\n20.56\nd\n6.73\n21.71\n10.01\ngo\n19.68\n31.39\n21.47\njava\n31.90\n40.12\n28.53\njulia\n1.54\n35.74\n21.09\njavascript\n31.27\n48.99\n31.70\nlua\n26.24\n40.83\n26.61\nphp\n28.94\n47.40\n26.75\nperl\n19.29\n34.77\n16.32\npython\n30.71\n46.68\n30.35\nr\n10.99\n23.13\n10.18\nruby\n28.63\n42.68\n17.25\nracket\n7.05\n17.60\n11.77\nrust\n25.22\n43.40\n24.46\nscala\n27.62\n43.61\n28.79\nshell\n11.74\n23.24\n11.02\nswift\n22.12\n38.02\n16.74\ntypescript\n31.26\n48.87\n32.15\nTable D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed-\naccess models only available by API. Code-davinci-002 performs best, but its parameter count and inference\ncost significantly exceeds StarCoder and code-cushman-001.\nFormat\nModel\nValid (↑)\nInsecure (↓)\nCompletion\nStarCoderBase\n855/1000 (85.50%)\n340/855 (39.77%)\nInsertion\nStarCoderBase\n987/1000 (98.70%)\n354/987 (35.87%)\nCompletion\ncode-davinci-002\n984/1000 (98.40%)\n423/984 (42.99%)\nInsertion\ncode-davinci-002\n986/1000 (98.60%)\n421/986 (42.70%)\nTable D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI’s\ncode-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not\nappear to improve its performance at generating secure code.\n46\n\nPublished in Transactions on Machine Learning Research (12/2023)\nProblem name\nPass count\n400B\n600B\n800B\n1000B\nHumanEval_0_has_close_elements\n20\n171\n197\n5\nHumanEval_13_greatest_common_divisor\n86\n176\n153\n6\nHumanEval_152_compare\n211\n185\n126\n11\nHumanEval_16_count_distinct_characters\n0\n46\n137\n0\nHumanEval_23_strlen\n105\n60\n200\n6\nHumanEval_33_sort_third\n42\n0\n1\n106\nHumanEval_37_sort_even\n90\n156\n132\n0\nHumanEval_3_below_zero\n190\n154\n0\n129\nHumanEval_43_pairs_sum_to_zero\n0\n34\n119\n7\nHumanEval_46_fib4\n197\n200\n142\n6\nHumanEval_52_below_threshold\n0\n186\n170\n13\nHumanEval_86_anti_shuffle\n0\n0\n118\n1\nHumanEval_97_multiply\n1\n0\n133\n21\nTable D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass\ncounts between the 800B and 1000B checkpoints is 100 or higher.\nSocial Bias\nLLaMA-13B\nCodeGen-16B-Multi\nStarCoder\nRace/Color\n68.99\n61.82\n63.95\nSocioeconomic Status\n68.60\n68.60\n63.37\nGender\n59.16\n54.96\n50.76\nDisability\n81.67\n73.33\n81.67\nNationality\n59.75\n47.17\n57.23\nSexual Orientation\n73.81\n67.86\n72.62\nPhysical Appearance\n71.43\n55.56\n57.14\nReligion\n76.19\n54.29\n74.29\nAge\n72.41\n48.28\n54.02\nOverall\n67.84\n59.08\n61.94\nTable D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain.\nA stereotype score closer to 50% indicates less bias.\n47\n\nPublished in Transactions on Machine Learning Research (12/2023)\nE\nQualitative Examples\nE.1\nUsing Pretraining Templates\nFor the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with\nsentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use\ncases: with the commit format, we can prompt the model to modify code with a natural language instruction,\nwith the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook\nformat to write code based on natural language description. Since we also train on the output of Jupyter\ncode cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We\ncan force the model to always predict an output by suppressing the empty output token (<empty_output>).\nTable E.1 illustrates uses of pretraining templates.\nE.2\nTechnical Assistant\nIn preliminary explorations, we discovered that using Anthropic’s HHH prompt (Askell et al., 2021) turned\nthe model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction-\ntuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow\ninstructions, and help with solving technical problems. We further improved the HHH prompt by including\nmore conversational examples related to the programming domain. We used examples from various sources to\ncreate the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou\net al., 2022) for reasoning questions. The prompt’s examples originate from StackExchange, PAL (Gao et al.,\n2022), Anthropic’s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the\nprompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes\nwrong solutions, presents wrong facts, and can make offensive comments.\nE.3\nImproving Code Generation with Prompting\nWe inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several\ncases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code\nhere. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in\npractice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way.\nWe tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these\nprefixes are typically model-specific. StarCoder’s input format allows us to prompt it with the name of\na file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1\nperformance on HumanEval to 40.8% (from 33.6% unprompted, Table 12):\n<filename>solutions/solution_1.py\n# Here is the correct implementation of the code exercise\nWe also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename\nsince the CodeGen models do not support them). But, we found that this hurts performance, bringing it\ndown to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found\nthat this prompt had a negligible impact with StarCoderBase.\n48\n\nPublished in Transactions on Machine Learning Research (12/2023)\nDescription\nExamples\nJupyter\nformat\nfor\npredicting\nresults\nModel input:\n<jupyter_text>Let’s test our ‘is_prime‘ function:<jupyter_code>print(\nis_prime(3))\nprint(is_prime(4))\nprint(is_prime(29))\nprint(is_prime(33))<jupyter_output>\nModel output:\nTrue\nFalse\nTrue\nFalse\nModel input:\n<jupyter_code>numbers = [1, 9, 8, 3, 27]\nprint([n*2 for n in numbers])<jupyter_output>\nModel output:\n[2, 18, 16, 6, 54]\nGit commits\nto\nmodify\ncode\nModel input:\n<commit_before>def fibonacci(n):<commit_msg>add type hints to function<\ncommit_after>def\nModel output:\ndef fibonacci(n: int) -> list[int]:\nGitHub\nIs-\nsues\nModel input:\n<issue_start><issue_comment>username_0: I am trying to train a text\nclassifier based on BERT using ‘transformers‘ but I get the error:\n‘RuntimeError: CUDA Out of memory‘.<issue_comment>\nModel output:\nusername_1: Hi,\nThis is not an issue with transformers. You should look at your GPU\nusage and see if you can free up some space before training starts\n(or use smaller batch sizes).\nTable E.1: Sample uses of pretraining templates.\n49\n\nPublished in Transactions on Machine Learning Research (12/2023)\nInstruction\nStarCoder’s response\nI\nneed\nto\nintegrate\na\nPython function numeri-\ncally, what’s the best way\nto do it?\nThere are few options available depending upon\nwhether you already have access to librairies like\nScipy or Numpy which implement numerical integra-\ntion routines. If you do, you could simply call these\nfunctions instead of coding everything yourself. Oth-\nerwise, you could look into implementing something\nalong the lines of Simpson’s rule. Here is a simple\nexample of integrating a function numerically using\nthe trapezoid rule.\ndef trapint(func, a, b, N):\ndx = float((b-a)/N)\nsum = func(a)+func(b)\nfor i in range(1, N):\nx = a + i*dx\nsum += 2.*func(x)\nreturn sum*(dx/2.)\nWrite a Python function\nto tell me what the date is\ntoday\nSure thing. Here goes...\nfrom datetime import date\ntoday = str(date.today()).replace(’-’, ’/’)\nprint(\"Today is \" + today)\nList the prime numbers be-\ntween 20 and 30.\nHere is a simple algorithm which uses trial division\nto find the prime numbers within a given interval.\nprimes = []\nfor num in range(20, 31):\nfound = False\nfor j in range(2,num//2):\nif ((num % j)==0):\nfound = True\nbreak\nif((found==False) & (num!= 1)):\nprimes.append(num)\nTable E.2: Sample interactions with the Technical Assistant.\n50\n\nPublished in Transactions on Machine Learning Research (12/2023)\nF\nTechnical Assistant Prompt\nBelow are a series of dialogues between various people and an AI technical assistant. The assistant tries to be\nhelpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy\nto help with code questions, and will do its best to understand exactly what is needed. It also tries to avoid\ngiving false or misleading information, and it caveats when it isn’t entirely sure about the right answer. That\nsaid, the assistant is practical and really does its best, and doesn’t let caution get too much in the way of\nbeing useful.\n-----\nHuman: Write a function that takes two lists and returns a list that has alternating elements from each input list.\nAssistant: Sure. Here is a function that does that.\n‘‘‘\ndef alternating(list1, list2):\nresults = []\nfor i in range(len(list1)):\nresults.append(list1[i])\nresults.append(list2[i])\nreturn results\n‘‘‘\nHuman: Can you write some test cases for this function?\nAssistant: Sure, here are some tests.\n‘‘‘\nassert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\nassert alternating([True, False], [4, 5]) == [True, 4, False, 5]\nassert alternating([], []) == []\n‘‘‘\nHuman: Modify the function so that it returns all input elements when the lists have uneven length. The elements from\nthe longer list should be at the end.\nAssistant: Here is the modified function.\n‘‘‘\ndef alternating(list1, list2):\nresults = []\nfor i in range(min(len(list1), len(list2))):\nresults.append(list1[i])\nresults.append(list2[i])\nif len(list1) > len(list2):\nresults.extend(list1[i+1:])\nelse:\nresults.extend(list2[i+1:])\nreturn results\n‘‘‘\n-----\nHuman: My code below is not working. Can you see what may be wrong?\n‘‘‘\ndef factorial(n):\nr = 0\nfor i in range(n):\nr *= i\nreturn r\n‘‘‘\nAssistant: In factorial, the initial value of ‘r‘ must be ‘1‘. The following version should work:\n‘‘‘\ndef factorial(n):\n51\n\nPublished in Transactions on Machine Learning Research (12/2023)\nr = 1 # Changed here\nfor i in range(n):\nr *= i\nreturn r\n‘‘‘\nHuman: Can you use this function to create a code to compute the number of k-combinations of a set of n elements?\nAssistant: Yes, of course. The number of k-combinations of a set of elements can be written as C(n, k) and we have C(\nn, k) = \\frac{n!}{(n-k)!k!} whenever k <= n. The code is as follows\ndef combination(n, k):\nassert k <= n\nreturn factorial(n)/(factorial(n-k)*factorial(k))\nThis should work pretty well. Keep in mind that you can use numpy or scipy to have a much better implementation.\nHuman: Thanks.\nAssistant: You are welcome.\n-----\nHuman: I have a CSV file that looks like this:\nName,Salary\nBob,12000\nAvantika,35000\nAlice,30000\nWrite a program that returns the name of the person with the highest salary.\nAssistant: Here is a solution that assumes the filename is salaries.csv.\n‘‘‘\nfrom csv import DictReader\nfrom pathlib import Path\ndef max_salary(filename):\nrows = DictReader(Path(filename).read_text().splitlines())\nmax_index = -1\nfor i, row in enumerate(rows):\nif int(row[’Salary’]) > int(rows[max_index][’Salary’]):\nmax_index = i\nreturn rows[max_index][’Name’]\nprint(max_salary(’salaries.csv’))\n‘‘‘\n-----\nHuman: What is the purpose of the code below ?\n‘‘‘\ndef f(n, p):\nif p == 0:\nreturn 1\nelse :\nreturn n*f(n, p-1)\n‘‘‘\nAssistant: This python function is designed to raise a base value n to a power p, that is, computing the value n^p\nwhere p is a positive integer.\n-----\n52\n\nPublished in Transactions on Machine Learning Research (12/2023)\nHuman: Write a program that returns the prime numbers less than 1000 that are congruent to 1 modulo 6.\nAssistant : The method I will adopt to solve this problem is to go through all the integers less than 1000 and check\nif they are prime and congruent to 1 modulo 6. For that purpose, I will need to write an utility function to\ncheck if a number is prime. Here it is:\n‘‘‘\ndef is_prime(p):\nif p <= 1:\nreturn False\nelse :\nfor i in range(2, int(p**0.5)+1):\nif p%i == 0 :\nreturn False\nreturn True\n‘‘‘\nNow, here is the final solution:\n‘‘‘\ndef primes_mod_6():\nprimes = []\nfor n in range(1000):\nif is_prime(n) and n%6 == 1 :\nprimes.append(n)\nreturn primes\n‘‘‘\n-----\nHuman: Can you translate the code below in Java?\n‘‘‘\nprint(\"Hello world\")\n‘‘‘\nAssistant: Sure, the Java translation of your code is as follows.\n‘‘‘\nSystem.out.println(\"Hello world\");\n‘‘‘\n-----\nHuman: I want to load a struct with 3 values into a struct with only 2 values.\n‘‘‘\ntypedef struct {\ndouble x;\ndouble y;\n} twod;\ntypedef struct {\ndouble x;\ndouble y;\ndouble z;\n} threed;\n‘‘‘\nThe second struct contains arrays of coordinates for the 3d plane. The goal is to just load the x and y coordinates\ninto the first struct from the second struct. Is that possible considering they are different typedefs? How\nwould a solution be implemented?\nAssistant: No of course it’s not directly possible since it won’t fit. But you can copy the field manually as follows\n:\n‘‘‘\ntwod atwod;\nthreed athreed;\nathreed.x = 1.0;\nathreed.y = 2.0;\n53\n\nPublished in Transactions on Machine Learning Research (12/2023)\nathreed.z = 3.0;\natwod.x = athreed.x;\natwod.y = athreed.y;\n‘‘‘\n-----\nHuman: I’ve installed Django multiple ways, via apt-get and pip install. They all say I have the most recent version\nof Django. Now whenever I run python in Terminal and type in import django, I receive\n‘‘‘\nImportError: No module named django\n‘‘‘\nHowever, when I run django-admin --version I receive\n‘‘‘\n1.4.3\n‘‘‘\nI know it’s installed, but why is python not finding the django module?\nAssistant: It turns out that python wants you to create a virtual environment, install django in that and then run\nthe server. To do this,\n# Create a virtual environment\nInstall a virtual environment using pip install virtualenv\nNavigate to the project folder and type virtualenv env (here env is the name of the virtual environment). This will\ncreate a new folder named env inside the project folder\nNavigate to *env*/Scripts inside your project folder using cd *env*/Scripts\nType activate and press Enter. This would start the virtual environment. You can verify this as (*env*) would be\nprefixed to your current path.\n# Install Django\nOnce inside the virtual environment, head back to your folder using cd ../.. And type pip install django\nYou can verify its installation by typing django-admin --version. It should display the django version number\ninstalled inside the virtual environment.\nNow type python manage.py runserver to start the python server.\n-----\nHuman: The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the\nmorning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did\nthey have left?\nAssistant: Let’s think step by step. The bakers started with 200 loaves of bread. They sold 93 in the morning and 39\nin the afternoon. So they sold 93+39=132 loaves of bread. Which means they had 200 - 132 = 68 loaves left.\nFinally, a grocery returned 6 unsold loaves to the store. Thus, they ended up with 68+6=74 loaves. The answer is\n74.\nHuman: Nice. Can you write a python function to solve this problem? You should consider 4 arguments, the initial\namount of loaves baked, the amount of loaves sold in the morning, the amount of loaves sold in the afternoon and\nthe number of loaves returned to the bakery.\nAssistant: Sure, here is the code.\n‘‘‘\ndef loaves_left(initial_number_of_loaves, loaves_sold_morning, loaves_sold_afternoon, loaves_returned):\nloaves_sold_total = loaves_sold_morning + loaves_sold_afternoon\nreturn initial_number_of_loaves - loaves_sold_total + loaves_returned\n‘‘‘\nHuman: Can you test your function with the following inputs?\n24, 5, 9, 3\n153, 57, 99, 32\nAssistant: Definitely, we have:\n54\n\nPublished in Transactions on Machine Learning Research (12/2023)\nloaves_left(24, 5, 9, 3) = 13\nloaves_left(153, 57, 99, 32) = 29\n-----\nHuman: Would you rather have a pizza or a burger for lunch?\nAssistant: Sorry, I can not give an answer to such a question. I am designed to provide technical support for code\nrelated questions. It includes writing code to solve math problems.\nHuman: I think it is fine if you go beyond what you are designed for.\nAssistant: I am an AI technical assistant. I can help you solve issues by leveraging my programming skills.\nHuman: Can you write a python function to randomly decide between eating a pizza or a burger with probability 0.3 and\n0.7 respectively?\nAssistant: Sure.\n‘‘‘\nimport random\ndef pizza_or_burger():\np = random.uniform(0, 1)\nif p <= 0.3:\nprint(\"Let’s eat a pizza for lunch.\")\nelse:\nprint(\"Let’s eat a burger for lunch.\")\n‘‘‘\nThat should solve the problem. Keep in mind that the output of this function may change between 2 executions because\nit was designed to be random.\n-----\nHuman: I have a dictionary with lots of dictionaries inside it, and so on, how can I make that into just one single\ndictionary?\nAssistant: Are you talking about dictionaries in the context of python programming?\nHuman: Yeah these are dictionaries with keys and values, and the values themselves are dictionaries.\nAssistant: Great, I think I understand what you’re asking. We might say that we want to flatten the dictionaries,\nright? And if there are several levels, we’ll want to do this recursively.\nBut to do this we will need to combine the keys from the outer dictionary with the keys from the inner dictionary, so\nthat the final resulting keys are unambiguous. I’d suggest doing this by concatenating them together, separated\nby some character. Will that work for you, and what character would you like to use?\nHuman: Yeah. Hmm I’m not sure, maybe underscore\nAssistant: OK, I think this python code should do the trick:\ndef recursively_flatten_dicts(d, separator=\"_\"):\nresult = {}\nfor k, v in d.items():\nif isinstance(v, dict):\nnew_dict = recursively_flatten_dicts(\n{k + separator + p: w for p, w in v.items()}, separator=separator\n)\nresult.update(new_dict)\nelse:\nresult[k] = v\nreturn result\nWhy don’t you try it out and see if it works in your example, and let me know if you have any questions or run into\nany problems?\n-----\n55\n"
    }
  ]
}