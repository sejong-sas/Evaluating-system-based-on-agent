{
  "1-1 (Weights)": "The project explicitly states that both StarCoder and its companion model StarCoderBase are being released as “open-access Code LLMs.”  Multiple sentences confirm that the weight files themselves are publicly downloadable: “We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages …” and again, “We take several important steps towards a safe open-access model release … and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.”  The technical report emphasizes that these are “15.5 B parameter large language models trained on code,” so the community gains access to the full 15.5-billion-parameter checkpoints rather than a pared-down or distilled variant.  The authors stress that the public weights come with additional safeguards: an “improved PII redaction pipeline” and “a novel attribution tracing tool” were used before publication.  A table labelled “D.1” shows evaluation results obtained from the released checkpoints, further indicating that the exact same weights were made available for third-party benchmarking.  In short, the full pretrained weights for StarCoder (and StarCoderBase) are downloadable by anyone, under the accompanying license, without gating through an API-only service.",
  "1-2 (Code)": "One sentence makes the status of the training-related source code explicit: “By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities.”  The phrase “all code repositories for building the model” indicates that the authors have published the scripts, configuration files, data-processing utilities, and other components needed to reproduce the pre-training pipeline—not merely inference or serving snippets.  Therefore, both the data-preparation and model-training workflow are available to the public on GitHub alongside the inference code, facilitating full end-to-end reproducibility.",
  "1-3 (License)": "Several quotes establish that StarCoder is distributed under a variant of the Responsible AI license family: “• We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access, use, and distribution of the model while embedding a set of use restrictions in identified critical scenarios.”  Another line reiterates the same point: “The model is released with an OpenRAIL-M license that places enforceable use restrictions that apply to the model and its modifications, and to applications using the model.”  Earlier sentences add that the authors “make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.”  Collectively, these passages describe a license that (a) allows use, modification, and redistribution—including commercial use—on a royalty-free basis, but (b) imposes specific, legally enforceable restrictions for sensitive or high-risk scenarios.  Those limits extend to derivative models and downstream applications, meaning redistributors and fine-tuners must also respect the same clauses.",
  "1-4 (Paper)": "The primary manuscript is titled “StarCoder: may the source be with you!”  Supporting sentences confirm it is both a paper and a technical report: “In this paper, we describe StarCoder and StarCoderBase, open-access code LLMs developed and released by the BigCode community …” and “In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder …”  The report documents model design, training process, and ethical safeguards.  It also introduces an online demo and an “integrated attribution tool” released with the model, and it presents “an extensive evaluation of the StarCoder models,” including multi-language HumanEval scores shown in Table D.1.  Thus, comprehensive technical documentation, quantitative benchmarks, and usage guidance are publicly available in the accompanying paper/technical report.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we describe StarCoder and StarCoderBase, open-access code LLMs developed and released by the BigCode community, with a focus on respecting copyright, privacy, transparency, and community-driven"
    },
    {
      "source": "[pdf_text]",
      "quote": "• We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder, open-access 15.5B parameter large language models trained on code."
    },
    {
      "source": "[sections/Published in Transactions on Machine Learning Research Appendix D]",
      "quote": "Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed-access models only available by API."
    },
    {
      "source": "[abstract]",
      "quote": "We take several important steps towards a safe open-access model release ... and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[sections/2305.06161]",
      "quote": "We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities"
    },
    {
      "source": "[pdf_text]",
      "quote": "We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[sections/Conclusion]",
      "quote": "By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access, use, and distribution of the model while embedding a set of use restrictions in identified critical scenarios."
    },
    {
      "source": "[sections/Model limitations]",
      "quote": "Deployments of StarCoder need to further challenge and adapt the model to prevent such behavior, e.g., through red-teaming (Perez et al., 2022), adversarial testing (Wan et al., 2023), and/or by adding a robust safety layer (OpenAI, 2023b). The model is released with an OpenRAIL-M license that places enforceable use restrictions that apply to the model and its modifications, and to applications using the model."
    },
    {
      "source": "[abstract]",
      "quote": "… and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access, use, and distribution of the model while embedding a set of use restrictions in identified critical scenarios."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "StarCoder is subject to typical limitations of LLMs, including the potential to generate content that is inaccurate, offensive, misleading, discriminatory towards age or gender, or reinforces other stereotypes. The model is released with an OpenRAIL-M license that places enforceable use restrictions that apply to the model and its modifications, and to applications using the model."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[title]",
      "quote": "StarCoder: may the source be with you!"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we describe StarCoder and StarCoderBase, open-access code LLMs developed and released by the BigCode community, with a focus on respecting copyright, privacy, transparency, and community-driven"
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder, open-access 15.5B parameter large language models trained on code."
    },
    {
      "source": "[pdf_text]",
      "quote": "We present an extensive evaluation of the StarCoder models and release a demo along with an integrated attribution tool that can help users locate model generations that may have been copied from the training set."
    }
  ],
  "1-5 (Architecture)": "The available material repeatedly emphasizes that StarCoder and its sibling StarCoderBase are 15.5 billion-parameter Code LLMs created by the BigCode community.  Several sentences underscore that both models were “trained on permissively licensed data from The Stack” and that they are “open-access Code LLMs trained on 80+ programming languages.”  Architectural highlights are described as a “novel combination of capabilities and architectural features”: an extended 8 000-token context window, explicit “infilling capabilities through Fill-in-the-Middle (FIM),” and “fast large-batch inference enabled by Multi-Query Attention (MQA).”  One quote clarifies that “StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files,” while another elaborates that FIM “allows the model to generate code conditioned on prefix and suffix code surrounding the insertion point.”  The text also notes the practical impact of these design choices, remarking that “without instruction-tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems.”  Collectively, the quotes portray a 15.5 B-parameter transformer equipped with an 8 K context, FIM-based infilling, and MQA-based efficiency, all tailored for multilingual code generation and reasoning.",
  "1-6 (Tokenizer)": "The sole tokenizer detail given is that “StarCoder’s input format allows us to prompt it with the name of a file using the <filename> token,” indicating the presence of a dedicated <filename> special token for file-name conditioning in prompts.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[pdf_text]",
      "quote": "Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    },
    {
      "source": "[sections/Perplexity With Long Contexts]",
      "quote": "StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files."
    },
    {
      "source": "[sections/Fill in the Middle Benchmarks]",
      "quote": "The StarCoder models support fill in the middle (FIM) or infilling, which allows the model to generate code conditioned on prefix and suffix code surrounding the insertion point."
    },
    {
      "source": "[abstract]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible de- velopment of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
    },
    {
      "source": "[sections/2305.06161]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[sections/2305.06161]",
      "quote": "Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the- Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[sections/E.2 Technical Assistant]",
      "quote": "We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems."
    },
    {
      "source": "[pdf_text/Table D.1 caption]",
      "quote": "Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/E.3 Improving Code Generation with Prompting]",
      "quote": "StarCoder’s input format allows us to prompt it with the name of a file using the <filename> token."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The material explicitly states that the StarCoder family is being released as an “open-access model” and that the authors have “made the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.” Although no dedicated hosted inference service or REST-style endpoint is described, the quote confirms public release, legal terms for commercial use, and the presence of supporting safety infrastructure (an improved PII-redaction pipeline plus a novel attribution-tracing tool) intended to accompany that release. Taken together, these points establish that StarCoder is accessible to external users in a manner consistent with an openly available model distribution, even though the excerpt does not detail a turnkey API comparable to GPT-style SaaS products.",
  "3-1 (Pre-training)": "StarCoderBase and StarCoder share an identical core architecture with 15.5 billion parameters. The base model, StarCoderBase, was trained on a massive 1 trillion-token corpus drawn from The Stack—a permissively licensed collection of GitHub code that spans more than 80 programming languages and also includes GitHub issues, commits, and Jupyter notebooks. Training proceeded for 250 000 iterations with a very large effective batch size of 4 million tokens, yielding the full 1 trillion tokens of compute consumption. Checkpoints were evaluated every 200 billion tokens to monitor progress across the 1 000 billion-token schedule. The context window was set to 8 k tokens, enabling both conditioning on and generation of long code files. In addition to linguistic diversity, the data selection process included an opt-out mechanism and inspection tools designed to respect repository licensing constraints. Both StarCoderBase and the subsequently derived StarCoder model therefore inherit the same high-capacity transformer backbone, the permissively licensed data origin, and the extensive multi-language coverage established during this pre-training phase.",
  "3-2 (Fine-tuning)": "StarCoder itself is produced by fine-tuning StarCoderBase on a focused Python subset. Specifically, the authors conduct an additional training stage on 35 billion Python-specific tokens, running for two full epochs. Hyperparameters are largely copied from the base pre-training setup, but the learning rate is explicitly set to 5 × 10⁻⁵ with a linear warm-up over the first 1 000 iterations, after which it decays to 5 × 10⁻⁶. Despite this language-targeted specialization, empirical results indicate that StarCoder retains strong multi-language competence and even surpasses StarCoderBase on some non-Python benchmarks. Preliminary experiments also revealed that, without any separate instruction-tuning, the 8 k context length allows the fine-tuned model to act as a “somewhat capable yet brittle” technical assistant when prompted with Anthropic’s HHH template. Thus, the fine-tuning pipeline both sharpens Python performance and preserves—or occasionally improves—broader code-generation abilities while maintaining full reproducibility through clearly stated data volume, epoch count, and learning-rate scheduling details.",
  "3-3 (Reinforcement Learning)": "The supplied excerpts include no discussion of reinforcement-learning-based post-training methods such as RLHF, RLAIF, DPO, or related preference-optimization procedures for StarCoder. Consequently, no RL-specific objectives, reward models, hyperparameters, or training stages can be summarized from the provided material.",
  "2-3 (API)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[sections/Model training]",
      "quote": "StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We evaluate the performance of StarCoderBase at several training checkpoints after every 200B tokens seen out of the total 1000B."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model."
    },
    {
      "source": "[sections/Model training]",
      "quote": "Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 × 10−5 and decayed it to 5 × 10−6 after 1,000 iterations of linear warmup."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section also shows that StarCoder, despite being fine-tuned on Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some languages."
    },
    {
      "source": "[pdf_text]",
      "quote": "What is more surprising is that StarCoder slightly outperforms StarCoderBase on certain languages, despite being fine-tuned on Python."
    },
    {
      "source": "[sections/E.2 Technical Assistant]",
      "quote": "In preliminary explorations, we discovered that using Anthropic’s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems."
    },
    {
      "source": "[sections/Model training]",
      "quote": "StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 × 10−5 and decayed it to 5 × 10−6 after 1,000 iterations of linear warmup."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite fine-tuning on Python, StarCoder remains competitive on most languages, and also outperforms other open models."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "According to the authors, StarCoderBase—the foundation on which StarCoder is built—was \"trained on 1 trillion tokens sourced from The Stack,\" a dataset consisting of permissively-licensed GitHub repositories that is distributed with \"inspection tools and an opt-out process.\"  The training corpus was restricted to \"The Stack v1.2 … which exclusively contains data from permissively licensed GitHub repositories,\" thereby guaranteeing that only code with appropriate usage rights was included.  The material covers \"80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks.\"  While StarCoder is principally a code model, roughly one-fifth of its pre-training mix is natural-language text; the breakdown given is \"7 % GitHub issues, 10 % Markdown, 2 % Jupyter notebooks, and 4 % HTML.\"  Both StarCoder and StarCoderBase are \"15.5 B parameter models\" that share the same permissively-licensed origin data, although \"StarCoder was trained on a subset of The Stack v1.2 dataset.\"  Altogether, the pre-training stage uses a trillion-token, license-screened, multi-language corpus whose composition is explicitly quantified and whose licensing status is clearly stated to be permissive.",
  "4-2 (Fine-tuning Data)": "The quoted material states that the transition from StarCoderBase to the released StarCoder model involved an additional, language-focused fine-tuning step: \"We fine-tuned StarCoderBase on 35 B Python tokens, resulting in the creation of StarCoder.\"  Elsewhere the text reiterates, \"We fine-tuned StarCoderBase on another 35 B Python tokens, leading to the StarCoder model.\"  Although this extra corpus is entirely Python code, the authors emphasize that \"StarCoder, despite being fine-tuned on Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some languages.\"  Thus, the fine-tuning dataset is precisely quantified (35 billion Python tokens), its single-language focus (Python) is declared, and its effect on final model capabilities is briefly characterized.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "The release emphasizes multiple safety-oriented data-cleaning mechanisms.  First, the authors \"take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool,\" measures designed to limit privacy leaks and enable license compliance.  Second, when describing training-set preparation, they note: \"Below, we describe how we further cleaned the data by combining heuristic filtering and manual inspection,\" indicating a two-pronged approach that blends automated rules with human review.  The impact of these steps is summarized in \"Table 1,\" which \"show[s] the number of files and data volume after near-deduplication, as well as after filtering\" for each selected programming language, demonstrating measurable reductions at successive pipeline stages.  Despite these efforts, the authors acknowledge residual risk: \"Despite our best efforts to remove PII … StarCoder may still produce PII,\" and the accompanying license explicitly restricts harmful uses of any such content.  Collectively, the filtering process involves near-duplicate removal, heuristic and manual inspection, automated PII redaction, and attribution tracing, all documented as prerequisites for the public release under a more commercially viable variant of the Open Responsible AI Model License.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[abstract]",
      "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "StarCoder was trained on a subset of The Stack v1.2 dataset."
    },
    {
      "source": "[sections/Natural Language Evaluation]",
      "quote": "Although the StarCoder models are principally developed to be Code LLMs, they have also been trained on a significant amount of natural language text. Roughly 20% of its training tokens are natural language data: 7% GitHub issues, 10% Markdown, 2% Jupyter notebooks, and 4% HTML."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[abstract]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[sections/Data Curation and Cleaning]",
      "quote": "This section describes how we processed the training data of StarCoderBase. We restrict the training set to The Stack v1.2 (Kocetkov et al., 2022), which exclusively contains data from permissively licensed GitHub repositories."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model."
    },
    {
      "source": "[sections/Multi-Language Evaluation]",
      "quote": "This section also shows that StarCoder, despite being fine-tuned on Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some languages."
    },
    {
      "source": "[abstract]",
      "quote": "We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model."
    },
    {
      "source": "[sections/6.2 Multi-Language Evaluation]",
      "quote": "This section also shows that StarCoder, despite being fine-tuned on Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some languages."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[sections/Data Curation and Cleaning]",
      "quote": "This section describes how we processed the training data of StarCoderBase. Below, we describe how we further cleaned the data by combining heuristic filtering and manual inspection."
    },
    {
      "source": "[tables/Table 1 caption]",
      "quote": "Table 1: Overview of the training data for StarCoder. For the selected programming languages, we show the number of files and data volume after near-deduplication, as well as after filtering."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "Despite our best efforts to remove PII (Section 4), StarCoder may still produce PII (however, note that the model license restricts use that aims to generate or disseminate PII with the purpose of harming others)."
    },
    {
      "source": "[table 1 caption]",
      "quote": "Table 1: Overview of the training data for StarCoder. For the selected programming languages, we show the number of files and data volume after near-deduplication, as well as after filtering."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "unknown"
  }
}