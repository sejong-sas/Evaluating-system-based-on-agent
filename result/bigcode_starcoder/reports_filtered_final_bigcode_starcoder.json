{
  "1-1 (Weights)": "The quotes state repeatedly that both \"StarCoderBase and StarCoder\" are being released as \"open-access Code LLMs.\" The language \"we release\" and \"make the StarCoder models publicly available\" makes it explicit that the actual weight files are downloadable. The release is positioned as an effort to \"increase access, reproducibility, and transparency of Code LLMs in the research and developer communities.\" The authors highlight that StarCoderBase \"generally obtains substantially stronger performance than all other models with released weights,\" reinforcing that the weights themselves are indeed shared. They also emphasize complementary safety measures—\"an improved PII redaction pipeline and a novel attribution tracing tool\"—that accompany the public weight release. Finally, the weights are made available \"under a more commercially viable version of the Open Responsible AI Model license,\" confirming that anyone can obtain them provided they respect that license.",
  "1-2 (Code)": "The single relevant quote states that, alongside releasing the model weights, the team is \"open-sourcing all code repositories for building the model on GitHub.\" This means the full training pipeline—not merely inference scripts—is public. The explicit motivation is to \"increase access, reproducibility, and transparency,\" implying that data-preparation scripts, configuration files, training schedules, and other components necessary to recreate StarCoder/StarCoderBase are included in those repositories.",
  "1-3 (License)": "Multiple quotes specify that StarCoder is distributed under the \"OpenRAIL-M\" (Open Responsible AI License – Modified) agreement. The license grants \"royalty-free access, use, and distribution of the model\"—thereby affirming permissions for (a) use, (b) redistribution, and (d) commercial use—yet it simultaneously \"embedd[es] a set of use restrictions in identified critical scenarios.\" Those restrictions are enforceable and apply \"to the model and its modifications, and to applications using the model,\" thus covering (c) modification. Concrete prohibited use-cases are listed: the license \"includes a use restriction against generating and/or disseminating malware (including — but not limited to — ransomware) or any other content that can be used to harm electronic systems,\" and it also restricts generation or dissemination of PII \"with the purpose of harming others.\" The text underscores that the OpenRAIL-M version used here is described as \"a more commercially viable version of the Open Responsible AI Model license,\" signaling that commercial deployment is allowed subject to the stated safeguards. A URL—\"https://www.bigcode-project.org/docs/pages/model-license/\"—is provided for the full legal text.",
  "1-4 (Paper)": "The official technical report is titled \"StarCoder: may the source be with you!\" and is noted twice in the quotes. It has been \"Published in Transactions on Machine Learning Research (12/2023),\" confirming peer-reviewed venue and date. The document covers both models—\"StarCoderBase and StarCoder\"—described as \"open-access 15.5B parameter large language models trained on code.\" Dedicated sections detail: (1) \"how we processed the training data\" (Section 5.1), (2) \"decontaminated the training data\" (Section 5.2), (3) tokenizer design (Section 5.3), (4) model architecture (Section 5.4), (5) training process (Section 5.5), (6) \"multi-node GPU setup\" (Section 5.6), and (7) CO2 emissions (Section 5.7). Additional parts include tables—e.g., \"Table 1\" showing near-deduplicated versus filtered data volumes for each programming language, and \"Table D.1\" comparing StarCoder’s pass@1 on MultiPL-E HumanEval to closed-access API models. The paper reports that StarCoderBase was trained on \"1 trillion tokens\" while StarCoder is a fine-tune on \"another 35B Python tokens (roughly 2 epochs).\" Qualitative observations—such as experiments with \"Anthropic’s HHH prompt\" enabling the 8k-context model to act as a \"somewhat capable yet brittle technical assistant\"—are also included. Overall, the paper serves as the comprehensive public record of data curation, architecture, training details, evaluation, and qualitative analysis for the StarCoder family.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "• We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    },
    {
      "source": "[pdf_text]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."
    },
    {
      "source": "[pdf_text]",
      "quote": "By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities."
    },
    {
      "source": "[pdf_text]",
      "quote": "We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    },
    {
      "source": "[pdf_text]",
      "quote": "…and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "– We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access, use, and distribution of the model while embedding a set of use restrictions in identified critical scenarios."
    },
    {
      "source": "[pdf_text]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "Deployments of StarCoder need to further challenge and adapt the model to prevent such behavior, e.g., through red-teaming (Perez et al., 2022), adversarial testing (Wan et al., 2023), and/or by adding a robust safety layer (OpenAI, 2023b). The model is released with an OpenRAIL-M license that places enforceable use restrictions that apply to the model and its modifications, and to applications using the model."
    },
    {
      "source": "[pdf_text]",
      "quote": "The StarCoder OpenRAIL-M license, therefore, includes a use restriction against generating and/or disseminating malware (including — but not limited to — ransomware) or any other content that can be used to harm electronic systems."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder may still produce PII (however, note that the model license restricts use that aims to generate or disseminate PII with the purpose of harming others)."
    },
    {
      "source": "[pdf_text]",
      "quote": "…and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "By releasing the StarCoder models with an Open Responsible AI Model license, and by open-sourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility, and transparency of Code LLMs in the research and developer communities."
    },
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Title: StarCoder: may the source be with you!"
    },
    {
      "source": "[pdf_text]",
      "quote": "Published in Transactions on Machine Learning Research (12/2023) StarCoder: may the source be with you!"
    },
    {
      "source": "[pdf_text]",
      "quote": "This section presents information on the training process of the StarCoder models."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder, open-access 15.5B parameter large language models trained on code."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2305.06161]",
      "quote": "Title: StarCoder: may the source be with you!"
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder: may the source be with you!"
    },
    {
      "source": "[pdf_text]",
      "quote": "This section describes how we processed the training data of StarCoderBase."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Overview of the training data for StarCoder. For the selected programming languages, we show the number of files and data volume after near-deduplication, as well as after filtering."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3. StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs)."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section presents information on the training process of the StarCoder models. Throughout the following, we show how we formatted the training data (Section 5.1), decontaminated the training data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture (Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions (Section 5.7)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed-access models only available by API."
    },
    {
      "source": "[pdf_text]",
      "quote": "In preliminary explorations, we discovered that using Anthropic’s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction-tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems."
    }
  ],
  "1-5 (Architecture)": "Across the supplied passages, the authors repeatedly emphasize that both StarCoder and StarCoderBase are large-language models specifically tailored for code. Every quote agrees on the headline architectural scale: each model contains 15.5 billion parameters.  Several sentences stress that the models were built with an extended 8 000-token context window, enabling them to condition on and generate very long code files, and that they successfully exploit this length in downstream usage (e.g., answering questions or following instructions without any separate instruction-tuning).  A distinctive architectural capability is “Fill-in-the-Middle” (FIM) infilling, letting the model generate code inside an insertion point defined by prefix + suffix; the text notes that this provides infilling functionality as well as ‘fast large-batch inference’ that is made possible through Multi-Query Attention (MQA).  The training corpus design is also mentioned as part of the architecture narrative: StarCoderBase was trained on one trillion tokens spanning 80+ programming languages, GitHub issues, commits, and Jupyter notebooks, while the final StarCoder model results from an additional fine-tuning pass on 35 billion Python-only tokens.  Although an explicit layer/heads breakdown is not given in the excerpts, a referenced “Table 11: Model architecture of StarCoder” indicates that detailed architectural specifications exist and that SantaCoder is included there for comparison.  Finally, a performance comparison (Table D.1) situates StarCoder’s multi-language pass@1 accuracy against other models, indirectly underscoring its parameter/compute efficiency relative to larger proprietary systems.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "The only hardware-related details provided concern the energy and compute footprint recorded during training.  Training StarCoderBase consumed 320 256 GPU-hours; assuming an average draw of 280 W per GPU, this corresponds to approximately 89 671.68 kWh of electricity.  These figures implicitly indicate large-scale GPU clusters were employed, but no specific accelerator model (e.g., A100, H100) or node count is named in the available passages.",
  "2-2 (Software)": "Training hyper-parameters and optimizer choices are given for both the base pre-training and the subsequent Python-centric fine-tuning.  For StarCoderBase pre-training, the run lasted 250 000 iterations with a batch size of four million tokens, yielding precisely one trillion training tokens.  Optimization used Adam with β₁ = 0.9, β₂ = 0.95, ε = 1 × 10⁻⁸, and weight decay = 0.1.  Fine-tuning to obtain StarCoder preserved these settings except for the learning-rate schedule: it started at 5 × 10⁻⁵ and decayed to 5 × 10⁻⁶ after a linear warm-up of 1 000 iterations, and the fine-tuning was run for two full epochs over the Python subset.  No other software-stack specifics (framework, distributed-training library, or specialized kernels) are disclosed in the quoted material.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks. We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model. Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[sections/5.4]",
      "quote": "Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder, open-access 15.5B parameter large language models trained on code."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[pdf_text]",
      "quote": "Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community)."
    },
    {
      "source": "[pdf_text]",
      "quote": "The StarCoder models support fill in the middle (FIM) or infilling, which allows the model to generate code conditioned on prefix and suffix code surrounding the insertion point."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files."
    },
    {
      "source": "[pdf_text]",
      "quote": "We were surprised that, without instruction-tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed-access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/5.7]",
      "quote": "StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase We report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU, this adds up to 89671.68 kWh of electricity consumed during the training process."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/5.5]",
      "quote": "StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.95, ϵ = 10−8 and a weight decay of 0.1."
    },
    {
      "source": "[sections/5.5]",
      "quote": "StarCoder Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data. We used the same settings as StarCoderBase, except that we used a learning rate of 5 × 10−5 and decayed it to 5 × 10−6 after 1,000 iterations of linear warmup."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens. We used Adam (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.95, ϵ = 10−8 and a weight decay of 0.1."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "StarCoder and StarCoderBase are open-access, 15.5 billion-parameter code-specialised LLMs produced by the BigCode community. The models support an 8 k token context window, infilling, and efficient large-batch inference through multi-query attention. Their pre-training corpus totals one trillion tokens drawn from The Stack—an openly released collection of permissively licensed GitHub repositories that comes with inspection tools and an author opt-out mechanism. The dataset covers 80 + programming languages as well as GitHub issues, commit messages, and Jupyter notebooks. Training of StarCoderBase lasted 250 000 iterations with a batch size of 4 million tokens, amounting to the full trillion-token budget. Both StarCoder and StarCoderBase therefore share the same 15.5 B architecture and are the first models trained on this curated trillion-token code dataset; StarCoder differs only by later fine-tuning but its original weights were obtained from the same subset of The Stack (v1.2).",
  "3-2 (Fine-tuning)": "StarCoder was derived from StarCoderBase via a dedicated Python-centric fine-tuning stage. Specifically, the team continued training the base model on 35 billion additional Python tokens—roughly two epochs over the Python portion of the data—to create the final StarCoder checkpoint. The documentation repeats that \"another 35 B\" Python tokens were used, confirming the single-language focus and the data volume. One experiment also produced a Python variant by running the same two-epoch schedule on that subset. Although no separate instruction-tuning or RLHF was applied, Section E.2 of the report shows that prompting strategies alone can elicit helpful assistant behaviour from the resulting StarCoder model.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase\nThe model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we described the efforts of the BigCode community in creating StarCoderBase and StarCoder, open-access 15.5B parameter large language models trained on code."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder was trained on a subset of The Stack v1.2 dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Section E.2, we demonstrate how to prompt StarCoder to act as a technical assistant without any instruction-tuning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "StarCoderBase and its derivative StarCoder are both 15.5-billion-parameter models whose pre-training corpus comes entirely from The Stack v1.2, a collection of permissively licensed GitHub repositories that offers inspection tools and an opt-out mechanism. The authors restricted the dataset to code from more than 80 programming languages as well as natural-language artefacts that appear in software projects. In total, one trillion tokens were used. Roughly 20 % of those tokens are natural-language content subdivided into about 7 % GitHub issues, 10 % Markdown, 2 % Jupyter notebooks and 4 % HTML. The training run for StarCoderBase lasted 250 000 iterations with a batch size of 4 million tokens, yielding the one-trillion-token target. Progress was monitored every 200 billion tokens. A cited Table 1 provides counts of files and data volumes after near-deduplication and subsequent filtering for the languages that were finally selected.",
  "4-2 (Fine-tuning Data)": "Fine-tuning converts StarCoderBase into StarCoder. The procedure consisted of training for two additional epochs on a Python-only slice of the original corpus, amounting to about 35 billion Python tokens. All quotes emphasize that this single-domain, two-epoch, 35 B-token run is the sole extra data step separating StarCoder from its base model, and no instruction-tuning or other specialised datasets were introduced.",
  "4-3 (Reinforcement Learning Data)": "The supplied quotations do not contain any information about reinforcement-learning data; no RL datasets, sources, or procedures are mentioned.",
  "4-4 (Data Filtering)": "Several layers of data cleaning were applied before and after assembling The Stack subset used for StarCoder. First, a license-detection filter ensured that only permissively licensed source-code files were retained. The team also combined heuristic filters with manual inspection to further clean the corpus. Near-deduplication was performed, and the resulting file and token counts after this step—as well as after additional filtering—are summarised in the referenced Table 1. To mitigate privacy risks, the release includes an improved personally identifiable information (PII) redaction pipeline. The authors trained an encoder-only classifier specifically to detect PII in both code and text; they caution that the detector can yield false positives or negatives. An attribution-tracing tool was added as another safety measure. Despite these efforts, the paper notes that StarCoder may still emit PII, and the licence explicitly forbids using the model to generate or disseminate such information with harmful intent.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described in Section 3."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoderBase\nThe model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder was trained on a subset of The Stack v1.2 dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section describes how we processed the training data of StarCoderBase. We restrict the training set to The Stack v1.2 (Kocetkov et al., 2022), which exclusively contains data from permissively licensed GitHub repositories."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Overview of the training data for StarCoder. For the selected programming languages, we show the number of files and data volume after near-deduplication, as well as after filtering."
    },
    {
      "source": "[pdf_text]",
      "quote": "We evaluate the performance of StarCoderBase at several training checkpoints after every 200B tokens seen out of the total 1000B."
    },
    {
      "source": "[pdf_text]",
      "quote": "Although the StarCoder models are principally developed to be Code LLMs, they have also been trained on a significant amount of natural language text. Roughly 20% of its training tokens are natural language data: 7% GitHub issues, 10% Markdown, 2% Jupyter notebooks, and 4% HTML."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs)."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder\nStarting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Section E.2, we demonstrate how to prompt StarCoder to act as a technical assistant without any instruction-tuning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Starting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the Python subset of the training data."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder was trained on a subset of The Stack v1.2 dataset. This dataset has been filtered using a license detector to only include permissively licensed source code."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite our best efforts to remove PII (Section 4), StarCoder may still produce PII (however, note that the model license restricts use that aims to generate or disseminate PII with the purpose of harming others). As mentioned in Section 4.2, we trained an encoder-only model to detect PII for both code- and text-related tasks and noted that there is a possibility of false positives and negatives, which could lead to unintended consequences when processing sensitive data."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section describes how we processed the training data of StarCoderBase. Below, we describe how we further cleaned the data by combining heuristic filtering and manual inspection."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Overview of the training data for StarCoder. For the selected programming languages, we show the number of files and data volume after near-deduplication, as well as after filtering."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite our best efforts to remove PII (Section 4), StarCoder may still produce PII (however, note that the model license restricts use that aims to generate or disseminate PII with the purpose of harming others)."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}