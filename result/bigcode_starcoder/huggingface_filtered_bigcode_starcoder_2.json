{
  "1-5 (Architecture)": "According to the provided documentation for bigcode/starcoder, the StarCoder model is a 15.5-billion-parameter GPT-2–style network that incorporates Multi-Query Attention. It was trained on 1 trillion tokens drawn from 80+ programming languages contained in The Stack (v1.2) after removing all opt-out requests. The model supports a context window of 8,192 tokens and was optimized with a Fill-in-the-Middle (FIM) training objective. The quotes emphasize that the underlying backbone is explicitly described as “GPT-2 model with multi-query attention,” making that the central architectural choice.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "The StarCoder models are 15.5B parameter models trained on 80+ programming languages from [The Stack (v1.2)](https://huggingface.co/datasets/bigcode/the-stack), with opt-out requests excluded. The model uses [Multi Query Attention](https://arxiv.org/abs/1911.02150), [a context window of 8192 tokens](https://arxiv.org/abs/2205.14135),  and was trained using the [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) on 1 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "- **Architecture:** GPT-2 model with multi-query attention and Fill-in-the-Middle objective"
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer for bigcode/starcoder is directly obtainable through the Hugging Face Transformers API: `AutoTokenizer.from_pretrained(\"bigcode/starcoder\")`. The example snippet shows that the same identifier string is used whether running on GPU (`device=\"cuda\"`) or CPU (`device=\"cpu\"`), indicating that the tokenizer is fully packaged and downloadable from the StarCoder model repository.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigcode/starcoder\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)"
    }
  ],
  "2-1 (Hardware)": "Training StarCoder required large-scale GPU resources: 512 NVIDIA Tesla A100 accelerators were employed. The reported wall-clock training duration is 24 days, broken down into 320,256 GPU-hours devoted to pre-training and an additional 11,208 GPU-hours for Python-specific fine-tuning, summing to 331,464 GPU-hours overall. Total training compute is quantified as 8.46 × 10²² FLOPs.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **GPUs:** 512 Tesla A100\n- **Training time:** 24 days (320,256 GPU hours pretraining + 11,208 GPU hours Python fine-tuning)\n- **Training FLOPS:** 8.46E+22"
    }
  ],
  "2-2 (Software)": "The training software stack for bigcode/starcoder is anchored on PyTorch for neural-network execution, orchestrated by Megatron-LM for large-scale distributed training. Mixed-precision (BP16) operations, when used, rely on NVIDIA’s Apex library.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Orchestration:** [Megatron-LM](https://github.com/bigcode-project/Megatron-LM)\n- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch)\n- **BP16 if applicable:** [apex](https://github.com/NVIDIA/apex)"
    }
  ]
}