{
  "1-5 (Architecture)": "The available material repeatedly emphasizes that StarCoder and its sibling StarCoderBase are 15.5 billion-parameter Code LLMs created by the BigCode community.  Several sentences underscore that both models were “trained on permissively licensed data from The Stack” and that they are “open-access Code LLMs trained on 80+ programming languages.”  Architectural highlights are described as a “novel combination of capabilities and architectural features”: an extended 8 000-token context window, explicit “infilling capabilities through Fill-in-the-Middle (FIM),” and “fast large-batch inference enabled by Multi-Query Attention (MQA).”  One quote clarifies that “StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files,” while another elaborates that FIM “allows the model to generate code conditioned on prefix and suffix code surrounding the insertion point.”  The text also notes the practical impact of these design choices, remarking that “without instruction-tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems.”  Collectively, the quotes portray a 15.5 B-parameter transformer equipped with an 8 K context, FIM-based infilling, and MQA-based efficiency, all tailored for multilingual code generation and reasoning.",
  "1-6 (Tokenizer)": "The sole tokenizer detail given is that “StarCoder’s input format allows us to prompt it with the name of a file using the <filename> token,” indicating the presence of a dedicated <filename> special token for file-name conditioning in prompts.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
    },
    {
      "source": "[pdf_text]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[pdf_text]",
      "quote": "Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs."
    },
    {
      "source": "[sections/Perplexity With Long Contexts]",
      "quote": "StarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code files."
    },
    {
      "source": "[sections/Fill in the Middle Benchmarks]",
      "quote": "The StarCoder models support fill in the middle (FIM) or infilling, which allows the model to generate code conditioned on prefix and suffix code surrounding the insertion point."
    },
    {
      "source": "[abstract]",
      "quote": "The BigCode community, an open-scientific collaboration working on the responsible de- velopment of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
    },
    {
      "source": "[sections/2305.06161]",
      "quote": "StarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from The Stack."
    },
    {
      "source": "[sections/2305.06161]",
      "quote": "Both StarCoder models come with a novel combination of architectural features, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the- Middle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA; Shazeer, 2019)."
    },
    {
      "source": "[sections/E.2 Technical Assistant]",
      "quote": "We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems."
    },
    {
      "source": "[pdf_text/Table D.1 caption]",
      "quote": "Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/E.3 Improving Code Generation with Prompting]",
      "quote": "StarCoder’s input format allows us to prompt it with the name of a file using the <filename> token."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": []
}