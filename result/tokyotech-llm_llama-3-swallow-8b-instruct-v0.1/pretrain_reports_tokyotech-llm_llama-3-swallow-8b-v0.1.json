{
  "model_id": "tokyotech-llm/llama-3-swallow-8b-v0.1",
  "pretrain_method": "To train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization.",
  "pretrain_data": "Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources.",
  "__evidence": {
    "3-1 (Pre-training)": [
      {
        "source": "art1",
        "quote": "To train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization."
      }
    ],
    "4-1 (Pre-training Data)": [
      {
        "source": "art1",
        "quote": "Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources."
      }
    ]
  }
}