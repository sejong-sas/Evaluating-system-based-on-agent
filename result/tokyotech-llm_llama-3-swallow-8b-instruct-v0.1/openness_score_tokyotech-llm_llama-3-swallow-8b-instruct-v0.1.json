{
  "model": "tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Distributed under the Meta Llama 3 Community License, which the rubric treats as Open despite its ‘no-competition’ clause."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "Provider-authored blog/model-card exists but no formal paper/tech report."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quoted information on GPU/TPU type or quantities used for training."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list Megatron-LM and Transformers 4.40.1, giving partial but not full stack details."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Mentions continual pre-training and parallelisation strategy, but lacks reproducible hyper-parameters."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "States that supervised fine-tuning (SFT) with ‘Chat Vector’ was applied, but omits full procedural detail."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No RL method information provided for the Swallow model. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Discloses that Llama 3 corpus was used plus extra Japanese data, but with no sizes or source list."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists concrete instruction-tuning datasets (OASST EN top-1, OASST JA with synthetic replies) but no statistics."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RLHF/preference dataset information quoted for this model."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Quotes describe heuristic, NSFW and semantic-dedup pipelines for Llama 3, giving partial filtering details."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Distributed under the Meta Llama 3 Community License, which the rubric treats as Open despite its ‘no-competition’ clause."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "Provider-authored blog/model-card exists but no formal paper/tech report."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quoted information on GPU/TPU type or quantities used for training."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list Megatron-LM and Transformers 4.40.1, giving partial but not full stack details."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Mentions continual pre-training and parallelisation strategy, but lacks reproducible hyper-parameters."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "States that supervised fine-tuning (SFT) with ‘Chat Vector’ was applied, but omits full procedural detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Discloses that Llama 3 corpus was used plus extra Japanese data, but with no sizes or source list."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists concrete instruction-tuning datasets (OASST EN top-1, OASST JA with synthetic replies) but no statistics."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Quotes describe heuristic, NSFW and semantic-dedup pipelines for Llama 3, giving partial filtering details."
    }
  },
  "final_score_10pt": 5.357,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "not_used"
    },
    "excluded": [
      "3-3 Reinforcement Learning",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 14,
    "raw_sum": 7.5,
    "scale": "10/14",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": true
  }
}