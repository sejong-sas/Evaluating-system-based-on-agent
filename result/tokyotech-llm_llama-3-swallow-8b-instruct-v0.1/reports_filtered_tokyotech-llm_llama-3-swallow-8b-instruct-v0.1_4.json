{
  "4-1 (Pre-training Data)": "The model is reported to have been pretrained on a corpus exceeding 15 trillion tokens, all of which were gathered from publicly available sources. Relative to its immediate predecessor, the overall pre-training dataset is said to be seven times larger and to contain four times more code. To prepare for future multilingual use cases, more than 5 % of this corpus consists of high-quality non-English text that spans over thirty different languages.",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "For the reinforcement-learning phase, the developers state that they learned from human preference rankings using both Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO). This preference-based RL setup is credited with greatly improving the modelâ€™s performance on reasoning and coding benchmarks.",
  "4-4 (Data Filtering)": "Before pre-training, the authors describe a multi-stage data-filtering pipeline designed to maximize quality. The pipeline relies on a combination of heuristic filters, NSFW filters, semantic deduplication techniques, and text-quality classifiers. To build those classifiers, they leveraged outputs generated by Llama 2, which they found to be particularly effective at identifying high-quality data. Collectively, these steps constitute the filtering process that determines what data is finally admitted into the Llama 3 training set.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code."
    },
    {
      "source": "[pdf_text]",
      "quote": "To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages."
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.meta.com/blog/meta-llama-3/]",
      "quote": "Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code."
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.meta.com/blog/meta-llama-3/]",
      "quote": "To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Learning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks."
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.meta.com/blog/meta-llama-3/]",
      "quote": "Learning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3."
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.meta.com/blog/meta-llama-3/]",
      "quote": "To ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality."
    }
  ]
}