{
    "model_id": "tokyotech-llm/llama-3-swallow-8b-instruct-v0.1",
    "files": [
        ".gitattributes",
        "LICENSE",
        "Notice",
        "README.md",
        "USE_POLICY.md",
        "config.json",
        "generation_config.json",
        "logo.png",
        "model-00001-of-00004.safetensors",
        "model-00002-of-00004.safetensors",
        "model-00003-of-00004.safetensors",
        "model-00004-of-00004.safetensors",
        "model.safetensors.index.json",
        "special_tokens_map.json",
        "tokenizer.json",
        "tokenizer_config.json"
    ],
    "readme": "---\nlanguage:\n  - en\n  - ja\nlibrary_name: transformers\npipeline_tag: text-generation\nlicense: llama3\nmodel_type: llama\n---\n\n# Llama3 Swallow - Built with Meta Llama 3\n\nOur Swallow model has undergone continual pre-training from the [Llama 3 family](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6), primarily with the addition of Japanese language data. The Instruct versions use supervised fine-tuning (SFT) and Chat Vector. Links to other models can be found in the index.\n\n\n# Model Release Updates\n\nWe are excited to share the release schedule for our latest models:\n- **July 1, 2024**: Released the [Llama-3-Swallow-8B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1), [Llama-3-Swallow-8B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1), [Llama-3-Swallow-70B-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1), and [Llama-3-Swallow-70B-Instruct-v0.1](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1).\n\n## Swallow Model Index\n\n|Model|Llama-3-Swallow|Llama3 Swallow Instruct|\n|---|---|---|\n|8B| [Link](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1) | [Link](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) |\n|70B| [Link](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1) | [Link](https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1) |\n\n![logo](./logo.png)\n\nThis repository provides large language models developed by [Swallow-LLM](https://swallow-llm.github.io/).\nRead our [blog post](https://zenn.dev/tokyotech_lm/articles/f65989d76baf2c).\n\n## Model Details\n\n* **Model type**: Please refer to [Llama 3 MODEL_CARD](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) for details on the model architecture.\n* **Language(s)**: Japanese English\n* **Library**: [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) \n* **Tokenizer**: Please refer to [Llama 3 blog](https://ai.meta.com/blog/meta-llama-3/) for details on the tokenizer.\n* **Contact**: swallow[at]nlp.c.titech.ac.jp \n\n## Model Performance\n\n### Japanese tasks\n\n|Model|Size|JCom.|JEMHopQA|NIILC|JSQuAD|XL-Sum|MGSM|WMT20-en-ja|WMT20-ja-en|JMMLU|JHumanEval|Ja Avg|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|   |   |4-shot|4-shot|4-shot|4-shot|1-shot|4-shot|4-shot|4-shot|5-shot|0-shot|   |\n|   |   |EM acc|Char-F1|Char-F1|Char-F1|ROUGE-2|EM acc|BLEU|BLEU|EM acc|pass@1|   |\n|calm2-7b-chat|7B|0.2413|0.5128|0.4956|0.7729|0.0551|0.0480|0.2208|0.1384|0.2482|0.0000|0.2733|\n|Swallow-7b-instruct-v0.1|7B|0.6059|0.4760|0.5284|0.8396|0.1546|0.1360|0.2285|0.1783|0.3510|0.0256|0.3524|\n|Swallow-MS-7b-instruct-v0.1|7B|0.7435|0.5066|0.4268|0.8594|0.1582|0.1760|0.2260|0.1880|0.4177|0.2244|0.3927|\n|RakutenAI-7B-chat|7B|0.9035|0.2600|0.4619|0.8647|0.1339|0.2120|0.2667|0.1966|0.4504|0.2299|0.3980|\n|Qwen2-7B-Instruct|7B|0.8856|0.3902|0.3859|0.8967|0.1277|0.5720|0.2041|0.1909|0.5713|0.5683|0.4793|\n|Meta-Llama-3-8B-Instruct|8B|0.8785|0.3812|0.3936|0.8955|0.1273|0.4160|0.2143|0.2035|0.4719|0.2872|0.4269|\n|Llama-3-ELYZA-JP-8B|8B|0.9017|0.5124|0.5016|0.9113|0.1677|0.4600|0.2509|0.1846|0.4829|0.3811|0.4754|\n|Llama-3-Swallow-8B-Instruct-v0.1|8B|0.9178|0.4963|0.5168|0.9088|0.1296|0.4880|0.2522|0.2254|0.4835|0.3927|0.4811|\n\n### English tasks\n\n|Model|Size|OpenBookQA|TriviaQA|HellaSWAG|SQuAD2.0|XWINO|MMLU|GSM8K|BBH|HumanEval|En Avg|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|   |   |4-shot|4-shot|4-shot|4-shot|4-shot|5-shot|4-shot|3-shot|0-shot|   |\n|   |   |Acc|EM acc|Acc|EM acc|Acc|Acc|EM acc|CoT EM Acc|pass@1|   |\n|calm2-7b-chat|7B|0.2860|0.3528|0.5042|0.2524|0.8413|0.3860|0.0546|0.2990|0.0000|0.3307|\n|Swallow-7b-instruct-v0.1|7B|0.3280|0.4810|0.5501|0.2720|0.8774|0.4066|0.1251|0.3646|0.0866|0.3879|\n|Swallow-MS-7b-instruct-v0.1|7B|0.3600|0.4999|0.5858|0.3030|0.8834|0.5273|0.2108|0.4386|0.2512|0.4511|\n|RakutenAI-7B-chat|7B|0.4160|0.5971|0.6465|0.3091|0.8886|0.5757|0.3139|0.4958|0.2671|0.5011|\n|Qwen2-7B-Instruct|7B|0.4000|0.5468|0.6146|0.3518|0.8852|0.7073|0.6300|0.3101|0.6354|0.5646|\n|Meta-Llama-3-8B-Instruct|8B|0.3880|0.6687|0.5834|0.3743|0.8903|0.6567|0.7453|0.6478|0.5415|0.6107|\n|Llama-3-ELYZA-JP-8B|8B|0.3200|0.5502|0.5224|0.3631|0.8809|0.5875|0.5701|0.3213|0.4604|0.5084|\n|Llama-3-Swallow-8B-Instruct-v0.1|8B|0.3720|0.6557|0.5861|0.3648|0.9002|0.6315|0.5959|0.6391|0.4238|0.5743|\n\n## MT-Bench JA\n\n|Model|Size|coding|extraction|humanities|math|reasoning|roleplay|stem|writing|JMTAvg|\n|---|---|---|---|---|---|---|---|---|---|---|\n|calm2-7b-chat|7B|0.1198|0.3793|0.4231|0.1011|0.1799|0.4760|0.3568|0.4583|0.3118|\n|Swallow-7b-instruct-v0.1|7B|0.1947|0.3156|0.4991|0.1900|0.2141|0.5330|0.4535|0.4624|0.3578|\n|Swallow-MS-7b-instruct-v0.1|7B|0.2235|0.3743|0.4611|0.1060|0.3404|0.4287|0.3969|0.3877|0.3398|\n|RakutenAI-7B-chat|7B|0.2475|0.3522|0.4692|0.2140|0.3926|0.4427|0.3977|0.4434|0.3699|\n|Qwen2-7B-Instruct|7B|0.4635|0.6909|0.6857|0.5970|0.5042|0.6667|0.5353|0.6808|0.6030|\n|Meta-Llama-3-8B-Instruct|8B|0.3744|0.6876|0.6225|0.2070|0.5032|0.5248|0.5326|0.4884|0.4926|\n|Llama-3-ELYZA-JP-8B|8B|0.2908|0.6421|0.6406|0.3088|0.5500|0.6740|0.5251|0.6744|0.5382|\n|Llama-3-Swallow-8B-Instruct-v0.1|8B|0.3547|0.6508|0.5371|0.2718|0.4007|0.5493|0.4752|0.5730|0.4766|\n\n## Evaluation Benchmarks\n\n### Japanese evaluation benchmarks\n\nWe used llm-jp-eval(v1.3.0), JP Language Model Evaluation Harness(commit #9b42d41) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows:\n\n- Multiple-choice question answering (JCommonsenseQA [Kurihara et al., 2022])\n- Open-ended question answering (JEMHopQA [Ishii et al., 2024])\n- Open-ended question answering (NIILC [関根, 2003])\n- Machine reading comprehension (JSQuAD [Kurihara et al., 2022])\n- Automatic summarization (XL-Sum [Hasan et al., 2021])\n- Machine translation (WMT2020 ja-en [Barrault et al., 2020])\n- Machine translation (WMT2020 en-ja [Barrault et al., 2020])\n- Mathematical reasoning (MGSM [Shi et al., 2023])\n- Academic exams (JMMLU [尹ら, 2024])\n- Code generation (JHumanEval [佐藤ら, 2024])\n\n### English evaluation benchmarks\n\nWe used the Language Model Evaluation Harness(v.0.4.2) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows:\n\n- Multiple-choice question answering (OpenBookQA [Mihaylov et al., 2018])\n- Open-ended question answering (TriviaQA [Joshi et al., 2017])\n- Machine reading comprehension (SQuAD2 [Rajpurkar et al., 2018])\n- Commonsense reasoning (XWINO [Tikhonov and Ryabinin, 2021])\n- Natural language inference (HellaSwag [Zellers et al., 2019])\n- Mathematical reasoning (GSM8K [Cobbe et al., 2021])\n- Reasoning (BBH (BIG-Bench-Hard) [Suzgun et al., 2023])\n- Academic exams (MMLU [Hendrycks et al., 2021])\n- Code generation (HumanEval [Chen et al., 2021])\n\n### MT-Bench JA\n\nWe used [Japanese MT-Bench](https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question) to assess the instruction-following capabilities of models.\nWe utilized the following settings:\n\n- Implemantation: FastChat [Zheng+, 2023] (commit #e86e70d0)\n- Question: [Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3](https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question/v3)\n- Reference Answer: [Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1](https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_referenceanswer/v1)\n- Prompt for Judge: [Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1](https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_prompt/v1)\n- Judge: `gpt-4-1106-preview`\n- Scoring: Absolute scale normalized to a 0-1 range, averaged over five runs.\n\n## Usage\n\n```sh\npip install vllm\n```\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmodel_name = \"tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(\n    model=model_name,\n    tensor_parallel_size=1,\n)\n\nsampling_params = SamplingParams(\n    temperature=0.6, top_p=0.9, max_tokens=512, stop=\"<|eot_id|>\"\n)\n\n\nmessage = [\n    {\"role\": \"system\", \"content\": \"あなたは誠実で優秀な日本人のアシスタントです。\"},\n    {\n        \"role\": \"user\",\n        \"content\": \"東京の夜空に打ち上がっている花火の下、向かい合っている燕とラマの温かい物語を書いてください。\",\n    },\n]\nprompt = tokenizer.apply_chat_template(\n    message, tokenize=False, add_generation_prompt=True\n)\n\noutput = llm.generate(prompt, sampling_params)\n\nprint(output[0].outputs[0].text)\n\n```\n\n## Training Datasets\n\n### Instruction Tuning\n\nThe following datasets were used for the instruction tuning. \n\n- [OpenAssistant Conversations Dataset EN top-1 thread](https://huggingface.co/datasets/OpenAssistant/oasst2)\n- [OpenAssistant Conversations Dataset](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) was used, where human utterances are included but the responses are not used. Instead, the responses were generated using the [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) model.\n\n \n## Risks and Limitations\n\nThe models released here are still in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations.\n\n## Acknowledgements\n\nWe thank Meta Research for releasing Llama 3 under an open license for others to build on.\n\nOur project is supported by the [Large Generative AI Development Support Program](https://abci.ai/en/link/lfm_support_program.html) of the National Institute of Advanced Industrial Science and Technology. \n\n## License\n\n[META LLAMA 3 COMMUNITY LICENSE](https://llama.meta.com/llama3/license/)\n\n## Authors\n\nHere are the team members:\n- From [Tokyo Institute of Technology Okazaki Laboratory](https://www.nlp.c.titech.ac.jp/index.en.html), the following members:\n  - [Naoaki Okazaki](https://www.chokkan.org/index.ja.html)\n  - [Sakae Mizuki](https://s-mizuki-nlp.github.io/)\n  - [Youmi Ma](https://www.nlp.c.titech.ac.jp/member/youmi.en.html)\n  - [Koki Maeda](https://sites.google.com/view/silviase)\n  - [Kakeru Hattori](https://aya-se.vercel.app/)\n  - [Masanari Ohi](https://sites.google.com/view/masanariohi)\n  - [Taihei Shiotani](https://github.com/inatoihs)\n  - [Koshiro Saito](https://sites.google.com/view/koshiro-saito)\n- From [Tokyo Institute of Technology YOKOTA Laboratory](https://www.rio.gsic.titech.ac.jp/en/index.html), the following members:\n  - [Rio Yokota](https://twitter.com/rioyokota)\n  - [Kazuki Fujii](https://twitter.com/okoge_kaz)\n  - [Taishi Nakamura](https://twitter.com/Setuna7777_2)\n  - [Takumi Okamoto](https://www.linkedin.com/in/takumi-okamoto)\n  - [Ishida Shigeki](https://www.wantedly.com/id/reborn27)\n- From [Artificial Intelligence Research Center, AIST, Japan](https://www.airc.aist.go.jp/en/teams/), the following members:\n  - [Hiroya Takamura](https://sites.google.com/view/hjtakamura)\n\n## How to cite\n\nIf you find our work helpful, please feel free to cite us.\n\n```\n@inproceedings{Fujii:COLM2024,\n   title={Continual Pre-Training for Cross-Lingual LLM Adaptation:\nEnhancing Japanese Language Capabilities},\n   author={Kazuki Fujii and Taishi Nakamura and Mengsay Loem and Hiroki\nIida and Masanari Ohi and Kakeru Hattori and Hirai Shota and Sakae\nMizuki and Rio Yokota and Naoaki Okazaki},\n   booktitle=\"Proceedings of the First Conference on Language Modeling\",\n   series={COLM},\n   pages=\"(to appear)\",\n   year=\"2024\",\n   month=oct,\n   address={University of Pennsylvania, USA},\n}\n\n@inproceedings{Okazaki:COLM2024,\n   title={Building a Large Japanese Web Corpus for Large Language Models},\n   author={Naoaki Okazaki and Kakeru Hattori and Hirai Shota and Hiroki\nIida and Masanari Ohi and Kazuki Fujii and Taishi Nakamura and Mengsay\nLoem and Rio Yokota and Sakae Mizuki},\n   booktitle=\"Proceedings of the First Conference on Language Modeling\",\n   series={COLM},\n   pages=\"(to appear)\",\n   year=\"2024\",\n   month=oct,\n   address={University of Pennsylvania, USA},\n}\n```\n\n### Citations\n\n```tex\n@article{llama3modelcard,\n    title={Llama 3 Model Card},\n    author={AI@Meta},\n    year={2024},\n    url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}\n```",
    "config": "{\n  \"_name_or_path\": \"tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128001,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n",
    "generation_config": "{\n  \"_from_model_config\": true,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [128001, 128009],\n  \"transformers_version\": \"4.40.1\"\n}\n",
    "license_file": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT\nMeta Llama 3 Version Release Date: April 18, 2024\n\n“Agreement” means the terms and conditions for use, reproduction, distribution and modification of the\nLlama Materials set forth herein.\n\n“Documentation” means the specifications, manuals and documentation accompanying Meta Llama 3\ndistributed by Meta at https://llama.meta.com/get-started/.\n\n“Licensee” or “you” means you, or your employer or any other person or entity (if you are entering into\nthis Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or\nregulations to provide legal consent and that has legal authority to bind your employer or such other\nperson or entity if you are entering in this Agreement on their behalf.\n\n“Meta Llama 3” means the foundational large language models and software and algorithms, including\nmachine-learning model code, trained model weights, inference-enabling code, training-enabling code,\nfine-tuning enabling code and other elements of the foregoing distributed by Meta at\nhttps://llama.meta.com/llama-downloads.\n\n“Llama Materials” means, collectively, Meta’s proprietary Meta Llama 3 and Documentation (and any\nportion thereof) made available under this Agreement.\n\n“Meta” or “we” means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your\nprincipal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located\noutside of the EEA or Switzerland).\n\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials,\nyou agree to be bound by this Agreement.\n\n1. License Rights and Redistribution.\n\n  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free\nlimited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama\nMaterials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the\nLlama Materials.\n\n  b. Redistribution and Use.\n\n      i. If you distribute or make available the Llama Materials (or any derivative works\nthereof), or a product or service that uses any of them, including another AI model, you shall (A) provide\na copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Meta\nLlama 3” on a related website, user interface, blogpost, about page, or product documentation. If you\nuse the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is\ndistributed or made available, you shall also include “Llama 3” at the beginning of any such AI model\nname.\n\n      ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part \nof an integrated end user product, then Section 2 of this Agreement will not apply to you.\n\n      iii. You must retain in all copies of the Llama Materials that you distribute the following\nattribution notice within a “Notice” text file distributed as a part of such copies: “Meta Llama 3 is\nlicensed under the Meta Llama 3 Community License, Copyright © Meta Platforms, Inc. All Rights\nReserved.”\n\n      iv. Your use of the Llama Materials must comply with applicable laws and regulations\n(including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama\nMaterials (available at https://llama.meta.com/llama3/use-policy), which is hereby incorporated by\nreference into this Agreement.\n\n      v. You will not use the Llama Materials or any output or results of the Llama Materials to\nimprove any other large language model (excluding Meta Llama 3 or derivative works thereof).\n\n2. Additional Commercial Terms. If, on the Meta Llama 3 version release date, the monthly active users\nof the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700\nmillion monthly active users in the preceding calendar month, you must request a license from Meta,\nwhich Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the\nrights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY\nOUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF\nANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED,\nINCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT,\nMERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR\nDETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND\nASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND\nRESULTS.\n\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\nOUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\nINCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED\nOF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n5. Intellectual Property.\n\n  a. No trademark licenses are granted under this Agreement, and in connection with the Llama\nMaterials, neither Meta nor Licensee may use any name or mark owned by or associated with the other\nor any of its affiliates, except as required for reasonable and customary use in describing and\nredistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to\nuse “Llama 3” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will\ncomply with Meta’s brand guidelines (currently accessible at\nhttps://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use\nof the Mark will inure to the benefit of Meta.\n\n  b. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\nrespect to any derivative works and modifications of the Llama Materials that are made by you, as\nbetween you and Meta, you are and will be the owner of such derivative works and modifications.\n\n  c. If you institute litigation or other proceedings against Meta or any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or\nresults, or any portion of any of the foregoing, constitutes infringement of intellectual property or other\nrights owned or licensable by you, then any licenses granted to you under this Agreement shall\nterminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold\nharmless Meta from and against any claim by any third party arising out of or related to your use or\ndistribution of the Llama Materials.\n\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this\nAgreement or access to the Llama Materials and will continue in full force and effect until terminated in\naccordance with the terms and conditions herein. Meta may terminate this Agreement if you are in\nbreach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete\nand cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this\nAgreement.\n\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of\nthe State of California without regard to choice of law principles, and the UN Convention on Contracts\nfor the International Sale of Goods does not apply to this Agreement. The courts of California shall have\nexclusive jurisdiction of any dispute arising out of this Agreement.",
    "py_files": {}
}