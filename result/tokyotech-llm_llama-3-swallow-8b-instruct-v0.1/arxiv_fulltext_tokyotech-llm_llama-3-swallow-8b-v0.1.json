{
  "model_id": "tokyotech-llm/llama-3-swallow-8b-v0.1",
  "full_texts": [
    {
      "arxiv_id": "2404.17733",
      "full_text": "Preprint (under review)\nBuilding a Large Japanese Web Corpus\nfor Large Language Models\nNaoaki Okazaki†, Kakeru Hattori†, Hirai Shota†, Hiroki Iida†, Masanari Ohi†,\nKazuki Fujii†, Taishi Nakamura†, Mengsay Loem†, Rio Yokota‡, Sakae Mizuki†\n†Department of Computer Science, School of Computing, Tokyo Institute of Technology\n‡Global Scientific Information and Computing Center, Tokyo Institute of Technology\n2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8550 Japan\n{okazaki@c, kakeru.hattori@nlp.c, shota.hirai@nlp.c, hiroki.iida@nlp.c,\nmasanari.ohi@nlp.c, kazuki.fujii@rio.gsic, taishi.nakamura@rio.gsic,\nmengsay.loem@nlp.c, rioyokota@rio.gsic, sakae.mizuki@nlp.c}.titech.ac.jp\nAbstract\nOpen Japanese large language models (LLMs) have been trained on the\nJapanese portions of corpora such as CC-100, mC4, and OSCAR. However,\nthese corpora were not created for the quality of Japanese texts. This study\nbuilds a large Japanese web corpus by extracting and refining text from the\nCommon Crawl archive (21 snapshots of approximately 63.4 billion pages\ncrawled between 2020 and 2023). This corpus consists of approximately\n312.1 billion characters (approximately 173 million pages), which is the\nlargest of all available training corpora for Japanese LLMs, surpassing\nCC-100 (approximately 25.8 billion characters), mC4 (approximately 239.7\nbillion characters) and OSCAR 23.10 (approximately 74 billion characters).\nTo confirm the quality of the corpus, we performed continual pre-training\non Llama 2 7B, 13B, 70B, Mistral 7B v0.1, and Mixtral 8x7B Instruct as base\nLLMs and gained consistent (6.6–8.1 points) improvements on Japanese\nbenchmark datasets. We also demonstrate that the improvement on Llama\n2 13B brought from the presented corpus was the largest among those from\nother existing corpora.\n1\nIntroduction\nChatGPT, released by OpenAI in late 2022, established a milestone toward achieving general-\npurpose artificial intelligence. Research topics using large language models (LLMs) attract\nmuch attention including chain of thoughts (Wei et al., 2022; Kojima et al., 2022; Wang et al.,\n2023a; Trivedi et al., 2023), instruction tuning (Wang et al., 2023b;c), evaluation (Suzgun et al.,\n2023; Zheng et al., 2023), hallucination (Rawte et al., 2023; Zhang et al., 2023), bias (Ladhak\net al., 2023; Feng et al., 2023), detecting generated text (Mitchell et al., 2023) watermark-\ning (Kirchenbauer et al., 2023), poisoning (Wan et al., 2023), detecting pre-training data (Shi\net al., 2024), unlearning (Yao et al., 2023), and efficient inference (Dettmers et al., 2023).\nWhile there are various motivations, such as raising the level of research and development in\nnatural language processing, elucidating the mechanisms of LLM intelligence, the security\nrisks of relying on a handful of foreign companies, and achieving responsible artificial\nintelligence, several Japanese companies and universities have actively been developing\nopen LLMs that achieve good performance on Japanese text. However, in many cases, the\nquality of the training data for building Japanese LLMs was not satisfactory because they\nwere mostly developed overseas as a part of multilingual corpora.\nTable 1 lists representative corpora used for training LLMs. Some popular corpora such as\nPile-CC (Gao et al., 2020), ROOTS (Laurenc¸on et al., 2023), Dolma (Soldaini et al., 2023), and\nRedPajama-Data-v21 cannot be used for training Japanese LLMs, including no Japanese text.\n1https://www.together.ai/blog/redpajama-data-v2\n1\narXiv:2404.17733v1  [cs.CL]  27 Apr 2024\n\nPreprint (under review)\nCorpus\nSize (en)\nSize (ja)\nSource\nText\nLangdet\nstatmt.org/ngrams\n976 BT\n—\n2012, 2013\nWET?\ncld2\n(Buck et al., 2014)\nDedup: Remove lines with exact hash values (MurmurHash, 64 bit).\nClean: Remove email addresses and HTML fragments; normalize\npunctuation and capitalization (Moses truecaser).\nCC-100 (CCNet)\n532 BT\n26 BL\n2018\nWET\nfastText\n(Wenzek et al., 2020)\nDedup: Remove paragraphs with exact hash values (SHA-1, 64 bit).\nClean: Use documents with high likelihood computed by the lan-\nguage model trained on Wikipedia.\nC4\n156 BT\n—\nApril 2019\nWET\nlangdetect\n(Raffel et al., 2020)\nDedup: Remove exact matches in three-sentence units.\nClean: Keep only lines ending in a punctuation mark; remove docu-\nments with one or two sentences; remove lines with 4- words; etc.\nmC4\n2,733 BT\n240 BL\n71 months\nWET\ncld3\n(Xue et al., 2021)\nDedup: Same as C4\nClean: Same as C4, but they use a modified rule for keeping text\n(because the punctuation filter of C4 is specific to English): the text\nmust be at least 200 characters long and contain at least 3 lines.\nOSCAR 23.01\n523 BW\n74 BL\nNov/Dec 2022\nWET\nfastText\n(Abadji et al., 2022)\nDedup: TLSH\nClean: Remove documents with high likelihood computed by the\nlanguage models trained on harmful content or with URLs registered\nin Blacklists UT1.\nPile-CC\n2,312 BL\n—\n2013–2020\njusText\ncld2\n(Gao et al., 2020)\nDedup: MinHash (Jaccard coefficient threshold is approximately 0.5).\nClean: Use documents similar to those in OpenWebText2 (documents\nwith a Reddit score of 3 or higher) based on a fastText classifier.\nROOTS\n484 BB\n—\n(539 domains)\n(original)\nfastText\n(Laurenc¸on et al., 2023)\nDedup: SimHashLSH (6-gram)\nClean: Remove documents with too few words, with a high percent-\nage of repeated characters, with a high percentage of repeated words,\nwith too many emojis, or with too few function words, etc.\nRefinedWeb\n600 BT\n—\nUntil Jun 2023\ntrafilatura fastText\n(Penedo et al., 2023)\nDedup: MinHash (character 5-gram, approximate threshold 0.8)\n.\nClean: Blacklist URL filter by UT1; remove repetitions; per-document\nfilter; per-line modification; remove lines matching more than 50\ntokens based on suffix array.\nDolma\n2,415 BT\n—\nMay 2020 to Jun 2023\nWET\nfastText\n(Soldaini et al., 2023)\nDedup: Remove documents with identical URLs; remove identical\nparagraphs within documents.\nClean: Paragraph filtering of MassiveWeb (Rae et al., 2021); punctua-\ntion rules of C4; etc.\nClueWeb 22\n*16.7 TT\n3,301 BT\nSearch engine\nBlingFire\n(original)\n(Overwijk et al., 2022)\n(Non-commercial use)\nDedup and Clean: Documents were sampled from a commercial\nsearch engine based on the distribution of page visits.\nRedPajama-Data-v2\n20.5 TT\n—\nAll (84 dumps)\nWET\nfastText\nDedup: Same as CCNet\nClean: Text-quality estimation rules used by C4, RefinedWeb, etc.\nThis study (raw)\n3,684 BL\n3,684 BL\n2020-40 to 2023-23\ntrafilatura (original)\nDedup and Clean: None.\nThis study (clean)\n312 BL\n312 BL\n2020-40 to 2023-23\ntrafilatura (original)\nDedup: MinHash (character 5-gram, Jaccard coefficient threshold is\napproximately 0.9).\nClean: Repetition detection of RefinedWeb and quality filter specially\ndesigned for Japanese text.\nTable 1: Representative corpora that can be used to pre-train LLMs. BL, BW, BT, and TT\nstand for billion letters, billion words, billion tokens, and trillion tokens, respectively. Ex-\ntracted from the corresponding papers, numbers in English size are not directly comparable\nbecause of the difference in units. The number of ClueWeb22 may include text in other\nlanguages. Numbers in Japanese size are in Unicode characters and are directly comparable.\nText, Langdet, Dedup, and Clean explain methods for text extraction, language detection,\ndeduplication, and cleaning. WET in text extraction indicates that the corpus uses the text\nextraction results distributed by Common Crawl in WET format.\n2\n\nPreprint (under review)\nTherefore, CC-100 (Wenzek et al., 2020), mC4 (Xue et al., 2021), and OSCAR 23.01 (Abadji\net al., 2022) are candidates for training Japanese LLMs, including a certain amount of\nJapanese text and released with permissive licenses. However, the quality of Japanese text in\nthese corpora is unsatisfactory because they contain noise in the HTML-to-text conversion\n(Common Crawl WET format) and because they have incorporated no special efforts to\nimprove text quality for Japanese. Although we recognize the importance of efforts in\nbuilding multilingual corpora, it is difficult to build a useful and reliable Japanese corpus\nwithout the knowledge of Japanese language.\nTherefore, this paper explores a method to construct a large-scale, high-quality Japanese\nweb corpus that can be used for training Japanese LLMs. The presented method includes\nlightweight language detection that boosts the processing speed of text extraction in the\ntarget language. This strategy is applicable not only to Japanese but also to other languages\nwith less text than English. In addition, we specially design a filtering method to find\ngood-quality Japanese text. The corpus developed in this study is made from 21 snapshots\nof Common Crawl (from CC-MAIN-2020-40 to CC-MAIN-2023-23), and the size of the\ncorpus after cleaning is 173,350,375 pages and 312,093,428,689 characters. To confirm the\nquality of the corpus, we perform continual pre-training on Llama 2 7B, 13B, 70B (Touvron\net al., 2023), Mistral 7B v0.1 (Jiang et al., 2023), and Mixtral 8x7B Instruct (Jiang et al., 2024)\nas base LLMs. Experimental results demonstrate that continual pre-training consistently\nimproves the base model’s performance by 6.6–8.1 points on Japanese benchmark datasets.\nWe also demonstrate that the improvement on Llama 2 13B brought from the presented\ncorpus was the largest among those from other existing corpora. The models trained on the\npresented corpus are available on Hugging Face2.\n2\nRelated work\nA number of large corpora were built from Common Crawl3 archives. Common Crawl is a\nnon-profit organization that crawls websites and provides their archives. The crawled data\nwas initially distributed in ARC format4, but since the summer of 2013, it has been stored in\nWeb ARChive (WARC)5 and Web Text (WET) formats. According to Statistics of Common\nCrawl Monthly Archives6, the total amount of accessible archives (data crawled since 2013\ntill 2023) is 251,325,355,174 pages. Compact Language Detector 2 (cld2)7 estimated that\nabout 5 percent of these web pages are written in Japanese.\nTable 1 summarizes the corpora derived from Common Crawl (except for ClueWeb22, where\ndocuments are sampled from a commercial search engine). All permissive corpora that\ncontain Japanese text rely on the data in WET format (HTML pages were converted to text\nby Common Crawl). The advantage of processing data in WET format is the reduction\nof processing time and data transfer size. Wenzek et al. (2020) proposed a pipeline to\nextract multilingual text from Common Crawl to train a multilingual BERT model (XLM-\nR) (Conneau et al., 2020), and released its implementation (CCNet)8 and data (CC-100)9.\nXue et al. (2021) constructed mC4, a multilingual extension of C4 (Raffel et al., 2020) to train\nmT5, a multilingual variant of T5. Abadji et al. (2022) released OSCAR 23.01 with adult\ncontent filtering (based on an n-gram language model) and duplicate removal (based on\nlocality sensitive hashing) added to the previous release.\nHowever, the corpus construction procedure often introduces aggressive filtering rules\nbecause the WET data usually includes irrelevant text, e.g., noises in HTML-to-text conver-\nsion, JavaScript codes, navigation menus, and footers. For example, C4 (Raffel et al., 2020)\n2https://huggingface.co/tokyotech-llm\n3https://commoncrawl.org/\n4https://archive.org/web/researcher/ArcFileFormat.php\n5https://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.1/\n6https://commoncrawl.github.io/cc-crawl-statistics/\n7https://github.com/CLD2Owners/cld2\n8https://github.com/facebookresearch/cc net\n9https://huggingface.co/datasets/cc100\n3\n\nPreprint (under review)\nDownloading WARC files\nStep 1\nRapid Japanese detection\nStep 2\nText extraction\nStep 3\nPrecise Japanese detection\nStep 4\nQuality filtering\nStep 5\nDeduplication\nStep 6\nFiltering by hostnames\nStep 7\nNormalizing punctuations\nStep 8\nRemoving footersd\nStep 9\n63,352,266,406 pages in Common Crawl\nThis step reduces processing time for Steps 3 and 4\nExtract text from HTML (Trafilatura)\n2,686,080,919 Japanese pages extracted\nFind high-quality text based on several rules\nRemove duplicated text (to avoid overfitting)\nRemove pages that may be unuseful to LLMs\nNormalize Japanese punctuations into “、” and “。”\nRemove footers that were left at Step 3\nFigure 1: The pipeline for building a large Japanese web corpus.\nhad to introduce filtering rules such as “remove lines with the word JavaScript” (to remove\nJavaScript code) and “remove pages with curly brackets {}” (because curly brackets are\nrarely used in a natural language but often used in a programming language). Therefore,\nwe use WARC instead of WAT as the source format of Common Crawl archives.\nWe follow the design principle of RefinedWeb (Penedo et al., 2023): scale first (avoid labour\nintensive human curation process), strict deduplication (respect the value of deduplication\nfor large language models), and neutral filtering (avoid undesirable biases introduced by\nmachine-learning-based filtering). The construction procedure of RefinedWeb has much\nin common with MassiveWeb (Rae et al., 2021). Collecting web pages with their own web\ncrawler, Rae et al. (2021) used Google’s SafeSearch filter to remove harmful content, and\nextracted text based on the HTML DOM structure. They also removed documents that\ncontained a lot of repetitions, low-quality text, or duplicated information. In this study, we\nextend their pipelines to extract high-quality Japanese documents efficiently.\nWe also considered using ClueWeb22 (Overwijk et al., 2022) as a source for building a large\nJapanese corpus. Although ClueWeb22 has nice and unique properties such as real distri-\nbution (web pages are extracted by the crawler of a commercial search engine and follow\nthe distribution of web search activities), large-scale quality content (content extraction\npipeline based on the search engine’s production-quality content understanding system),\nand rich information (annotations of content structure, in-links/out-links of web pages,\nvisual features of contents, etc), we could not use ClueWeb22 in this study because an LLM\ntrained on ClueWeb22 cannot be released under an open license.\n3\nMethod\nFigure 1 shows the corpus building pipeline. This pipeline is roughly divided into the\nfollowing three stages: (1) Extracting Japanese text from WARC files in Common Crawl\n(Section 3.1); (2) Selecting Japanese text carefully with quality filtering (Section 3.2), dedu-\nplication (Section 3.3), and host filtering (Section 3.4); (3) Cleaning extracted text (Section\n3.5).\n4\n\nPreprint (under review)\n3.1\nExtracting Japanese text\nCommon Crawl snapshots are stored as buckets in Amazon S3 and can be accessed via S3\nor the web server10. We extract HTML content from WARC data using the WARCIO library11\n(Step 1, Figure 1). We will defer to the explanation of Step 2 until later. We then apply\nTrafilatura12 (Barbaresi, 2021) for extracting text from HTML markups (Step 3). Step 4\ndetects the language of a text based on a linear binary classifier (see Appendix A.1 for the\ndetail) and extracts Japanese text only.\nBecause about 5% of the entire Common Crawl is written in Japanese, it is possible to reduce\nthe processing time of Steps 3 and 4 by 95% if we can target Japanese web pages only.\nHowever, in order to improve the accuracy of language detection, it is desirable to remove\nHTML markup before applying the language detection. Therefore, we should apply text\nextraction first (Step 3), followed by language detection (Step 4). To make matters worse,\ntext extraction takes more processing time than language detection; we cannot reduce the\nprocessing time in this processing order.\nThis is the motivation behind Step 2: we select web pages that are highly likely to be in\nJapanese in a rapid manner without text extraction. A web page proceeds to Steps 3 and 4\nonly when one of the following two conditions is met: (1) the HTML declares the language\nas Japanese, for example, <html lang=\"ja\">; (2) the precise Japanese detection (Section A.1)\nrecognizes the content within <title> tag as Japanese.\nTo evaluate the validity of this rule, we compared the results of rapid and precise language\ndetection. More specifically, assuming that the result of the precise language detection is\ncorrect, we measured the accuracy of rapid language detection on 10 WARC files13 (394,192\npages, 12,052,992,707 bytes in total with gzip compression). The precision, recall, and F1\nscore were 0.888, 0.967, and 0.926, respectively. This result indicates that although rapid\nJapanese language detection may discard about 3.3% of Japanese web pages, the relatively\nhigh precision reduces the processing time for Steps 3 and 4 for non-Japanese web pages.\nIn a benchmark experiment using 1 CPU of Intel Xeon Gold 6130 processor (2.1 GHz), the\nprocessing time for Steps 1–4 was about 15 times faster than Steps 1, 3, and 4 (without rapid\nlanguage detection) because Step 2 removes a number of web pages from the target.\nThis stage obtained 2,686,080,919 pages and 3,684,320,219,695 Japanese characters from 21\nsnapshots of Common Crawl (from CC-MAIN-2020-40 to CC-MAIN-2023-23).\n3.2\nQuality filtering\nThis process (1) removes web pages with many repetitions, (2) selects web pages that contain\ngood-quality Japanese text, and (3) removes web pages that may contain harmful expres-\nsions. For processing (1), we adopted the rules of Rae et al. (2021) to remove documents\nwith many duplicated contents (see Appendix A.2).\nAs for processing (2), no standard has been established for assessing the quality of Japanese\ntext. Therefore, we designed several rules for (2) selecting Japanese text of good quality.\nThis study removes a text that satisfies either of the following conditions as irrelevant for\nLLMs (thresholds in parentheses).\n1. Number of characters (less than 400 characters)\n2. Percentage of hiragana characters (less than 0.2)\n3. Percentage of katakana characters (greater than 0.5)\n4. Percentage of Japanese characters (hiragana, katakana, kanji, punctuations) (less\nthan 0.5)\n5. Average number of characters in a sentence (less than 20 or more than 90)\n10https://data.commoncrawl.org/\n11https://github.com/webrecorder/warcio\n12https://trafilatura.readthedocs.io/\n13CC-MAIN-20230527223515-20230528013515-0000?.warc.gz from 0 to 9 in CC-MAIN-2023-23.\n5\n\nPreprint (under review)\n6. Number of characters in the longest sentence (more than 200)\n7. Percentage of sentences ending with an ellipsis symbol (greater than 0.2)\nFor example, Rule 2 ensures that a text includes a certain amount of function words such as\nga (subject case marker), wo (object case marker) ni (position case maker roughly correspond-\ning to to in English). Rule 3 rejects a web page containing a lot of product or service names\n(e.g., found in SEO sites), which are often expressed in katakana characters. Rule 7 removes\na web page that looks like an RSS feed (a collection of snippets of other web pages). These\nrules were adjusted manually as the authors examined the text to be deleted and extracted.\nAlthough RefinedWeb used UT1 blocklist14 for removing harmful content, its coverage for\nJapanese pages may be insufficient. In order to remove web pages that may contain harmful\nexpressions (3), we manually created a list of NG expressions. If the percentage of characters\nthat match NG expressions is greater than 0.05, we exclude the text from the corpus.\nThis quality filtering reduced the size of the corpus to 646,238,066 pages (1,202,868,044,631\ncharacters). Examining the text before and after the quality filtering, we observe that web\npages that may be unuseful for training LLMs (e.g., e-commerce sites) have disappeared.\n3.3\nDeduplication\nBecause Common Crawl visits and crawls the same website multiple times, the archive\nincludes web pages that are identical or similar because of minor modifications or repro-\nductions. Lee et al. (2022) reported that deduplication, removing duplicated text from the\ncorpus, not only reduces memorization of LLMs but also improves pre-training efficiency.\nTherefore, we performed deduplication using MinHash (Broder, 1997) in a similar manner\nto Lee et al. (2022). When using MinHash for deduplication, it is common to create r buckets,\nwhere each bucket is a concatenation of b MinHash values, compare r pairs of buckets of two\ndocuments, recognize the two documents as a duplicate if any of bucket pairs are identical.\nIn this study, we adopted a setting where b = 20, r = 40 so that a pair of documents with a\nJackard coefficient15 of 0.9 can be approximately detected as a duplicate with a probability\nof 92.5%. When a pair of documents was recognized as a duplicate, we kept the one crawled\nrecently and removed the older one. In the snapshot used in this experiment, the non-\nduplicate rate of web pages collected between March and June 2023 ranged from 77.8 to\n87.9 percent, while the non-duplicate rate of web pages collected before February 2023\ndropped to less than 40 percent, and the rate of web pages collected around 2020 was less\nthan 20 percent. This deduplication process reduced the corpus size to 185,766,463 pages\n(353,191,872,805 characters).\n3.4\nFiltering by hostnames\nEven after the quality filtering in Section 3.2, we found some irrelevant content in the corpus.\nTherefore, we created a block list consisting of hostnames that met the following criteria.\n1. Included in the UT1 blocklist.\n2. Percentage of pages containing the name of a dating site (greater than 0.001).\n3. Percentage of pages containing NG expressions (greater than 0.005).\n4. *wikipedia.org\n5. *.5ch.net\nAlthough the UT1 blocklist includes some Japanese websites, we introduced Criteria 2 and 3\nto improve the coverage. The authors manually adjusted the thresholds for the percentage16.\n14https://dsi.ut-capitole.fr/blacklists/\n15In this study, features were constructed with 5-grams of characters.\n16We placed emphasis on recall at the expense of precision to reduce harmful behaviors of trained\nmodels. This design results in including a lot of non-harmful websites in the block list.\n6\n\nPreprint (under review)\nWe applied Criterion 4 because we planned to use Wikipedia text extracted from the dump.\nThis filtering process resulted in a corpus of 173,350,375 pages (312,674,522,606 characters).\n3.5\nCleaning extracted text\nSections 3.1 to 3.4 process text at the document level, i.e., filter out irrelevant documents\nwithout altering the text within a document. In this study, to avoid an unexpected side\neffect, we minimally edit the text: punctuation normalization (replacing Western-style\npunctuations with Japanese-style ones) and footer trimming (removing footers that were\nleft by Trafilatura). Refer to Appendix A.3 for the details. The process does not delete web\npages, but the number of characters decreased from 312,674,522,606 to 312,093,428,689.\n4\nExperiments\n4.1\nModels and training data\nIn order to evaluate the usefulness of the presented corpus, we conducted continual pre-\ntraining of popular LLMs that excel in English17. We used Llama 2 7B, 13B, 70B, Mistral 7B\nv0.1, and Mixtral 8x7B Instruct v0.1 as base models and performed continual pre-training\nof these base models on the presented corpus to examine whether the corpus improves\ntheir Japanese knowledge and skill. The training data for continual pre-training was a\nmixture of the presented corpus, Japanese Wikipedia, RefinedWeb, and arXiv component in\nThe Pile18 (Gao et al., 2020). Specifically, we prepared approximately 104.9 BT of training\ndata, assuming a sequence length of 4,096 tokens and a batch size of 1,024 with 25,000\nsteps for continuous pre-training. The ratio of Japanese to English tokens was set to 9:1,\nwith 5% of the training data being the English text from RefinedWeb, 5% being The Pile’s\narXiv paper text (English), and the remaining 90% being Japanese text. The breakdown\nof the Japanese text was about 1.6 BT of Japanese Wikipedia, and the presented Japanese\ncorpus occupied the rest. We also used AlgebraicStack (Azerbayev et al., 2024) for Mistral’s\ncontinual pre-training. For Mixtral, we used both AlgebraicStack and The Vault (Manh\net al., 2023)19. We used a different ratio (72:28) of Japanese to English tokens for continual\npre-training on Mixtral 8x7B Instruct20.\nWe did not change the architecture of base LLMs; the embedding sizes of tokens, hidden\nlayers, feed-forward layers, the number of attention heads, and the number of layers were\nunchanged from the base models in continual pre-training. Before continual pre-training,\nwe added Japanese vocabulary to Llama 2 and Mistral 7B tokenizers. The total number of\nvocabulary was 43,176 for LLMs based on Llama 2 and 42,800 for those based on Mistral 7B.\nWe employed AdamW (Loshchilov & Hutter, 2019), with hyperparameters set to β1 = 0.9,\nβ2 = 0.95, ϵ = 1.0 × 10−8. We used re-warming and re-decaying the learning rate (Ibrahim\net al., 2024) with 1,000 warm-up steps. For Llama 2, the maximum learning rate was set\nat 1.0 × 10−4 with a decay rate of 1/30, while for Mistral and Mixtral, it was 2.0 × 10−5\nwith a decay rate of 1/10. The batch size was 1,024. We used 0.1 for weight decay and 1.0\nfor gradient clipping. In addition, we used Flash Attention (Dao et al., 2022) to improve\ncomputational and memory efficiency.\n17It may be straightforward to evaluate the corpus by training LLMs from scratch, but we chose\ncontinual pre-training because the broader goal of this effort was to build high-performance LLMs\nthat excel in Japanese under the limited amount of computing budget. We can assess the quality of the\ncorpus by checking performance gains from the base models and also comparing the performance\nwith other models.\n18We expected that the RefinedWeb and arXiv would help maintain the base LLMs’ English knowl-\nedge and skill during continual pre-training.\n19We completed the continual pre-training experiments on Llama 2 well before we began with\nMistral 7B. By then, our objective had expanded to enhance logical inference capabilities in Mistral\nand Mixtral, leading us to incorporate AlgebraicStack and The Vault.\n20We changed this ratio as an attempt to improve the performance in English.\n7\n\nPreprint (under review)\nIn order to examine the impact of a corpus on the performance of LLMs, we performed con-\ntinual pre-training on ClueWeb22 and llm-jp-corpus v1.0.121 as well as the presented corpus.\nWe also compared our LLMs to others with similar numbers of parameters: CyberAgentLM2\n7B (CALM2 7B, a Japanese LLM trained from scratch); Japanese-StableLM-Base-Beta-7B\n(JSLMB 7B, a continual pre-training LLM on Llama 2 7B); Youri 7B (a continual pre-training\nLLM on Llama 2 7B); Qwen 7B; Nekomata 7B (a continual pre-training LLM on Qwen 7B);\nJapanese Stable LM Base Gamma 7B (JSLMG 7B, a continual pre-training LLM on Mistral\n7B); Qwen 14B; KARAKURI LM 70B (Karakuri 70B, a continual pre-training LLM on Llama\n2 70B); Japanese-StableLM-Base-Beta-70B (JSLMB 70B, a continual pre-training LLM on\nLLama 2 70B); and Qwen 72B. Refer to Appendix A.4 for the complete list of URLs of the\nLLMs used in the experiments.\n4.2\nEvaluation Dataset\nWe used llm-jp-eval22 and lm-evaluation-harness23 as Japanese evaluation benchmarks. llm-\njp-eval is a benchmark consisting of tasks for multiple choice (MC) question answering, open\nquestion answering (QA), reading comprehension (RC), and natural language inference\n(NLI) tasks. JCommonsenseQA (Kurihara et al., 2022) is employed for MC, JEMHopQA (Ishii\net al., 2023) and NIILC (Sekine, 2003) for QA, JSQuAD (Kurihara et al., 2022) for RC.\nWe decided to exclude the NLI dataset from the benchmark because the distribution of\nground-truth NLI labels is highly imbalanced, which made the evaluation unstable24. For\nlm-evaluation-harness, we used Japanese subsets of XL-Sum (Hasan et al., 2021) for the\nautomatic summarization task and MGSM (Shi et al., 2023) for the arithmetic reasoning task,\nrespectively. In addition, WMT 2020 (Barrault et al., 2020) was used for the Japanese-English\nand English-Japanese machine translation.\n4.3\nResults\nTable 2 reports the performance of LLMs on the Japanese benchmark datasets. The row “+\nthis study” shows the performance of the LLM after we applied continual pre-training to the\nbase model on the presented corpus. The rows “+ this study” demonstrate that the presented\ncorpus consistently improves the performance of the base models via continual pre-training\nby 6.6–8.1 points on average. The models built by this study outperform other models built\nfrom scratch or continual pre-training, establishing the state-of-the-art performance in each\nmodel size (7B, 13B, 8x7B, and 70B). This fact shows the usefulness of the presented corpus\nin building LLMs that excel in Japanese.\nWe can compare different Japanese corpora in continual pre-training of Llama 2 13B. All\ncorpora (ClueWeb22, llm-jp-corpus v1.0.1, and presented corpus) improved the base model’s\nperformance by 5.0–7.0 points, which verifies the effectiveness of the continual pre-training.\nThe presented corpus yielded the best improvement (7.0 points) of all corpora; in particular,\nit drastically enhances the performance of question answering (JCom, JEMHop, NIILC),\nreading comprehension (JSQuAD), and arithmetic reasoning (MGSM). Although we did not\nincorporate a special effort to improve these tasks, the model acquired knowledge about\nJapan and the Japanese language from the presented corpus. In contrast, the presented\ncorpus did not improve the summarization (XL-Sum) task. This trend is observed across\nother base models, except for Mixtral 8x7B Instruct, potentially indicating that adding\nJapanese vocabulary may have a detrimental effect on this task. In addition, we found\nthat the continual pre-training on the presented corpus improved the performance in\nEnglish-Japanese translation but degraded that in the reversed direction (Japanese-English\ntranslation). We will explore mitigation of this phenomenon in the future, e.g., by changing\nthe ratio of Japanese to English tokens in the training data and promoting language transfer\nusing an English-Japanese parallel corpus during continual pre-training.\n21https://github.com/llm-jp/llm-jp-corpus\n22https://github.com/llm-jp/llm-jp-eval v1.0.0\n23https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable commit #9b42d41\n24An LLM may obtain a high score only if the model happens to predict the majority label without\nunderstanding and solving the NLI task.\n8\n\nPreprint (under review)\nCorpus\nQA\nQA\nQA\nRC\nSum\nJa-En\nEn-Ja\nMath\nAvg\nJCom\nJEMHop\nNIILC\nJSQuAD\nXL-Sum\nWMT20\nWMT20\nMGSM\nCALM2 7B\n21.98\n50.47\n50.66\n77.99\n2.33\n14.99\n23.45\n6.00\n30.98\nJSLMB 7B\n36.10\n44.78\n44.32\n83.18\n21.95\n12.26\n19.46\n7.20\n33.66\nYouri 7B\n46.20\n47.76\n49.99\n85.06\n19.57\n19.71\n26.71\n6.40\n37.67\nLlama 2 7B\n38.52\n42.40\n34.10\n79.17\n19.05\n17.37\n17.83\n7.60\n32.01\n+ this study\n48.08\n50.78\n59.68\n85.73\n18.30\n15.11\n25.10\n12.40\n39.40\nQwen 7B\n77.12\n42.34\n23.76\n85.94\n13.71\n18.01\n16.89\n21.60\n37.42\nNekomata 7B\n74.17\n49.28\n50.22\n87.07\n16.76\n18.15\n26.73\n12.40\n41.85\nJSLMG 7B\n73.64\n46.43\n55.68\n89.10\n22.93\n15.61\n23.90\n16.80\n43.01\nMistral 7B\n73.01\n42.45\n27.22\n85.63\n20.06\n17.33\n14.05\n17.60\n37.17\n+ this study\n85.70\n49.15\n55.19\n88.02\n19.88\n16.67\n24.94\n22.40\n45.24\nLlama 2 13B\n68.19\n44.55\n41.74\n85.51\n21.33\n19.81\n21.36\n13.20\n39.46\n+ ClueWeb22\n76.76\n49.29\n56.02\n89.55\n20.15\n23.32\n29.08\n11.60\n44.47\n+ llm-jp\n74.71\n50.85\n60.34\n90.28\n21.91\n17.89\n25.89\n18.00\n44.98\n+ this study\n78.37\n50.63\n64.06\n90.07\n21.68\n17.71\n27.37\n21.60\n46.44\nQwen 14B\n88.29\n42.43\n32.20\n89.80\n18.51\n22.24\n22.23\n38.80\n44.31\nMixtral 8x7B\n84.00\n50.33\n31.07\n88.08\n20.02\n20.63\n19.56\n45.20\n44.86\n+ this study\n92.58\n58.43\n56.87\n91.48\n25.89\n20.74\n27.05\n43.60\n52.08\nKarakuri 70B\n85.79\n51.25\n57.13\n91.00\n14.64\n21.13\n25.40\n27.20\n46.69\nJSLMB 70B\n91.15\n49.25\n60.42\n91.92\n25.73\n23.35\n27.65\n41.60\n51.38\nLlama 2 70B\n86.86\n46.56\n52.56\n90.80\n23.61\n23.98\n26.43\n35.60\n48.30\n+ this study\n93.48\n62.90\n69.60\n91.76\n22.66\n22.98\n30.43\n48.40\n55.28\nQwen 72B\n92.94\n55.66\n45.18\n91.59\n21.79\n23.56\n25.61\n63.20\n52.44\nTable 2: Benchmark evaluation on Japanese tasks. A horizontal line groups LLMs with\nthe same number of parameters. A horizontal dash line groups LLMs from the same base\nmodel. A bold number indicates the maximum value in the LLMs with the same number of\nparameters.\n5\nConclusion\nIn this paper, we built a large Japanese web corpus by extracting and refining text from\nthe Common Crawl archive (21 snapshots of approximately 63.4 billion pages crawled\nbetween 2020 and 2023). This corpus consists of approximately 312.1 billion characters\n(approximately 173 million pages), which is the largest of all available training corpora\nfor Japanese LLMs. We confirmed the usefulness of the corpus by performing continual\npre-training on Llama 2 7B, 13B, 70B, Mistral 7B v0.1, and Mixtral 8x7B Instruct as base\nLLMs. The experiments demonstrated consistent (6.6–8.1 points) improvements in Japanese\nbenchmark datasets, and established the state-of-the-art performance in each model size\n(7B, 13B, 8x7B, and 70B). We also observed that the improvement on Llama 2 13B brought\nfrom the presented corpus was the largest among those from other existing corpora.\nFuture directions include efforts towards the safety of LLMs, such as reducing harmful\ngenerations (e.g., discrimination, exclusion, toxicity, hallucination). Currently, we only\nuse the lists of NG expressions and hostnames, but it is desirable to establish more robust\nfiltering methods to remove harmful text for pre-training Japanese LLMs. In addition,\nalthough our study focused on the continual pre-training setting, we want to evaluate the\npresented corpus by training Japanese LLMs from scratch. Although we evaluated the LLMs\nin downstream tasks such as question-answering and summarization, it is questionable\nwhether this can measure the “general intelligence” of an LLM. At the same time, training an\nLLM on a pre-training corpus requires huge computations. Therefore, we want to explore a\nlightweight method for assessing the effectiveness of pre-training corpora without building\nLLMs.\n9\n\nPreprint (under review)\n6\nLimitations\nWe could not present an ablation study of each step in Figure 1 because of a large amount of\ncomputational resources required to train LLMs. Instead, we reported the performance of\neach step of the corpus construction procedure in Section 3.\nThis study focuses on Japanese, and the construction of corpora for other languages is\noutside the scope. For English, it may be a good strategy to make use of good quality\ncorpora that have already been developed. Although some ideas in this paper could be\nhelpful in building corpora for other languages, for example, reducing processing time\nwith rapid language detection, we believe that it would be better for researchers from\ndifferent countries to share the task of building corpora for their own, as assessing text\nquality requires the knowledge of the language.\n7\nEthics statement\nArticle 30-4 of the Copyright Act in Japan permits to use of a copyrighted work without the\npermission of the copyright holder as long as the use is not a person’s purpose to personally\n“enjoy” or cause another person to “enjoy” the work. This provides the justification for\nbuilding the presented corpus and LLMs. To the best of our knowledge, we have taken\nevery measure to keep the corpus as non-toxic and unbiased as possible, but we are unaware\nof any direct ethical consequences caused by the LLMs trained on the presented corpus.\n8\nReproducibility statement\nAll models developed in this study (continual pre-training on Llama 2 7B, 13B, 70B, Mistral\n7B v0.1, and Mixtral 8x7B Instruct) have already been released on Hugging Face. The bench-\nmark datasets used in this study are also publicly available. Therefore, it is straightforward\nto reproduce our experimental results reported in Table 2.\nAcknowledgements\nThis paper is based on results obtained from a project, JPNP18002, commissioned by the\nNew Energy and Industrial Technology Development Organization (NEDO). In addition,\nthe experiments of continual pre-training of LLMs was supported by the “Support Program\nfor Building Large Language Models” of the AI Bridging Cloud Infrastructure (ABCI)\ndeveloped and operated by the National Institute of Advanced Industrial Science and\nTechnology (AIST). We used the datasets and findings released by the Japanese LLM Study\nGroup (LLM-jp) in the evaluation experiments. We also received suggestions on tokenization\nfrom Tatsuya Hiraoka of Fujitsu Ltd.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoˆıt Sagot. Towards a cleaner\ndocument-oriented multilingual crawled corpus. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pp. 4344–4355, 2022. URL https://aclanthology.\norg/2022.lrec-1.463.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus\nMcAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An\nopen language model for mathematics. In the Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=4WnqRR915j.\nAdrien Barbaresi. Trafilatura: A web scraping library and command-line tool for text\ndiscovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing: System Demonstrations, pp. 122–131, 2021. URL https://aclanthology.org/\n2021.acl-demo.15.\n10\n\nPreprint (under review)\nLo¨ıc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-juss`a, Christian Fed-\nermann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric\nJoanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubeˇsi´c, Christof Monz, Makoto\nMorishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Mar-\ncos Zampieri. Findings of the 2020 conference on machine translation (WMT20). In\nProceedings of the Fifth Conference on Machine Translation, pp. 1–55, 2020. URL https:\n//aclanthology.org/2020.wmt-1.1.\nAndrei Z Broder. On the resemblance and containment of documents. In Proceedings of the\nCompression and Complexity of Sequences 1997, pp. 21, 1997.\nChristian Buck, Kenneth Heafield, and Bas van Ooyen. N-gram counts and language\nmodels from the Common Crawl. In Proceedings of the Ninth International Conference on\nLanguage Resources and Evaluation, pp. 3579–3584, 2014. URL http://www.lrec-conf.org/\nproceedings/lrec2014/pdf/1097 Paper.pdf.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume\nWenzek, Francisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin\nStoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440–8451, 2020.\nURL https://aclanthology.org/2020.acl-main.747.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e.\nFlashAtten-\ntion:\nFast and memory-efficient exact attention with IO-awareness.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 35,\npp. 16344–16359,\n2022.\nURL\nhttps://proceedings.neurips.cc/paper files/paper/2022/file/\n67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient\nfinetuning of quantized LLMs. In Advances in Neural Information Processing Systems,\nvolume 36, pp. 10088–10115, 2023. URL https://proceedings.neurips.cc/paper files/\npaper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf.\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. From pretraining data\nto language models to downstream tasks: Tracking the trails of political biases leading\nto unfair NLP models. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 11737–11762, 2023. URL https:\n//aclanthology.org/2023.acl-long.656.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The\nPile: An 800GB dataset of diverse text for language modeling. arXiv:2101.00027, 2020.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-\nBin Kang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual\nabstractive summarization for 44 languages. In Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pp. 4693–4703, 2021. URL https://aclanthology.org/\n2021.findings-acl.413.\nAdam Ibrahim, Benjamin Th´erien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Tim-\noth´ee Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continu-\nally pre-train large language models. arXiv:2403.08763, 2024.\nAi Ishii, Naoya Inoue, and Satoshi Sekine. Construction of a Japanese multi-hop QA\ndataset for QA systems capable of explaining the rationale. In the 29th Annual Meeting of\nJapanese Association for Natural Language Processing (NLP2023), pp. 2088–2093, 2023. URL\nhttps://www.anlp.jp/proceedings/annual meeting/2023/pdf dir/Q8-14.pdf.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample,\nLucile Saulnier, L´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7B.\narXiv:2310.06825, 2023.\n11\n\nPreprint (under review)\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Th´eophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth´ee Lacroix, and William El Sayed. Mixtral of experts. arXiv:2401.04088, 2024.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.\nA watermark for large language models. In Proceedings of the 40th International Conference\non Machine Learning, volume 202, pp. 17061–17084, 2023. URL https://proceedings.mlr.\npress/v202/kirchenbauer23a.html.\nTakeshi\nKojima,\nShixiang\n(Shane)\nGu,\nMachel\nReid,\nYutaka\nMatsuo,\nand\nYusuke Iwasawa.\nLarge language models are zero-shot reasoners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 35,\npp. 22199–22213,\n2022.\nURL\nhttps://proceedings.neurips.cc/paper files/paper/2022/file/\n8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.\nKentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. JGLUE: Japanese general\nlanguage understanding evaluation. In Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pp. 2957–2966, 2022. URL https://aclanthology.org/2022.\nlrec-1.317.\nFaisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKe-\nown, and Tatsunori Hashimoto. When do pre-training biases propagate to downstream\ntasks? a case study in text summarization. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, pp. 3206–3219, 2023. URL\nhttps://aclanthology.org/2023.eacl-main.234.\nHugo Laurenc¸on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova\ndel Moral, Teven Le Scao, et al. The BigScience ROOTS corpus: A 1.6TB composite\nmultilingual dataset. arXiv:2303.03915, 2023.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini.\nDeduplicating training data makes language\nmodels better. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 8424–8445, 2022. URL https://aclanthology.org/\n2022.acl-long.577.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nDung Nguyen Manh, Nam Le Hai, Anh T. V. Dau, Anh Minh Nguyen, Khanh Nghiem,\nJin Guo, and Nghi D. Q. Bui. The vault: A comprehensive multilingual dataset for\nadvancing code understanding and generation. In Proceedings of the 3rd Workshop for\nNatural Language Processing Open Source Software (NLP-OSS 2023), pp. 219–244, 2023. URL\nhttps://aclanthology.org/2023.nlposs-1.25.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea\nFinn. DetectGPT: Zero-shot machine-generated text detection using probability cur-\nvature.\nIn Proceedings of the 40th International Conference on Machine Learning, vol-\nume 202 of Proceedings of Machine Learning Research, pp. 24950–24962, 2023.\nURL\nhttps://proceedings.mlr.press/v202/mitchell23a.html.\nArnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, and Jamie Callan.\nClueWeb22:\n10 billion web documents with visual and semantic information.\narXiv:2211.15848, 2022.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data,\nand web data only. arXiv:2306.01116, 2023.\n12\n\nPreprint (under review)\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,\nTom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,\nLisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl,\nSumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena\nMartens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,\nDomenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria\nTsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby\nPohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer,\nOriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &\ninsights from training Gopher. arXiv:2112.11446, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\nURL http://jmlr.org/papers/v21/20-074.html.\nVipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam\nTonmoy, Aman Chadha, Amit Sheth, and Amitava Das. The troubling emergence of\nhallucination in large language models - an extensive definition, quantification, and\nprescriptive remediations. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pp. 2541–2573, 2023. URL https://aclanthology.org/2023.\nemnlp-main.155.\nSatoshi Sekine. Development of a question answering system for encyclopedias. the 9th\nAnnual Meeting of Japanese Association for Natural Language Processing (NLP2003), pp. 637–\n640, 2003. URL https://www.anlp.jp/proceedings/annual meeting/2003/pdf dir/C7-6.\npdf.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.\nLanguage models are multilingual chain-of-thought reasoners. In the Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?id=\nfR3wGCk-IXp.\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins,\nDanqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language\nmodels. In the Twelfth International Conference on Learning Representations, 2024. URL\nhttps://openreview.net/forum?id=zWqr3MQuNs.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Khyathi Chandu, Jennifer Dumas, Li Lucy, Xinxi Lyu, Ian Magnusson, Aakanksha\nNaik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Zejiang Shen, Emma\nStrubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi,\nNoah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle\nLo. Dolma: An open corpus of 3 trillion tokens for language model pretraining research.\nTechnical report, Allen Institute for AI, 2023. URL https://allenai.github.io/dolma/\ndocs/assets/dolma-datasheet-v0.1.pdf.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Chal-\nlenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings\nof the Association for Computational Linguistics: ACL 2023, pp. 13003–13051, 2023. URL\nhttps://aclanthology.org/2023.findings-acl.824.\n13\n\nPreprint (under review)\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv:2307.09288, 2023.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\nretrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 10014–10037, 2023. URL https://aclanthology.org/2023.\nacl-long.557.\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models\nduring instruction tuning. In Proceedings of the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine Learning Research, pp. 35413–35425, 2023.\nURL https://proceedings.mlr.press/v202/wan23b.html.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought\nreasoning in language models. In the Eleventh International Conference on Learning Represen-\ntations, 2023a. URL https://openreview.net/forum?id=1PL1NIMMrw.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nHow far can camels go? Exploring the state of instruction tuning on open resources. In\nThirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack, 2023b. URL https://openreview.net/forum?id=w4zZNC4ZaV.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated\ninstructions. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 13484–13508, 2023c. URL https://aclanthology.\norg/2023.acl-long.754.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi,\nQuoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large lan-\nguage models. In Advances in Neural Information Processing Systems, volume 35, pp.\n24824–24837, 2022.\nURL https://proceedings.neurips.cc/paper files/paper/2022/\nfile/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco\nGuzm´an, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality mono-\nlingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources\nand Evaluation Conference, pp. 4003–4012, 2020. URL https://aclanthology.org/2020.\nlrec-1.494.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text\ntransformer. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pp. 483–498, 2021.\nURL https://aclanthology.org/2021.naacl-main.41.\n14\n\nPreprint (under review)\nYuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In Socially\nResponsible Language Modelling Research, 2023. URL https://openreview.net/forum?id=\nwKe6jE065x.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language\nmodel hallucinations can snowball. arXiv:2305.13534, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\nURL https://openreview.net/forum?id=uccHPGDlao.\nA\nAppendix\nA.1\nPrecise language detection\nFollowing the pipeline of RefinedWeb (Penedo et al., 2023), we built a language detector\nwith a linear discriminator using character n-grams as features. We used multilingual\nWikipedia texts as the training data for the linear discriminator, and the feature space was\nconstructed from the training data with character unigrams, bi-grams, and trigrams that\nsatisfy one of the following criteria.\n1. Within the top 400,000 occurrences in the training data for all languages.\n2. Within the top 400,000 occurrences in the training data of Japanese.\n3. Within the top 100,000 occurrences in Chinese training data.\n4. Within the top 10,000 occurrences in the training data for each language.\nIf only Criterion (1) was used, there would be a possibility that features related to Japanese\ncharacters would not be obtained sufficiently. Therefore, Criterion (2) aimed to obtain\nfeatures specific to Japanese. Criterion (3) made it easier to distinguish between Japanese\nand Chinese, which share Chinese characters. We aimed to stabilize the detection results for\ntexts in languages with insufficient training data using Criterion (4). The total number of\ndistinct features was 821,484.\nFor training the linear discriminator, we used the dump of Wikipedia CirrusSearch25 (as of\nJune 12, 2023). In training the language detector, we reduced the amount of training data to\n1/20th of the dump because the total amount was too large for training the discriminator.\nWe sampled 100,000 instances in the remaining data for development and evaluation sets.\nWe trained an L2-regularized L2-loss Support Vector Machine on the training data using\nLIBLINEAR26 2.46. The regularization coefficient was set to C = 10 empirically by the search\non the development set. We confirmed that the performance of this classifier was quite\ngood: 0.996 F1 score on the test set and 0.989 F1 score on whatlang-corpora27.\nA.2\nRules for removing repetitions\nThe removal of documents that contain many repetitions aims to filter out documents\nthat do not seem to have useful information and to prevent the behavior of LLMs from\nrepeatedly generating the same words. Specifically, we follow the rules of Rae et al. (2021)\nand remove a document when the percentage of any of the following exceeds the threshold\n(shown in parentheses).\n1. Number of lines duplicated in other lines / total number of all lines (0.30)\n25https://dumps.wikimedia.org/other/cirrussearch/\n26https://www.csie.ntu.edu.tw/∼cjlin/liblinear/\n27https://github.com/whatlang/whatlang-corpora\n15\n\nPreprint (under review)\n2. Number of paragraphs that are duplicates of other paragraphs / total number of\nparagraphs (0.30)\n3. Number of characters that appear in other lines / total number of all characters\n(0.20)\n4. Number of characters in paragraphs that appear in other paragraphs / total number\nof characters (0.20)\n5. Number of occurrences of the most frequent 2-gram / total number of occurrences\nof all 2-grams (0.20)\n6. Number of occurrences of the most frequent 3-gram / total number of occurrences\nof all 3-grams (0.18)\n7. Number of occurrences of the most frequent 4-gram / total number of occurrences\nof all 4-grams (0.16)\n8. Total number of occurrences of 5-grams appearing more than once / Total number\nof occurrences of all 5-grams (0.15)\n9. Total number of occurrences of 6-grams appearing more than once / Total number\nof occurrences of all 6-grams (0.14)\n10. Total number of occurrences of 7-grams appearing more than once / Total number\nof occurrences of all 7-grams (0.13)\n11. Total number of occurrences of 8-grams appearing more than once / Total number\nof occurrences of all 8-grams (0.12)\n12. Total number of occurrences of 9-grams appearing more than once / Total number\nof occurrences of all 9-grams (0.11)\n13. Total number of occurrences of 10-grams appearing more than once / Total number\nof occurrences of all 10-grams (0.10)\nA.3\nClearning the text\nIn order to normalize the punctuation in the corpus to “、” and “。”, we replaced “，” with\n“、” and “．” with “。”, respectively, based on the following rule.\n1. Check the number of occurrences the symbols “、” and “，” appear in the document.\nIf “，” appears more often than “、”, replace “，” with “、”. However, “，”\npreceding an alphanumeric character is excluded from the replacement with “、”.\n2. Check the number of occurrences the symbols “。” and “．” appear in the document.\nIf “．” appears more often than “。”, replace “．” with “。”. However, “。”\npreceding an alphanumeric character is excluded from the replacement with “．”.\nThis punctuation normalization process replaced “．” with “。” in 290,318 documents\n(0.17%) and “，” with “、” in 1,107,319 documents (0.64%).\nAlthough Trafilatura used for text extraction removes the navigation and footer text of web\npages, we sometimes see footer text that have not be removed by Trafilatura. Therefore,\nexpressions such as ”Trackback list for this article”, ”All rights reserved”, and ”Click” in\nthe last three lines of text were removed from the text if they occupy more than 30% of\nthe text in character. This footer removal process removed footers at the end of 12,617,787\ndocuments (7.3%) of the pages.\nA.4\nList of LLMs used in the experiments\nBase models\n• Llama 2 7B: https://huggingface.co/meta-llama/Llama-2-7b-hf\n• Llama 2 13B: https://huggingface.co/meta-llama/Llama-2-13b-hf\n16\n\nPreprint (under review)\n• Llama 2 70B: https://huggingface.co/meta-llama/Llama-2-70b-hf\n• Mistral 7B v0.1: https://huggingface.co/mistralai/Mistral-7B-v0.1\n• Mixtral 8x7B Instruct v0.1:\nhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\nOther models for comparison\n• CyberAgentLM2 7B: https://huggingface.co/cyberagent/calm2-7b\n• Japanese-StableLM-Base-Beta-7B:\nhttps://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b\n• Rinna Youri 7B: https://huggingface.co/rinna/youri-7b\n• Qwen 7B: https://huggingface.co/Qwen/Qwen-7B\n• Rinna Nekomata 7B: https://huggingface.co/rinna/nekomata-7b\n• Japanese Stable LM Base Gamma 7B:\nhttps://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b\n• Qwen 14B: https://huggingface.co/Qwen/Qwen-14B\n• KARAKURI LM 70B:\nhttps://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1\n• Japanese-StableLM-Base-Beta-70B:\nhttps://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b\n• Qwen 72B: https://huggingface.co/Qwen/Qwen-72B\n17\n"
    }
  ]
}