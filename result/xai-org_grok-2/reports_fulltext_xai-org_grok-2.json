{
  "model_id": "xai-org/grok-2",
  "full_texts": [
    {
      "arxiv_id": "https://docs.sglang.ai/basic_usage/send_request.html",
      "full_text": " Sending Requests &#8212; SGLang Skip to main content Back to top Ctrl + K Get Started Install SGLang Basic Usage Sending Requests OpenAI-Compatible APIs Offline Engine API SGLang Native APIs Sampling Parameters DeepSeek Usage GPT OSS Usage Llama4 Usage Qwen3-Next Usage Advanced Features Server Arguments Hyperparameter Tuning Speculative Decoding Structured Outputs Structured Outputs For Reasoning Models Tool Parser Reasoning Parser Quantization LoRA Serving PD Disaggregation Query Vision Language Model SGLang Router Observability Attention Backend Supported Models Large Language Models Multimodal Language Models Embedding Models Reward Models Rerank Models How to Support New Models Transformers fallback in SGLang Use Models From ModelScope Hardware Platforms AMD GPUs Blackwell GPUs CPU Servers TPU NVIDIA Jetson Orin Ascend NPUs Developer Guide Contribution Guide Development Guide Using Docker Benchmark and Profiling Bench Serving Guide References Troubleshooting and Frequently Asked Questions Environment Variables Production Metrics Multi-Node Deployment Custom Chat Template Frontend Language Learn more Repository Show source Suggest edit Open issue .ipynb .pdf Sending Requests Contents Launch A Server Using cURL Using Python Requests Using OpenAI Python Client Streaming Using Native Generation APIs Streaming Sending Requests # This notebook provides a quick-start guide to use SGLang in chat completions after installation. For Vision Language Models, see OpenAI APIs - Vision . For Embedding Models, see OpenAI APIs - Embedding and Encode (embedding model) . For Reward Models, see Classify (reward model) . Launch A Server # [1]: from sglang.test.doc_patch import launch_server_cmd from sglang.utils import wait_for_server , print_highlight , terminate_process # This is equivalent to running the following command in your terminal # python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --host 0.0.0.0 server_process , port = launch_server_cmd ( &quot;&quot;&quot; python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct \\ --host 0.0.0.0 --log-level warning &quot;&quot;&quot; ) wait_for_server ( f &quot;http://localhost: { port } &quot; ) W0913 09:53:11.935000 1927155 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. W0913 09:53:11.935000 1927155 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures. `torch_dtype` is deprecated! Use `dtype` instead! WARNING:transformers.configuration_utils:`torch_dtype` is deprecated! Use `dtype` instead! All deep_gemm operations loaded successfully! W0913 09:53:21.307000 1928188 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. W0913 09:53:21.307000 1928188 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures. W0913 09:53:21.308000 1928189 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. W0913 09:53:21.308000 1928189 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ[&#39;TORCH_CUDA_ARCH_LIST&#39;] to specific architectures. `torch_dtype` is deprecated! Use `dtype` instead! [2025-09-13 09:53:21] `torch_dtype` is deprecated! Use `dtype` instead! [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0 [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0 [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0 [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0 [2025-09-13 09:53:23] CUDA-fused xIELU not available (No module named &#39;xielu&#39;) – falling back to a Python version. For CUDA xIELU (experimental), `pip install git+https://github.com/nickjbrowning/XIELU` [2025-09-13 09:53:23] MOE_RUNNER_BACKEND is not initialized, using triton backend All deep_gemm operations loaded successfully! Loading safetensors checkpoint shards: 0% Completed | 0/1 [00:00&lt;?, ?it/s] Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 5.56it/s] Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 5.56it/s] Capturing batches (bs=1 avail_mem=58.70 GB): 100%|██████████| 3/3 [00:00&lt;00:00, 4.81it/s] NOTE: Typically, the server runs in a separate terminal. In this notebook, we run the server and notebook code together, so their outputs are combined. To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue. To reduce the log length, we set the log level to warning for the server, the default log level is info. We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance. Using cURL # [2]: import subprocess , json curl_command = f &quot;&quot;&quot; curl -s http://localhost: { port } /v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d &#39; {{ &quot;model&quot;: &quot;qwen/qwen2.5-0.5b-instruct&quot;, &quot;messages&quot;: [ {{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France?&quot; }} ] }} &#39; &quot;&quot;&quot; response = json . loads ( subprocess . check_output ( curl_command , shell = True )) print_highlight ( response ) {'id': '3ce1df8d3d5f45e2a3dbcbf1a1279acb', 'object': 'chat.completion', 'created': 1757757213, 'model': 'qwen/qwen2.5-0.5b-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The capital of France is Paris.', 'reasoning_content': None, 'tool_calls': None}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 151645}], 'usage': {'prompt_tokens': 36, 'total_tokens': 44, 'completion_tokens': 8, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}} Using Python Requests # [3]: import requests url = f &quot;http://localhost: { port } /v1/chat/completions&quot; data = { &quot;model&quot; : &quot;qwen/qwen2.5-0.5b-instruct&quot; , &quot;messages&quot; : [{ &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;What is the capital of France?&quot; }], } response = requests . post ( url , json = data ) print_highlight ( response . json ()) {'id': 'e333610f2e8b40689c9bcdb12e3feba9', 'object': 'chat.completion', 'created': 1757757213, 'model': 'qwen/qwen2.5-0.5b-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The capital of France is Paris.', 'reasoning_content': None, 'tool_calls': None}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 151645}], 'usage': {'prompt_tokens': 36, 'total_tokens': 44, 'completion_tokens': 8, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}} Using OpenAI Python Client # [4]: import openai client = openai . Client ( base_url = f &quot;http://127.0.0.1: { port } /v1&quot; , api_key = &quot;None&quot; ) response = client . chat . completions . create ( model = &quot;qwen/qwen2.5-0.5b-instruct&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;List 3 countries and their capitals.&quot; }, ], temperature = 0 , max_tokens = 64 , ) print_highlight ( response ) ChatCompletion(id='9894207e8a1d4761afda131922e84a8a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure, here are three countries and their respective capitals:\\n\\n1. **United States** - Washington, D.C.\\n2. **Canada** - Ottawa\\n3. **Australia** - Canberra', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=151645)], created=1757757213, model='qwen/qwen2.5-0.5b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=37, total_tokens=76, completion_tokens_details=None, prompt_tokens_details=None, reasoning_tokens=0), metadata={'weight_version': 'default'}) Streaming # [5]: import openai client = openai . Client ( base_url = f &quot;http://127.0.0.1: { port } /v1&quot; , api_key = &quot;None&quot; ) # Use stream=True for streaming responses response = client . chat . completions . create ( model = &quot;qwen/qwen2.5-0.5b-instruct&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;List 3 countries and their capitals.&quot; }, ], temperature = 0 , max_tokens = 64 , stream = True , ) # Handle the streaming output for chunk in response : if chunk . choices [ 0 ] . delta . content : print ( chunk . choices [ 0 ] . delta . content , end = &quot;&quot; , flush = True ) Sure, here are three countries and their respective capitals: 1. **United States** - Washington, D.C. 2. **Canada** - Ottawa 3. **Australia** - Canberra Using Native Generation APIs # You can also use the native /generate endpoint with requests, which provides more flexibility. An API reference is available at Sampling Parameters . [6]: import requests response = requests . post ( f &quot;http://localhost: { port } /generate&quot; , json = { &quot;text&quot; : &quot;The capital of France is&quot; , &quot;sampling_params&quot; : { &quot;temperature&quot; : 0 , &quot;max_new_tokens&quot; : 32 , }, }, ) print_highlight ( response . json ()) {'text': ' Paris. It is the largest city in Europe and the second largest city in the world. It is located in the south of France, on the banks of the', 'output_ids': [785, 6722, 315, 9625, 374, 12095, 13, 1084, 374, 279, 7772, 3283, 304, 4505, 323, 279, 2086, 7772, 3283, 304, 279, 1879, 13, 1084, 374, 7407, 304, 279, 9806, 315, 9625, 11, 389, 279, 13959, 315, 279], 'meta_info': {'id': '903b3f8f5df140f58de885e6d4d8983b', 'finish_reason': {'type': 'length', 'length': 32}, 'prompt_tokens': 5, 'weight_version': 'default', 'completion_tokens': 32, 'cached_tokens': 2, 'e2e_latency': 0.07062387466430664}} Streaming # [7]: import requests , json response = requests . post ( f &quot;http://localhost: { port } /generate&quot; , json = { &quot;text&quot; : &quot;The capital of France is&quot; , &quot;sampling_params&quot; : { &quot;temperature&quot; : 0 , &quot;max_new_tokens&quot; : 32 , }, &quot;stream&quot; : True , }, stream = True , ) prev = 0 for chunk in response . iter_lines ( decode_unicode = False ): chunk = chunk . decode ( &quot;utf-8&quot; ) if chunk and chunk . startswith ( &quot;data:&quot; ): if chunk == &quot;data: [DONE]&quot; : break data = json . loads ( chunk [ 5 :] . strip ( &quot; \\n &quot; )) output = data [ &quot;text&quot; ] print ( output [ prev :], end = &quot;&quot; , flush = True ) prev = len ( output ) Paris. It is the largest city in Europe and the second largest city in the world. It is located in the south of France, on the banks of the [8]: terminate_process ( server_process ) previous Install SGLang next OpenAI-Compatible APIs Contents Launch A Server Using cURL Using Python Requests Using OpenAI Python Client Streaming Using Native Generation APIs Streaming By SGLang Team © Copyright 2023-2025, SGLang. Last updated on Sep 13, 2025. so the DOM is not blocked --> ",
      "fetch_method": "direct-html"
    }
  ]
}