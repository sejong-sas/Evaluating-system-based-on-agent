{
  "model_id": "xai-org/grok-2",
  "full_texts": [
    {
      "arxiv_id": "2409.09281",
      "full_text": "Language Models “Grok” to Copy\nAng Lv1,2,\nRuobing Xie2*,\nXingwu Sun2,\nZhanhui Kang2,\nRui Yan1,3*\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2Machine Learning Platform Department, Tencent\n3School of Computer Science, Wuhan University\n{anglv, ruiyan}@ruc.edu.cn\nruobingxie@tencent.com\nAbstract\nWe examine the pre-training dynamics of lan-\nguage models, focusing on their ability to\ncopy text from preceding context—a funda-\nmental skill for various LLM applications, in-\ncluding in-context learning (ICL) and retrieval-\naugmented generation (RAG). We propose a\nnovel perspective that Transformer-based lan-\nguage models develop copying abilities simi-\nlarly to grokking, which refers to sudden gen-\neralization on test set long after the model fit\nto the training set. Our experiments yield three\narguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of\nmodels initially lags and then abruptly saturates.\n(2) The speed of developing copying ability is\nindependent of the number of tokens trained,\nsimilarly to how grokking speed is unaffected\nby dataset size as long as the data distribution\nis preserved. (3) Induction heads, the attention\nheads responsible for copying, form from shal-\nlow to deep layers during training, mirroring\nthe development of circuits in deeper layers\nduring grokking. We contend that the connec-\ntion between grokking and context copying can\nprovide valuable insights for more effective lan-\nguage model training, ultimately improving in-\ncontext performance. For example, we demon-\nstrated that techniques that enhance grokking,\nsuch as regularization, either accelerate or en-\nhance the development of context copying.\n1\nIntroduction\nLarge language models (LLMs) can learn, re-\ntrieve, and reason from input context, facilitating\nvarious applications such as in-context learning\n(ICL, Brown et al., 2020) and retrieval-augmented\ngeneration (RAG, Lewis et al., 2020). Despite these\nachievements, several shortcomings have been re-\nported regarding LLMs’ in-context capacities. For\n*\nCorresponding\nauthors:\nRuobing\nXie\n(ruob-\ningxie@tencent.com) and Rui Yan (ruiyan@ruc.edu.cn)\ninstance, the order of ICL demonstrations mat-\nters (Lu et al., 2022) and LLMs’ awareness of dif-\nferent contextual positions fluctuates (Liu et al.,\n2023). We believe that studying the mechanisms\nbehind the development of in-context capabilities\nduring pre-training offers valuable insights for en-\nhancing LLMs from a novel perspective.\nIn this paper, we examine the pre-training dy-\nnamics of language models, focusing specifically\non their context copying capabilities. These capa-\nbilities are crucial for various LLM applications,\nincluding ICL and RAG. For example, Olsson\net al. (2022) interpret ICL as a process that en-\ntails copying and then fuzzy pattern completion.\nSimilarly, RAG exhibits this characteristic, as it\nrequires the in-context retrieval of key information,\nwhich is then copied (or integrated with additional\nparaphrasing and reasoning) as the output. This\npaper presents empirical evidence demonstrating\nthat Transformer-based language models (Vaswani\net al., 2017) develop context copying capabilities in\na manner akin to “grokking” (Power et al., 2022).\nGrokking refers to the abrupt improvement in test\nset generalization long after models have overfit.\nOur experimental method is summarized as fol-\nlows: We trained 12-layer Llama models (Touvron\net al., 2023) using 40 billion tokens and saved\ncheckpoints at regular intervals. To evaluate con-\ntext copying, we presented the models with an input\ncontext comprising multiple random token subse-\nquences, each beginning with a unique prefix, and\nlet them complete one of the prefixes presented in\nthe context. The accuracy of these completions\nserved as a measure of the models’ context copy-\ning abilities. By analyzing the evolution of con-\ntext copying accuracy and the development of cir-\ncuits (i.e., the subnetworks responsible for complet-\ning the specific task) across the saved checkpoints,\nwe argue there is a potential connection between\narXiv:2409.09281v2  [cs.CL]  6 Feb 2025\n\ngrokking and the development of context copying\ncapabilities, as outlined in the following arguments:\nArgument 1: Grokked Context Copying.\nWe\nobserve that context copying accuracy shows a sud-\nden increase long after the training loss stabilizes,\nakin to “grokking” on the test set when neural net-\nworks trained on small training sets.\nArgument\n2:\nToken-Count-Independent\nGrokking Speed.\nWe adjust the batch size to\nmanage the number of tokens trained at specific\nupdate steps. Results indicate that context copying\nis developed after certain updates, rather than\nafter processing a specific quantity of tokens.\nSimilarly,\nthe\ndata-amount-independent\n(i.e.,\ntoken-count-independent) generalization speed is a\ncharacteristic of grokking (Wang et al., 2024).\nWe found that a higher learning rate speeds up\ngrokked copying, suggesting it occurs at a specific\noptimization intensity, determined by the learning\nrate and update steps. These experiments under-\nscore the importance of careful hyperparameter\nselection in training language models for capaci-\nties like context copying, as their development isn’t\nnecessarily reflected in pre-training loss reduction.\nArgument 3: Deeper Circuit Formation.\nWe\nnote that induction heads (Olsson et al., 2022), at-\ntention heads responsible for copying tokens, form\nfrom shallow to deep layers during training, consis-\ntent with research showing deeper circuits form in\nTransformers after grokking (Wang et al., 2024).\nBased on the novel perspective that language\nmodels grok to copy, we pre-trained language\nmodels using regularization techniques, which are\nknown to enhance grokking. These techniques\nlead to either faster copying acquisition or higher\naccuracy.\nOur findings highlight a promising\nand efficient research approach: developing im-\nproved language models with enhanced in-context\nperformance by leveraging an understanding of\ngrokking. This efficiency arises from the fact that\nstudies on grokking can utilize smaller, synthe-\nsized datasets, thereby avoiding the extensive and\nresource-intensive trials required for directly pre-\ntraining language models.\n2\nGeneral Setup\nModel Architecture and Hyper-parameters.\nWe train small Llama models (Touvron et al., 2023)\non a subset of the RedPajama dataset (Computer,\n2023), comprising 40 billion tokens, with the task\n94071d6780d7ba717e\\n\n5015906d28e186d2bb\\n\n……\n8bc19970dd5b8e7350\\n\n94071d6780d7\n50 random token\nsubsequences\nThe prefix of \nthe 1st\nsubsequence\nFigure 1: An test input example when i = 1. The\ncorrect completion of this input should be ba717e.\nof next-token prediction. Our model has 162M pa-\nrameters (12 layers, each with 12 attention heads;\nThe hidden state dimension is 768, and the inter-\nmediate dimension of MLP layers is 3,072.) The\ncontext length is 1,024 tokens. We use the Llama\ntokenizer with a vocabulary of 32,000 tokens. Un-\nless otherwise specified, the following hyperparam-\neters are used: The AdamW optimizer (Loshchilov\nand Hutter, 2019) with (β1, β2) = (0.9, 0.999), a\nlearning rate of 0.1, 2000 warmup steps, and the\nnorm clip value of 1. Our training is conducted on\n8 A100 GPUs, with a batch size of 64 per GPU.\nEvaluating Context Copying.\nEach test sample\nconsists of 50 random-token sequences, which are\nconcatenated to form a single long sequence. These\nsequences have an average length of 18 tokens, and\nwe ensure that the 12-gram prefix and 6-gram suffix\nof each sequence is unique. We append the prefix\nof the i-th sequence to the end of the concatenated\nsequences, which together serve as the model’s\ninput. An example input case is shown in Figure 1.\nOur test set includes 500 samples.\nWe ask the model to continue the input. An out-\nput is correct if it copies the suffix of the queried\nprefix from the context, since random token se-\nquences lack meaningful semantics and the most\nnatural continuation is to generate the suffix of the\nprefix that has appeared in the context (Olsson et al.,\n2022). To comprehensively assess context copying\ncapabilities across different contextual positions,\nwe evaluate the model for every i mod 5 = 0.\nUnless specifically indicated, we report the aver-\nage accuracy across these positions, from models\ntrained with 3 different random seeds.\n3\nLanguage Models “Grok” to Copy\nWe propose that language models develop context\ncopying in a manner similar to “grokking”. This\nsection presents three arguments, along with sup-\nporting experiments and analyses.\n\nTokens\nLoss\nAcc.\nFigure 2: We illustrate the average context copying\naccuracy by the bars, and the pre-training loss by the\nline. The X-axis represents the number of tokens trained.\nA clear grokked copying occurs at 15B tokens.\nBatch Size = 32\nAcc.\nBatch Size = 64\nBatch Size = 128\nTokens\nSteps\nFigure 3: We manage the token count trained at specific\nsteps by adjusting the batch size. Three models trained\nwith different batch size develop fundamental copying\nabilities after around 38,000 update steps, despite train-\ning on varying numbers of tokens.\nFor Argument 1, we present the context copy-\ning accuracy and pre-training loss in Figure 2. The\ntraining loss stabilizes after 5B tokens, indicating\nthat the fundamental language modeling has been\nestablished (i.e., fitted to the training distribution).\nHowever, the accuracy is low until 10B tokens have\nbeen trained. A surge in accuracy occurs at 15B\ntokens. This pattern of developing robust context\ncopying resembles grokking (Power et al., 2022).\nFor argument 2, we trained another two models\nusing the same setups and same initial weights as\ndescribed in Section 2, but with batch sizes of 32\nand 128. Our results indicate that grokked context\ncopying is independent of the token count. Figure 3\nshows that with a fixed learning rate, to achieve\nsimilar accuracy to models using a batch size of\n64, models trained with a batch size of 128 (32)\nrequire twice (half) the token count, as their update\nsteps are equal. This finding aligns with observa-\nTokens\nLoss\nAcc.\nTokens\nLoss\nAcc.\nBatch Size=32\nBatch Size=128\nFigure 4: With a fixed learning rate,the convergence rate\non the training set, as indicated by the training loss, is\nrelated to the token count. However, under similar con-\nvergence rates, the copying capacity varies significantly,\nwhich is influenced by the number of update steps.\ntions (Wang et al., 2024) that data quantity does\nnot affect the grokking speed. The consistency en-\nhances the connection between grokking and the\ndevelopment of context copying.\nNotably, we observed that the convergence on\nthe training set is token-count-dependent, although\ncopying performance is slowed down with larger\nbatch sizes, as shown in Figure 4. We assume that\nusing an appropriately smaller batch size to update\nthe models with more steps within a single epoch\nmay facilitate the development of capacities that\nare not reflected in the training loss reduction.\nMoreover, we examine the impact of learning\nrates. Figure 5 indicates that an increased learn-\ning rate facilitates earlier and stronger grokking.\nConsequently, we assume that the grokked con-\ntext copying doesn’t emerge until the optimization\nreaches a specific intensity, which is influenced by\nboth the learning rate and the number of update\nsteps.\nFor argument 3, we examined the evolution of\ninduction heads in our models. Induction heads (El-\nhage et al., 2021) are the primary circuit for condi-\n\nLoss\nTokens\nLoss\nLoss\nLR = 1e-4\nLR = 2e-4\nLR = 4e-4\nAcc.\nAcc.\nAcc.\nFigure 5: With a fixed batch size (64), a larger learning\nrate accelerates the grokking to copy.\ntional copying in Transformer-based language mod-\nels and have been identified as a general mechanism\nacross various models (Lv et al., 2024). Consider\na sequence “A, B, ..., A” input to the language\nmodel, where A and B are arbitrary tokens. In-\nduction heads work based on collaboration across\nlayers, enabling the model to output B. In shal-\nlower layers, certain attention heads move each\ntoken’s information to its next position; in deeper\nlayers, induction heads at the final position (i.e., the\nsecond A) attend to B (since a subspace of hidden\nstates at B’s position contains information from the\nfirst A) and copy the attended B as the output.\nWe introduce the induction score I(L,H), which\nquantifies the similarity between the behavior of the\nH-th head in layer L—referred to as (L, H)—and\nthat of an ideal induction head. We establish I(L,H)\nas a value within the range of [−1, 1], defined as:\nI(L,H) = ¯A(L,H) · EP (L,H).\n(1)\nIn Eq. 1, ¯A(L,H) ∈[0, 1] measures the induction\nattention pattern: when inputting a random token\nsequence of length 2s which contains two identical\nsubsequences of length s (set to 100), we denote\nthe average attention weight assigned from position\ns + i −1 to i as ¯A(L,H), i ∈[1, s −1]. Induction\nheads are expected to exhibit a high ¯A(L,H) score.\nEP (L,H) ∈[−1, 1] in Eq. 1 is the eigenvalue\npositivity of the OV circuit (Elhage et al., 2021)\nTokens\n𝐼(\",$)\nFigure 6: The evolution of induction heads during train-\ning. A bar’s height represents the I(L,H) value. Bars\nexhibiting larger values positioned nearer to the X-axis.\nThe results in this figure are from a single model.\nAcc.\nTokens\nFigure 7: Regularization positively impacts the grokked\ncopying. Compared with vanilla models, dropout ac-\ncelerates the grokking process, advancing the abrupt\naccuracy increase from 15B tokens to 10B tokens, albeit\nwith increased fluctuation in the evolutionary dynamics.\nBoth techniques improve the final accuracy.\nof the head: EP (L,H) = P\ni λi/ P\ni |λi|. λi is\nthe i-th eigenvalue of (WUW (L,H)\nO\nW (L,H)\nV\nWE),\nand W (L,H)\nO\nand W (L,H)\nV\nare weights of the value\nand output projection in head (L, H), while WE\nand WU are model’s embedding and unembedding\nmatrices. A high EP (L,H) implies that the head\ncopies the tokens it attends to as output. Overall, a\nhigher I(L,H) indicates a stronger induction head.\nFigure 6 illustrates the evolution of induction\nheads during training, revealing that they develop\nfrom shallower to deeper layers. This findings\nechos Wang et al. (2024), who proposes that after\ngrokking, models develop circuits in deeper layers.\n\n4\nApplication\nViewing the development of context copying as a\nspecial grokking inspires us to examine the impact\nof regularization, as it enhances grokking (Nanda\net al., 2023). We train models using (1) 10% atten-\ntion dropout and (2) weight decay (λ = 0.1). Fig-\nure 7 shows that their positive impact: with dropout,\nthe model groks to copy earlier; both techniques im-\nprove the accuracy compared to the vanilla model.\n5\nDiscussions\nWe sincerely appreciate the anonymous reviewers\nfor their valuable feedback. In this section, we\naddress key points raised in their reviews, which\nmay also be of interest to a broader audience.\n1. Our motivation for using copying tasks to\nmeasure in-context ability.\nInduction heads, the\nkey components responsible for in-context learn-\ning, are known to perform “copy and paste,” as\ndescribed by (Olsson et al., 2022). In essence, in-\nduction heads “complete the pattern” by copying\nand extending sequences that have occurred previ-\nously. This behavior motivates our exploration of\ncopying, which are foundational to understanding\nin-context abilities.\nMoreover, the copying task employed in this\nstudy has proven effective in previous research\non RAG (Tan et al., 2025) and in-context abili-\nties (Chen et al., 2024).\n2.\nWe suggest evaluating grokking through\ndownstream performance rather than training\nloss.\nIn our task, the training objective is natural\nlanguage modeling, while the testing task focuses\non general copying. As a result, the training loss\ndoesn’t fully capture the performance saturation\nseen in traditional grokking tasks. This is because\ncopying can be viewed as a skill learned during\npretraining, and once copying proficiency saturates,\nfurther improvements in other abilities can still lead\nto a decrease in training loss.\nTo demonstrate that copying on the training\ndata has reached saturation, we measured the “ICL\nscore” proposed by (Olsson et al., 2022), which\ntracks the development of in-context abilities. Our\nresults show that after approximately 4,000 train-\ning steps (about 1.85 billion tokens), the ICL score\nstabilizes at -0.5 nats. Since testing accuracy con-\ntinues to improve well after this saturation point,\nwe infer that once copying accuracy is “grokked,”\nreductions in training loss primarily stem from im-\nprovements in other abilities, rather than further\nprogress in in-context copying.\n3. The trade-offs between knowledge acquisition\nand in-context ability.\nSome studies (Chang\net al., 2024) suggest that large batch sizes enhance\nknowledge acquisition but hinder the development\nof in-context abilities, highlighting a trade-off be-\ntween the two (Nafar et al., 2024; Yu et al., 2023).\nWhile large batch sizes slow down in-context abil-\nity acquisition, their overall effect in real-world\napplications remains difficult to quantify, necessi-\ntating further research.\n4. Properties of Grokking\nThe properties of\ngrokking are not limited to the three arguments we\nhave exemplified. Many studies (Miller et al., 2024;\nFan et al., 2024; Liu et al., 2022; Lee et al., 2024)\nexplore various aspects of grokking; we list some\nfor readers who may be interested.\n6\nConclusions\nThis paper introduces a novel perspective that\nthe development of context copying is a special\ngrokking. It holds the potential to provide meaning-\nful insights that can be applied to language models,\nas we did in Section 4. We hope a better under-\nstanding of grokking in future works provide more\ninsights for developing stronger language models.\nLimitations\nThis paper focuses on the copying task to reflect\nthe development of in-context capacities. Future\ninnovations on improving the language model with\nbetter in-context capacities (e.g., ICL) might bene-\nfit from the correlations with grokking. However,\nit is important to note that ICL presents a higher\nlevel of complexity compared to simple copying\ntasks. Due to our limited computational resources,\nwe were unable to train language models to achieve\nrobust ICL performance, and therefore did not eval-\nuate ICL tasks.\nAcknowledgement\nRuobing\nXie\nis\nsupported\nby\nthe\nYoung\nElite Scientists Sponsorship Program by CAST\n(2023QNRC001). Ang Lv is supported by the\nOutstanding Innovative Talents Cultivation Funded\nPrograms 2024 of Renmin University of China.\n\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nHoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee\nYang, Youngkyung Seo, Du-Seong Chang, and Min-\njoon Seo. 2024. How do large language models ac-\nquire factual knowledge during pretraining?\nYuhan Chen, Ang Lv, Jian Luan, Bin Wang, and Wei Liu.\n2024. Hope: A novel positional encoding without\nlong-term decay for enhanced context awareness and\nextrapolation.\nTogether Computer. 2023. Redpajama: An open source\nrecipe to reproduce llama training dataset.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nSimin Fan, Razvan Pascanu, and Martin Jaggi. 2024.\nDeep grokking: Would deep neural networks gener-\nalize better?\nJaerin Lee, Bong Gyun Kang, Kihoon Kim, and Ky-\noung Mu Lee. 2024. Grokfast: Accelerated grokking\nby amplifying slow gradients.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts.\nZiming Liu, Ouail Kitouni, Niklas S Nolte, Eric\nMichaud, Max Tegmark, and Mike Williams. 2022.\nTowards understanding grokking: An effective theory\nof representation learning. In Advances in Neural\nInformation Processing Systems, volume 35, pages\n34651–34663. Curran Associates, Inc.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAng Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang,\nLifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan.\n2024. Interpreting key mechanisms of factual recall\nin transformer-based language models.\nJack Miller, Charles O’Neill, and Thang Bui. 2024.\nGrokking beyond neural networks: An empirical ex-\nploration with model complexity.\nAliakbar Nafar, Kristen Brent Venable, and Parisa Ko-\nrdjamshidi. 2024. Learning vs retrieval: The role of\nin-context examples in regression with llms.\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess\nSmith, and Jacob Steinhardt. 2023. Progress mea-\nsures for grokking via mechanistic interpretability.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nAlethea Power, Yuri Burda, Harri Edwards, Igor\nBabuschkin, and Vedant Misra. 2022.\nGrokking:\nGeneralization beyond overfitting on small algorith-\nmic datasets.\nTao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao\nWu, yongbo wang, Feng Wang, Jingtong Wu, xin\nlu, and Rui Yan. 2025. PEAR: Position-embedding-\nagnostic attention re-weighting enhances retrieval-\naugmented generation with zero inference overhead.\nIn THE WEB CONFERENCE 2025.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nBoshi Wang, Xiang Yue, Yu Su, and Huan Sun. 2024.\nGrokked transformers are implicit reasoners: A mech-\nanistic journey to the edge of generalization.\n\nQinan Yu, Jack Merullo, and Ellie Pavlick. 2023. Char-\nacterizing mechanisms for factual recall in language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9924–9959, Singapore. Association for Com-\nputational Linguistics.\n"
    }
  ]
}