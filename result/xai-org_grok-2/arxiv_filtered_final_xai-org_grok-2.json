{
  "1-1 (Weights)": "",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "",
  "1-1 (Weights)__evidence": [],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [],
  "1-5 (Architecture)": "",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Grokking refers to the abrupt improvement in test set generalization long after models have overfit. Our experimental method is summarized as follows: We trained 12-layer Llama models (Touvron et al., 2023) using 40 billion tokens and saved checkpoints at regular intervals."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The materials that explicitly mention the target tokens describe a pre-training approach for grok-2 that is centered on the observation that “language models grok to copy.” To exploit this phenomenon, the authors say they “pre-trained language models using regularization techniques, which are known to enhance grokking.” They further provide empirical insight into the role of the learning-rate schedule: “Figure 5 indicates that an increased learning rate facilitates earlier and stronger grokking,” and they directly tie this to copying behaviour, noting that “a higher learning rate speeds up grokked copying, suggesting it occurs at a specific optimization intensity, determined by the learning rate and update steps.” In short, the quoted passages emphasize (1) the deliberate use of regularization during pre-training to strengthen the model’s ability to “grok,” and (2) a systematic exploration of learning-rate magnitude, showing that larger rates accelerate and intensify the emergent copying capability, with the critical point governed jointly by learning rate and the number of optimization steps.",
  "3-2 (Fine-tuning)": "",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Based on the novel perspective that language models grok to copy, we pre-trained language models using regularization techniques, which are known to enhance grokking."
    },
    {
      "source": "[pdf_text]",
      "quote": "Figure 5 indicates that an increased learning rate facilitates earlier and stronger grokking."
    },
    {
      "source": "[pdf_text]",
      "quote": "We found that a higher learning rate speeds up grokked copying, suggesting it occurs at a specific optimization intensity, determined by the learning rate and update steps."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}