{
  "1-1 (Weights)": "The project openly states that \"our model checkpoints are stored in the block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct).\"  In other words, the complete Kimi-K2-Instruct weights are publicly hosted on Hugging Face and downloadable by anyone, and they are provided in the specialised block-FP8 precision format rather than standard FP16/FP32 files.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Our model checkpoints are stored in the block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct)."
    }
  ],
  "1-2 (Code)": "A single explicit sentence covers the code release status: \"Both the code repository and the model weights are released under the [Modified MIT License](LICENSE).\"  This confirms that source code accompanying Kimi-K2-Instruct is publicly available alongside the weights.  While the quote does not enumerate which exact pipeline stages (pre-training, fine-tuning, RLHF) are present, it does make clear that whatever code is in the repository is licensed for public use under the same terms as the weights.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Both the code repository and the model weights are released under the [Modified MIT License](LICENSE)."
    }
  ],
  "1-3 (License)": "Licensing is governed by a \"Modified MIT License.\"  The repository highlights this with a badge (\"License-Modified_MIT\") and the LICENSE file itself.  The authors spell out the sole deviation from the vanilla MIT terms: \"if the Software (or any derivative works thereof) is used for any of your commercial products or services that have more than 100 million monthly active users, or more than 20 million US dollars in monthly revenue, you shall prominently display \\\"Kimi K2\\\" on the user interface of such product or service.\"  Apart from this attribution clause for very large-scale commercial deployments, all standard MIT freedoms‚Äîuse, copy, modification, redistribution, and commercial exploitation‚Äîremain intact for both the code and the weights.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "Both the code repository and the model weights are released under the [Modified MIT License](LICENSE)."
    },
    {
      "source": "[license_file]",
      "quote": "Our only modification part is that, if the Software (or any derivative works thereof) is used for any of your commercial products or services that have more than 100 million monthly active users, or more than 20 million US dollars (or equivalent in other currencies) in monthly revenue, you shall prominently display \"Kimi K2\" on the user interface of such product or service."
    },
    {
      "source": "[readme]",
      "quote": "lign=\"center\" style=\"line-height: 1;\">\n <a href=\"https://github.com/moonshotai/Kimi-K2/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-Modified_MIT-f5de53?&color=f5de53\"/></a>\n</div>\n\n<p align=\"center\">\n<b>üì∞&nbsp;&nbsp;<a href=\"https://moonshotai.github.io/Kimi-K2/\">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE"
    }
  ],
  "1-4 (Paper)": "Kimi-K2-Instruct is documented through two official resources: (1) a Tech Blog at https://moonshotai.github.io/Kimi-K2/ and (2) a full technical report available as \"tech_report.pdf\" in the GitHub repository (linked via a üìÑ Paper icon).  These constitute the formal write-ups that describe the model‚Äôs design, training, and capabilities.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<b>üì∞&nbsp;&nbsp;<a href=\"https://moonshotai.github.io/Kimi-K2/\">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>üìÑ&nbsp;&nbsp;<a href=\"https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf\">Paper</a></b>"
    }
  ],
  "1-5 (Architecture)": "Kimi K2 is described in the quotes as a ‚Äústate-of-the-art mixture-of-experts (MoE) language model.‚Äù The architecture activates 32 billion parameters for any given input while the total parameter pool across all experts reaches 1 trillion parameters, underscoring its large-scale MoE design. A configuration line explicitly labels the model with \"model_type\": \"kimi_k2\", confirming that these architectural characteristics belong to the Kimi K2 variant.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters."
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"kimi_k2\","
    }
  ],
  "1-6 (Tokenizer)": "The model uses the Tiktoken tokenizer implementation. The code reference ‚Äúmegatron/tokenizer/tiktoken_tokenizer.py‚Äù indicates that tokenization, as well as encoding and decoding, are handled through this file. The tokenizer class inherits from the Hugging Face [`PreTrainedTokenizer`], meaning it reuses the standard interfacing methods provided by that base class for fundamental operations such as tokenization, conversion, and serialization. The vocabulary resource is defined in the constant `VOCAB_FILES_NAMES = {\"vocab_file\": \"tiktoken.model\"}`, which shows that the core vocabulary resides in the file named `tiktoken.model`.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[py_files/tokenization_kimi.py]",
      "quote": "Tokenizing and encoding/decoding text using the Tiktoken tokenizer. See megatron/tokenizer/tiktoken_tokenizer.py."
    },
    {
      "source": "[py_files/tokenization_kimi.py]",
      "quote": "This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods."
    },
    {
      "source": "[py_files/tokenization_kimi.py]",
      "quote": "VOCAB_FILES_NAMES = {\"vocab_file\": \"tiktoken.model\"}"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The available material specifies that developers can reach the Kimi K2 interface at ‚Äúhttps://platform.moonshot.ai.‚Äù  The provider explicitly states that they ‚Äúprovide OpenAI/Anthropic-compatible API for you,‚Äù signalling that request and response schemas align with the two most common industry conventions.  One concrete compatibility rule is spelled out in a second sentence: ‚ÄúThe Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications.‚Äù  Taken together, these two sentences convey every known public detail: the endpoint location, its high-level compatibility promise, and the exact rescaling formula applied to the temperature parameter so legacy Anthropic-tuned clients work unmodified.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you."
    },
    {
      "source": "[readme]",
      "quote": "The Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications."
    }
  ],
  "3-1 (Pre-training)": "The documentation summarises the core pre-training facts in one compound statement: ‚ÄúKimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. ‚Äì Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.‚Äù  From this we learn that Kimi K2 adopts a Mixture-of-Experts architecture, that the full parameter count reaches one trillion while only 32 billion are active per token, and that the training run processed 15.5 trillion tokens without encountering instability.  No additional hyper-parameters, optimiser settings, curriculum design, or data-pipeline specifics are supplied in the provided quotes.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. - Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning information is limited to a single sentence: ‚Äú**Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences.‚Äù  This tells us that, after base pre-training, Kimi K2 undergoes a post-training or instruction-tuning phase that yields the ‚ÄúKimi-K2-Instruct‚Äù variant, and that this derivative is considered optimal for general chat and agent-style deployments.  No further details about datasets, objectives, or procedural steps are provided in the quoted material.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The only disclosed pre-training detail for moonshotai/kimi-k2-instruct is that ‚ÄúKimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters ‚Ä¶ Pre-trained a 1 T parameter MoE model on 15.5 T tokens with zero training instability,‚Äù and that the training run used the Muon optimizer. From these sentences we can infer that the developers fed the model an extremely large corpus‚Äî15.5 trillion tokens‚Äîduring pre-training, achieving stable convergence throughout. No additional information about the nature of those tokens (data domains, languages, sources, licenses, or geographic/temporal coverage) is revealed in the provided material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n- Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}