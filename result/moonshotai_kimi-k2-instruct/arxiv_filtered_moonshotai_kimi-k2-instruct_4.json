{
  "4-1 (Pre-training Data)": "The quotes describe a very large-scale, carefully assembled corpus for kimi K2.  The trillion-parameter mixture-of-experts (MoE) base model is “pre-trained on 15.5 trillion high-quality tokens,” a figure that is repeated several times and is explicitly credited with producing “zero loss spike” during training.  All quoted sentences agree that the 15.5 T tokens are deliberately drawn from four major domains—“Web Text, Code, Mathematics, and Knowledge.”  The data were “curated, high-quality,” and their collection involved “rigorous correctness and quality validation” together with “targeted data experiments” so that the final mix would be both diverse and effective.  An innovation over the preceding K1.5 generation is “the introduction of a synthetic data generation strategy to increase token utility,” signalling that not only raw scraping but also purpose-built synthetic material entered the corpus.  Finally, the training regime is linked to the “token-efficient MuonClip optimizer,” whose use together with the 15.5 T-token corpus is credited with “stable, scalable pre-training.”",
  "4-2 (Fine-tuning Data)": "Fine-tuning (referred to as “post-training” in the quotes) for kimi K2 is multi-staged and data-intensive.  One quote states that the model “undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline,” indicating that substantial portions of the instruction-tuning data are machine-generated through agentic simulations.  The team “construct[s] a large-scale instruction-tuning dataset spanning diverse domains,” following two explicit principles: “maximizing prompt diversity and ensuring high response quality.”  Throughout post-training they “employ the Muon optimizer… and recommend its use for fine-tuning with K2,” suggesting continuity of the optimizer used in pre-training.  For response generation, they “adopt K1.5 and other in-house domain-specialized expert models to generate candidate responses for various tasks,” after which “LLMs or human-based judges… perform automated quality evaluation and filtering.”  Together, the quotes portray an instruction-tuning corpus that is (1) partly synthetic, (2) breadth-oriented in prompts, (3) quality-screened by both automated and human mechanisms, and (4) produced under an optimization regime consistent with earlier stages.",
  "4-3 (Reinforcement Learning Data)": "The reinforcement-learning phase is explicitly tied to data obtained from both real and synthetic environments.  One quote notes “a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.”  Building on earlier K1.5 work, the authors “continue to scale RL in both task diversity and training FLOPs in K2,” signalling that the RL dataset is broader and computationally heavier than before.  An internal “Gym-like extensible framework” is said to “facilitate RL across a wide range of scenarios,” and the first loop step is that “the K2 actor generates responses for general prompts that cover a wide range of use cases.”  Another sentence summarises the data signal: post-training “combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards and self-critic feedbacks.”  Therefore, the RL data used by kimi K2 consist of wide-coverage prompts, model-generated candidate actions, and reward signals that mix external verification with self-critique, all orchestrated in a scalable, gym-style environment.",
  "4-4 (Data Filtering)": "Multiple filtering mechanisms are highlighted.  For the 15.5 T-token pre-training set, the team “performed rigorous correctness and quality validation” for every one of the four domains and ran “targeted data experiments” to guarantee both diversity and effectiveness.  In synthetic augmentation, “fidelity checks… compare the semantic alignment of each rephrased passage with its source,” ensuring that rewritten content stays faithful.  During instruction-tuning, candidate outputs from K1.5 and other expert models are passed through “LLMs or human-based judges to perform automated quality evaluation and filtering,” adding another quality gate.  The quotes also reveal that kimi-K2-Instruct maintains “rigorous decontamination procedures,” which are mentioned in connection with achieving a 60.0 % accuracy on the Aider-Polyglot benchmark—evidence that aggressive filtering did not harm, and may have enhanced, downstream performance.  Across all stages—pre-training curation, synthetic data consistency checking, instruction-tuning response vetting, and benchmark-oriented decontamination—the project employs layered, domain-specific filters designed to remove low-quality, mis-aligned, or contaminating material before it ever reaches the final model.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[abstract]",
      "quote": "Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike."
    },
    {
      "source": "[pdf_text]",
      "quote": "The base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer model, pre-trained on 15.5 trillion high-quality tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "The base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer [72] model, pre-trained on 15.5 trillion high-quality tokens."
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge."
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[abstract]",
      "quote": "During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments."
    },
    {
      "source": "[pdf_text]",
      "quote": "We employ the Muon optimizer [33] in our post-training and recommend its use for fine-tuning with K2."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We employ the Muon optimizer [33] in our post-training and recommend its use for fine-tuning with K2. We construct a large-scale instruction-tuning dataset spanning diverse domains, guided by two core principles: maximizing prompt diversity and ensuring high response quality."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We employ the Muon optimizer [33] in our post-training and recommend its use for fine-tuning with K2. We adopt K1.5 [35] and other in-house domain-specialized expert models to generate candidate responses for various tasks, followed by LLMs or human-based judges to perform automated quality evaluation and filtering."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[abstract]",
      "quote": "During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments."
    },
    {
      "source": "[pdf_text]",
      "quote": "Reinforcement learning (RL) is believed to have better token efficiency and generalization than SFT. Based on the work of K1.5 [35], we continue to scale RL in both task diversity and training FLOPs in K2."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the first core process of the learning loop, the K2 actor generates responses for general prompts that cover a wide range of use cases."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "Based on the work of K1.5 [35], we continue to scale RL in both task diversity and training FLOPs in K2. To support this, we develop a Gym-like extensible framework that facilitates RL across a wide range of scenarios."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "In the first core process of the learning loop, the K2 actor generates responses for general prompts that cover a wide range of use cases."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Kimi K2 achieves stable, scalable pre-training. Post-training combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards and self-critic feedbacks."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge. For each domain, we performed rigorous correctness and quality validation and designed targeted data experiments to ensure the curated dataset achieved both high diversity and effectiveness."
    },
    {
      "source": "[sections/Evaluation Details]",
      "quote": "Moreover, on the Aider-Polyglot benchmark, Kimi-K2-Instruct attains a 60.0% accuracy while employing rigorous decontamination procedures, further illustrating its strength and reliability across diverse coding environments."
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility. To ensure consistency between original and rewritten content, we perform fidelity checks that compare the semantic alignment of each rephrased passage with its source."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We employ the Muon optimizer [33] in our post-training and recommend its use for fine-tuning with K2. We adopt K1.5 [35] and other in-house domain-specialized expert models to generate candidate responses for various tasks, followed by LLMs or human-based judges to perform automated quality evaluation and filtering."
    }
  ]
}