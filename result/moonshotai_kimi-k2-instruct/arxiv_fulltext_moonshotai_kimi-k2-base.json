{
  "model_id": "moonshotai/Kimi-K2-Base",
  "full_texts": [
    {
      "arxiv_id": "2507.20534",
      "full_text": "KIMI K2: OPEN AGENTIC INTELLIGENCE\nTECHNICAL REPORT OF KIMI K2\nKimi Team\nABSTRACT\nWe introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated\nparameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon\nMuon with a novel QK-clip technique to address training instability while enjoying the advanced\ntoken efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero\nloss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a\nlarge-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the\nmodel improves its capabilities through interactions with real and synthetic environments.\nKimi K2 achieves state-of-the-art performance among open-source non-thinking models, with\nstrengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench\n(En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual — surpassing most open\nand closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding,\nmathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025,\n75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position\nKimi K2 as one of the most capable open-source large language models to date, particularly in\nsoftware engineering and agentic tasks. We release our base and post-trained model checkpoints1 to\nfacilitate future research and applications of agentic intelligence.\nSWE-bench Veriﬁed\n0\n20\n40\n60\n80\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\n72.5\n54.6\n34.4\n38.8\n65.8\nSWE-bench Multilingual\n0\n20\n40\n60\n80\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Sonnet\n51.0\n31.5\n20.9\n25.8\n47.3\nLiveCodeBench v6\n0\n20\n40\n60\n80\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n44.7\n47.4\n44.7\n37.0\n46.9\n53.7\nOJBench\n0\n20\n40\n60\n80\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n19.5\n19.6\n19.5\n11.3\n24.0\n27.1\n0\n25\n50\n75\n100\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n41.0\n67.6\n54.4\n37.3\n48.8\n66.1\nAceBench (en)\n0\n25\n50\n75\n100\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n74.5\n75.6\n80.1\n70.5\n72.7\n76.5\nAgentic and Competitive Coding\nTool Use\nAIME 2025\n0\n25\n50\n75\n100\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n46.6\n33.9\n37.0\n24.7\n46.7\n49.5\nGPQA-Diamond\n0\n25\n50\n75\n100\nKimi-K2-Instruct\nDeepSeek-V3-0324\nQwen3-235B-A22B\nOpenAI GPT-4.1\nClaude 4 Opus\nGemini 2.5 Flash non-thinking\n68.2\n74.9\n66.3\n62.9\n68.4\n75.1\nMath & STEM\nTau2-bench micro-average\nFigure 1: Kimi K2 main results.2\n1https://huggingface.co/moonshotai/Kimi-K2-Instruct\n2All models evaluated above are non-thinking models. For SWE-bench Multilingual, we evaluated only Claude 4 Sonnet because\nthe cost of Claude 4 Opus was prohibitive.\narXiv:2507.20534v1  [cs.LG]  28 Jul 2025\n\nKimi K2\nTECHNICAL REPORT\n1\nIntroduction\nThe development of Large Language Models (LLMs) is undergoing a profound paradigm shift towards Agentic\nIntelligence – the capabilities for models to autonomously perceive, plan, reason, and act within complex and dynamic\nenvironments. This transition marks a departure from static imitation learning towards models that actively learn\nthrough interactions, acquire new skills beyond their training distribution, and adapt behavior through experiences [63].\nIt is believed that this approach allows an AI agent to go beyond the limitation of static human-generated data, and\nacquire superhuman capabilities through its own exploration and exploitation. Agentic intelligence is thus rapidly\nemerging as a defining capability for the next generation of foundation models, with wide-ranging implications across\ntool use, software development, and real-world autonomy.\nAchieving agentic intelligence introduces challenges in both pre-training and post-training. Pre-training must en-\ndow models with broad general-purpose priors under constraints of limited high-quality data, elevating token effi-\nciency—learning signal per token—as a critical scaling coefficient. Post-training must transform those priors into\nactionable behaviors, yet agentic capabilities such as multi-step reasoning, long-term planning, and tool use are rare\nin natural data and costly to scale. Scalable synthesis of structured, high-quality agentic trajectories, combined with\ngeneral reinforcement learning (RL) techniques that incorporate preferences and self-critique, are essential to bridge\nthis gap.\nIn this work, we introduce Kimi K2, a 1.04 trillion-parameter Mixture-of-Experts (MoE) LLM with 32 billion activated\nparameters, purposefully designed to address the core challenges and push the boundaries of agentic capability. Our\ncontributions span both the pre-training and post-training frontiers:\n• We present MuonClip, a novel optimizer that integrates the token-efficient Muon algorithm with a stability-\nenhancing mechanism called QK-Clip. Using MuonClip, we successfully pre-trained Kimi K2 on 15.5 trillion\ntokens without a single loss spike.\n• We introduce a large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations\nvia simulated and real-world environments. This system constructs diverse tools, agents, tasks, and trajectories to\ncreate high-fidelity, verifiably correct agentic interactions at scale.\n• We design a general reinforcement learning framework that combines verifiable rewards (RLVR) with a self-\ncritique rubric reward mechanism. The model learns not only from externally defined tasks but also from evaluating\nits own outputs, extending alignment from static into open-ended domains.\nKimi K2 demonstrates strong performance across a broad spectrum of agentic and frontier benchmarks. It achieves\nscores of 66.1 on Tau2-bench, 76.5 on ACEBench (en), 65.8 on SWE-bench Verified, and 47.3 on SWE-bench\nMultilingual, outperforming most open- and closed-weight baselines under non-thinking evaluation settings, closing the\ngap with Claude 4 Opus and Sonnet. In coding, mathematics, and broader STEM domains, Kimi K2 achieves 53.7\non LiveCodeBench v6, 27.1 on OJBench, 49.5 on AIME 2025, and 75.1 on GPQA-Diamond, further highlighting\nits capabilities in general tasks. On the LMSYS Arena leaderboard (July 17, 2025)3, Kimi K2 ranks as the top 1\nopen-source model and 5th overall based on over 3,000 user votes.\nTo spur further progress in Agentic Intelligence, we are open-sourcing our base and post-trained checkpoints, enabling\nthe community to explore, refine, and deploy agentic intelligence at scale.\n2\nPre-training\nThe base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer [72] model, pre-trained\non 15.5 trillion high-quality tokens. Given the increasingly limited availability of high-quality human data, we posit\nthat token efficiency is emerging as a critical coefficient in the scaling of large language models. To address this,\nwe introduce a suite of pre-training techniques explicitly designed for maximizing token efficiency. Specifically, we\nemploy the token-efficient Muon optimizer [33, 46] and mitigate its training instabilities through the introduction of\nQK-Clip. Additionally, we incorporate synthetic data generation to further squeeze the intelligence out of available\nhigh-quality tokens. The model architecture follows an ultra-sparse MoE with multi-head latent attention (MLA) similar\nto DeepSeek-V3 [10] , derived from empirical scaling law analysis. The underlying infrastructure is built to optimize\nboth training efficiency and research efficiency.\n3https://lmarena.ai/leaderboard/text\n2\n\nKimi K2\nTECHNICAL REPORT\n2.1\nMuonClip: Stable Training with Weight Clipping\nWe train Kimi K2 using the token-efficient Muon optimizer [33], incorporating weight decay and consistent update\nRMS scaling [46]. Experiments in our previous work Moonlight [46] show that, under the same compute budget and\nmodel size — and therefore the same amount of training data — Muon substantially outperforms AdamW [36, 48],\nmaking it an effective choice for improving token efficiency in large language model training.\nTraining instability when scaling Muon\nDespite its efficiency, scaling up Muon training reveals a challenge: training\ninstability due to exploding attention logits, an issue that occurs more frequently with Muon but less with AdamW\nin our experiments. Existing mitigation strategies are insufficient. For instance, logit soft-cap [69] directly clips the\nattention logits, but the dot products between queries and keys can still grow excessively before capping is applied. On\nthe other hand, Query-Key Normalization (QK-Norm) [11, 81] is not applicable to multi-head latent attention (MLA),\nbecause its Key matrices are not fully materialized during inference.\nTaming Muon with QK-Clip\nTo address this issue, we propose a novel weight-clipping mechanism QK-Clip to\nexplicitly constrain attention logits. QK-Clip works by rescaling the query and key projection weights post-update to\nbound the growth of attention logits.\nLet the input representation of a transformer layer be X. For each attention head h, its query, key, and value projections\nare computed as\nQh = XWh\nq ,\nKh = XWh\nk,\nVh = XWh\nv.\nwhere Wq, Wk, Wv are model parameters. The attention output is:\nOh = softmax\n\u0012 1\n√\nd\nQhKh⊤\n\u0013\nVh.\nWe define the max logit, a per-head scalar, as the maximum input to softmax in this batch B:\nSh\nmax =\n1\n√\nd\nmax\nX∈B max\ni,j Qh\ni Kh⊤\nj\nwhere i, j are indices of different tokens in a training sample X.\nThe core idea of QK-Clip is to rescale Wk, Wq whenever Sh\nmax exceeds a target threshold τ. Importantly, this operation\ndoes not alter the forward/backward computation in the current step — we merely use the max logit as a guiding signal\nto determine the strength to control the weight growth.\nA naïve implementation clips all heads at the same time:\nWh\nq ←γαWh\nq\nWh\nk ←γ1−αWh\nk\nwhere γ = min(1, τ/Smax) with Smax = maxh Sh\nmax, and α is a balancing parameter typically set to 0.5, applying\nequal scaling to queries and keys.\nHowever, we observe that in practice, only a small subset of heads exhibit exploding logits. In order to minimize our\nintervention on model training, we determine a per-head scaling factor γh = min(1, τ/Sh\nmax), and opt to apply per-head\nQK-Clip. Such clipping is straightforward for regular multi-head attention (MHA). For MLA, we apply clipping only\non unshared attention head components:\n• qC and kC (head-specific components): each scaled by √γh\n• qR (head-specific rotary): scaled by γh,\n• kR (shared rotary): left untouched to avoid effect across heads.\nMuonClip: The New Optimizer\nWe integrate Muon with weight decay, consistent RMS matching, and QK-Clip\ninto a single optimizer, which we refer to as MuonClip (see Algorithm 1).\nWe demonstrate the effectiveness of MuonClip from several scaling experiments. First, we train a mid-scale 9B activated\nand 53B total parameters Mixture-of-Experts (MoE) model using the vanilla Muon. As shown in Figure 2 (Left), we\nobserve that the maximum attention logits quickly exceed a magnitude of 1000, showing that attention logits explosion\nis already evident in Muon training to this scale. Max logits at this level usually result in instability during training,\nincluding significant loss spikes and occasional divergence.\n3\n\nKimi K2\nTECHNICAL REPORT\nAlgorithm 1 MuonClip Optimizer\n1: for each training step t do\n2:\n// 1. Muon optimizer step\n3:\nfor each weight W ∈Rn×m do\n4:\nMt = µMt−1 + Gt\n▷M0 = 0, Gt is the grad of Wt, µ is momentum\n5:\nOt = Newton-Schulz(Mt) ·\np\nmax(n, m) · 0.2\n▷Match Adam RMS\n6:\nWt = Wt−1 −η\n\u0000Ot + λWt−1\n\u0001\n▷learning rate η, weight decay λ\n7:\nend for\n8:\n// 2. QK-Clip\n9:\nfor each attention head h in every attention layer of the model do\n10:\nObtain Sh\nmax already computed during forward\n11:\nif Sh\nmax > τ then\n12:\nγ ←τ/Sh\nmax\n13:\nWh\nqc ←Wh\nqc · √γ\n14:\nWh\nkc ←Wh\nkc · √γ\n15:\nWh\nqr ←Wh\nqr · γ\n16:\nend if\n17:\nend for\n18: end for\n0\n2500\n5000\n7500\n10000\n12500\n15000\nTraining Steps\n0\n200\n400\n600\n800\n1000\n1200\nMax Logits\nVanilla run with Muon\n0\n50000\n100000\n150000\n200000\nTraining Steps\n0\n20\n40\n60\n80\n100\nMax Logits\nKimi K2 with MuonClip\nFigure 2: Left: During a mid-scale training run, attention logits rapidly exceed 1000, which could lead to potential\nnumerical instabilities and even training divergence. Right: Maximum logits for Kimi K2 with MuonClip and τ = 100\nover the entire training run. The max logits rapidly increase to the capped value of 100, and only decay to a stable range\nafter approximately 30% of the training steps, demonstrating the effective regulation effect of QK-Clip.\nNext, we demonstrate that QK-Clip does not degrade model performance and confirm that the MuonClip optimizer\npreserves the optimization characteristics of Muon without adversely affecting the loss trajectory. A detailed discussion\nof the experiment designs and findings is provided in the Appendix D.\nFinally, we train Kimi K2, a large-scale MoE model, using MuonClip with τ = 100 and monitor the maximum attention\nlogits throughout the training run (Figure 2 (Right)). Initially, the logits are capped at 100 due to QK-Clip. Over the\ncourse of training, the maximum logits gradually decay to a typical operating range without requiring any adjustment to\nτ. Importantly, the training loss remains smooth and stable, with no observable spikes, as shown in Figure 3, validating\nthat MuonClip provides robust and scalable control over attention dynamics in large-scale language model training.\n2.2\nPre-training Data: Improving Token Utility with Rephrasing\nToken efficiency in pre-training refers to how much performance improvement is achieved for each token consumed\nduring training. Increasing token utility—the effective learning signal each token contributes—enhances the per-token\nimpact on model updates, thereby directly improving token efficiency. This is particularly important when the supply of\nhigh-quality tokens is limited and must be maximally leveraged. A naive approach to increasing token utility is through\nrepeated exposure to the same tokens, which can lead to overfitting and reduced generalization.\n4\n\nKimi K2\nTECHNICAL REPORT\n0\n2\n4\n6\n8\n10\n12\n14\n16\nTokens (Trillion)\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\nLoss\nFigure 3: Per-step training loss curve of Kimi K2, without smoothing or sub-sampling. It shows no spikes throughout\nthe entire training process. Note that we omit the very beginning of training for clarity.\nA key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation\nstrategy to increase token utility. Specifically, a carefully designed rephrasing pipeline is employed to amplify the volume\nof high-quality tokens without inducing significant overfitting. In this report, we describe two domain-specialized\nrephrasing techniques—targeted respectively at the Knowledge and Mathematics domains—that enable this controlled\ndata augmentation.\nKnowledge Data Rephrasing\nPre-training on natural, knowledge-intensive text presents a trade-off: a single epoch\nis insufficient for comprehensive knowledge absorption, while multi-epoch repetition yields diminishing returns and\nincreases the risk of overfitting. To improve the token utility of high-quality knowledge tokens, we propose a synthetic\nrephrasing framework composed of the following key components:\n• Style- and perspective-diverse prompting: To enhance linguistic diversity while maintaining factual integrity, we\napply a range of carefully engineered prompts. These prompts guide a large language model to generate faithful\nrephrasings of the original texts in varied styles and from different perspectives.\n• Chunk-wise autoregressive generation: To preserve global coherence and avoid information loss in long\ndocuments, we adopt a chunk-based autoregressive rewriting strategy. Texts are divided into segments, rephrased\nindividually, and then stitched back together to form complete passages. This method mitigates implicit output\nlength limitations that typically exist with LLMs. An overview of this pipeline is presented in Figure 4.\n• Fidelity verification: To ensure consistency between original and rewritten content, we perform fidelity checks\nthat compare the semantic alignment of each rephrased passage with its source. This serves as an initial quality\ncontrol step prior to training.\nWe compare data rephrasing with multi-epoch repetition by testing their corresponding accuracy on SimpleQA. We\nexperiment with an early checkpoint of K2 and evaluate three training strategies: (1) repeating the original dataset for\n10 epochs, (2) rephrasing the data once and repeating it for 10 epochs, and (3) rephrasing the data 10 times with a\nsingle training pass. As shown in Table 1, the accuracy consistently improves across these strategies, demonstrating the\nefficacy of our rephrasing-based augmentation. We extended this method to other large-scale knowledge corpora and\nobserved similarly encouraging results, and each corpora is rephrased at most twice.\nTable 1: SimpleQA Accuracy under three rephrasing-epoch configurations\n# Rephrasings\n# Epochs\nSimpleQA Accuracy\n0 (raw wiki-text)\n10\n23.76\n1\n10\n27.39\n10\n1\n28.94\n5\n\nKimi K2\nTECHNICAL REPORT\npartial input excerpt 1\n256 tokens\nfull input excerpt\npartial input excerpt 2\n...\nrewrite model\nrewrite model\n...\npartial output excerpt 1\npartial output excerpt 2\n...\nfull output excerpt\n4096 tokens\nauto-regressive\nauto-regressive\nsplit\ntogether as context\nconcat\nFigure 4: Auto-regressive chunk-wise rephrasing pipeline for long input excerpts. The input is\nsplit into smaller chunks with preserved context, rewritten sequentially, and then concatenated\ninto a full rewritten passage.\nMathematics Data Rephrasing\nTo enhance mathematical reasoning capabilities, we rewrite high-quality mathemati-\ncal documents into a “learning-note” style, following the methodology introduced in SwallowMath [15]. In addition,\nwe increased data diversity by translating high-quality mathematical materials from other languages into English.\nAlthough initial experiments with rephrased subsets of our datasets show promising results, the use of synthetic data\nas a strategy for continued scaling remains an active area of investigation. Key challenges include generalizing the\napproach to diverse source domains without compromising factual accuracy, minimizing hallucinations and unintended\ntoxicity, and ensuring scalability to large-scale datasets.\nPre-training Data Overall\nThe Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality\ndata spanning four primary domains: Web Text, Code, Mathematics, and Knowledge. Most data processing pipelines\nfollow the methodologies outlined in Kimi K1.5 [35]. For each domain, we performed rigorous correctness and\nquality validation and designed targeted data experiments to ensure the curated dataset achieved both high diversity and\neffectiveness.\n2.3\nModel Architecture\nKimi K2 is a 1.04 trillion-parameter Mixture-of-Experts (MoE) transformer model with 32 billion activated parameters.\nThe architecture follows a similar design to DeepSeek-V3 [10] , employing Multi-head Latent Attention (MLA) [44] as\nthe attention mechanism, with a model hidden dimension of 7168 and an MoE expert hidden dimension of 2048. Our\nscaling law analysis reveals that continued increases in sparsity yield substantial performance improvements, which\nmotivated us to increase the number of experts to 384, compared to 256 in DeepSeek-V3. To reduce computational\noverhead during inference, we cut the number of attention heads to 64, as opposed to 128 in DeepSeek-V3. Table 2\npresents a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3.\nTable 2: Architectural comparison between Kimi K2 and DeepSeek-V3\nDeepSeek-V3\nKimi K2\n∆\n#Layers\n61\n61\n=\nTotal Parameters\n671B\n1.04T\n↑54%\nActivated Parameters\n37B\n32.6B\n↓13%\nExperts (total)\n256\n384\n↑50%\nExperts Active per Token\n8\n8\n=\nShared Experts\n1\n1\n=\nAttention Heads\n128\n64\n↓50%\nNumber of Dense Layers\n3\n1\n↓67%\nExpert Grouping\nYes\nNo\n-\n6\n\nKimi K2\nTECHNICAL REPORT\nSparsity Scaling Law\nWe develop a sparsity scaling law tailored for the Mixture-of-Experts (MoE) model family\nusing Muon. Sparsity is defined as the ratio of the total number of experts to the number of activated experts. Through\ncarefully controlled small-scale experiments, we observe that — under a fixed number of activated parameters (i.e.,\nconstant FLOPs) — increasing the total number of experts (i.e., increasing sparsity) consistently lowers both the training\nand validation loss, thereby enhancing overall model performance (Figure 5). Concretely, under the compute-optimal\nsparsity scaling law, achieving the same validation loss of 1.5, sparsity 48 reduces FLOPs by 1.69×, 1.39×, and 1.15×\ncompared to sparsity levels 8, 16, and 32, respectively. Though increasing sparsity leads to better performance, this\ngain comes with increased infrastructure complexity. To balance model performance with cost, we adopt a sparsity of\n48 for Kimi K2, activating 8 out of 384 experts per forward pass.\n1020\n1021\nTraining FLOPs\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nValidation Loss\nsparsity 8\nsparsity 16\nsparsity 32\nsparsity 48\nsparsity 64\nFigure 5: Sparsity Scaling Law. Increasing sparsity leads\nto improved model performance. We fixed the number of\nactivated experts to 8 and the number of shared experts\nto 1, and varied the total number of experts, resulting in\nmodels with different sparsity levels.\n1011\nTraining Tokens\n1.35\n1.40\n1.45\n1.50\n1.55\n1.60\n1.65\n1.70\n1.75\nValidation Loss\nmodels with number of attention heads\nequals to number of layers\ncounterparts with doubled attention heads\n1.2e+20 FLOPs\n2.2e+20 FLOPs\n4.5e+20 FLOPs\n9.0e+20 FLOPs\nFigure 6: Scaling curves for models with number of atten-\ntion heads equals to number of layers and their counter-\nparts with doubled attention heads. Doubling the number\nof attention heads leads to a reduction in validation loss\nof approximately 0.5% to 1.2%.\nNumber of Attention Heads\nDeepSeek-V3 [10] sets the number of attention heads to roughly twice the number of\nmodel layers to better utilize memory bandwidth and enhance computational efficiency. However, as the context length\nincreases, doubling the number of attention heads leads to significant inference overhead, reducing efficiency at longer\nsequence lengths. This becomes a major limitation in agentic applications, where efficient long context processing is\nessential. For example, with a sequence length of 128k, increasing the number of attention heads from 64 to 128, while\nkeeping the total expert count fixed at 384, leads to an 83% increase in inference FLOPs. To evaluate the impact of\nthis design, we conduct controlled experiments comparing configurations where the number of attention heads equals\nthe number of layers against those with double number of heads, under varying training FLOPs. Under iso-token\ntraining conditions, we observe that doubling the attention heads yields only modest improvements in validation loss\n(ranging from 0.5% to 1.2%) across different compute budgets (Figure 6). Given that sparsity 48 already offers strong\nperformance, the marginal gains from doubling attention heads do not justify the inference cost. Therefore we choose\nto 64 attention heads.\n2.4\nTraining Infrastructure\n2.4.1\nCompute Cluster\nKimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB\nRAM and 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, 8×400 Gbps RoCE\ninterconnects are utilized to facilitate communications.\n2.4.2\nParallelism for Model Scaling\nTraining of large language models often progresses under dynamic resource availability. Instead of optimizing one\nparallelism strategy that’s only applicable under specific amount of resources, we pursue a flexible strategy that allows\nKimi K2 to be trained on any number of nodes that is a multiple of 32. Our strategy leverages a combination of 16-way\n7\n\nKimi K2\nTECHNICAL REPORT\n1\n2\n3\n4\n1\n2\n3\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n1\n2\n3\n4\n5\n6\n1\n3\n2\n4\n3\n4\n5\n6\n1\n7\n2\n8\n3\n5\n6\n5\n7\n6\n8\n4\n7\n1\n3\n2\n4\n3\n4\n5\n6\n1\n7\n2\n8\n3\n5\n6\n4\n5\n1\n3\n2\n4\n3\n4\n5\n6\n1\n7\n2\n8\n3\n1\n3\n2\n4\n3\n6\n1\n5\n6\n7\n8\n8\n5\n6\n7\n8\n8\n5\n6\n7\n8\n5\n6\n7\n8\n7\n6\n7\n5\n6\n7\n5\n4\n3\n2\n2\n2\n2\n8\n6\n4\n2\n3\n1\n7\n5\nAttn\nMLP\nEP-D\nEP-C\nOffload\nEP-C\nMLP\nAttn\nWGrad\nEP-D\nLoad\nAttn\nMLP\nEP-D\nEP-C\nOffload\nEP-C\nMLP\nAttn\nWGrad\nEP-D\nPP\nOnload\n4\n4\n5\n8\nComputation\nCommunication\nOffload\nVPP + 1 warmup\nEP-C\nEP-D\nEP-D\nEP-C\nForward pass\nBackward pass\nPP communication\nEP dispatch and combine\nPP\nFigure 7: Computation, communication and offloading overlapped in different PP phases.\nPipeline Parallelism (PP) with virtual stages [28, 53, 38, 57, 47, 21], 16-way Expert Parallelism (EP) [39], and ZeRO-1\nData Parallelism [60].\nUnder this setting, storing the model parameters in BF16 and their gradient accumulation buffer in FP32 requires\napproximately 6 TB of GPU memory, distributed over a model-parallel group of 256 GPUs. Placement of optimizer\nstates depends on the training configurations. When the total number of training nodes is large, the optimizer states are\ndistributed, reducing its per-device memory footprint to a negligible level. When the total number of training nodes is\nsmall (e.g., 32), we can offload some optimizer states to CPU.\nThis approach allows us to reuse an identical parallelism configuration for both small- and large-scale experiments,\nwhile letting each GPU hold approximately 30 GB of GPU memory for all states. The rest of the GPU memory are used\nfor activations, as described in Sec. 2.4.3. Such a consistent design is important for research efficiency, as it simplifies\nthe system and substantially accelerates experimental iteration.\nEP communication overlap with interleaved 1F1B\nBy increasing the number of warm-up micro-batches, we can\noverlap EP all-to-all communication with computation under the standard interleaved 1F1B schedule [21, 53]. In\ncomparison, DualPipe [10] doubles the memory required for parameters and gradients, necessitating an increase in\nparallelism to compensate. Increasing PP introduces more bubbles, while increasing EP, as discussed below, incurs\nhigher overhead. The additional costs are prohibitively high for training a large model with over 1 trillion parameters\nand thus we opted not to use DualPipe.\nHowever, interleaved 1F1B splits the model into more stages, introducing non-trivial PP communication overhead. To\nmitigate this cost, we decouple the weight-gradient computation from each micro-batch’s backward pass and execute\nit in parallel with the corresponding PP communication. Consequently, all PP communications can be effectively\noverlapped except for the warm-up phase.\nSmaller EP size\nTo ensure full computation-communication overlap during the 1F1B stage, the reduced attention\ncomputation time in K2 (which has 64 attention heads compared to 128 heads in DeepSeek-V3) necessitates minimizing\nthe time of EP operations. This is achieved by adopting the smallest feasible EP parallelization strategy, specifically\nEP = 16. Utilizing a smaller EP group also relaxes expert-balance constraints, allowing for near-optimal speed to be\nachieved without further tuning.\n2.4.3\nActivation Reduction\nAfter reserving space for parameters, gradient buffers, and optimizer states, the remaining GPU memory on each device\nis insufficient to hold the full MoE activations. To ensure the activation memory fits within the constraints, especially\nfor the initial pipeline stages that accumulate the largest activations during the 1F1B warm-up phase, the following\ntechniques are employed.\nSelective recomputation\nRecomputation is applied to inexpensive, high-footprint stages, including LayerNorm,\nSwiGLU, and MLA up-projections [10]. Additionally, MoE down-projections are recomputed during training to further\nreduce activation memory. While optional, this recomputation maintains adequate GPU memory, preventing crashes\ncaused by expert imbalance in early training stages.\nFP8 storage for insensitive activations\nInputs of MoE up-projections and SwiGLU are compressed to FP8-E4M3 in\n1× 128 tiles with FP32 scales. Small-scale experiments show no measurable loss increase. Due to potential risks of\nperformance degradation that we observed during preliminary study, we do not apply FP8 in computation.\n8\n\nKimi K2\nTECHNICAL REPORT\nActivation CPU offload\nAll remaining activations are offloaded to CPU RAM. A copy engine is responsible for\nstreaming the offload and onload, overlapping with both computation and communication kernels. During the 1F1B\nphase, we offload the forward activations of the previous micro-batch while prefetching the backward activations of the\nnext. The warm-up and cool-down phases are handled similarly and the overall pattern is shown in Figure 7. Although\noffloading may slightly affect EP traffic due to PCIe traffic congestion, our tests show that EP communication remains\nfully overlapped.\n2.5\nTraining recipe\nWe pre-trained the model with a 4,096-token context window using the MuonClip optimizer (Algorithm 1) and the\nWSD learning rate schedule [25], processing a total of 15.5T tokens. The first 10T tokens were trained with a constant\nlearning rate of 2e-4 after a 500-step warm-up, followed by 5.5T tokens with a cosine decay from 2e-4 to 2e-5. Weight\ndecay was set to 0.1 throughout, and the global batch size was held at 67M tokens. The overall training curve is shown\nin Figure 3.\nTowards the end of pre-training, we conducted an annealing phase followed by a long-context activation stage. The\nbatch size was kept constant at 67M tokens, while the learning rate was decayed from 2e-5 to 7e-6. In this phase, the\nmodel was trained on 400 billion tokens with a 4k sequence length, followed by an additional 60 billion tokens with a\n32k sequence length. To extend the context window to 128k, we employed the YaRN method [55].\n3\nPost-Training\n3.1\nSupervised Fine-Tuning\nWe employ the Muon optimizer [33] in our post-training and recommend its use for fine-tuning with K2. This follows\nfrom the conclusion of our previous work [46] that a Muon-pre-trained checkpoint produces the best performance with\nMuon fine-tuning.\nWe construct a large-scale instruction-tuning dataset spanning diverse domains, guided by two core principles: max-\nimizing prompt diversity and ensuring high response quality. To this end, we develop a suite of data generation\npipelines tailored to different task domains, each utilizing a combination of human annotation, prompt engineering, and\nverification processes. We adopt K1.5 [35] and other in-house domain-specialized expert models to generate candidate\nresponses for various tasks, followed by LLMs or human-based judges to perform automated quality evaluation and\nfiltering. For agentic data, we create a data synthesis pipeline to teach models tool-use capabilities through multi-step,\ninteractive reasoning.\n3.1.1\nLarge-Scale Agentic Data Synthesis for Tool Use Learning\nA critical capability of modern LLM agents is their ability to autonomously use unfamiliar tools, interact with external\nenvironments, and iteratively refine their actions through reasoning, execution, and error correction. Agentic tool use\ncapability is essential for solving complex, multi-step tasks that require dynamic interaction with real-world systems.\nRecent benchmarks such as ACEBench [6] and τ-bench [85] have highlighted the importance of comprehensive tool-use\nevaluation, while frameworks like ToolLLM [58] and ACEBench [6] have demonstrated the potential of teaching\nmodels to use thousands of tools effectively.\nHowever, training such capabilities at scale presents a significant challenge: while real-world environments provide\nrich and authentic interaction signals, they are often difficult to construct at scale due to cost, complexity, privacy\nand accessibility constraints. Recent work on synthetic data generation (AgentInstruct [51]; Self-Instruct [75];\nStableToolBench [20]; ZeroSearch [66]) has shown promising results in creating large-scale data without relying on\nreal-world interactions. Building on these advances and inspired by ACEBench [6]’s comprehensive data synthesis\nframework, we developed a pipeline that simulates real-world tool-use scenarios at scale, enabling the generation of\ntens of thousands of diverse and high-quality training examples.\nThere are three stages in our data synthesis pipeline, depicted in Fig. 8.\n• Tool spec generation: we first construct a large repository of tool specs from both real-world tools and LLM-\nsynthetic tools;\n• Agent and task generation: for each tool-set sampled from the tool repository, we generate an agent to use the\ntoolset and some corresponding tasks;\n• Trajectory generation: for each agent and task, we generate trajectories where the agent finishes the task by\ninvoking tools.\n9\n\nKimi K2\nTECHNICAL REPORT\nTool Repository\nMCP tools\nDomains\nApplications\nAgents\nTasks\nwith rubrics\nsynthesized\ntool specs\nreal-world\ntool specs\n(a) Synthesizing tool specs, agents and tasks\nAgent\nTask\nRubrics\nFiltered\nData\nobservation\ncall\ninteraction\ntrajectories\nJudge\nAgent\nUser\nAgent\nTool\nSimulator\n(b) Generating agent trajectories\nFigure 8: Data synthesis pipeline for tool use. (a) Tool specs are from both real-world tools and LLMs; agents and tasks\nare the generated from the tool repo. (b) Multi-agent pipeline to generate and filter trajectories with tool calling.\n75\n50\n25\n0\n25\n50\n75\nt-SNE 1\n60\n40\n20\n0\n20\n40\n60\nt-SNE 2\nt-SNE of MCP tools by Category\ndatabases\nimage-and-video-processing\ncloud-platforms\ncalendar-management\ncryptocurrency\nvector-databases\nlocation-services\ncommunication\nshell-access\nSearch\nmultimedia-processing\nfile-systems\nweb-scraping\necommerce-and-retail\nsearch\ncustomer-data-platforms\napp-automation\ndeveloper-tools\nos-automation\nhealth-and-wellness\nvirtualization\nversion-control\ncloud-storage\nResearch & Data\nentertainment-and-media\nother\ngames-and-gamification\nAIGC\ntravel-and-transportation\nnote-taking\nbrowser-automation\nrag-systems\nlanguage-translation\nsocial-media\nsecurity-and-iam\nhome-automation-and-iot\nmonitoring\naigc\nresearch-and-data\nweather-services\nart-and-culture\ncustomer-support\nblockchain\nfinance\nknowledge-and-memory\nspeech-processing\nmarketing\n(a) t-SNE visualization of real MCP tools, colored by their\noriginal source categories\n100\n75\n50\n25\n0\n25\n50\n75\n100\nt-SNE 1\n100\n75\n50\n25\n0\n25\n50\n75\n100\nt-SNE 2\nt-SNE of synthetic tools by Category\nenterprise_business_intelligence\ntransportation_logistics\niphone_android\nsmart_home\nreal_estate_property\nunknown\nsoftware_apps\nlegal_compliance\neducation_elearning\nrobot_control\nagriculture_environmental\nhealthcare_medical\nmanufacturing_industrial_iot\ndesktop_systems\nfinancial_trading\nwebsite_control\ngaming_entertainment\n(b) t-SNE visualization of synthetic tools, colored by pre-defined\ndomain categories\nFigure 9: t-SNE visualizations of tool embeddings. (a) Real-world MCP tools exhibit natural clustering based on their\noriginal source categories. (b) Synthetic tools are organized into pre-defined domain categories, providing systematic\ncoverage of the tool space. Together, they ensure comprehensive representation across different tool functionalities.\nDomain Evolution and Tool Generation.\nWe construct a comprehensive tool repository through two complementary\napproaches. First, we directly fetch 3000+ real MCP (Model Context Protocol) tools from GitHub repositories,\nleveraging existing high-quality tool specs. Second, we systematically evolve [82] synthetic tools through a hierarchical\ndomain generation process: we begin with key categories (e.g., financial trading, software applications, robot control),\nthen evolve multiple specific application domains within each category. Specialized tools are then synthesized for each\ndomain, with clear interfaces, descriptions, and operational semantics. This evolution process produces over 20,000\nsynthetic tools. Figure 9 visualizes the diversity of our tool collection through t-SNE embeddings, demonstrating that\nboth MCP and synthetic tools cover complementary regions of the tool space.\nAgent Diversification.\nWe generate thousands of distinct agents by synthesizing various system prompts and\nequipping them with different combinations of tools from our repository. This creates a diverse population of agents\nwith varied capabilities, areas of expertise, and behavioral patterns, ensuring a broad coverage of potential use cases.\nRubric-Based Task Generation.\nFor each agent configuration, we generate tasks that range from simple to complex\noperations. Each task is paired with an explicit rubric that specifies success criteria, expected tool-use patterns, and\nevaluation checkpoints. This rubric-based approach ensures a consistent and objective evaluation of agent performance.\nMulti-turn Trajectory Generation.\nWe simulate realistic tool-use scenarios through several components:\n• User Simulation: LLM-generated user personas with distinct communication styles and preferences engage in\nmulti-turn dialogues with agents, creating naturalistic interaction patterns.\n10\n\nKimi K2\nTECHNICAL REPORT\n• Tool Execution Environment: A sophisticated tool simulator (functionally equivalent to a world model) executes\ntool calls and provides realistic feedback. The simulator maintains and updates state after each tool execution,\nenabling complex multi-step interactions with persistent effects. It introduces controlled stochasticity to produce\nvaried outcomes including successes, partial failures, and edge cases.\nQuality Evaluation and Filtering.\nAn LLM-based judge evaluates each trajectory against the task rubrics. Only\ntrajectories that meet the success criteria are retained for training, ensuring high-quality data while allowing natural\nvariation in task-completion strategies.\nHybrid Approach with Real Execution Environments.\nWhile simulation provides scalability, we acknowledge\nthe inherent limitation of simulation fidelity. To address this, we complement our simulated environments with real\nexecution sandboxes for scenarios where authenticity is crucial, particularly in coding and software engineering tasks.\nThese real sandboxes execute actual code, interact with genuine development environments, and provide ground-truth\nfeedback through objective metrics such as test suite pass rates. This combination ensures that our models learn from\nboth the diversity of simulated scenarios and the authenticity of real executions, significantly strengthening practical\nagent capabilities.\nBy leveraging this hybrid pipeline that combines scalable simulation with targeted real-world execution, we generate\ndiverse, high-quality tool-use demonstrations that balance coverage and authenticity. The scale and automation of our\nsynthetic data generation, coupled with the grounding provided by real execution environments, effectively implements\nlarge-scale rejection sampling [26, 87] through our quality filtering process. This high-quality synthetic data, when\nused for supervised fine-tuning, has demonstrated significant improvements in the model’s tool-use capabilities across a\nwide range of real-world applications.\n3.2\nReinforcement Learning\nReinforcement learning (RL) is believed to have better token efficiency and generalization than SFT. Based on the work\nof K1.5 [35], we continue to scale RL in both task diversity and training FLOPs in K2. To support this, we develop a\nGym-like extensible framework that facilitates RL across a wide range of scenarios. We extend the framework with a\nlarge number of tasks with verifiable rewards. For tasks that rely on subjective preferences, such as creative writing and\nopen-ended question answering, we introduce a self-critic reward in which the model performs pairwise comparisons to\njudge its own outputs. This approach allows tasks from various domains to all benefit from the RL paradigm.\n3.2.1\nVerifiable Rewards Gym\nMath, STEM and Logical Tasks\nFor math, stem and logical reasoning domains, our RL data preparation follows\ntwo key principles, diverse coverage and moderate difficulty.\nDiverse Coverage. For math and stem tasks, we collect high-quality QA pairs using a combination of expert annotations,\ninternal QA extraction pipelines, and open datasets [41, 52]. During the collection process, we leverage a tagging\nsystem to deliberately increase coverage of under-covered domains. For logical tasks, our dataset comprises a variety of\nformats, including structured data tasks (e.g., multi-hop tabular reasoning, cross-table aggregation) and logic puzzles\n(e.g., the 24-game, Sudoku, riddles, cryptarithms, and Morse-code decoding).\nModerate Difficulty. The RL prompt-set should be neither too easy nor too hard, both of which may produce little signal\nand reduce learning efficiency. We assess the difficulty of each problem using the SFT model’s pass@k accuracy and\nselect only problems with moderate difficulty.\nComplex Instruction Following\nEffective instruction following requires not only understanding explicit constraints\nbut also navigating implicit requirements, handling edge cases, and maintaining consistency over extended dialogues.\nWe address these challenges through a hybrid verification framework that combines automated verification with\nadversarial detection, coupled with a scalable curriculum generation pipeline. Our approach employs a dual-path system\nto ensure both precision and robustness:\nHybrid Rule Verification. We implement two verification mechanisms: (1) deterministic evaluation via code interpreters\nfor instructions with verifiable outputs (e.g., length, style constraints), and (2) LLM-as-judge evaluation for instructions\nrequiring nuanced understanding of constraints. To address potential adversarial behaviors where models might claim\ninstruction fulfillment without actual compliance, we incorporate an additional hack-check layer that specifically detects\nsuch deceptive claims.\nMulti-Source Instruction Generation. To construct our training data, we employ three distinct generation strategies to\nensure comprehensive coverage: (1) expert-crafted complex conditional prompts and rubrics developed by our data\n11\n\nKimi K2\nTECHNICAL REPORT\nteam (2) agentic instruction augmentation inspired by AutoIF [12], and (3) a fine-tuned model specialized for generating\nadditional instructions that probe specific failure modes or edge cases. This multipronged approach ensures both breadth\nand depth in instruction coverage.\nFaithfulness\nFaithfulness is essential for an agentic model operating in scenarios such as multi-turn tool use, self-\ngenerated reasoning chains, and open-environment interactions. Inspired by the evaluation framework from FACTS\nGrounding [30], we train a sentence-level faithfulness judge model to perform automated verification. The judge is\neffective in detecting sentences that make a factual claim without supporting evidence in context. It serves as a reward\nmodel to enhance overall faithfulness performance.\nCoding & Software Engineering\nTo enhance our capability in tackling competition-level programming problems,\nwe gather problems and their judges from both open-source datasets [27, 83] and synthetic sources. To ensure the\ndiversity of the synthetic data and the correctness of reward signals, we incorporate high-quality human-written unit\ntests retrieved from pre-training data.\nFor software engineering tasks, we collect a vast amount of pull requests and issues from GitHub to build software\ndevelopment environment that consists of user prompts/issues and executable unit tests. This environment was built on\na robust sandbox infrastructure, powered by Kubernetes for scalability and security. It supports over 10,000 concurrent\nsandbox instances with stable performance, making it ideal for both competitive coding and software engineering tasks.\nSafety\nOur work to enhance the safety begins with a human-curated set of seed prompts, manually crafted to\nencompass prevalent risk categories such as violence, fraud, and discrimination.\nTo simulate sophisticated jailbreak attempts (e.g., role-playing, literary narratives, and academic discourse), we employ\nan automated prompt evolution pipeline with three key components:\n• Attack Model: Iteratively generates adversarial prompts designed to elicit unsafe responses from the target LLM.\n• Target Model: Produces responses to these prompts, simulating potential vulnerabilities.\n• Judge Model: Evaluates the interaction to determine if the adversarial prompt successfully bypasses safety\nmechanisms.\nEach interaction is assessed using a task-specific rubric, enabling the judge model to provide a binary success/failure\nlabel.\n3.2.2\nBeyond Verification: Self-Critique Rubric Reward\nTo extend model alignment beyond tasks with verifiable reward, we introduce a framework for general reinforcement\nlearning from self-critic feedbacks. This approach is designed to align LLMs with nuanced human preferences,\nincluding helpfulness, creativity, depth of reasoning, factuality, and safety, by extending the capabilities learned from\nverifiable scenarios to a broader range of subjective tasks. The framework operates using a Self-Critique Rubric Reward\nmechanism, where the model evaluates its own outputs to generate preference signals. To bootstrap K2 as a competent\njudge, we curated a mixture of open-source and in-house preference datasets and initialize its critic capability in the\nSFT stage.\nSelf-Critiqued Policy Optimization\nIn the first core process of the learning loop, the K2 actor generates responses\nfor general prompts that cover a wide range of use cases. The K2 critic then ranks all results by performing pairwise\nevaluations against a combination of rubrics, which incorporates both core rubrics (Appendix. F.1), which represent the\nfundamental values of our AI assistant that Kimi cherish, prescriptive rubrics (Appendix. F.2) that aim to eliminate\nreward hacking, and human-annotated rubrics crafted by our data team for specific instructional contexts. Although\ncertain rubrics can be designated as mandatory, K2 retains the flexibility to weigh them against its internal priors. This\ncapacity enables a dynamic and continuous alignment with its evolving on-policy behavior, ensuring that the model’s\nresponses remain coherent with its core identity while adapting to specific instructions.\nClosed-Loop Critic Refinement and Alignment\nDuring RL training, the critic model is refined using verifiable\nsignals. On-policy rollouts generated from verifiable-reward prompts are used to continuously update the critic, a crucial\nstep that distills objective performance signals from RLVR directly into its evaluation model. This transfer learning\nprocess grounds its more subjective judgments in verifiable data, allowing the performance gains from verifiable\ntasks to enhance the critic’s judgment on complex tasks that lack explicit reward signals. This closed-loop process\nensures that the critic continuously recalibrates its evaluation standards in lockstep with the policy’s evolution. By\n12\n\nKimi K2\nTECHNICAL REPORT\ngrounding subjective evaluation in verifiable data, the framework enables robust and scalable alignment with complex,\nnon-verifiable human objectives.\nConsequently, this holistic alignment yields comprehensive performance improvements across a wide spectrum of do-\nmains, including user intent understanding, creative writing, complex reasoning, and nuanced language comprehension.\n3.2.3\nRL Algorithm\nWe adopt the policy optimization algorithm introduced in K1.5 [35] as the foundation for K2. For each problem x,\nwe sample K responses {y1, . . . , yk} from the previous policy πold, and optimize the model πθ with respect to the\nfollowing objective:\nLRL(θ) = Ex∼D\n\"\n1\nK\nK\nX\ni=1\n\"\u0012\nr(x, yi) −¯r(x) −τ log πθ(yi|x)\nπold(yi|x)\n\u00132##\n,\nwhere ¯r(x) = 1\nk\nPk\ni=1 r(x, yi) is the mean rewards of the sampled responses, τ > 0 is a regularization parameter that\npromotes stable learning. As in SFT, we employ the Muon optimizer [33] to minimize this objective. As we scale\nRL training to encompass a broader range of tasks in K2, a primary challenge is achieving consistent performance\nimprovements across all domains. To address this, we introduce several additions to the RL algorithm.\nBudget Control\nIt has been widely observed that RL often results in a substantial increase in the length of model-\ngenerated responses [35, 19]. While longer responses can enable the model to utilize additional test-time compute for\nimproved performance on complex reasoning tasks, the benefits often do not justify its inference cost in non-reasoning\ndomains. To encourage the model to properly distribute inference budget, we enforce a per-sample maximum token\nbudget throughout RL training, where the budget is determined based on the type of task. Responses that exceed\nthis token budget are truncated and assigned a penalty, which incentivizes the model to generate solutions within the\nspecified limit. Empirically, this approach significantly enhances the model’s token efficiency, encouraging concise yet\neffective solutions across all domains.\nPTX Loss\nTo prevent the potential forgetting of valuable, high-quality data during joint RL training, we curate a\ndataset comprising hand-selected, high-quality samples and integrate it into the RL objective through an auxiliary PTX\nloss [54]. This strategy not only leverages the advantages of high-quality data, but also mitigates the risk of overfitting\nto the limited set of tasks explicitly present in the training regime. This augmentation substantially improves the model’s\ngeneralization across a broader range of domains.\nTemperature Decay\nFor tasks such as creative writing and complex reasoning, we find that promoting exploration\nvia a high sampling temperature during the initial stages of training is crucial. A high temperature allow the model to\ngenerate diverse and innovative responses, thereby facilitating the discovery of effective strategies and reducing the risk\nof premature convergence to suboptimal solutions. However, retaining a high temperature in the later stages of training\nor during evaluation can be detrimental, as it introduces excessive randomness and compromises the reliability and\nconsistency of the model’s outputs. To address this, we employ a temperature decay schedule, to shift from exploration\nto exploitation throughout the training. This strategy ensures that the model leverages exploration when it is most\nbeneficial, while ultimately converge on stable and high-quality outputs.\n3.3\nRL Infrastructure\n3.3.1\nColocated Architecture\nSimilar to K1.5 [35], we adopt a hybrid colocated architecture for our synchronized RL training, where the training and\ninference engines live on the same workers. When one engine is actively working, the other engine releases or offloads\nits GPU resources to accommodate. In each iteration of RL training, a centralized controller first calls the inference\nengine to generate new data for training. It then notifies the training engine to train on the new data, and send updated\nparameters to the inference engine for the next iteration.\nEach engine is heavily optimized for throughput. In addition, as the model scales to the size of K2, the latency of engine\nswitching and failure recovery becomes significant. We present our system design considerations in these aspects.\n13\n\nKimi K2\nTECHNICAL REPORT\ncheckpoint engine\nckpt\nckpt\ntrain engine\ntrain\ntrain\ninference engine\ninference\ninference\npod\nbroadcast\nFigure 10: Parameter update utilizing a checkpoint engine\n3.3.2\nEfficient Engine Switching\nDuring rollout, the parameters of the training engine are offloaded to DRAM. Bringing up the training engine is\ntherefore a simple step of H2D transmission. However, bringing up the inference engine is a bigger challenge, as it\nmust obtain updated parameters from the training engine with a different sharding paradigm.\nGiven the scale of K2 and the vast number of devices involved, using a network file system for resharding and\nbroadcasting parameters is impractical. The aggregate bandwidth required to keep overhead low reaches several\npetabytes per second. To address this challenge, we developed a distributed checkpoint engine co-located on training\nnodes to manage parameter states. To perform a parameter update, each checkpoint engine worker obtains a local copy\nof parameters from the training engine, then broadcasts the full parameter set across all checkpoint engine workers.\nSubsequently, the inference engine retrieves only the parameter shard it requires from the checkpoint engine. This\nprocess is illustrated in Figure 10. To enable this for a 1T model, updates are performed parameter-by-parameter in a\npipelined manner, minimizing memory footprint (see Appendix G).\nWe opt to broadcast the full parameter set across the entire cluster, regardless of the specific sharding schemes on each\ninference worker. While this transfers several times more data than a theoretically optimal approach, it offers a simpler\nsystem design that is less intrusive to the training and inference engines. We chose to trade off this minor overhead to\nfully decouple the training engine and the inference engine, significantly simplifying maintenance and testing.\nNotably, this approach outperforms the transfer-what-you-need method due to reduced synchronization overhead and\nhigher network bandwidth utilization. Our system can complete a full parameter update for Kimi K2 with less than 30\nseconds, a negligible duration for a typical RL training iteration.\n3.3.3\nEfficient System Startup\nAs large-scale training is prone to system failure, optimizing the startup time is crucial for models as large as Kimi K2.\nTo start the training engine, we let each training worker selectively read part or none of the parameters from disk, and\nbroadcast necessary parameters to its peers. The design goal is to ensure all workers collectively read the checkpoint\nonly once, minimizing expensive disk IO.\nAs the inference engines are independent replicas, we would like to avoid introducing extra synchronization barriers\nbetween them. Therefore, we opt to reuse checkpoint engine for startup: we let checkpoint engine collectively read the\ncheckpoint from disk, similar to how the training engine starts. Then it updates the state of the uninitialized inference\nengine, using the approach introduced in the previous section. By leveraging the dedicated checkpoint engine, the\nsystem also becomes robust to single-point failures, because an inference replica can restart without communicating\nwith other replicas.\n3.3.4\nAgentic Rollout\nOur RL infrastructure supports the training of long-horizon, multi-turn agentic tasks. During rollout, these tasks present\ndistinct challenges, such as complex environmental interactions and prolonged rollout durations. Here we introduce a\nfew optimizations to alleviate these issues.\nDue to the diversity of environments, certain interactions may be blocked on waiting for environment feedback (e.g., a\nvirtual machine or a code interpreter), leaving the GPUs idle. We employ two strategies to maximize GPU utilization:\n14\n\nKimi K2\nTECHNICAL REPORT\n(i) we deploy heavy environments as dedicated services that can scale up more easily; (ii) we employ a large number of\nconcurrent rollouts to amortize the latency induced by certain expensive interactions.\nAnother challenge in agentic rollout is that individual rollout trajectories can be extremely long. To prevent long-tail\ntrajectories from blocking the entire rollout process, we employ the partial rollout [35] technique. This strategy allows\nlong-tail unfinished tasks to be paused, and resumed in the next RL iteration.\nTo improve research efficiency, we also design a unified interface inspired by the OpenAI Gym framework [49] to\nstreamline the integration of new environments. We hope to scale our RL infrastructure to more diverse interactive\nenvironments in the future.\n4\nEvaluations\nThis section begins with the post-training evaluation of Kimi-K2-Instruct, followed by a brief overview of the capabilities\nof Kimi-K2-Base. We conclude with a comprehensive safety evaluation.\n4.1\nPost-training Evaluations\n4.1.1\nEvaluation Settings\nBenchmarks\nWe assess Kimi-K2-Instruct across different areas. For coding, we adopt LiveCodeBench v6 [31](ques-\ntions from August 2024 to May 2025), OJBench [77], MultiPL-E [5], SWE-bench Verified [32, 84], TerminalBench [71],\nMulti-SWE-bench [86], SWE-Lancer [50], PaperBench [65], and Aider-Polyglot [16]. For tool use tasks, we evaluate\nperformance on τ 2-Bench [3] and AceBench [6], which emphasize multi-turn tool-calling capabilities. In reasoning,\nwe include a wide range of mathematical, science and logical tasks: AIME 2024/2025, MATH-500, HMMT 2025,\nCNMO 2024, PolyMath-en, ZebraLogic [43], AutoLogi [91], GPQA-Diamond [61], SuperGPQA [13], and Humanity’s\nLast Exam (Text-Only) [56]. We benchmark the long-context capabilities on: MRCR4 for long-context retrieval, and\nDROP [14], FRAMES [37] and LongBench v2 [2] for long-context reasoning. For factuality, we evaluate FACTS\nGrounding [30], the Vectara Hallucination Leaderboard [73], and FaithJudge [68]. Finally, general capabilities are\nassessed using MMLU [23], MMLU-Redux [17], MMLU-Pro [76], IFEval [90], Multi-Challenge [64], SimpleQA [78],\nand LiveBench [80] (as of 2024-11-25).\nBaselines\nWe benchmark against both open-source and proprietary frontier models, ensuring every candidate is\nevaluated under its non-thinking configuration to eliminate additional gains from test-time compute. Open-source\nbaselines: DeepSeek-V3-0324 and Qwen3-235B-A22B, with the latter run in the vendor-recommended no-thinking\nregime. Proprietary baselines: Claude Sonnet 4, Claude Opus 4, GPT-4.1, and Gemini 2.5 Flash Preview (2025-05-20).\nEach invoked in its respective non-thinking mode via official APIs under unified temperature and top-p settings.\nEvaluation Configurations All runs query models in their non-thinking mode. Output token length is capped at\n8192 tokens everywhere except SWE-bench Verified (Agentless), which is raised to 16384. For benchmarks with high\nper-question variance, we adopt repeated sampling k times and average the results to obtain stable scores, denoted as\nAvg@k. For long-context tasks, we set the context window size to 128K tokens during evaluation, truncating any input\nthat exceeds this limit to fit within the window. SWE-bench Verified is evaluated in two modes: Agentless Coding\nvia Single Patch without Test (Acc) and Agentic Coding via bash/editor tools under both Single Attempt (Acc) and\nMultiple Attempts (Acc) using best-of-N selection with an internal verifier; SWE-bench Multilingual is tested only in\nthe single-attempt agentic setting. Some data points have been omitted due to prohibitively expensive evaluation costs.\n4.1.2\nEvaluation Results\nA comprehensive evaluation results of Kimi-K2-Instruct is shown in Table 3, with detailed explanation provided in the\nAppendix C. Below, we highlight key results across four core domains:\nAgentic and Competitive Coding\nKimi-K2-Instruct demonstrates state-of-the-art open-source performance on\nreal-world SWE tasks. It outperforms most baselines on SWE-bench Verified (65.8%, 71.6% with multiple attemps),\nSWE-bench Multilingual (47.3%), and SWE-lancer (39.1%), significantly closing the gap with Claude 4 Opus and\nSonnet. On competitive coding benchmarks (e.g., LiveCodeBench v6 53.7%, OJBench 27.1%), it also leads among all\nmodels, highlighting its practical coding proficiency across difficulty levels.\n4https://huggingface.co/datasets/openai/mrcr\n15\n\nKimi K2\nTECHNICAL REPORT\nTable 3: Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across\ndiverse tasks. Bold denotes the global SOTA; underlined bold indicates the best open-source result. Data points\nmarked with * are taken directly from the model’s technical report or blog.\nOpen Source\nProprietary\nBenchmark\nKimi-K2-\nInstruct\nDeepSeek-\nV3-0324\nQwen3-\n235B-\nA22B\nClaude\nSonnet 4\nClaude\nOpus 4\nGPT-4.1\nGemini\n2.5 Flash\nCoding Tasks\nLiveCodeBench v6 (Pass@1)\n53.7\n46.9\n37.0\n48.5\n47.4\n44.7\n44.7\nOJBench (Pass@1)\n27.1\n24.0\n11.3\n15.3\n19.6\n19.5\n19.5\nMultiPL-E (Pass@1)\n85.7\n83.1\n78.2\n88.6\n89.6\n86.7\n85.6\nSWE-bench Verified\nAgentless-Single-Patch (Pass@1)\n51.8\n36.6\n39.4\n50.2\n53.0\n40.8\n32.6\nSWE-bench Verified\nAgentic-Single-Attempt (Pass@1)\n65.8\n38.8\n34.4\n72.7*\n72.5*\n54.6\n—\nSWE-bench Verified\nAgentic-Multi-Attempt (Pass@1)\n71.6\n—\n—\n80.2*\n79.4*\n—\n—\nSWE-bench Multilingual (Pass@1)\n47.3\n25.8\n20.9\n51.0\n—\n31.5\n—\nMulti-SWE-bench (Pass@1)\n18.3\n8.0\n9.0\n29.2\n—\n11.7\n14.0\nSWE-Lancer (Pass@1)\n39.1\n30.5\n24.1\n40.8\n—\n23.0\n38.5\nPaper Bench Code-Dev (Acc.)\n27.8\n12.2\n13.2\n43.3\n—\n29.9\n5.7\nTerminal Bench In-House (Acc.)\n30.0\n—\n—\n35.5\n43.2\n8.3\n—\nTerminal Bench Terminus (Acc.)\n25.0\n16.3\n6.6\n—\n—\n30.3\n16.8\nAider-Polyglot (Acc.)\n60.0\n55.1\n61.8\n56.4\n70.7\n52.4\n44.0\nTool Use Tasks\nTau2 retail (Avg@4)\n70.6\n69.1\n57.0\n75.0\n81.8\n74.8\n64.3\nTau2 airline (Avg@4)\n56.5\n39.0\n26.5\n55.5\n60.0\n54.5\n42.5\nTau2 telecom (Avg@4)\n65.8\n32.5\n22.1\n45.2\n57.0\n38.6\n16.9\nAceBench (Acc.)\n76.5\n72.7\n70.5\n76.2\n75.6\n80.1\n74.5\nMath & STEM Tasks\nAIME 2024 (Avg@64)\n69.6\n59.4*\n40.1*\n43.4\n48.2\n46.5\n61.3\nAIME 2025 (Avg@64)\n49.5\n46.7\n24.7*\n33.1*\n33.9*\n37.0\n46.6\nMATH-500 (Acc.)\n97.4\n94.0*\n91.2*\n94.0\n94.4\n92.4\n95.4\nHMMT 2025 (Avg@32)\n38.8\n27.5\n11.9\n15.9\n15.9\n19.4\n34.7\nCNMO 2024 (Avg@16)\n74.3\n74.7\n48.6\n60.4\n57.6\n56.6\n75.0\nPolyMath-en (Avg@4)\n65.1\n59.5\n51.9\n52.8\n49.8\n54.0\n49.9\nZebraLogic (Acc.)\n89.0\n84.0\n37.7*\n79.7\n59.3\n58.5\n57.9\nAutoLogi (Acc.)\n89.5\n88.9\n83.3*\n89.8\n86.1\n88.2\n84.1\nGPQA-Diamond (Avg@8)\n75.1\n68.4*\n62.9*\n70.0*\n74.9*\n66.3\n68.2\nSuperGPQA (Acc.)\n57.2\n53.7\n50.2\n55.7\n56.5\n50.8\n49.6\nHumanity’s Last Exam (Acc.)\n4.7\n5.2\n5.7\n5.8\n7.1\n3.7\n5.6\nGeneral Tasks\nMMLU (EM)\n89.5\n89.4\n87.0\n91.5\n92.9\n90.4\n90.1\nMMLU-Redux (EM)\n92.7\n90.5\n89.2*\n93.6\n94.2\n92.4\n90.6\nMMLU-Pro (EM)\n81.1\n81.2*\n77.3\n83.7\n86.6\n81.8\n79.4\nIFEval (Prompt Strict)\n89.8\n81.1\n83.2*\n87.6\n87.4\n88.0\n84.3\nMulti-Challenge (Acc.)\n54.1\n31.4\n34.0\n46.8\n49.0\n36.4\n39.5\nSimpleQA (Correct)\n31.0\n27.7\n13.2\n15.9\n22.8\n42.3\n23.3\nLivebench (Pass@1)\n76.4\n72.4\n67.6\n74.8\n74.6\n69.8\n67.8\nArena Hard v2.0\nHard Prompt (Win rate)\n54.5\n39.9\n39.9\n51.6\n59.7\n51.7\n48.7\nArena Hard v2.0\nCreative Writing (Win rate)\n85.0\n59.3\n59.8\n54.6\n68.5\n61.5\n72.8\nFACTS Grounding (Adjusted)\n88.5\n68.3\n68.5\n83.6\n—\n79.2\n86.6\nHHEM v2.1 (1-Hallu.)\n98.9\n88.9\n94.5\n94.5\n—\n96.7\n97.8\nFaithJudge (1-Hallu.)\n92.6\n83.4\n75.7\n83.0\n—\n91.0\n93.2\nLongBench v2 (Acc.)\n49.1\n51.1\n—\n52.5\n—\n54.3\n55.5\nFRAMES (Acc.)\n77.1\n79.2\n—\n76.3\n—\n87.4\n72.9\nMRCR (Acc.)\n55.0\n50.8\n—\n74.4\n—\n66.9\n81.7\nDROP (Acc.)\n93.5\n91.2\n84.3\n92.0\n—\n79.1\n81.7\n16\n\nKimi K2\nTECHNICAL REPORT\nAgentic Tool Use\nOn multi-turn tool-use benchmarks, Kimi-K2-Instruct sets a new standard. It achieves 66.1 Pass@1\non τ 2-Bench and 76.5 on ACEBench, substantially outperforming all baselines. These results affirm its strength in\ngrounded, controlled, and agent-driven tool orchestration across domains.\nGeneral Capabilities\nKimi-K2-Instruct exhibits strong, balanced performance across general knowledge, math,\ninstruction following, and long-context tasks. It surpasses open-source peers on SimpleQA (31.0%), MMLU (89.5%)\nand MMLU-Redux (92.7%), and leads all models on instruction benchmarks (IFEval: 89.8%, Multi-Challenge: 54.1%).\nIn math and STEM, it achieves top-tier scores (AIME 2024: 69.6%, GPQA-Diamond: 75.1%), and remains competitive\non long-context factuality and retrieval (DROP: 93.5%, MRCR: 55.0%). These results position Kimi-K2-Instruct as a\nwell-rounded and capable generalist across both short- and long-context settings.\nOpen-Ended Evaluation\nOn the LMSYS Arena leaderboard (July 17, 2025), Kimi-K2-Instruct ranks as the top-1\nopen-source model and 5th overall based on over 3,000 user votes. This real-world preference signal—across diverse,\nblind prompts—underscores Kimi-K2’s strengths in generating high-quality responses on open-ended tasks.\n4.2\nPre-training Evaluations\n4.2.1\nEvaluation Settings\nBenchmarks\nWe evaluate Kimi-K2-Base across diverse capability areas. For general capabilities, we assess on\nMMLU [23], MMLU-Pro [76], MMLU-Redux [17], BBH [67], TriviaQA [34], SuperGPQA [13], SimpleQA [78], Hel-\nlaSwag [88], AGIEval [89], GPQA-Diamond [61], ARC-Challenge [8], and WinoGrande [62]. For coding capabilities,\nwe employ EvalPlus [45] (averaging HumanEval [7], MBPP [1], HumanEval+, and MBPP+), LiveCodeBench v6 [31],\nand CRUXEval [18]. For mathematical reasoning, we utilize GSM8K [9], GSM8K-Platinum [74], MATH [24], and\nCMATH [79]. For Chinese language capabilities, we evaluate on C-Eval [29], CMMLU [40], and CSimpleQA [22].\nBaselines\nWe benchmark against leading open-source foundation models: DeepSeek-V3-Base [10], Qwen2.5-72B-\nBase [59] (Note that Qwen3-235B-A22B-Base is not open-sourced, and the largest open-sourced base model in the\nQwen series is Qwen2.5-72B-Base), and Llama 4-Maverick [70] (Llama 4-Behemoth is also not open-sourced). All\nmodels are evaluated under identical configurations to ensure fair comparison.\nEvaluation Configurations\nWe employ perplexity-based evaluation for MMLU, MMLU-Redux, GPQA-Diamond,\nHellaSwag, ARC-Challenge, C-Eval, and CMMLU. Generation-based evaluation is used for MMLU-Pro, SuperGPQA,\nTriviaQA, BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench, and\nEvalPlus. To mitigate the high variance inherent to GPQA-Diamond, we report the mean score across eight independent\nruns. All evaluations are conducted using our internal framework derived from LM-Harness-Evaluation [4], ensuring\nconsistent settings across all models.\n4.2.2\nEvaluation Results\nTable 4 presents a comprehensive comparison of Kimi-K2-Base against leading open-source foundation models across\ndiverse evaluation benchmarks. The results demonstrate that Kimi-K2-Base achieves state-of-the-art performance\nacross the majority of evaluated tasks, establishing it as a leading foundation model in the open-source landscape.\nGeneral Language Understanding\nKimi-K2-Base achieves state-of-the-art performance on 10 out of 12 English\nlanguage benchmarks. Notable results include MMLU (87.79%), MMLU-Pro (69.17%), MMLU-Redux (90.17%),\nSuperGPQA (44.67%), and SimpleQA (35.25%), significantly outperforming all baselines.\nCoding Capabilities\nOn coding benchmarks, Kimi-K2-Base sets new standards with leading performance across all\nmetrics. It achieves 74.00% on CRUXEval-I-cot, 83.50% on CRUXEval-O-cot, 26.29% on LiveCodeBench v6, and\n80.33% on EvalPlus, demonstrating superior code generation and comprehension abilities, particularly in scenarios\nrequiring step-by-step reasoning.\nMathematical Reasoning\nKimi-K2-Base exhibits exceptional mathematical capabilities, leading on three out of\nfour benchmarks: MATH (70.22%), GSM8K (92.12%), and GSM8K-Platinum (94.21%). It maintains competitive\nperformance on CMATH (90.26%), narrowly behind DeepSeek-V3-Base (90.53%). These results highlight the model’s\nrobust mathematical problem-solving abilities across varying difficulty levels.\n17\n\nKimi K2\nTECHNICAL REPORT\nChinese Language Understanding\nThe model demonstrates superior multilingual capabilities, achieving state-of-the-\nart results across all Chinese language benchmarks: C-Eval (92.50%), CMMLU (90.90%), and CSimpleQA (77.57%).\nThese results establish Kimi-K2-Base as a leading model for Chinese language understanding while maintaining strong\nperformance across other languages.\nTable 4: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\nBenchmark (Metric)\n#Shots Kimi-K2-Base DeepSeek-V3-Base Llama4-Maverick-Base Qwen2.5-72B-Base\nArchitecture\n-\nMoE\nMoE\nMoE\nDense\n# Activated Params\n-\n32B\n37B\n17B\n72B\n# Total Params\n-\n1043B\n671B\n400B\n72B\nEnglish\nMMLU\n5-shots\n87.79\n87.10\n84.87\n86.08\nMMLU-pro\n5-shots\n69.17\n60.59\n63.47\n62.80\nMMLU-redux\n5-shots\n90.17\n89.53\n88.18\n87.77\nSuperGPQA\n5-shots\n44.67\n39.20\n38.84\n34.23\nGPQA-Diamond(avg@8) 5-shots\n48.11\n50.51\n49.43\n40.78\nSimpleQA\n5-shots\n35.25\n26.49\n23.74\n10.31\nTriviaQA\n5-shots\n85.09\n84.11\n79.25\n76.03\nBBH\n3-shots\n88.71\n88.37\n87.10\n84.09\nHellaSwag\n5-shots\n94.60\n89.44\n86.02\n95.27\nAGIEval\n-\n84.23\n81.57\n67.55\n76.87\nARC-Challenge\n0-shot\n95.73\n93.77\n94.03\n95.56\nWinoGrande\n5-shots\n85.32\n84.21\n77.58\n84.14\nCode\nCRUXEval-I-cot\n0-shots\n74.00\n62.75\n67.13\n61.12\nCRUXEval-O-cot\n0-shots\n83.50\n75.25\n75.88\n66.13\nLiveCodeBench(v6)\n1-shots\n26.29\n24.57\n25.14\n22.29\nEvalPlus\n-\n80.33\n65.61\n65.48\n66.04\nMath\nMATH\n4-shots\n70.22\n61.70\n63.02\n62.68\nGSM8k\n8-shots\n92.12\n91.66\n86.35\n90.37\nGSM8k-platinum\n8-shots\n94.21\n93.38\n88.83\n92.47\nCMATH\n6-shots\n90.26\n90.53\n88.07\n86.98\nChinese\nC-Eval\n5-shots\n92.50\n90.04\n80.91\n90.86\nCMMLU\n5-shots\n90.90\n88.84\n81.24\n90.55\nCSimpleQA\n5-shots\n77.57\n72.13\n53.47\n50.53\n4.3\nSafety Evaluation\n4.3.1\nExperiment Settings\nWe conducted red-teaming evaluations on Kimi K2 compare with other open-source LLMs. The evaluation covered a\nrange of attack scenarios—including harmful content, privacy content, and security content, as well as different attack\nstrategies such as prompt injection and iterative jailbreak.\nWe choose Promptfoo5 to generate adversarial prompts and analyze the responses. By this way, we can evaluate model\nin a scalable ways.\nModel Selection We compare Kimi K2 with three other open-source LLMs: DeepSeek-V3, DeepSeek-R1, and Qwen3.\nPromptfoo Settings Table 5 lists plugins and strategies evaluated, with each plugin paired with all strategies to assess\ntheir performance.\nTest Case Count Given the inherent non-determinism of large language model inference, single-pass outputs may\nexhibit variability. To account for this, we generated 3 attack prompts per plugin for each strategy.\nPrompt Language Settings We pre-tested the language compatibility for each plugin-strategy combination. Some\nplugins support both English and Chinese, while others only support English. For combinations that support both, we\ngenerated 3 prompts in each language, resulting in 6 prompts per combination.\n5https://github.com/promptfoo/promptfoo\n18\n\nKimi K2\nTECHNICAL REPORT\nTable 5: Enabled Plugins and Strategies\nPlugin\nHarmful\nGraphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self\nHarm, Sexual Content, ToxicChat\nCriminal\nChemical&Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal\nActivities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent\nCrime, Violent Crime, Sex Crimes\nMisinformation Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misin-\nformation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance,\nPolitical Opinions, Religious Sensitivity\nPrivacy\nPrivacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social\nEngineering\nSecurity\nASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAn-\nswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery\nStrategy Basic, Prompt Injection, Iterative Jailbreak, Crescendo\nManual Review We incorporated human review into the evaluation process. To minimize subjectivity problem, we\nconducted multiple rounds of review and assigned the same reviewer to evaluate all cases within a given test set to\nensure consistency and reduce variability in judgment.\n4.3.2\nSafety Evaluation Results\nTable 6 presents the passing rates of different models under various plugin–strategy combinations.\nTable 6: Safety Evaluation Results\nPlugin\nStrategy\nKimi-K2-Instruct DeepSeek-V3-0324 DeepSeek-R1 Qwen3-235B-A22B\nHarmful\nBasic\n98.04\n90.45\n99.02\n98.53\nBase64\n100\n90.20\n100\n100\nPrompt Injection\n93.14\n100\n95.10\n99.02\nIterative Jailbreak\n92.16\n66.67\n72.55\n74.51\nCrescendo\n64.71\n64.71\n80.39\n86.27\nCriminal\nBasic\n100\n99.62\n95.45\n99.24\nBase64\n96.97\n89.39\n84.85\n98.48\nPrompt Injection\n75.76\n91.67\n69.70\n98.47\nIterative Jailbreak\n57.57\n21.21\n25.76\n53.03\nCrescendo\n56.06\n31.81\n42.42\n59.09\nMisinformation\nBasic\n97.28\n92.57\n92.46\n94.84\nBase64\n98.48\n90.48\n96.83\n93.65\nPrompt Injection\n98.39\n86.51\n93.65\n93.65\nIterative Jailbreak\n63.97\n53.97\n84.13\n69.84\nCrescendo\n85.71\n55.56\n88.89\n84.13\nPrivacy\nBasic\n100\n100\n100\n100\nBase64\n100\n100\n100\n100\nPrompt Injection\n88.33\n98.33\n100\n91.67\nIterative Jailbreak\n76.67\n100\n93.33\n96.67\nCrescendo\n96.67\n100\n96.67\n100\nSecurity\nBasic\n77.84\n75.57\n70.46\n90.09\nBase64\n82.93\n82.93\n63.41\n95.12\nPrompt Injection\n87.80\n97.56\n65.85\n84.13\nIterative Jailbreak\n43.90\n60.97\n43.90\n78.04\nCrescendo\n68.29\n87.80\n68.29\n87.80\nWithout targeted optimization for specific evaluation scenarios, the passing rate of some complex cases (e.g., Harm-\nful–Iterative Jailbreak) was relatively higher compared to other models.\nAcross different attack strategies, the models exhibited varying trends. Under the Base64 strategy, passing rates\ngenerally approached or reached 100%, suggesting that encoding transformations had minimal impact on the models’\n19\n\nKimi K2\nTECHNICAL REPORT\nbasic robustness. In contrast, the Crescendo strategy led to a general drop in passing rates, indicating stronger adversarial\neffectiveness.\nIn addition, complex attack strategies do not always outperform basic prompts. Some originally adversarial prompts\nmay lose their intended meaning after multiple rounds of transformation, rendering the resulting model outputs less\nmeaningful.\nAutomated Red-teaming Limitations Due to the involvement of human review, the evaluation results inevitably\ncontain a degree of subjectivity. Additionally, certain plugin types involve API misuse or external tool invocation, which\nare more suitable for evaluating agent models with tool-calling capabilities. In the context of base LLMs, such tests\nmay have limited relevance.\n5\nLimitations\nIn our internal tests, we have identified some limitations in current Kimi K2 models. When dealing with hard reasoning\ntasks or unclear tool definition, the model may generate excessive tokens, sometimes leading to truncated outputs or\nincomplete tool calls. Additionally, performance may decline on certain tasks if tool use is unnecessarily enabled. When\nbuilding complete software projects, the success rate of one-shot prompting is not as good as using K2 under an agentic\ncoding framework. We are working to address these issues in future releases and looking forward to more feedbacks.\n6\nConclusions\nWe introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence. Leveraging the token-\nefficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training.\nPost-training combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards\nand self-critic feedbacks. Kimi K2 sets new state-of-the-art on agentic and reasoning benchmarks, establishing itself as\nthe most capable open-weight LLM to date.\n7\nAcknowledgments\nWe would like to acknowledge the valuable support provided by the OpenHands and Multi-SWE-bench teams in\nevaluating the SWE-bench Verified and Multi-SWE-bench experimental results.\n20\n\nKimi K2\nTECHNICAL REPORT\nReferences\n[1]\nJacob Austin et al. Program Synthesis with Large Language Models. 2021. arXiv: 2108.07732 [cs.PL]. URL:\nhttps://arxiv.org/abs/2108.07732.\n[2]\nYushi Bai et al. LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context\nMultitasks. 2025. arXiv: 2412.15204 [cs.CL]. URL: https://arxiv.org/abs/2412.15204.\n[3]\nVictor Barres et al. τ 2-Bench: Evaluating Conversational Agents in a Dual-Control Environment. 2025. arXiv:\n2506.07982 [cs.AI]. URL: https://arxiv.org/abs/2506.07982.\n[4]\nStella Biderman et al. “Lessons from the trenches on reproducible evaluation of language models”. In: arXiv\npreprint arXiv:2405.14782 (2024).\n[5]\nFederico Cassano et al. “MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Genera-\ntion”. In: IEEE Transactions on Software Engineering 49.7 (2023), pp. 3675–3691. DOI: 10.1109/TSE.2023.\n3267446.\n[6]\nChen Chen et al. “ACEBench: Who Wins the Match Point in Tool Learning?” In: arXiv e-prints (2025), arXiv–\n2501.\n[7]\nMark Chen et al. “Evaluating Large Language Models Trained on Code”. In: (2021). arXiv: 2107.03374\n[cs.LG].\n[8]\nPeter Clark et al. “Think you have solved question answering? try arc, the ai2 reasoning challenge”. In: arXiv\npreprint arXiv:1803.05457 (2018).\n[9]\nKarl Cobbe et al. Training Verifiers to Solve Math Word Problems. 2021. arXiv: 2110.14168 [cs.LG]. URL:\nhttps://arxiv.org/abs/2110.14168.\n[10]\nDeepSeek-AI. DeepSeek-V3 Technical Report. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.\norg/abs/2412.19437.\n[11]\nMostafa Dehghani et al. “Scaling vision transformers to 22 billion parameters”. In: International conference on\nmachine learning. PMLR. 2023, pp. 7480–7512.\n[12]\nGuanting Dong et al. Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large\nLanguage Models. 2024. arXiv: 2406.13542 [cs.CL]. URL: https://arxiv.org/abs/2406.13542.\n[13]\nXinrun Du et al. “Supergpqa: Scaling llm evaluation across 285 graduate disciplines”. In: arXiv preprint\narXiv:2502.14739 (2025).\n[14]\nDheeru Dua et al. “DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Para-\ngraphs”. In: CoRR abs/1903.00161 (2019). arXiv: 1903.00161. URL: http://arxiv.org/abs/1903.00161.\n[15]\nKazuki Fujii et al. Rewriting Pre-Training Data Boosts LLM Performance in Math and Code. 2025. arXiv:\n2505.02881 [cs.LG]. URL: https://arxiv.org/abs/2505.02881.\n[16]\nPaul Gauthier. Aider LLM Leaderboards. https://aider.chat/docs/leaderboards/. 2025.\n[17]\nAryo Pradipta Gema et al. “Are we done with mmlu?” In: arXiv preprint arXiv:2406.04127 (2024).\n[18]\nAlex Gu et al. “Cruxeval: A benchmark for code reasoning, understanding and execution”. In: arXiv preprint\narXiv:2401.03065 (2024).\n[19]\nDaya Guo et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning”. In: arXiv\npreprint arXiv:2501.12948 (2025).\n[20]\nZhicheng Guo et al. “StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large\nLanguage Models”. In: arXiv preprint arXiv:2403.07714 (2025).\n[21]\nAaron Harlap et al. “Pipedream: Fast and efficient pipeline parallel dnn training”. In: arXiv preprint\narXiv:1806.03377 (2018).\n[22]\nY He et al. “Chinese simpleqa: A chinese factuality evaluation for large language models, 2024a”. In: URL\nhttps://arxiv. org/abs/2411.07140 ().\n[23]\nDan Hendrycks et al. “Measuring massive multitask language understanding”. In: arXiv preprint\narXiv:2009.03300 (2020).\n[24]\nDan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset. 2021. arXiv: 2103.\n03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.\n[25]\nShengding Hu et al. “Minicpm: Unveiling the potential of small language models with scalable training strategies”.\nIn: arXiv preprint arXiv:2404.06395 (2024).\n[26]\nJiaxin Huang et al. “Large language models can self-improve”. In: arXiv preprint arXiv:2210.11610 (2022).\n[27]\nSiming Huang et al. OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models. 2025. arXiv:\n2411.04905 [cs.CL]. URL: https://arxiv.org/abs/2411.04905.\n21\n\nKimi K2\nTECHNICAL REPORT\n[28]\nYanping Huang et al. “Gpipe: Efficient training of giant neural networks using pipeline parallelism”. In: Advances\nin neural information processing systems 32 (2019).\n[29]\nYuzhen Huang et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.\n2023. arXiv: 2305.08322 [cs.CL]. URL: https://arxiv.org/abs/2305.08322.\n[30]\nAlon Jacovi et al. The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to\nLong-Form Input. 2025. arXiv: 2501.03200 [cs.CL]. URL: https://arxiv.org/abs/2501.03200.\n[31]\nNaman Jain et al. “Livecodebench: Holistic and contamination free evaluation of large language models for\ncode”. In: arXiv preprint arXiv:2403.07974 (2024).\n[32]\nCarlos E Jimenez et al. “SWE-bench: Can Language Models Resolve Real-world Github Issues?” In: The Twelfth\nInternational Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=\nVTF8yNQM66.\n[33]\nKeller Jordan et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https : / /\nkellerjordan.github.io/posts/muon/.\n[34]\nMandar Joshi et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.\n2017. arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.org/abs/1705.03551.\n[35]\nKimi Team. “Kimi k1. 5: Scaling reinforcement learning with llms”. In: arXiv preprint arXiv:2501.12599 (2025).\n[36]\nDiederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.\n[37]\nSatyapriya Krishna et al. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation.\n2025. arXiv: 2409.12941 [cs.CL]. URL: https://arxiv.org/abs/2409.12941.\n[38]\nJoel Lamy-Poirier. “Breadth-first pipeline parallelism”. In: Proceedings of Machine Learning and Systems 5\n(2023), pp. 48–67.\n[39]\nDmitry Lepikhin et al. “Gshard: Scaling giant models with conditional computation and automatic sharding”. In:\narXiv preprint arXiv:2006.16668 (2020).\n[40]\nHaonan Li et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2024. arXiv:\n2306.09212 [cs.CL]. URL: https://arxiv.org/abs/2306.09212.\n[41]\nJia Li et al. “Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems\nand solutions”. In: Hugging Face repository 13.9 (2024), p. 9.\n[42]\nTianle Li et al. “From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline”.\nIn: arXiv preprint arXiv:2406.11939 (2024).\n[43]\nBill Yuchen Lin et al. ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning. 2025. arXiv: 2502.\n01100 [cs.AI]. URL: https://arxiv.org/abs/2502.01100.\n[44]\nAixin Liu et al. “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model”. In:\narXiv preprint arXiv:2405.04434 (2024).\n[45]\nJiawei Liu et al. “Is your code generated by chatgpt really correct? rigorous evaluation of large language models\nfor code generation”. In: Advances in Neural Information Processing Systems 36 (2023), pp. 21558–21572.\n[46]\nJingyuan Liu et al. “Muon is scalable for LLM training”. In: arXiv preprint arXiv:2502.16982 (2025).\n[47]\nZiming Liu et al. “Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training\nEfficiency”. In: Proceedings of the International Conference for High Performance Computing, Networking,\nStorage and Analysis. SC ’23. ACM, Nov. 2023, pp. 1–13. DOI: 10.1145/3581784.3607073. URL: http:\n//dx.doi.org/10.1145/3581784.3607073.\n[48]\nIlya Loshchilov and Frank Hutter. “Decoupled Weight Decay Regularization”. In: International Conference on\nLearning Representations. 2019. URL: https://openreview.net/forum?id=Bkg6RiCqY7.\n[49]\nJan Ludziejewski et al. OpenAI Gym. 2025. arXiv: 2502.05172 [cs.LG]. URL: https://arxiv.org/abs/\n2502.05172.\n[50]\nSamuel Miserendino et al. “SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\nSoftware Engineering?” In: arXiv preprint arXiv:2502.12115 (2025).\n[51]\nArindam Mitra et al. “Agentinstruct: Toward generative teaching with agentic flows”. In: arXiv preprint\narXiv:2407.03502 (2024).\n[52]\nIvan Moshkov et al. “Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with\nopenmathreasoning dataset”. In: arXiv preprint arXiv:2504.16891 (2025).\n[53]\nDeepak Narayanan et al. “Efficient large-scale language model training on gpu clusters using megatron-lm”. In:\nProceedings of the international conference for high performance computing, networking, storage and analysis.\n2021, pp. 1–15.\n22\n\nKimi K2\nTECHNICAL REPORT\n[54]\nLong Ouyang et al. “Training language models to follow instructions with human feedback”. In: Advances in\nneural information processing systems 35 (2022), pp. 27730–27744.\n[55]\nBowen Peng et al. “Yarn: Efficient context window extension of large language models”. In: arXiv preprint\narXiv:2309.00071 (2023).\n[56]\nLong Phan et al. Humanity’s Last Exam. 2025. arXiv: 2501.14249 [cs.LG]. URL: https://arxiv.org/\nabs/2501.14249.\n[57]\nPenghui Qi et al. “Zero bubble pipeline parallelism”. In: arXiv preprint arXiv:2401.10241 (2023).\n[58]\nYujia Qin et al. “Toolllm: Facilitating large language models to master 16000+ real-world apis”. In: arXiv\npreprint arXiv:2307.16789 (2023).\n[59]\nQwen et al. Qwen2.5 Technical Report. 2025. arXiv: 2412.15115 [cs.CL]. URL: https://arxiv.org/abs/\n2412.15115.\n[60]\nSamyam Rajbhandari et al. “Zero: Memory optimizations toward training trillion parameter models”. In: SC20:\nInternational Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2020,\npp. 1–16.\n[61]\nDavid Rein et al. “Gpqa: A graduate-level google-proof q&a benchmark”. In: First Conference on Language\nModeling. 2024.\n[62]\nKeisuke Sakaguchi et al. “Winogrande: An adversarial winograd schema challenge at scale”. In: Communications\nof the ACM 64.9 (2021), pp. 99–106.\n[63]\nDavid Silver and Richard S Sutton. “Welcome to the era of experience”. In: Google AI 1 (2025).\n[64]\nVed Sirdeshmukh et al. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging\nto Frontier LLMs. 2025. arXiv: 2501.17399 [cs.CL]. URL: https://arxiv.org/abs/2501.17399.\n[65]\nGiulio Starace et al. “PaperBench: Evaluating AI’s Ability to Replicate AI Research”. In: arXiv preprint\narXiv:2504.01848 (2025).\n[66]\nHao Sun et al. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. 2025. arXiv: 2505.\n04588 [cs.CL]. URL: https://arxiv.org/abs/2505.04588.\n[67]\nMirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. 2022. arXiv:\n2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.\n[68]\nManveer Singh Tamber et al. “Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards”. In: arXiv\npreprint arXiv:2505.04847 (2025).\n[69]\nGemma Team et al. “Gemma 2: Improving open language models at a practical size”. In: arXiv preprint\narXiv:2408.00118 (2024).\n[70]\nLlaMA Team. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation — ai.meta.com.\nhttps://ai.meta.com/blog/llama-4-multimodal-intelligence/. [Accessed 15-07-2025].\n[71]\nThe Terminal-Bench Team. Terminal-Bench: A Benchmark for AI Agents in Terminal Environments. Apr. 2025.\nURL: https://github.com/laude-institute/terminal-bench.\n[72]\nAshish Vaswani et al. “Attention is All you Need”. In: Advances in Neural Information Processing Systems.\nEd. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings.neurips.cc/\npaper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[73]\nVectara. Hallucination Evaluation Model (Revision 7437011). 2024. URL: https://huggingface.co/\nvectara/hallucination_evaluation_model.\n[74]\nJoshua Vendrow et al. “Do large language model benchmarks test reliability?” In: arXiv preprint\narXiv:2502.03461 (2025).\n[75]\nYizhong Wang et al. “Self-instruct: Aligning language models with self-generated instructions”. In: arXiv\npreprint arXiv:2212.10560 (2022).\n[76]\nYubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark.\n2024. arXiv: 2406.01574 [cs.CL]. URL: https://arxiv.org/abs/2406.01574.\n[77]\nZhexu Wang et al. OJBench: A Competition Level Code Benchmark For Large Language Models. 2025. arXiv:\n2506.16395 [cs.CL]. URL: https://arxiv.org/abs/2506.16395.\n[78]\nJason Wei et al. “Measuring short-form factuality in large language models”. In: arXiv preprint arXiv:2411.04368\n(2024).\n[79]\nTianwen Wei et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023.\narXiv: 2306.16636 [cs.CL]. URL: https://arxiv.org/abs/2306.16636.\n[80]\nColin White et al. “LiveBench: A Challenging, Contamination-Free LLM Benchmark”. In: The Thirteenth\nInternational Conference on Learning Representations. 2025.\n23\n\nKimi K2\nTECHNICAL REPORT\n[81]\nMitchell Wortsman et al. “Small-scale proxies for large-scale transformer training instabilities, 2023”. In: URL\nhttps://arxiv. org/abs/2309.14322 ().\n[82]\nCan Xu et al. WizardLM: Empowering large pre-trained language models to follow complex instructions. 2025.\narXiv: 2304.12244 [cs.CL]. URL: https://arxiv.org/abs/2304.12244.\n[83]\nZhangchen Xu et al. KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding. 2025. arXiv:\n2503.02951 [cs.LG]. URL: https://arxiv.org/abs/2503.02951.\n[84]\nJohn Yang et al. SWE-smith: Scaling Data for Software Engineering Agents. 2025. arXiv: 2504.21798 [cs.SE].\nURL: https://arxiv.org/abs/2504.21798.\n[85]\nShunyu Yao et al. “tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains”. In: arXiv\npreprint arXiv:2406.12045 (2024).\n[86]\nDaoguang Zan et al. “Multi-swe-bench: A multilingual benchmark for issue resolving”. In: arXiv preprint\narXiv:2504.02605 (2025).\n[87]\nEric Zelikman et al. “Star: Bootstrapping reasoning with reasoning”. In: Advances in Neural Information\nProcessing Systems 35 (2022), pp. 15476–15488.\n[88]\nRowan Zellers et al. “Hellaswag: Can a machine really finish your sentence?” In: arXiv preprint arXiv:1905.07830\n(2019).\n[89]\nWanjun Zhong et al. “Agieval: A human-centric benchmark for evaluating foundation models”. In: arXiv preprint\narXiv:2304.06364 (2023).\n[90]\nJeffrey Zhou et al. “Instruction-Following Evaluation for Large Language Models”. In: ArXiv abs/2311.07911\n(2023). URL: https://arxiv.org/abs/2311.07911.\n[91]\nQin Zhu et al. AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large\nLanguage Models. 2025. arXiv: 2502.16906 [cs.CL]. URL: https://arxiv.org/abs/2502.16906.\n24\n\nKimi K2\nTECHNICAL REPORT\nAppendix\nA\nContributions\nThe listing of authors is in alphabetical order based on their last names. Names marked with an asterisk (*) indicate\npeople who are no longer part of our team.\nYifan Bai\nYiping Bao\nGuanduo Chen\nJiahao Chen\nNingxin Chen\nRuijue Chen\nYanru Chen\nYuankun Chen\nYutian Chen\nZhuofu Chen*\nJialei Cui\nHao Ding\nMengnan Dong\nAng’ang Du\nChenzhuang Du\nDikang Du\nYulun Du\nYu Fan\nYichen Feng\nKelin Fu\nBofei Gao\nHongcheng Gao\nPeizhong Gao\nTong Gao\nXinran Gu\nLongyu Guan\nHaiqing Guo*\nJianhang Guo\nHao Hu\nXiaoru Hao\nTianhong He\nWeiran He\nWenyang He\nChao Hong\nYangyang Hu\nZhenxing Hu\nWeixiao Huang\nZhiqi Huang\nZihao Huang\nTao Jiang\nZhejun Jiang\nXinyi Jin\nYongsheng Kang*\nGuokun Lai\nCheng Li\nFang Li\nHaoyang Li\nMing Li\nWentao Li\nYanhao Li\nYiwei Li\nZhaowei Li\nZheming Li\nHongzhan Lin*\nXiaohan Lin\nZongyu Lin\nChengyin Liu\nChenyu Liu\nHongzhang Liu\nJingyuan Liu*\nJunqi Liu\nLiang Liu\nShaowei Liu\nT.Y. Liu\nTianwei Liu\nWeizhou Liu\nYangyang Liu\nYibo Liu\nYiping Liu\nYue Liu\nZhengying Liu\nEnzhe Lu\nLijun Lu\nShengling Ma\nXinyu Ma\nYingwei Ma\nShaoguang Mao\nJie Mei\nXin Men\nYibo Miao\nSiyuan Pan\nYebo Peng\nRuoyu Qin\nBowen Qu\nZeyu Shang\nLidong Shi\nShengyuan Shi\nFeifan Song\nJianlin Su\nZhengyuan Su\nXinjie Sun*\nFlood Sung\nHeyi Tang\nJiawen Tao\nQifeng Teng\nChensi Wang\nDinglu Wang\nFeng Wang\nHaiming Wang\nJianzhou Wang*\nJiaxing Wang\nJinhong Wang\nShengjie Wang\nShuyi Wang\nYao Wang\nYejie Wang\nYiqin Wang\nYuxin Wang\nYuzhi Wang\nZhaoji Wang\nZhengtao Wang\nZhexu Wang\nChu Wei\nQianqian Wei\nWenhao Wu\nXingzhe Wu\nYuxin Wu\nChenjun Xiao\nXiaotong Xie\nWeimin Xiong*\nBoyu Xu\nJing Xu*\nJinjing Xu\nL.H. Xu\nLin Xu\nSuting Xu\nWeixin Xu\nXinran Xu\nYangchuan Xu\nZiyao Xu\nJunjie Yan\nYuzi Yan\nXiaofei Yang\nYing Yang\nZhen Yang\nZhilin Yang\nZonghan Yang\nHaotian Yao\nXingcheng Yao\nWenjie Ye\nZhuorui Ye\nBohong Yin\nLonghui Yu\nEnming Yuan\nHongbang Yuan*\nMengjie Yuan\nHaobing Zhan\nDehao Zhang\nHao Zhang\nWanlu Zhang\nXiaobin Zhang\nYangkun Zhang\nYizhi Zhang\nYongting Zhang\nYu Zhang\nYutao Zhang\nYutong Zhang\nZheng Zhang\nHaotian Zhao\nYikai Zhao\nHuabin Zheng\nShaojie Zheng\nJianren Zhou\nXinyu Zhou\nZaida Zhou\nZhen Zhu\nWeiyu Zhuang\nXinxing Zu\nKimi K2\n25\n\nKimi K2\nTECHNICAL REPORT\nB\nToken Template of Tool Calling\nThere are three components in the token structure for tool-calling:\n• Tool declaration message: defines the list of available tools and the schema of the arguments;\n• Tool invoking section in assistant message: encodes the model’s request to invoke tools;\n• Tool result message: encapsulates the invoked tool’s execution result.\nThe raw tokens of the tool declaration message are formatted as follows:\n<|im_begin|>\ntool_declare\n<|im_middle|>\n# Tools\n{{ tool declaration content }}\n<|im_end|>\nThe blue highlighted marks represent special tokens, and the green part, quoted by brackets, is the tool declaration\ncontent. We use TypeScript to express the tool declaration content, since TypeScript is a concise language with a\ncomprehensive type system, able to express the types and constraints of tool parameters with brief text. The code 1\nshows an example for two simple tools in JSON format compatible with OpenAI’s chat completion API, as a comparison,\nthe same tools defined in TypeScript (listed in Code 2) is much shorter. To improve compatibility, part of our training\ndata also uses JSON as the tool declaration language, so that 3rd-party frameworks need not additional development to\nsupport our tool calling scheme.\nListing 1: Tool definition with JSON in OpenAI compatible API\n[{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get\nweather\nfor a location\nand date\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"City and\ncountry e.g. Beijing , China\"\n},\n\"date\": {\n\"type\": \"string\",\n\"description\": \"Date to query , format in\n‘%Y-%m-%d’\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"Calculator\",\n\"description\": \"Simple\ncalculator\",\n\"parameters\": {\n\"properties\": {\n\"expr\": {\n\"type\": \"string\",\n\"description\": \"Arithmetic\nexpression in javascript\"\n}\n},\n26\n\nKimi K2\nTECHNICAL REPORT\n\"type\": \"object\"\n}\n}\n}]\nListing 2: Tool definition in TypeScript\nnamespace\nfunctions {\n// Get weather\nfor a location\nand date\ntype\nget_weather = (_: {\n// City and\ncountry e.g. Beijing , China\nlocation: string ,\n// Date to query , format in\n‘%Y-%m-%d’\ndate ?: string\n}) => any;\n// Simple\ncalculator\ntype\nCalculator = (_: {\n// Arithmetic\nexpression in javascript\nexpr ?: string\n}) => any;\n}\nThe token template of the tool invoking section in the model’s response messages is listed as follows:\n<tool_call_section_begin|>\n<|tool_call_begin|>\n// call_id part\nfunctions.{{tool name}}:{{counter}}\n<|tool_arguments_begin|>\n{{ json serialized call arguments }}\n<|tool_call_end|>\n<|tool_call_begin|>\n// more tool calls\n<|tool_call_end|>\n<|tool_call_section_end|>\nAs shown in the template, we support parallel tool calling by placing multiple tool calls in a single response turn. Each\ntool call has a unique call id, formatted as functions.{tool-name}:{counter}, where tool-name is the name of\nthe tool, and counter is an auto-increasing counter of all tool calls starting from 0 in the dialog.\nDuring inference, the model may occasionally generate unexpected tokens, leading to format errors when parsing a tool\ncall. To solve this issue, we developed a constrained decoding module named enforcer, inspired by lm-format-enforcer6.\nWhen a <tool_call_section_begin|> token is generated, it ensures that the upcoming tool-related tokens follow\nthe predefined template, and the JSON argument string follows the declared schema.\nThe tool result message is simply a text message encoded with the tool’s call id and the corresponding results.\n<|im_begin|>\ntool\n<|im_middle|>\n## Results of {{call_id}}\n{{ execution result content }}\n<|im_end|>\nC\nEvaluation Details\nCoding Tasks.\nWe evaluate Kimi-K2-Instruct’s capabilities on competitive coding benchmarks, LiveCodeBench and\nOJBench, where Kimi-K2-Instruct attains superior performance with scores of 53.7% and 27.1%, respectively. This\nexcellence spans both medium-level coding challenges, such as LeetCode and AtCoder, and hard-level contests like NOI\nand ICPC, outperforming leading open-source and proprietary models. For multilingual programming proficiency, we\nemploy MultiPL-E, covering languages including C++, C#, Java, JavaScript, PHP, Go, Kimi-K2-Instruct surpasses top\n6https://github.com/noamgat/lm-format-enforcer\n27\n\nKimi K2\nTECHNICAL REPORT\nopen-source models with an accuracy of 85.7%, compared with 83.1% for DeepSeek-V3-0324 and 78.2% for Qwen3-\n235B-A22B. In software engineering tasks, Kimi-K2-Instruct demonstrates robust performance on SWE-bench Verified\n(Python), SWE-lancer (Python), SWE-bench Multilingual, and Multi-SWE-bench datasets. It significantly outperforms\nopen-source counterparts in resolving real-world code repository issues and notably narrows the performance gap with\nproprietary models. For example:\n• SWE-bench Verified (multiple attempts): 71.6% (Kimi-K2-Instruct) vs. 80.2% (Claude 4 Sonnet)\n• SWE-bench Multilingual: 47.3% (Kimi-K2-Instruct) vs. 51.0% (Claude 4 Sonnet)\n• SWE-lancer: 39.1% (Kimi-K2-Instruct) vs. 40.8% (Claude 4 Sonnet)\nOn PaperBench, Kimi-K2-Instruct achieves an accuracy of 27.8%, closely matching GPT-4.1 and outperforming\nDeepSeek-V3-0324 (12.2%) and Qwen3-235B-A22B (8.2%) by a substantial margin. In terminal interaction tasks\nmeasured by TerminalBench, Kimi-K2-Instruct attains 25.0% using the default Terminus framework and rises to\n30% within Moonshot’s in-house agentic framework, underscoring its capabilities in real-world agentic programming\nscenarios. Moreover, on the Aider-Polyglot benchmark, Kimi-K2-Instruct attains a 60.0% accuracy while employing\nrigorous decontamination procedures, further illustrating its strength and reliability across diverse coding environments.\nTool Use Tasks.\nWe evaluate multi-turn tool use with two complementary suites: τ 2-Bench and ACEBench. τ 2-Bench\nextends the original τ-bench single-control setup to a dual-control environment in which both the agent and an LLM-\nsimulated user have constrained tool affordances over a shared state, adding a realistic Telecom troubleshooting domain\nalongside the prior Airline/Retail TAU tasks and enabling analysis of coordination vs. pure reasoning. ACEBench is a\nlarge bilingual (En/Zh) API-grounded benchmark (4.5K APIs across 8 domains; 2K annotated eval items) partitioned\ninto NORMAL (basic/personalized/atomic), SPECIAL (imperfect or out-of-scope inputs), and AGENT (scenario-driven\nmulti-turn, multi-step sandbox) tracks with automated grading of calls and outcomes. All models run in non-thinking\nmode; we set the temperature to 0.0, use deterministic tool adapters, score τ 2 Airline/Retail/Telecom under Avg@4\nseeds with Pass@1/4, and report overall on ACEBench English. Kimi-K2-Instruct averages 66.1 micro Pass@1 across\nτ 2 vs DeepSeek-V3-0324 48.8 / Qwen3-235B-A22B 37.3. On ACEBench Overall Kimi-K2-Instruct scores 76.5 vs\nDeepSeek 72.7 / Qwen 70.5 and remains competitive with GPT-4.1 (80.1).\nMath & STEM & Logical Tasks.\nFor Math tasks, Kimi-K2-Instruct achieves consistently strong performance,\naveraging over Geimini-2.5-Flash by 5.3 percentage points, over DeepSeek-V3-0324 by 5.5 points and over GPT4.1 by\n15.8 points. For example, on AIME 2024, Kimi-K2-Instruct scores 69.6%, outperforming another two top open-source\nmodels by a large margin, DeepSeek-V3-0324 by 10.2 points and Qwen3-235B-A22B by 29.5 points. In STEM\nevaluations, Kimi-K2-Instruct achieves 75.1% on GPQA-Diamond, outperforming DeepSeek-V3-0324 (68.4%) and all\nnon-thinking baselines by at least 5 percentage points. On SuperGPQA, it also exceeds the previous best open-source\nmodel, DeepSeek-V3-0324, by 3.5 points. Kimi-K2-Instruct also surpasses the other two leading models in logical\nreasoning. It achieves 89.0% on ZebraLogic and 89.5% on AutoLogi, exceeding DeepSeek-V3-0324 (84.0%, 88.9%)\nand substantially outperforming Qwen3-235B-A22B (37.7%, 83.3%).\nGeneral Tasks.\nKimi-K2-Instruct ties DeepSeek-V3-0324 on MMLU and MMLU-Pro, and takes the lead on MMLU-\nRedux with a 92.7 EM score—slightly ahead of GPT-4.1 (92.4) and just 1.5 points behind Claude-Opus-4. Beyond\nmultiple-choice tasks, the model achieves 31.0% accuracy on the short-answer SimpleQA—3.3 points above DeepSeek-\nV3-0324 and more than twice that of Qwen3-235B-A22B—though still below GPT-4.1 (42.3%). On the adversarial\nfree-response LiveBench (2024-11-25 snapshot), it reaches 76.4%, surpassing Claude-Sonnet 4 (74.8%) and leading\nGemini 2.5 Flash Preview by 8.6 points. Across this challenging triad measuring breadth, depth, and robustness of world\nknowledge, Kimi-K2-Instruct secures a top-tier position among open-source models. We evaluate instruction-following\nwith IFEval and Multi-Challenge. On IFEval, Kimi-K2-Instruct scores 89.8%, higher than DeepSeek-V3-0324 (81.1%)\nand GPT-4.1 (88.0%). On Multi-Challenge, which involves multi-turn dialogues with conflicting instructions, it achieves\n54.1%, outperforming DeepSeek-V3-0324 (31.4%), GPT-4.1 (36.4%), and Claude-Opus-4 (49.0%). These results\ndemonstrate that Kimi-K2-Instruct integrates strong factual knowledge with consistent instruction adherence across\nboth single- and multi-turn settings, supporting robust and reliable real-world deployment.\nLong Context and Factuality Tasks.\nTo evaluate the factuality of Kimi-K2-Instruct, we employ three benchmarks:\nFACTS Grounding, which measures adherence to provided documents using the proprietary models GPT-4o, Gemini\n1.5 Pro and Claude 3.5 Sonnet; HHEM, which assesses summarization quality via the open-source HHEM-2.1-Open\njudge; and FaithJudge, which analyzes faithfulness in RAG tasks with o3-mini as the judge. Kimi-K2-Instruct scores\n88.5 on FACTS Grounding, substantially outperforming all open-source rivals and even surpassing the closed-source\nGemini 2.5 Flash. With HHEM-2.1-Open it achieves a hallucination rate of 1.1 %, reported in the tables as 1 minus the\n28\n\nKimi K2\nTECHNICAL REPORT\n0%\n20%\n40%\n60%\n80%\n100%\n% win rate\nKimi-K2-Instruct\nvs ChatGPT-4o-latest\nKimi-K2-Instruct\nvs Claude-Sonnet-4\nKimi-K2-Instruct\nvs DeepSeek-V3-0324\n65.4%\n17.6%\n17.0%\n64.6%\n18.8%\n16.6%\n59.6%\n23.5%\n16.9%\nKimi-K2-Instruct Open-Ended Evaluation\n(aggregated)\nWin\nTie\nLoss\nFigure 11: Chinese in-house benchmark evaluation.\nrate, i.e. 98.9. On FaithJudge’s RAG tasks the hallucination rate is 7.4 %, likewise present as 92.6 for table consistency.\nFor long-context capabilities, Kimi-K2-Instruct outperforms all open source and proprietary models on DROP (93.5%),\nand exceeds DeepSeek-V3-0324 on retrieval task MRCR (55.0% vs 50.8%). For long-context reasoning tasks FRAMES\nand LongBench v2, Kimi-K2-Instruct (77.1%, 49.1%) lags slightly behind DeepSeek-V3-0324 by around 2%.\nOpen-Ended Evaluation\nBeyond static, closed-ended benchmarks, we evaluate the model’s performance on open-\nended, nuanced tasks that more closely resemble real-world usage.\nFor English scenarios, we leverage the Arena-Hard-Auto v2.0 benchmark, which use LLM-as-a-judge protocols to\nassess generation quality across diverse, open-ended prompts [42]. These evaluations cover a wide range of high-\ndifficulty prompts and are widely recognized in the research community. On Arena-Hard-Auto v2.0, Kimi-K2-Instruct\nachieves state-of-the-art win-rate on both hard prompts (54.5%) and creative writing tasks (85.0%), outperforming all\nopen-source models and rivaling top proprietary systems such as GPT-4.1 and Claude Sonnet. These results underscore\nthe model’s strength in handling complex reasoning and nuanced generation under diverse, unconstrained settings.\nHowever, Arena-Hard-Auto provides limited coverage of Chinese-specific tasks. To address this gap, we developed\nan in-house held-out benchmark grounded in authentic user queries. To safeguard the integrity of the evaluation, the\nbenchmark data is access-restricted, thereby eliminating the risk of overfitting.\nAs shown in Figure 11, Kimi-K2-Instruct shows strong performance across all comparisons on Chinese in-house\nbenchmarks. It outperforms ChatGPT-4o-latest with a 65.4% win rate, Claude Sonnet 4 with 64.6%, and DeepSeek-V3-\n0324 with 59.6%. In all cases, the loss rate stays low (around 17%), indicating that Kimi-K2-Instruct rarely falls behind.\nThe high win rates and consistent margins demonstrate its strong ability on open-ended Chinese tasks.\nIn addition to controlled evaluations, we also consider real-world user preference through public human assessments.\nAs of July 17, 2025, Kimi-K2-Instruct ranked as the top open-source model and fifth overall on the LMSYS Arena\nleaderboard7, based on over 3,000 blind votes from real users. Unlike LLM-as-a-judge protocols, this leaderboard\nreflects direct human preference on diverse, user-submitted prompts, providing a complementary perspective on practical\nmodel performance.\nThe results on Arena-Hard-Auto, our in-house benchmark and votes from LMSYS Arena collectively offer a compre-\nhensive view of Kimi-K2-Instruct’s open-ended capabilities, showing that it is a highly preferred model in real-world\nuser experience across English and Chinese.\nD\nQK-Clip Does Not Impair Model Quality\nThe QK-Clip design follows a minimal intervention principle: it activates only when necessary, and deactivates after\ntraining stabilizes. Empirical evidence and analysis converge on its negligible impact on model quality.\n7https://lmarena.ai/leaderboard/text\n29\n\nKimi K2\nTECHNICAL REPORT\n0\n5000\n10000\n15000\n20000\nTraining Steps\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nValidation Loss\nw/ QK-Clip\nw/o QK-Clip\nFigure 12: Applying QK-Clip to Muon in a small-scale\nsetting with an aggresive threshold (τ = 30) has negligible\nimpact on loss, indicating that it is a safe and effective\nmethod for constraining attention logits.\nSmall-Scale Ablations\nWe train two small-scale 0.5B activated and 3B total parameters MoE models, one with vanilla\nMuon and the other with MuonClip using a low clipping threshold (τ = 30). As shown in Figure 12, applying MuonClip\nhas negligible effects on the loss curve, indicating that even aggressive clipping does not impair convergence or training\ndynamics with MuonClip. This demonstrates that MuonClip is a safe and effective method for bounding attention logits\nwithout degrading model performance. Furthermore, evaluation on downstream tasks reveals no statistically significant\ndegradation in performance. These results collectively demonstrate that MuonClip is a safe and effective method for\nbounding attention logits without compromising model quality.\nSelf-deactivation\nIn Kimi K2, QK-Clip was only transiently active:\n• Initial 70000 steps: 12.7% of attention heads triggered QK-Clip for at least once, clamping Smax to 100.\n• Post-70000 steps: All heads at some point reduced their Smax below 100, rendering QK-Clip inactive.\nWhen QK-Clip is active, it is applied per-head (rather than per-layer) to minimize potential over-regularization on other\nheads. After training stabilizes, QK-clip is deactivated and has no effect at all.\nE\nWhy Muon is More Prone to Logit Explosion\nLogit explosion occurs when the largest pre-softmax attention score\nSmax = max\ni,j\n\u0000qi· kj\n\u0001\n(1)\ngrows unboundedly during training. Since\n|qi·kj| ≤∥qi∥∥kj∥≤∥xi∥∥xj∥∥Wq∥∥Wk∥,\n(2)\nand RMS-Norm keeps ∥xi∥∥xj∥bounded, the phenomenon is primarily driven by the growing spectral-norm of Wq or\nWk. Empirically, we found that Muon is more susceptible to logit explosion. We give our hypothesis below.\nStructural difference in updates\nMuon produces a weight update coming from the msign operation; as a result, all\nsingular values of the update matrix are equal — its effective rank is full. In contrast, a typical update matrix produced\nby Adam exhibits a skewed spectrum: a few large singular values dominate, and the effective rank is low. This low-rank\nassumption for Adam is not new; higher-order muP makes the same assumption.\nSuch phenomenon is verified on the 16 B Moonlight model, which shows weights trained with Muon exhibit higher\nsingular-value entropy (i.e. higher effective rank) than those trained with Adam, corroborating the theoretical intuition.\nSVD formulation\nLet the parameter matrix at step t −1 have the singular value decomposition\nWt−1 =\nX\ni\nσi uiv⊤\ni\n(3)\n30\n\nKimi K2\nTECHNICAL REPORT\nWe write the update matrices as\n∆Wt =\nX\nj\n¯σ ¯uj¯v⊤\nj\n(4)\nThe next parameter update is therefore\nWt ←\nX\ni\nσiuiv⊤\ni +\nX\nj\n¯σ ¯uj¯v⊤\nj\n(5)\nIn Muon, as both the weights and the updates have a higher effective rank than Adam, we hypothesize there is a higher\nprobability for singular-vector pair uiv⊤\ni to align with ¯uj¯v⊤\nj . This could cause the corresponding singular value of Wt\nto increase additively.\nAttention-specific amplification\nAttention logits are computed via the bilinear form\nqi · kj = (xiWq) · (xjWk).\n(6)\nThe product WqW⊤\nk squares the spectral norm, so any singular-value increase in either matrix is compounded. Muon’s\ntendency to enlarge singular values therefore translates into a higher risk of logit explosion.\nF\nK2 Critic Rubrics for General RL\nF.1\nCore Rubrics\n• Clarity and Relevance: Assesses the extent to which the response is succinct while fully addressing the user’s\nintent. The focus is on eliminating unnecessary detail, staying aligned with the central query, and using efficient\nformats such as brief paragraphs or compact lists. Unless specifically required, long itemizations should be avoided.\nWhen a choice is expected, the response should clearly offer a single, well-defined answer.\n• Conversational Fluency and Engagement: Evaluates the response’s contribution to a natural, flowing dialogue that\nextends beyond simple question-answering. This includes maintaining coherence, showing appropriate engagement\nwith the topic, offering relevant observations or insights, potentially guiding the conversation constructively when\nappropriate, using follow-up questions judiciously, handling hypothetical or personal-analogy queries gracefully,\nand adapting tone effectively to suit the conversational context (e.g., empathetic, formal, casual).\n• Objective and Grounded Interaction: Assesses the response’s ability to maintain an objective and grounded\ntone, focusing squarely on the substance of the user’s request. It evaluates the avoidance of both metacommentary\n(analyzing the query’s structure, topic combination, perceived oddity, or the nature of the interaction itself) and\nunwarranted flattery or excessive praise directed at the user or their input. Excellent responses interact respectfully\nbut neutrally, prioritizing direct, task-focused assistance over commentary on the conversational dynamics or\nattempts to curry favor through compliments.\nF.2\nPrescriptive Rubrics\n• Initial Praise: Responses must not begin with compliments directed at the user or the question (e.g., “That’s a\nbeautiful question”, “Good question!”).\n• Explicit Justification: Any sentence or clause that explains why the response is good or how it successfully\nfulfilled the user’s request. This is different from simply describing the content.\nF.3\nLimitations\nOne potential side effect of this evaluation framework is that it may favor responses that appear confident and assertive,\neven in contexts involving ambiguity or subjectivity. This stems from two key constraints in the current rubric:\n• Avoidance of Self-Qualification: The prescriptive rules prohibit self-assessments, explicit disclaimers, or hedging\nlanguage (e.g., “this may not be accurate”, “I might be wrong”). While these phrases can reflect epistemic humility,\nthey are often penalized as non-informative or performative.\n• Preference for Clarity and Singularity: The rubric reward direct, decisive answers when users ask for a\nrecommendation or explanation. In complex or open-ended scenarios, this may disincentivize appropriately\ncautious or multi-perspective responses.\n31\n\nKimi K2\nTECHNICAL REPORT\nAs a result, the model may occasionally overstate certainty in areas where ambiguity, nuance, or epistemic modesty\nwould be more appropriate. Future iterations of the framework may incorporate more fine-grained handling of calibrated\nuncertainty.\nG\nEngine Switching Pipeline for RL Training\nH2D\nReload weights\nBroadcast (src)\nBroadcast (dst)\nH2D Buffer\nIPC Buffer\nDevice 0\nDevice 1\nDevice 2\nDevice 3\n(a) Theoretical perfect three-stage pipeline weight update\n(b) A PCIE bounded three-stage pipeline\n(c) Fixed two-stage pipeline\nFigure 13: pipeline for RL weight update\nThe checkpoint engine manages three equal-size device buffers on each GPU: an H2D buffer for loading the offloaded\nmodel parameters, and two IPC buffers for GPU-to-GPU broadcast. The IPC buffers are shared to inference engines,\nallowing it to directly access the same physical memory. These three buffers allow us to arrange the three steps in a\npipeline.\nTheoretical three-stage pipeline.\nAs illustrated in Figure 13a, a three-stage pipeline is introduced. (1) H2D: a shard\nof the latest weights is copied into the H2D buffer asynchronously. (2) Broadcast: Once the copy completes, the shard\nwill be copied to one IPC buffers and broadcast to all devices. (3) Reload: Inference engines simultaneously load\nparameters from the other IPC buffer.\nTwo-stage pipeline due to PCIe saturation.\nOn NVIDIA H800 clusters, concurrent H2D and broadcast saturate the\nshared PCIe fabric, collapsing the three stages into a sequential procedure (Figure 13b). We therefore adopt a simpler,\ntwo-stage scheme (Figure 13c): (1) All devices perform a single, synchronous H2D transfer. (2) The broadcast and\nreload proceed in parallel.\nThe two-stage pipeline will be bound by multiple synchronous H2D copy operations. But in large scale devices, model\nwill be split into small shards, the entire parameter set fits into the H2D buffer in one transfer, the overhead will\ndisappear.\nBy overlapping H2D, Broadcast, and Reload weights, we can obtain a high bandwidth to reshard the weights from train\nengines to all inference engines.\n32\n"
    }
  ]
}