{
  "4-1 (Pre-training Data)": "The available information paints a very concrete picture of Kimi K2’s pre-training stage. The base model is described as a trillion-parameter Mixture-of-Experts (MoE) transformer that was exposed to a colossal 15.5 trillion tokens. Multiple statements repeat the same figure, establishing that the full corpus size is indeed 15.5 T tokens and that training proceeded “with zero loss spike,” suggesting stable optimization throughout. All material is explicitly characterised as “high-quality” and “curated.” Four top-level domains are named—Web Text, Code, Mathematics, and Knowledge—indicating broad topical coverage. The team emphasises that every domain went through “rigorous correctness and quality validation,” implying that data were not merely scraped but examined for factual soundness and diversity. A notable methodological upgrade over the predecessor Kimi K1.5 is the “introduction of a synthetic data generation strategy to increase token utility,” signalling that part of the 15.5 T tokens are machine-generated to enrich training signals. Finally, the authors credit the token-efficient MuonClip optimizer as a key enabler, stating that by “leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training.”",
  "4-2 (Fine-tuning Data)": "Fine-tuning (post-training) of Kimi K2 relies on several distinct datasets and guiding principles. First, the project “construct[s] a large-scale instruction-tuning dataset spanning diverse domains,” intentionally maximising prompt diversity and emphasising “high response quality.” In addition to generic instruction data, the team “curated a mixture of open-source and in-house preference datasets” specifically to bootstrap K2’s ability to act as a “competent judge”; these examples are fed in the supervised-fine-tuning (SFT) stage to initialise a critic head. A further component is a “large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations via simulated and real-world environments,” designed to push K2’s agentic capabilities. All of these fine-tuning efforts employ the Muon optimizer, which the authors “recommend … for fine-tuning with K2,” hinting that hyper-parameter choices are aligned with the earlier MuonClip philosophy. Collectively, the fine-tuning corpus therefore contains (i) diverse instruction-following prompts and answers, (ii) preference and ranking data for critic training, and (iii) synthetic demonstrations of tool usage, each contributing to coverage and quality rather than raw volume alone.",
  "4-3 (Reinforcement Learning Data)": "For RL-based post-training, the team explicitly “continue[s] to scale RL in both task diversity and training FLOPs in K2,” building upon practices from K1.5. They have developed “a Gym-like extensible framework that facilitates RL across a wide range of scenarios,” implying programmatic generation or ingestion of environments and prompts. The RL loop uses an actor–critic setup in which “the K2 actor generates responses for general prompts … [and] the K2 critic then ranks all results by performing pairwise evaluations.” The critic’s scoring criteria blend three sources: (1) core rubrics that encode the fundamental values “that Kimi cherish,” (2) prescriptive rubrics to “eliminate reward hacking,” and (3) “human-annotated rubrics” drawn up for certain instructional contexts. In a “multi-stage post-training process,” RL occurs alongside the previously mentioned “large-scale agentic data synthesis pipeline,” allowing K2 to improve through interactions with both “real and synthetic environments.” Hence, the RL data are not a static corpus but are dynamically produced trajectories encompassing diverse tasks, rubric-based preference labels, and a scalable framework to expand coverage.",
  "4-4 (Data Filtering)": "Kimi K2’s data cleaning and filtering pipeline is repeatedly described as rigorous and multi-layered. For the pre-training corpus, the team states that they performed “rigorous correctness and quality validation” for each of the four major domains (Web Text, Code, Mathematics, Knowledge) and ran “targeted data experiments” to ensure high diversity and effectiveness, confirming proactive curation rather than passive scraping. A “synthetic data generation strategy” is highlighted, but crucially the authors note a dedicated “fidelity verification” step: every rewritten passage undergoes “fidelity checks that compare the semantic alignment of each rephrased passage with its source,” ensuring that synthetic augmentations remain truthful to originals. Additional decontamination safeguards appear during evaluation and fine-tuning: on the Aider-Polyglot benchmark “Kimi-K2-Instruct attains a 60.0 % accuracy while employing rigorous decontamination procedures,” signalling active removal or mitigation of test-set contamination and potential memorisation issues. Collectively, the filtering methodology therefore combines domain-specific validation, semantic fidelity checking for synthetic rewrites, and benchmark-oriented decontamination, all designed to maximise data reliability before, during, and after model training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike."
    },
    {
      "source": "[pdf_text]",
      "quote": "The base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer [72] model, pre-trained on 15.5 trillion high-quality tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility."
    },
    {
      "source": "[pdf_text]",
      "quote": "Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We employ the Muon optimizer [34] in our post-training and recommend its use for fine-tuning with K2. We construct a large-scale instruction-tuning dataset spanning diverse domains, guided by two core principles: maximizing prompt diversity and ensuring high response quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "To bootstrap K2 as a competent judge, we curated a mixture of open-source and in-house preference datasets and initialize its critic capability in the SFT stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "We introduce Kimi K2, a 1.04 trillion-parameter Mixture-of-Experts (MoE) LLM with 32 billion activated parameters, purposefully designed to address the core challenges and push the boundaries of agentic capability. We introduce a large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations via simulated and real-world environments."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Based on the work of K1.5 [36], we continue to scale RL in both task diversity and training FLOPs in K2. To support this, we develop a Gym-like extensible framework that facilitates RL across a wide range of scenarios."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the first core process of the learning loop, the K2 actor generates responses for general prompts that cover a wide range of use cases. The K2 critic then ranks all results by performing pairwise evaluations against a combination of rubrics, which incorporates both core rubrics (Appendix. F.1), which represent the fundamental values of our AI assistant that Kimi cherish, prescriptive rubrics (Appendix. F.2) that aim to eliminate reward hacking, and human-annotated rubrics crafted by our data team for specific instructional contexts."
    },
    {
      "source": "[pdf_text]",
      "quote": "During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge. For each domain, we performed rigorous correctness and quality validation and designed targeted data experiments to ensure the curated dataset achieved both high diversity and effectiveness."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility. Fidelity verification: To ensure consistency between original and rewritten content, we perform fidelity checks that compare the semantic alignment of each rephrased passage with its source."
    },
    {
      "source": "[pdf_text]",
      "quote": "Moreover, on the Aider-Polyglot benchmark, Kimi-K2-Instruct attains a 60.0% accuracy while employing rigorous decontamination procedures, further illustrating its strength and reliability across diverse coding environments."
    }
  ]
}