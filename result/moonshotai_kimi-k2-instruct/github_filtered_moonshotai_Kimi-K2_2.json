{
  "1-5 (Architecture)": "The statements that explicitly mention Kimi-K2 give four concrete architectural facts. First, the quote “| **Activated Parameters** | 32B |” establishes that the model keeps 32 billion parameters active during inference or training. Second, the line “| **MoE Hidden Dimension** (per Expert) | 2048 |” indicates that Kimi-K2 uses a Mixture-of-Experts design in which each expert has a hidden size of 2 048. Third, the row “| **Selected Experts per Token** | 8 |” reveals that eight experts are routed to every individual token, confirming a multi-expert routing strategy. Finally, “| **Context Length** | 128K |” specifies that the maximum sequence length processed in one pass is 128 k tokens. Taken together, these four figures describe Kimi-K2 as a 32-billion-parameter MoE architecture with 2 048-dimension experts, eight experts chosen per token, and an extended 128 k-token window.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "| **Activated Parameters** | 32B |"
    },
    {
      "source": "[readme]",
      "quote": "| **MoE Hidden Dimension** (per Expert) | 2048 |"
    },
    {
      "source": "[readme]",
      "quote": "| **Selected Experts per Token** | 8 |"
    },
    {
      "source": "[readme]",
      "quote": "| **Context Length** | 128K |"
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer information is limited to a single quantitative line: “| **Vocabulary Size** | 160K |.” This shows that Kimi-K2’s tokenizer is built with a 160 000-token vocabulary, but no further structural or download details are given in the supplied text.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "| **Vocabulary Size** | 160K |"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The only training-software detail provided is the sentence: “Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.” From this we can conclude that the Muon optimizer was employed during Kimi-K2’s training run; no additional frameworks, libraries, or version specifics are disclosed in the available excerpts.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities."
    }
  ]
}