{
  "pretrain_method": "Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike.",
  "pretrain_data": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge.",
  "__evidence": [
    {
      "source": "arxiv:2507.20534",
      "quote": "Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike."
    },
    {
      "source": "arxiv:2507.20534",
      "quote": "The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge."
    }
  ]
}