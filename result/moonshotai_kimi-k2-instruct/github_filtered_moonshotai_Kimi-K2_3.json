{
  "2-3 (API)": "The quotes specify that “You can access Kimi K2's API on https://platform.moonshot.ai,” underlining that the developer-facing endpoint is hosted at Moonshot’s platform domain. The same sentence clarifies compatibility, stating that Kimi-K2 exposes “OpenAI/Anthropic-compatible API” semantics, implying users can interact with it through request/response patterns and tooling originally designed for those well-known ecosystems. Thus, the project publicly advertises a self-service, remotely hosted, production API for the Kimi-K2 model family, reachable via a dedicated URL and designed to integrate with existing GPT-style or Anthropic-style client libraries.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "> You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you."
    }
  ],
  "3-1 (Pre-training)": "According to the provided material, the Kimi-K2 effort relies on “Large-Scale Training” of “a 1T parameter MoE model.” The model was pre-trained on an immense corpus amounting to “15.5T tokens,” and the training run achieved “zero training instability,” highlighting robustness during scale-up. Optimization relied on the “MuonClip Optimizer,” described as applying “the Muon optimizer to an unprecedented scale.” Novel optimization techniques were introduced specifically to “resolve instabilities while scaling up,” indicating that the team focused on stabilizing gradient updates and convergence when training a trillion-parameter mixture-of-experts architecture on tens of trillions of tokens.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "- Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    },
    {
      "source": "[readme]",
      "quote": "- MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up."
    }
  ],
  "3-2 (Fine-tuning)": "The fine-tuning layer of the release is split into at least two named checkpoints. First, “Kimi-K2-Base” is positioned as “the foundation model,” intended for “researchers and builders who want full control for fine-tuning and custom solutions.” This signals that the base checkpoint is delivered without instruction or preference specialization, giving downstream users a blank slate on which to apply their own supervised or alignment procedures. Second, “Kimi-K2-Instruct” is described as “the post-trained model best for drop-in, general-purpose chat and agentic experiences.” It is explicitly characterized as “a reflex-grade model without long thinking,” suggesting that the post-training stage produces fast, instruction-following outputs suitable for conversational applications.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Base**: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions."
    },
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": []
}