{
  "1-5 (Architecture)": "The provided material states that Kimi K2 is implemented as a Mixture-of-Experts (MoE) transformer. It contains a total of about 1.04 trillion parameters, of which 32 billion are active on any given token. Its design is patterned after DeepSeek-V3 and makes Multi-head Latent Attention (MLA) the core attention mechanism. Key dimensional choices are a 7 168-wide model hidden size and 2 048-wide MoE expert hidden size. Relative to DeepSeek-V3, the team expanded the expert count from 256 to 384 to boost capacity, but cut the number of attention heads from 128 to 64 to lower inference cost. Overall, the model is presented as a “1 T-parameter open-weight MoE model built for agentic intelligence.”",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "Training was carried out on a cluster built from NVIDIA H800 GPUs. Each cluster node offers 2 TB of system RAM and hosts eight H800 cards that interconnect via NVLink and NVSwitch on-node.",
  "2-2 (Software)": "For optimization, Kimi K2 employs the token-efficient Muon (also referenced as MuonClip) optimizer, augmented with weight decay and the consistent update RMS-scaling technique. The parallel-training strategy can scale to any node count that is a multiple of 32, combining 16-way Pipeline Parallelism with virtual stages, 16-way Expert Parallelism, and ZeRO-1 Data Parallelism. These choices were applied during pre-training on a 15.5-trillion-token corpus, aiming for stable and compute-efficient scaling.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Kimi K2 is a 1.04 trillion-parameter Mixture-of-Experts (MoE) transformer model with 32 billion activated parameters. The architecture follows a similar design to DeepSeek-V3 [10] , employing Multi-head Latent Attention (MLA) [44] as the attention mechanism, with a model hidden dimension of 7168 and an MoE expert hidden dimension of 2048."
    },
    {
      "source": "[pdf_text]",
      "quote": "Kimi K2 … motivated us to increase the number of experts to 384, compared to 256 in DeepSeek-V3. To reduce computational overhead during inference, we cut the number of attention heads to 64, as opposed to 128 in DeepSeek-V3."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence."
    },
    {
      "source": "[sections/Pre-training/Model Architecture]",
      "quote": "Kimi K2 is a 1.04 trillion-parameter Mixture-of-Experts (MoE) transformer model with 32 billion activated parameters. The architecture follows a similar design to DeepSeek-V3 [10] , employing Multi-head Latent Attention (MLA) [44] as the attention mechanism, with a model hidden dimension of 7168 and an MoE expert hidden dimension of 2048."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "We introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Kimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB RAM and 8 GPUs connected by NVLink and NVSwitch within nodes."
    },
    {
      "source": "[sections/Training Infrastructure/Compute Cluster]",
      "quote": "Kimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB RAM and 8 GPUs connected by NVLink and NVSwitch within nodes."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Kimi K2 using the token-efficient Muon optimizer [33], incorporating weight decay and consistent update RMS scaling [46]."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training."
    },
    {
      "source": "[sections/Pre-training/MuonClip: Stable Training with Weight Clipping]",
      "quote": "We train Kimi K2 using the token-efficient Muon optimizer [33], incorporating weight decay and consistent update RMS scaling [46]."
    },
    {
      "source": "[sections/Training Infrastructure/Parallelism for Model Scaling]",
      "quote": "…we pursue a flexible strategy that allows Kimi K2 to be trained on any number of nodes that is a multiple of 32. Our strategy leverages a combination of 16-way Pipeline Parallelism (PP) with virtual stages [28, 53, 38, 57, 47, 21], 16-way Expert Parallelism (EP) [39], and ZeRO-1 Data Parallelism [60]."
    }
  ]
}