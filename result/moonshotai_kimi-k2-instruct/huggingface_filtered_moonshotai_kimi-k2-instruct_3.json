{
  "2-3 (API)": "The available material specifies that developers can reach the Kimi K2 interface at “https://platform.moonshot.ai.”  The provider explicitly states that they “provide OpenAI/Anthropic-compatible API for you,” signalling that request and response schemas align with the two most common industry conventions.  One concrete compatibility rule is spelled out in a second sentence: “The Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications.”  Taken together, these two sentences convey every known public detail: the endpoint location, its high-level compatibility promise, and the exact rescaling formula applied to the temperature parameter so legacy Anthropic-tuned clients work unmodified.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you."
    },
    {
      "source": "[readme]",
      "quote": "The Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications."
    }
  ],
  "3-1 (Pre-training)": "The documentation summarises the core pre-training facts in one compound statement: “Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. – Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.”  From this we learn that Kimi K2 adopts a Mixture-of-Experts architecture, that the full parameter count reaches one trillion while only 32 billion are active per token, and that the training run processed 15.5 trillion tokens without encountering instability.  No additional hyper-parameters, optimiser settings, curriculum design, or data-pipeline specifics are supplied in the provided quotes.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. - Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning information is limited to a single sentence: “**Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences.”  This tells us that, after base pre-training, Kimi K2 undergoes a post-training or instruction-tuning phase that yields the “Kimi-K2-Instruct” variant, and that this derivative is considered optimal for general chat and agent-style deployments.  No further details about datasets, objectives, or procedural steps are provided in the quoted material.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": []
}