{
  "model_id": "Qwen/Qwen2.5-32B",
  "full_texts": [
    {
      "arxiv_id": "2407.10671",
      "full_text": "QWEN2 TECHNICAL REPORT\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren\nZhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei\nLi, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan,\nYang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang\nGuo, and Zhihao Fan\nQwen Team, Alibaba Group∗\nABSTRACT\nThis report introduces the Qwen2 series, the latest addition to our large lan-\nguage models and large multimodal models. We release a comprehensive suite of\nfoundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\nThe flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as\na base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains\n9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,\nQwen2 demonstrates robust multilingual capabilities, proficient in approximately\n30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Rus-\nsian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility\nand global reach.\nTo foster community innovation and accessibility, we have made the Qwen2 model\nweights openly available on Hugging Face1 and ModelScope2, and the supplemen-\ntary materials including example code on GitHub3. These platforms also include\nresources for quantization, fine-tuning, and deployment, facilitating a wide range\nof applications and research endeavors.\n∗Authors are ordered alphabetically by the first name.\n1https://huggingface.co/Qwen\n2https://modelscope.cn/organization/qwen\n3https://github.com/QwenLM/Qwen2\n1\narXiv:2407.10671v4  [cs.CL]  10 Sep 2024\n\nCONTENTS\n1\nIntroduction\n3\n2\nTokenizer & Model\n3\n2.1\nTokenizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nModel Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.1\nQwen2 Dense Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.2\nQwen2 Mixture-of-experts Model . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.3\nModel Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nPre-training\n5\n3.1\nPre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nLong-context Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nPost-training\n6\n4.1\nPost-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.1.1\nCollaborative Data Annotation . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.1.2\nAutomated Data Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.2\nSupervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nReinforcement Learning from Human Feedback . . . . . . . . . . . . . . . . . . .\n8\n5\nEvaluation\n8\n5.1\nBase Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.1.1\nCore Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nInstruction-tuned Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2.1\nOpen Benchmark Evaluation . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2.2\nIn-house Automatic Evaluation\n. . . . . . . . . . . . . . . . . . . . . . .\n14\n5.2.3\nLong Context Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n5.2.4\nMultilingual Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2.5\nSafety & Responsibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2.6\nContamination Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6\nConclusion\n20\n2\n\n1\nINTRODUCTION\nFollowing the emergence of ChatGPT (OpenAI, 2022), enthusiasm for large language models\n(LLMs) has escalated globally. The release of the Llama series (Touvron et al., 2023) has further\nignited interests within the open-source community, particularly regarding GPT-level local LLMs.\nRecently, Claude-3 Opus (Anthropic, 2024) and GPT-4o (omni) (OpenAI, 2024), the updated model\nfor ChatGPT, have ascended to the pinnacle of the Chatbot Arena (Chiang et al., 2024) in quick\nsuccession. This platform is well-regarded for its human evaluations of LLMs. Moreover, Llama-\n3 (AI@Meta, 2024) has emerged as the state-of-the-art open-weight model series, narrowing the\nperformance gap with leading proprietary models and widely acknowledged as GPT-4–level. An\nincreasing number of competitive LLMs are now pursuing advancements similar to those made by the\nGPT series from OpenAI. Many of these models, including Qwen (Bai et al., 2023a), Mistral (Jiang\net al., 2023a), Gemma (Mesnard et al., 2024), etc., have been released in an open-weight manner.\nOver recent months, we have successively introduced the Qwen series (Bai et al., 2023a) and\nprogressed to Qwen1.5 (Qwen Team, 2024a). In the meantime, we have unveiled the vision-language\nmodel Qwen-VL (Bai et al., 2023b), and launched the audio-language model Qwen-Audio (Chu\net al., 2023). In this work, we introduce the newest addition to the Qwen family of large language\nmodels and large multimodal modles: Qwen2. Qwen2 is a series of LLMs, grounded in the\nTransformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model\nseries encompasses foundational, i.e., base language models, pre-trained but unaligned to human\npreferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction-\nfollowing datasets suitable for chat and agent purposes. Our release comprises four dense models\nwith parameter counts of 0.5 billion, 1.5 billion, 7 billion, and 72 billion, plus a Mixture-of-Experts\n(MoE) model with 57 billion parameters, of which 14 billion are activated for each token. The smaller\nmodels, specifically Qwen2-0.5B and Qwen2-1.5B, are designed for easy deployment on portable\ndevices such as smartphones, earphones, and smart glasses. Conversely, the larger models cater to\ndeployment across GPUs of varying scales.\nAll models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,\ncovering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2\nincludes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathe-\nmatics content. This enrichment is hypothesized to improve reasoning abilities of LLMs. Regarding\npost-training, all models underwent supervised fine-tuning and direct preference optimization (DPO,\nRafailov et al., 2023), aligning them with human preferences through learning from human feedback.\nThis process endows the models with the capability to follow instructions effectively.\nWe have conducted a thorough evaluation of Qwen2, alongside a selection of baseline models includ-\ning both open-weight and proprietary models accessible via API. Qwen2 outperforms competing\nmodels in evaluations of both fundamental language capabilities and instruction-tuned functionalities\nSpecifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng\net al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al.,\n2024). Meanwhile, Qwen2-72B, the base language model, achieves 84.2 on MMLU (Hendrycks\net al., 2021a), 37.9 on GPQA (Rein et al., 2023), 64.6 on HumanEval (Chen et al., 2021), 89.5 on\nGSM8K (Cobbe et al., 2021), and 82.4 on BBH (Suzgun et al., 2023).\n2\nTOKENIZER & MODEL\nThis section introduces the tokenizer and model design of Qwen2. We detail the model architecture\nand configurations for different model sizes.\n2.1\nTOKENIZER\nFollowing Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-\npair encoding. Notably, this tokenizer exhibits high encoding efficiency, as evidenced by its better\ncompression rate relative to alternatives, facilitating the multilingual capabilities of Qwen2.\nModels of all sizes employ a common vocabulary consisting of 151,643 regular tokens and 3 control\ntokens. For more information, please refer to Bai et al. (2023a). It should be noted that, owing to\nconsiderations in distributed training, the effective size for the embeddings is larger.\n3\n\n2.2\nMODEL ARCHITECTURE\nThe Qwen2 series fundamentally constitute large language models based on the Transformer ar-\nchitecture, featuring self-attention with causal masks (Vaswani et al., 2017). Specifically, this\nseries encompasses dense language models of 4 scales and a Mixture-of-Experts (MoE) model. We\nintroduce the specifics of the dense models before delving into the MoE model’s distinctive attributes.\n2.2.1\nQWEN2 DENSE MODEL\nThe architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped\nwith causal attention mechanisms and feed-forward neural networks (FFNs). Key differences from\nQwen are described below:\nGrouped Query Attention\nWe adopt Grouped Query Attention (GQA, Ainslie et al., 2023) instead\nof conventional multi-head attention (MHA). GQA optimizes KV cache usage during inference,\nsignificantly enhancing throughput. Detailed KV head configurations for various model sizes are\nreported in Section 2.2.3.\nDual Chunk Attention with YARN\nTo expand the context window of Qwen2, we implement Dual\nChunk Attention (DCA, An et al., 2024), which segments long sequences into chunks of manageable\nlengths. If the input can be handled in a chunk, DCA produces the same result as the original\nattention. Otherwise, DCA facilitates effective capture of relative positional information between\ntokens within and across chunks, thereby improving long context performance. Moreover, we also\nemploy YARN (Peng et al., 2023) to rescale the attention weights for better length extrapolation.\nMoreover, we follow Qwen with the usage of SwiGLU (Dauphin et al., 2017) for activation, Rotary\nPositional Embeddings (RoPE, Su et al., 2024) for positional embedding, QKV bias (Su, 2023) for\nattention, RMSNorm (Jiang et al., 2023b) and pre-normalization for training stability.\n2.2.2\nQWEN2 MIXTURE-OF-EXPERTS MODEL\nThe architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team,\n2024c). As a substitute for the original FFN, the MoE FFN consists of n individual FFNs, each serving\nas an expert. Each token is directed to a specific expert Ei for computation based on probabilities\nassigned by a gated network G:\np = softmax (G (x)) ,\n(1)\ny =\nX\ni∈topk(p) piEi(x).\n(2)\nIn the following, we present critical design considerations of Qwen2 MoE.\nExpert Granularity\nThe key structural difference between MoE models and dense models is\nthat MoE layers incorporate multiple FFNs, each serving as an individual expert. Consequently,\none straightforward strategy to transition from a dense architecture to an MoE architecture is to set\nthe parameters of each expert equal to those of a single FFN from the original dense model. For\nexample, transitioning from Mistral-7B (Jiang et al., 2023a) to Mixtral 8x7B (Jiang et al., 2024),\ninvolves activating two of the eight experts at a time. Differently, our model employs fine-grained\nexperts (Dai et al., 2024), creating smaller-scale experts while activating a greater number of experts\nsimultaneously. Given an equal total number of expert parameters and activated parameters, fine-\ngrained experts offer a richer set of expert combinations. By leveraging these fine-grained experts,\nQwen2 MoE facilitates more diverse and dynamic expert utilization, thereby enhancing overall\nperformance and adaptability.\nExpert Routing\nThe design of expert routing mechanisms is crucial for enhancing the performance\nof MoE models. Recently, there has been a notable trend towards integrating both shared and\nrouting-specific experts within MoE layers (Rajbhandari et al., 2022; Dai et al., 2024). We adopt this\napproach, as it facilitates the application of shared experts across various tasks while reserving others\nfor selective use in specific routing scenarios. The introduction of shared and specialized experts\noffers a more adaptable and efficient method for developing MoE routing mechanisms.\n4\n\nTable 1: Architecture of Qwen2 dense and MoE models. For MoE models, 57B-A14B denotes that\nthe model has 57B parameters in total and for each token 14B parameters are active, the Intermediate\nsize denotes that of each expert, and # Activated Experts excludes the shared experts.\nConfiguration\n0.5B\n1.5B\n7B\n72B\n57B-A14B\nHidden Size\n896\n1,536\n3,584\n8,192\n3,584\n# Layers\n24\n28\n28\n80\n28\n# Query Heads\n14\n12\n28\n64\n28\n# KV Heads\n2\n2\n4\n8\n4\nHead Size\n64\n128\n128\n128\n128\nIntermediate Size\n4,864\n8,960\n18,944\n29,568\n2,560\n# Routed Experts\n-\n-\n-\n-\n64\n# Activated Experts\n-\n-\n-\n-\n8\n# Shared Experts\n-\n-\n-\n-\n8\nEmbedding Tying\nTrue\nTrue\nFalse\nFalse\nFalse\nVocabulary Size\n151,646\n151,646\n151,646\n151,646\n151,646\n# Trained Tokens\n12T\n7T\n7T\n7T\n4.5T\nExpert Initialization\nWe initialize the experts in a similar way to upcycling (Komatsuzaki et al.,\n2023), leveraging the weights of a dense model. In contrast, our approach emphasizes diversification\namong fine-grained experts to enhance the model’s representational breadth. Given the designated\nexpert intermediate size hE, the number of experts n, and the original FFN intermediate size hFFN, the\nFFN is replicated ⌈n×hE/hFFN⌉times. This replication ensures compatibility with the specified number\nof experts while accommodating any arbitrary expert intermediate size. To promote diversity within\neach FFN copy, parameters are shuffled along the intermediate dimension. This guarantees that each\nfine-grained expert exhibits unique characteristics, even across different FFN copies. Subsequently,\nthese experts are extracted from the FFN copies, and the remaining dimensions are discarded. For\neach fine-grained expert, 50% of its parameters are randomly reinitialized. This process introduces\nadditional stochasticity into expert initialization, potentially enhancing the model’s capacity for\nexploration during training.\n2.2.3\nMODEL CONFIGURATION\nIn the following, we provide the key configuration and information for the Qwen2 series.\nThe Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B,\nQwen2-57B-A14B, and Qwen2-72B. Table 1 lists the hyper-parameters and important information,\ne.g., the number of pre-trained tokens. Particularly, Qwen2-57B-A14B is upscaled from Qwen2-7B.\nNotably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative\nto Qwen1.5 models. This characteristic translates into a reduced memory footprint, particularly\nadvantageous in long-context inference tasks.\n3\nPRE-TRAINING\nIn the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating\nmethods to handle extended context lengths effectively.\n3.1\nPRE-TRAINING DATA\nThe pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality\nmultilingual dataset. This dataset represents an improvement over the corpora used in previous\nQwen and Qwen1.5 models (Bai et al., 2023a; Qwen Team, 2024a), enhancing the scale, quality, and\ndiversity of the pre-training data in several key areas:\nQuality Enhancement\nThe filtering algorithm has been refined with additional heuristic and model-\nbased methods, including the use of the Qwen models to filter out low-quality data. Moreover, these\nmodels are utilized to synthesize high-quality pre-training data.\n5\n\nData Expansion\nCompared to Qwen1.5 (Qwen Team, 2024a), we have collected a significantly\nlarger volume of high-quality code, mathematics, and multilingual data, enhancing the model’s capa-\nbilities in respective areas. This new dataset supports approximately 30 languages, such as English,\nChinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, and Vietnamese.\nDistribution Improvement\nTo ensure the model learns the distribution akin to human-like learning,\nwe conduct experiments on scaled-down models to optimize the mixing of data from various sources\nand domains.\nBased on these enhancements, the pre-training data was expanded from 3 trillion tokens in\nQwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold\nresulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a\nsignificant performance improvement over the 7 trillion token model. It is suspected that increasing\nthe volume of data does not necessarily benefit model pre-training. Considering training costs, we\nopted to use the higher-quality 7 trillion token dataset for training larger models, leaving further\nexploration for future model iterations.\nAll Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of\nover 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset. The MoE\nmodel received an additional 4.5 trillion tokens of pre-training, in line with the principle of upcycling.\nSimilar to previous Qwen models, high-quality multi-task instruction data is integrated into the\nQwen2 pre-training process to enhance in-context learning and instruction-following abilities.\n3.2\nLONG-CONTEXT TRAINING\nTo enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens\nto 32,768 tokens during the concluding phase of pre-training. This expansion was complemented by\nthe introduction of a significantly increased volume of high-quality, lengthy data. In conjunction with\nthese enhancements, we modified the base frequency of RoPE from 10,000 to 1,000,000 to optimize\nperformance in long-context scenarios (Xiong et al., 2023).\nTo fully leverage the model’s length extrapolation potential, we adopted the YARN mechanism (Peng\net al., 2023) and the Dual Chunk Attention mechanism (An et al., 2024). These strategies enable\nthe model to process sequences of up to 131,072 tokens while maintaining high performance, as\nevidenced by minimal perplexity degradation in preliminary experiments.\n4\nPOST-TRAINING\nFollowing extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This\nprocess is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding,\nmathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover,\nit ensures that the generation from the models is in harmony with human values, making it helpful,\nhonest, and harmless. Unlike traditional methods that heavily rely on extensive human supervision,\nour approach focuses on scalable alignment with minimal human annotation (Cao et al., 2024).\nSpecifically, we investigate methods to acquire high-quality demonstration and preference data for\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming\nto minimize the need for human labeling while maximizing the quality and reliability of the data.\n4.1\nPOST-TRAINING DATA\nThe post-training data primarily consists of two components: demonstration data D = {(xi, yi)} and\npreference data P = {(xi, y+\ni , y−\ni )}, where xi represents the instruction, yi represents a satisfactory\nresponse, and y+\ni and y−\ni are two responses to xi, with y+\ni being the preferred choice over y−\ni . The\nset D is utilized in SFT, whereas P is employed in RLHF.\nThe construction of training data entails a two-step process: collaborative data annotation and\nautomated data synthesis. First, we extract the data ontology from large-scale instruction corpora,\nleading to a broad and diverse set of high-quality instructions. These instructions are systematically\nenhanced to incorporate greater complexity. Through human annotation, we obtain the target response\nyi and their positive and negative counterparts (y+\ni , y−\ni ). Subsequently, a variety of automated\n6\n\nalignment strategies are employed to synthesize a substantial volume of artificially annotated data\nacross the domains of code, mathematics, instruction-following, creation, role-playing, and safety.\n4.1.1\nCOLLABORATIVE DATA ANNOTATION\nAutomatic Ontology Extraction\nThe process initiates with the application of InsTag (Lu et al.,\n2024c), an open-set fine-grained tagger, to extract the underlying ontology from a large-scale\ninstruction dataset. Subsequent manual refinement ensures the accuracy of the extracted ontology.\nInstruction Selection\nEach instruction, with tags annotated, is evaluated for tag diversity, semantic\nrichness, complexity, and intent completeness. Based on these criteria, we select a set of representative\ninstructions (Dong et al., 2023).\nInstruction Evolution\nTo enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024)\nis employed, prompting the Qwen models to add constraints or requirements to existing instructions,\nthereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset.\nHuman Annotation\nMultiple responses to an instruction are obtained using diverse generation\nstrategies and Qwen models of different scales. Annotators rank these responses based on their\npreferences, ensuring the best response meets established criteria, yielding both demonstration and\npreference data.\n4.1.2\nAUTOMATED DATA SYNTHESIS\nMaintaining the quality of annotations for responses to instructions presents significant challenges on\na large scale, particularly those that require expertise, experience, carefulness, or patience. To address\nthese challenges, we devised various automated alignment strategies to synthesize data at scale.\nRejection Sampling\nFor mathematical or similar tasks with definitive final answers, rejection\nsampling (Yuan et al., 2023) is applied to improve the quality of solutions. Large language models\n(LLMs) are tasked to generate multiple responses, namely the reasoning paths, for each instruction.\nPaths that result in accurate conclusions and are considered reasonable by the model are preserved,\nserving as demonstration data. Preference data is generated by contrasting correct and incorrect paths.\nExecution Feedback\nFor coding tasks, LLMs are employed to generate solutions and associated\ntest cases. The efficacy of these solutions is evaluated by compiling and executing them against the\ntest cases, thereby creating demonstration and preference data. This methodology is also applicable\nto assessing instruction following (Dong et al., 2024). For each instruction with constraints, e.g.,\nlength limit, the LLM is tasked to generate a Python verification function to ensure the response\naligns with the instruction requirements.\nData Repurposing\nCreating skilled responses in literary writing tasks is challenging for annotators\nwithout specialized training. To tackle this problem, we aggregate high-quality literary works\nfrom the public domain and employ LLMs to develop instructions with varying levels of detail.\nThese instructions, paired with the original works, serve as demonstration data. For example, to\ncompile roleplay data with vivid and engaging responses, we source detailed character profiles from\nknowledge repositories such as Wikipedia and instruct LLMs to generate corresponding instructions\nand responses (Lu et al., 2024b). This process, similar to a reading comprehension task, ensures that\nthe integrity of the character’s profile is maintained.\nConstitutional Feedback\nConstitutional AI refers to the process of guiding LLMs to generate\nresponses based on predefined sets of principles (Bai et al., 2022). To ensure adherence to guidelines\nsuch as safety and values, a constitution dataset was compiled. This dataset delineates principles to\nbe followed and those to be avoided. It was used to instruct LLMs to produce responses that either\nare aligned with or deviated from these guidelines, serving as a reference for demonstration and\npreference data.\n7\n\n4.2\nSUPERVISED FINE-TUNING\nWe have assembled an extensive instruction dataset featuring more than 500,000 examples that\ncover skills such as instruction following, coding, mathematics, logical reasoning, role-playing,\nmultilingualism, and safety. Our model was fine-tuned for two epochs with a sequence length of\n32,768 tokens. To optimize learning, the learning rate was gradually decreased from 7 × 10−6 to\n7 × 10−7. To address overfitting, we applied a weight decay of 0.1 and gradients were clipped at a\nmaximum value of 1.0.\n4.3\nREINFORCEMENT LEARNING FROM HUMAN FEEDBACK\nOur training regime for RLHF comprises two sequential stages: offline and online training. In the\noffline training stage, we use a pre-compiled preference dataset P to maximize the difference in\nlikelihood between y+\ni and y−\ni with Direct Preference Optimization (DPO, Rafailov et al., 2023). In\nthe online training stage, the model iteratively refines its performance in real-time, leveraging reward\nmodels for immediate feedback. Specifically, we sample multiple responses from the current policy\nmodel, and the reward model selects the most and the least preferred responses, forming preference\npairs that are used for DPO in each episode. Moreover, we employ Online Merging Optimizer (Lu\net al., 2024a) to mitigate the alignment tax, i.e., the performance degradation associated with aligning\nmodel generation with human preferences.\n5\nEVALUATION\nTo thoroughly assess the Qwen2 models, consisting of both base and instruction-tuned models,\nwe implement a comprehensive evaluation protocol. This protocol examines a range of compe-\ntencies, including general knowledge understanding, language comprehension, generation, coding,\nmathematics, reasoning, and additional areas of expertise. Specifically, base models are assessed\nusing established benchmark datasets for large language models (LLMs), with responses elicited\nthrough few-shot prompting, unless specified otherwise. For instruction-tuned models, in addition to\nbenchmark evaluations, we prioritize human preference assessments.\n5.1\nBASE LANGUAGE MODELS\nIn this section, we illustrate the evaluation of the base language models of the Qwen2 series. Specifi-\ncally, we evaluate the models on benchmark datasets for knowledge and basic capabilities and apply\nmultilingual benchmark datasets to evaluate their support of languages. As there are multiple model\nsizes, we compare them with the state-of-the-art (SOTA) models of similar or larger sizes.\n5.1.1\nCORE CAPABILITIES\nBenchmarks and Evaluation Protocol\nThe common practice of evaluating the core capabilities\nof base language models is the implementation of benchmark dataset evaluation with few-shot or\nzero-shot prompting. The evaluation mainly focuses on the model performance of natural language\nunderstanding, general question answering, coding, mathematics, scientific knowledge, reasoning, etc.\nThe datasets for evaluation include MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang\net al., 2024) (5-shot), GPQA (Rein et al., 2023) (5shot), Theorem QA (Chen et al., 2023a) (5-shot),\nBBH (Suzgun et al., 2023) (3-shot), HellaSwag (Zellers et al., 2019) (10-shot), Winogrande (Sak-\naguchi et al., 2021) (5-shot), TruthfulQA (Lin et al., 2022a) (0-shot), ARC-C (Clark et al., 2018)\n(25-shot), HumanEval (Chen et al., 2021) (0-shot), MBPP (Austin et al., 2021) (0-shot), EvalPlus(Liu\net al., 2023a) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot on Python, C++, Java, PHP, Type-\nScript, C#, Bash, and JavaScript), GSM8K (Cobbe et al., 2021) (5-shot), MATH (Hendrycks et al.,\n2021b) (4-shot), C-Eval (Huang et al., 2023) (5-shot), and CMMLU (Li et al., 2023) (5-shot). Multi-\nlingual datasets can be grouped into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova\net al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French,\nPortuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar\net al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023)\n(5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c)\n8\n\nTable 2: Performance of the 70B+ models. We compare Qwen2-72B with the baselines, including\nMixtral-8x22B, Llama-3-70B, Qwen1.5-110B, and Qwen1.5-72B. For most datasets, Qwen2-72B\ndemonstrates advantages over the baselines.\nDatasets\nMixtral-8x22B\nLlama-3-70B\nQwen1.5-72B\nQwen1.5-110B\nQwen2-72B\nEnglish\nMMLU\n77.8\n79.5\n77.5\n80.4\n84.2\nMMLU-Pro\n49.5\n52.8\n45.8\n49.4\n55.6\nGPQA\n34.3\n36.3\n36.3\n35.9\n37.9\nTheorem QA\n35.9\n32.3\n29.3\n34.9\n43.1\nBBH\n78.9\n81.0\n65.5\n74.8\n82.4\nHellaSwag\n88.7\n88.0\n86.0\n87.5\n87.6\nWinogrande\n85.0\n85.3\n83.0\n83.5\n85.1\nARC-C\n70.7\n68.8\n65.9\n69.6\n68.9\nTruthfulQA\n51.0\n45.6\n59.6\n49.6\n54.8\nCoding\nHumanEval\n46.3\n48.2\n46.3\n54.3\n64.6\nMBPP\n71.7\n70.4\n66.9\n70.9\n76.9\nEvalPlus\n54.1\n54.8\n52.9\n57.7\n65.4\nMultiPL-E\n46.7\n46.3\n41.8\n52.7\n59.6\nMathematics\nGSM8K\n83.7\n83.0\n79.5\n85.4\n89.5\nMATH\n41.7\n42.5\n34.1\n49.6\n51.1\nChinese\nC-Eval\n54.6\n65.2\n84.1\n89.1\n91.0\nCMMLU\n53.4\n67.2\n83.5\n88.3\n90.1\nMultilingual\nExam\n63.5\n70.0\n66.4\n75.6\n76.6\nUnderstanding\n77.7\n79.9\n78.2\n78.2\n80.7\nMathematics\n62.9\n67.1\n61.7\n64.4\n76.0\nTranslation\n23.3\n38.0\n35.6\n36.2\n37.8\nMathematics: MGSM (Goyal et al., 2022) (8-shot CoT); and (d) Translation: Flores-101 (Goyal et al.,\n2022) (5-shot).\nQwen2-72B\nIn terms of the largest model of Qwen2, we compare Qwen2-72B with competitive\nbaseline open-weight models, including Mixtral-8x22B (Jiang et al., 2024), Llama-3-70B (AI@Meta,\n2024), as well as Qwen1.5-72B (Qwen Team, 2024a) and Qwen1.5-110B (Qwen Team, 2024b).\nThe results are reported in Table 2. Qwen2-72B outperforms Llama-3-70B in general knowledge\nunderstanding on both MMLU and MMLU-Pro, achieving accuracy improvements of 4.7 and 2.8,\nrespectively. In scientific assessments, Qwen2-72B demonstrates superiority over Llama-3-70B with\nenhancements of 1.6 and 9.8 on GPQA and Theorem QA. Upon enrichment of coding data, Qwen2-\n72B exhibits a significant 18.3 and 10.0 percentage point advantage over Qwen1.5-72B in HumanEval\nand MBPP evaluations. Enhanced mathematics-related data allows Qwen2-72B to outperform\nQwen1.5-72B by 10.0 and 17.0 percentage points in the GSM8K and MATH benchmarks. Qwen2-\n72B displays reasoning capabilities equivalent to Llama-3-70B, considering BBH, Winogrande,\nand ARC-C, attributable to its improved coding and mathematical data. In assessing language\nunderstanding in Chinese, Qwen2-72B significantly outperforms Mixtral-8x22B and Llama-3-70B,\nand also outperforms Qwen1.5-72B.\nQwen2-57B-A14B\nFor the evaluation of the MoE model, Qwen2-57B-A14B is compared against\nbaselines of similar sizes. These baselines include other MoE models, such as Mixtral-8x7B (Jiang\net al., 2024) and Jamba (Lieber et al., 2024), and dense models, such as Yi-1.5-34B (Young et al., 2024)\n9\n\nTable 3: Performance of the 30B+ dense models and 40B+ MoE models. Qwen2-57B-A14B, an\nMoE model with a total of 57 billion parameters and 14 billion activated parameters, is designed\nto match the performance of 30 billion parameter dense models. This comparison includes dense\nmodel baselines: Yi-1.5-34B and Qwen1.5-32B, as well as MoE baselines: Mixtral-8x7B and Jamba.\nResults demonstrate that Qwen2-57B-A14B achieves competitive performance overall, with a notable\nsuperiority in coding and mathematics tasks.\nDatasets\nJamba\nMixtral-8x7B\nYi-1.5-34B\nQwen1.5-32B\nQwen2-57B-A14B\nArchitecture\nMoE\nMoE\nDense\nDense\nMoE\n# Act Params\n12B\n12B\n32B\n34B\n14B\n# Params\n52B\n47B\n32B\n34B\n57B\nEnglish\nMMLU\n67.4\n71.8\n77.1\n74.3\n76.5\nMMLU-Pro\n-\n41.0\n48.3\n44.0\n43.0\nGPQA\n-\n29.2\n-\n30.8\n34.3\nTheorem QA\n-\n23.2\n-\n28.8\n33.5\nBBH\n45.4\n50.3\n76.4\n66.8\n67.0\nHellaSwag\n87.1\n86.5\n85.9\n85.0\n85.2\nWinogrande\n82.5\n81.9\n84.9\n81.5\n79.5\nARC-C\n64.4\n66.0\n65.6\n63.6\n64.1\nTruthfulQA\n46.4\n51.1\n53.9\n57.4\n57.7\nCoding\nHumanEval\n29.3\n37.2\n46.3\n43.3\n53.0\nMBPP\n-\n63.9\n65.5\n64.2\n71.9\nEvalPlus\n-\n46.4\n51.9\n50.4\n57.2\nMultiPL-E\n-\n39.0\n39.5\n38.5\n49.8\nMathematics\nGSM8K\n59.9\n62.5\n82.7\n76.8\n80.7\nMATH\n-\n30.8\n41.7\n36.1\n43.0\nChinese\nC-Eval\n-\n-\n-\n83.5\n87.7\nCMMLU\n-\n-\n84.8\n82.3\n88.5\nMultilingual\nExam\n-\n56.1\n58.3\n61.6\n65.5\nUnderstanding\n-\n70.7\n73.9\n76.5\n77.0\nMathematics\n-\n45.0\n49.3\n56.1\n62.3\nTranslation\n-\n29.8\n30.0\n33.5\n34.5\nand Qwen1.5-32B (Qwen Team, 2024a), both of which have approximately 30 billion parameters.\nThe results are shown in Table 3. We anticipate that Qwen2-57B-A14B, which activates 14 billion\nparameters, will match the performance of a 30 billion parameter dense equivalent Qwen2 model. Our\nevaluation reveals that Qwen2-57B-A14B performs comparably to Yi-1.5-34B in natural language\nunderstanding tasks. Moreover, it outperforms the baseline models in coding and mathematics tasks.\nAdditionally, Qwen2-57B-A14B demonstrates robust Chinese language understanding capabilities,\nrivaling the larger Qwen2-72B model. In essence, Qwen2-57B-A14B is an efficient model that, while\nactivating only 14 billion parameters per forward pass, maintains the performance level of a 30 billion\nparameter dense model.\nQwen2-7B\nThe 7B model is widely utilized, as it enables the execution in 16-bit floating points on\naccelerators equipped with 16GB memory. Our focus is on comparing this model with other leading\n7B models, including Llama-3-8B, which has recently demonstrated exceptional performance in the\nChatbot Arena (Chiang et al., 2024). This comparison also includes Mistral-7B-v0.2 (Jiang et al.,\n2023a), Gemma-7B (Mesnard et al., 2024), and our predecessor, Qwen1.5-7B (Qwen Team, 2024a).\n10\n\nTable 4: Performance of the 7B+ models. We compare Qwen2-7B with previously released state-of-\nthe-art 7B+ models including Mixtral-7B, Gemma-7B, Llama-3-8B, and our previous Qwen1.5-7B.\nQwen2-7B demonstrates significant advantages over the baselines in most of the evaluation datasets.\nDatasets\nMistral-7B\nGemma-7B\nLlama-3-8B\nQwen1.5-7B\nQwen2-7B\nEnglish\nMMLU\n64.2\n64.6\n66.6\n61.0\n70.3\nMMLU-Pro\n30.9\n33.7\n35.4\n29.9\n40.0\nGPQA\n24.7\n25.7\n25.8\n26.7\n31.8\nTheorem QA\n19.2\n21.5\n22.1\n14.2\n31.1\nBBH\n56.1\n55.1\n57.7\n40.2\n62.6\nHellaSwag\n83.2\n82.2\n82.1\n78.5\n80.7\nWinogrande\n78.4\n79.0\n77.4\n71.3\n77.0\nARC-C\n60.0\n61.1\n59.3\n54.2\n60.6\nTruthfulQA\n42.2\n44.8\n44.0\n51.1\n54.2\nCoding\nHumanEval\n29.3\n37.2\n33.5\n36.0\n51.2\nMBPP\n51.1\n50.6\n53.9\n51.6\n65.9\nEvalplus\n36.4\n39.6\n40.3\n40.0\n54.2\nMultiPL-E\n29.4\n29.7\n22.6\n28.1\n46.3\nMathematics\nGSM8K\n52.2\n46.4\n56.0\n62.5\n79.9\nMATH\n13.1\n24.3\n20.5\n20.3\n44.2\nChinese\nC-Eval\n47.4\n43.6\n49.5\n74.1\n83.2\nCMMLU\n-\n-\n50.8\n73.1\n83.9\nMultilingual\nExam\n47.1\n42.7\n52.3\n47.7\n59.2\nUnderstanding\n63.3\n58.3\n68.6\n67.6\n72.0\nMathematics\n26.3\n39.1\n36.3\n37.3\n57.5\nTranslation\n23.3\n31.2\n31.9\n28.4\n31.5\nThe results can be found in Table 4. Qwen2-7B demonstrates superior performance across most\ndatasets compared to other models, particularly excelling in coding tasks, mathematics, and Chinese\nlanguage tasks. It also shows strong performance in multilingual understanding and exams. This\nindicates that Qwen2-7B has been optimized for a wide range of language and logic-based tasks,\nshowcasing its versatility and advanced capabilities.\nQwen2-1.5B & Qwen2-0.5B\nTo evaluate the performance of our smaller models, specifically\nQwen2-1.5B and Qwen2-0.5B, we compare them against established baselines: Phi-2 (Abdin et al.,\n2024), Gemma-2B (Mesnard et al., 2024), and Qwen1.5-1.8B (Qwen Team, 2024a). The results\nare given in Table 5. In language understanding, Qwen2-1.5B outperforms Phi-2, a model trained\non textbook-like data. For coding tasks, Qwen2-0.5B matches the performance of Gemma-2B and\nQwen1.5-1.8B, while Qwen2-1.5B surpasses these baselines, except for Phi-2. Both Qwen2 models\nexhibit superior performance in mathematics compared to their competitors. In terms of general\nreasoning, we find that Phi-2 generally outperforms all others, which to some extent reflects the\nsignificance of textbook data for reasoning capabilities. In TruthfulQA, Qwen2-1.5B performs the\nbest, demonstrating that smaller models does not necessarily suffer from hallucination. In Chinese\nlanguage understanding, both Qwen2 models outperform all the others, a trend consistent with larger\nmodels in their respective comparisons.\nIn general, the Qwen2 series demonstrates superior performance against the baselines across different\nmodel sizes. Notably, Qwen2-72B exhibits the highest performance among all Qwen2 models,\nunderscoring the efficacy of model size scaling.\n11\n\nTable 5: Performance of the smaller models. We compare our Qwen2-0.5B and Qwen2-1.5B\nwith the previous SOTA small models including Phi-2, Gemma-2B and Qwen1.5-1.8B. Qwen2-0.5B\nwith a much smaller model size achieves competitive performance, and Qwen2-1.5B significantly\noutperforms Qwen2-0.5B.\nDatasets\nPhi-2\nGemma-2B\nQwen1.5-1.8B\nQwen2-0.5B\nQwen2-1.5B\n# Non-Emb Params\n2.5B\n2.0B\n1.2B\n0.3B\n1.2B\nMMLU\n52.7\n42.3\n46.8\n45.4\n56.5\nMMLU-Pro\n-\n15.9\n-\n14.7\n21.8\nTheorem QA\n-\n-\n-\n8.9\n15.0\nBBH\n43.4\n35.2\n24.2\n28.4\n37.2\nHellaSwag\n73.1\n71.4\n61.4\n49.3\n66.6\nWinogrande\n74.4\n66.8\n60.3\n56.8\n66.2\nARC-C\n61.1\n48.5\n37.9\n31.5\n43.9\nTruthfulQA\n44.5\n33.1\n39.4\n39.7\n45.9\nHumanEval\n47.6\n22.0\n20.1\n22.0\n31.1\nMBPP\n55.0\n29.2\n18.0\n22.0\n37.4\nGSM8K\n57.2\n17.7\n38.4\n36.5\n58.5\nMATH\n3.5\n11.8\n10.1\n10.7\n21.7\nC-Eval\n23.4\n28.0\n59.7\n58.2\n70.6\nCMMLU\n24.2\n-\n57.8\n55.1\n70.3\n5.2\nINSTRUCTION-TUNED MODEL\nTo critically evaluate instruction-tuned models, we implement a multifaceted approach. Assessments\nof foundational skills and human preferences are conducted using open datasets and benchmarks. Our\ndetailed in-house examinations further probe model competencies in key areas. A particular focus is\nplaced on assessing long context capability. Safety measures include multilingual safety assessments\nand red teaming exercises. The following sections detail the evaluation methods and their outcomes.\n5.2.1\nOPEN BENCHMARK EVALUATION\nTo comprehensively evaluate the quality of instruction-tuned models, we compile automatic and\nhuman evaluation to assess the capabilities and human preference. For the evaluation of basic\ncapabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural\nlanguage understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU,\nMMLU-Pro, GPQA, and Theorem QA for language understanding and knowledge, HumanEval,\nMBPP, MultiPL-E, and LiveCodeBench v1 (Jain et al., 2024) for coding, GSM8K and MATH for\nmathematics. Additionally, we assess the performance of human preference alignment and instruction\nfollowing by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li\net al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate\nthose of Chatbot Arena, and IFEval (Zhou et al., 2023)4 for instruction following.\nQwen2-72B-Instruct\nWe compare Qwen2-72B-Instruct against the instruction-tuned models in-\ncluding Mixtral-8x22B-Instruct, Llama-3-70B-Instruct, as well as Qwen1.5-72B-Chat. The results are\npresented in Table 6. It can be found that a strong base language model can help boost the downstream\nperformance of the instruction-tuned model. Specifically, Qwen2-72B-Instruct outshines its peers in\nareas such as language understanding, coding, and mathematics, with the exception of GPQA and\nMBPP. Regarding human preference alignment and instruction following, Qwen2-72B has significant\nadvantages over the baselines. We assume this achievement is attributed to both the high-quality\npre-trained model and improvements in both data and training techniques for post-training.\n4For simplicity, we report the results of the subset strict-prompt.\n12\n\nTable 6: Performance of 70B+ instruction-tuned models. We compare Qwen2-72B-Instruct with\nMixtral-8x22B-Instruct, Llama-3-70B-Instruct, Qwen1.5-72B-Chat, and Qwen1.5-110B-Chat. “-\nInstruct” or “-Chat” is omitted in the table. Qwen2-72B-Instruct demonstrates advantages in core\ncapabilities, and superior performance in human preference alignment.\nDatasets\nMixtral-8x22B Llama-3-70B Qwen1.5-72B Qwen1.5-110B Qwen2-72B\nEnglish\nMMLU\n74.0\n82.0\n75.6\n76.5\n82.3\nMMLU-Pro\n56.1\n56.2\n51.7\n50.5\n64.4\nGPQA\n49.7\n41.9\n39.4\n32.8\n42.4\nTheorem QA\n40.8\n42.5\n28.8\n18.8\n44.4\nCoding\nHumanEval\n73.8\n81.7\n71.3\n74.4\n86.0\nMBPP\n75.9\n82.3\n71.9\n76.4\n80.2\nMultiPL-E\n61.1\n63.4\n48.1\n55.4\n69.2\nLiveCodeBench v1\n21.8\n29.3\n17.9\n25.3\n35.7\nMathematics\nGSM8K\n89.1\n93.0\n82.7\n84.5\n93.2\nMATH\n47.4\n50.4\n42.5\n42.0\n69.0\nAlignment\nMT-Bench\n8.66\n8.95\n8.61\n8.88\n9.12\nMixEval\n82.3\n84.0\n84.1\n85.7\n86.7\nArena-Hard\n36.4\n41.1\n36.1\n39.8\n48.1\nIFEval strict-prompt\n67.1\n77.3\n55.8\n57.5\n77.6\nAlignBench\n-\n7.42\n7.28\n7.87\n8.27\nQwen2-57B-A14B-Instruct\nFor medium-size models, we compare Qwen2-57B-A14B-Instruct\nwith Mixtral-8x7B-Instruct, another MoE baseline, as well as the dense SOTA models with over 30\nbillion parameters, e.g., Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. The results are provided in Table 7.\nCompared with Qwen1.5-32B-Chat, Qwen2-57B-A14B-Instruct reaches superior performance in\nalmost all benchmarks, and compared with the 30B SOTA model Yi-1.5-34B-Chat, Qwen2-57B-\nA14B-Instruct has gained advantages in most evaluations except for those for mathematics. In terms\nof the evaluation for alignment, the advantages of Qwen2-57B-A14B-Instruct are notably evident.\nQwen2-7B-Instruct\nWithin the spectrum of 7B to 9B models, we compare Qwen2-7B-Instruct\nwith Llama-3-8B-Instruct, Yi-1.5-9B-Chat, GLM-4-9B-Chat, and Qwen1.5-7B-Chat. The results\ncan be found in Table 8. Qwen2-7B-Instruct demonstrates substantial advancements compared\nto its predecessor, Qwen1.5-7B-Chat, across comprehensive evaluations, notably achieving higher\nscores in coding and mathematics-related tasks. Compared with the recent SOTA model, Llama-3-\n8B-Instruct, Qwen2-7B-Instruct demonstrates competitive performance and specifically it achieves\nsuperior performance in coding. Nonetheless, in terms of instruction following, Qwen2-7B-Instruct\ngreatly falls behind the competitor. To address this limitation, we plan to augment the 7B model’s\ninstruction-following ability by enhancing the quality of post-training data, ensuring a more robust\nunderstanding and execution of complex commands.\nQwen2-1.5B-Instruct & Qwen2-0.5B-Instruct\nIn the context of smaller models, we compare\nQwen2-0.5B-Instruct with Qwen1.5-0.5B-Chat, and Qwen2-1.5B-Instruct with Qwen1.5-1.8B-Chat.\nNotably, the complexity of certain datasets designed for larger models exceeds the capabilities of\nthese smaller models; thus, our analysis focuses on a selected subset. As detailed in Table 9, the\nQwen2 models demonstrate a marked advantage over their predecessors in both core capabilities and\ninstruction-following tasks. The achievement mainly attributes to the scaling of pre-training data.\nConsequently, our results affirm that data scaling remains an effective strategy for enhancing model\nperformance, even in the domain of sub-billion parameter models.\n13\n\nTable 7: Performance of 30B+ dense and 40B+ MoE instruction-tuned models. We compare\nQwen2-57B-A14B-Instruct with the similar-size MoE model Mixtral-8x7B-Instruct, 30B dense\nmodels such as Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. “-Instruct” or “-Chat” is omitted in the\ntable. Qwen2-57B-A14B-Instruct is competitive with the recent SOTA 30B dense models, and\nsignificantly outcompetes the MoE baseline.\nDatasets\nMixtral-8x7B\nYi-1.5-34B\nQwen1.5-32B\nQwen2-57B-A14B\nArchitecture\nMoE\nDense\nDense\nMoE\n# Act Params\n12B\n32B\n34B\n14B\n# Params\n47B\n32B\n34B\n57B\nEnglish\nMMLU\n71.4\n76.8\n74.8\n75.4\nMMLU-Pro\n43.3\n52.3\n46.4\n52.8\nGPQA\n-\n-\n30.8\n34.3\nTheorem QA\n-\n-\n30.9\n33.1\nCoding\nHumanEval\n45.1\n75.2\n68.3\n79.9\nMBPP\n59.5\n74.6\n67.9\n70.9\nMultiPL-E\n-\n-\n50.7\n66.4\nLiveCodeBench v1\n12.3\n-\n15.2\n25.5\nMathematics\nGSM8K\n65.7\n90.2\n83.6\n85.3\nMATH\n30.7\n50.1\n42.4\n49.1\nAlignment\nMT-Bench\n8.30\n8.50\n8.30\n8.55\nMixEval\n70.0\n81.7\n81.0\n82.3\nIFEval strict-prompt\n-\n-\n50.3\n59.9\nAlignBench\n5.70\n7.20\n7.19\n7.36\n5.2.2\nIN-HOUSE AUTOMATIC EVALUATION\nDespite a number of open benchmark datasets for the evaluation, we believe that it is far from\nsufficient to fully comprehend the capabilities of LLMs. Specifically, we have made a series of\nin-house datasets that assess different capabilities of the models, e.g., knowledge understanding,\ntext generation, coding, etc. The evaluation is in Chinese and English. The results are gathered in\nTable 10 and Table 11, respectively.\nChinese Evaluation\nFor the evaluations in Chinese, we focus on comparing the performance of\nQwen2 models with the Qwen1.5 counterparts. For the small models, Qwen2-1.5B-Instruct generally\noutperforms Qwen1.5-1.8B-Chat in almost all the evaluations even with fewer parameters. In terms of\nthe comparison of 7B models, the advantages of Qwen2 are more significant. Noteworthy is Qwen2-\n72B’s superior performance to Qwen1.5-110B-Chat, despite the latter’s greatly more parameters.\nThe MoE model displays superior performance across most domains relative to Qwen1.5-32B-Chat,\nexcluding knowledge understanding. This discrepancy may be attributed to a short of pre-training\ntokens. In the near future, we are about to continue the pre-training of the MoE model to discover its\nscaling behaviors.\nEnglish Evaluation\nFor English, we compare Qwen2 with both Qwen1.5 and Llama-3. Similarly,\nthe small models of Qwen2 significantly outcompete the Qwen1.5 counterparts. However, in com-\nparison with Llama-3-70B, Qwen2-72B-Instruct is falling behind by small margins especially in\ncomprehension and coding. We assume both the amount of English tokens for pre-training and the\nquantity and diversity of data for post-training lead to the performance gap in English.\n14\n\nTable 8: Performance of 7B+ instruction-tuned models. We compare Qwen2-7B-Instruct with the\nrecent SOTA models with 7-9 billion parameters, including Llama-3-8B-Instruct, Yi-1.5-9B-Chat,\nGLM-4-9B-Chat, and Qwen1.5-7B-Chat. “-Instruct” or “-Chat” is omitted in the table. Qwen2-7B-\nInstruct demonstrates competitive performance against Llama-3-8B-Instruct.\nDatasets\nLlama-3-8B\nYi-1.5-9B\nGLM-4-9B\nQwen1.5-7B\nQwen2-7B\nEnglish\nMMLU\n68.4\n69.5\n72.4\n59.5\n70.5\nMMLU-Pro\n41.0\n-\n-\n29.1\n44.1\nGPQA\n34.2\n-\n-\n27.8\n34.3\nTheorem QA\n23.0\n-\n-\n14.1\n25.3\nCoding\nHumanEval\n62.2\n66.5\n71.8\n46.3\n79.9\nMBPP\n67.9\n-\n-\n48.9\n67.2\nMultiPL-E\n48.5\n-\n-\n27.2\n59.1\nLiveCodeBench v1\n17.3\n-\n-\n6.0\n26.6\nMathematics\nGSM8K\n79.6\n84.8\n79.6\n60.3\n85.7\nMATH\n30.0\n47.7\n50.6\n23.2\n52.9\nAlignment\nMT-Bench\n8.05\n8.20\n8.35\n7.60\n8.41\nMixEval\n75.0\n74.2\n-\n71.4\n76.5\nIFEval strict-prompt\n72.1\n-\n69.0\n38.3\n54.7\nAlignBench\n6.20\n6.90\n7.01\n6.20\n7.21\nTable 9: Performance of smaller instruction-tuned models. We compare both Qwen2-0.5B-Instruct\nand Qwen2-1.5B-Instruct with Qwen1.5-0.5B-Chat and Qwen2-1.8B-Chat. “-Instruct” or “-Chat”\nis omitted in the table. Compared with the similar-size baselines, Qwen2 significant surpasses the\nperformance of Qwen1.5.\nDatasets\nQwen1.5-0.5B\nQwen2-0.5B\nQwen1.5-1.8B\nQwen2-1.5B\nMMLU\n35.0\n37.9\n43.7\n52.4\nHumanEval\n10.4\n29.9\n27.4\n47.0\nMBPP\n14.5\n37.8\n28.6\n51.9\nGSM8K\n11.3\n40.1\n35.3\n61.6\nIFEval strict-prompt\n14.6\n20.0\n16.8\n29.0\n5.2.3\nLONG CONTEXT CAPABILITIES\nThree methods to evaluate long context capabilities are employed: the Needle in a Haystack (NIAH,\nKamradt, 2023), NeedleBench (OpenCompass Contributors, 2023), and LV-Eval (Yuan et al., 2024).\nNeedle in a Haystack\nThis experiment assesses a model’s proficiency in pinpointing facts within\nvoluminous texts. Texts with 8K, 16K, ..., 128K tokens in length were crafted, with facts strategically\npositioned at varying depths. Each depth interval, e.g., from 0% to 10%, encompassed two instances.\nFor contexts over 32K, YARN (Peng et al., 2023) was applied in this evaluation. As illustrated in\nFigure 1, Qwen2-72B-Instruct exhibits exceptional accuracy in retrieving information from the entire\n128K context. Coupled with its inherent strength, this model emerges as the optimal choice for\nprocessing extensive texts, assuming sufficient resources are accessible. Additionally, models within\nthe same series showcases remarkable performance across different context lengths. Precisely, Qwen2-\n7B-Instruct achieves a high level of accuracy in handling contexts up to 128K tokens. Meanwhile,\nQwen2-57B-A14B-Instruct manages contexts up to 64K tokens proficiently, and the two smaller\nmodels in the Qwen2 series could support contexts of 32K tokens.\n15\n\nTable 10: Performances of Qwen2-Instruct models on our in-house Chinese automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 counterparts are\nin bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\nModels\nKnowledge\nExam\nComprehension\nCoding\nMath\nReasoning\nAvg.\nProprietary LLMs\nGPT-4o-2024-05-13\n66.68\n69.04\n76.85\n59.58\n71.16\n69.94\n68.87\nQwen-Max-0428\n76.65\n74.80\n73.66\n49.48\n66.01\n70.84\n68.57\nQwen1.5 Series\nQwen1.5-0.5B-Chat\n28.55\n36.99\n29.70\n3.82\n13.10\n25.47\n22.94\nQwen1.5-1.8B-Chat\n30.31\n44.98\n44.81\n6.86\n29.85\n34.61\n31.90\nQwen1.5-4B-Chat\n33.67\n47.17\n50.44\n14.05\n36.20\n39.98\n36.92\nQwen1.5-MoE-A2.7B-Chat\n52.76\n60.49\n52.84\n19.34\n38.45\n43.07\n44.49\nQwen1.5-7B-Chat\n56.77\n59.36\n55.50\n18.85\n46.41\n48.77\n47.61\nQwen1.5-14B-Chat\n63.35\n66.13\n60.06\n28.19\n54.80\n50.20\n53.79\nQwen1.5-32B-Chat\n68.63\n67.59\n64.67\n35.28\n60.62\n62.87\n59.94\nQwen1.5-72B-Chat\n71.52\n70.04\n66.70\n38.22\n63.09\n61.30\n61.81\nQwen1.5-110B-Chat\n76.26\n74.00\n71.25\n44.25\n64.92\n64.47\n65.86\nQwen2 Series\nQwen2-0.5B-Instruct\n28.18\n38.09\n35.90\n9.40\n21.20\n25.61\n26.40\nQwen2-1.5B-Instruct\n35.46\n51.93\n44.70\n14.05\n34.58\n35.94\n36.11\nQwen2-7B-Instruct\n61.54\n66.66\n59.63\n34.74\n60.99\n58.22\n56.96\nQwen2-57B-A14B-Instruct\n64.15\n73.67\n67.52\n40.66\n63.90\n59.89\n61.63\nQwen2-72B-Instruct\n76.19\n75.65\n74.72\n49.53\n70.80\n70.59\n69.58\nTable 11: Performances of Qwen2-Instruct models on our in-house English automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 and Llama-3\ncounterparts are in bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\nModels\nKnowledge\nComprehension\nCoding\nMath\nAvg.\nProprietary LLMs\nGPT-4o-2024-05-13\n87.29\n76.30\n55.87\n84.99\n76.11\nQwen-Max-0428\n80.73\n71.63\n48.76\n79.12\n70.06\nQwen1.5 Series\nQwen1.5-0.5B-Chat\n30.12\n25.44\n1.78\n15.48\n18.21\nQwen1.5-1.8B-Chat\n40.37\n41.87\n4.99\n29.71\n29.23\nQwen1.5-4B-Chat\n51.44\n50.16\n15.45\n44.83\n40.47\nQwen1.5-MoE-A2.7B-Chat\n61.64\n54.79\n21.28\n50.46\n47.04\nQwen1.5-7B-Chat\n64.86\n58.61\n20.79\n54.24\n49.62\nQwen1.5-14B-Chat\n74.41\n59.80\n28.18\n66.91\n57.32\nQwen1.5-32B-Chat\n76.38\n64.70\n37.39\n73.04\n62.88\nQwen1.5-72B-Chat\n77.59\n67.58\n37.30\n73.76\n64.06\nQwen1.5-110B-Chat\n78.29\n70.17\n44.12\n78.87\n67.86\nLlama-3 Series\nLlama-3-8B-Instruct\n71.01\n64.71\n42.56\n65.82\n61.03\nLlama-3-70B-Instruct\n83.06\n76.31\n57.18\n79.70\n74.06\nQwen2 Series\nQwen2-0.5B-Instruct\n43.19\n29.57\n6.95\n31.52\n27.81\nQwen2-1.5B-Instruct\n56.03\n45.08\n17.61\n50.44\n42.29\nQwen2-7B-Instruct\n73.75\n63.09\n36.41\n75.67\n62.23\nQwen2-57B-A14B-Instruct\n76.80\n67.92\n42.37\n77.04\n66.03\nQwen2-72B-Instruct\n83.00\n73.58\n53.03\n82.15\n72.94\n16\n\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nTesting Qwen2-Instruct via “Needle in A HayStack”\nRetrieve Facts from Given Documents across Context Lengths and Document Depth\nQwen2-72B-Instruct\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nQwen2-7B-Instruct\nQwen2-7B-Instruct\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\n100%\nAccuracy of \nRetrieval\n50%\nAccuracy of \nRetrieval\n0%\nAccuracy of \nRetrieval\nQwen2-57B-A14B-Instruct\nQwen2-1.5B-Instruct\nQwen2-0.5B-Instruct\nFigure 1: Performance of Qwen2 instruction-tuned models on Needle in A Haystack Test. All\nmodels that supports context lengths above 32k tokens integrates the YARN mechanism.\nTable 12: Performance of Qwen2-72B-Instruct and Qwen2-7B-Instruct on NeedleBench and\nLV-Eval. +YARN+DCA does not change the model behavior within 32k tokens.\nDatasets\nNeedleBench\nLV-Eval\n8k\n32k\n128k\n256k\n16k\n32k\n64k\n128k\n256k\nChatGLM4-9B-1M\n56.61\n49.15\n44.30\n45.29\n46.40\n43.23\n42.92\n40.41\n36.95\nQwen2-7B-Instruct\n87.07\n73.64\n38.77\n2.92\n49.77\n46.93\n28.03\n11.01\n0.55\n+ YARN + DCA\n66.32\n60.71\n42.14\n36.64\n34.72\nQwen2-72B-Instruct\n91.90\n92.01\n73.05\n17.13\n58.82\n56.70\n42.92\n31.79\n2.88\n+ YARN + DCA\n90.27\n85.21\n53.03\n48.83\n42.35\n17\n\nTable 13: Performance of Qwen2-72B-Instruct and proprietary LLMs in multilingual human\nevaluation. We compare Qwen2-72B-Instruct with GPT-3.5-Turbo-1106, GPT-4-Turbo-0409, GPT-\n4o-0513, Claude-3-Opus-0229. Scores range from 1 to 5. Overall, Qwen2-72B-Instruct performs\nsubstantially better than GPT-3.5-Turbo but there is progress to be made to be competitive with the\nproprietary models released in the last 6 months.\nLanguage\nGPT-3.5-Turbo GPT-4-Turbo\nGPT-4o\nClaude-3-Opus\nQwen2-72B-Instruct\nArabic\n2.52\n3.44\n3.55\n4.15\n3.86\nFrench\n3.47\n4.19\n4.16\n4.23\n4.01\nIndonesian\n3.56\n4.09\n4.39\n4.40\n3.83\nJapanese\n2.75\n3.68\n3.72\n3.85\n3.63\nKorean\n2.37\n4.24\n4.40\n4.23\n4.14\nPortuguese\n3.37\n3.86\n3.89\n4.09\n3.97\nRussian\n3.24\n4.27\n4.32\n4.25\n4.15\nSpanish\n4.07\n4.08\n4.26\n4.31\n4.10\nThai\n3.38\n4.11\n4.09\n4.01\n3.75\nVietnamese\n3.90\n3.84\n4.14\n3.98\n3.91\nAverage\n3.16\n3.98\n4.09\n4.15\n3.93\nNeedleBench\nNeedleBench ups the challenge on NIAH by including multiple facts (two to five) in\npassages, necessitating simultaneous identification and multi-hop reasoning. Table 12 reveals that\nthe integration of YARN and DCA (An et al., 2024) notably improves Qwen2 models’ long-context\nabilities. Qwen2-7B-Instruct surpasses ChatGLM4-9B-1M (Zeng et al., 2024), which claims a 1M\ncontext length. Moreover, Qwen2-72B-Instruct demonstrates strong performance, with an accuracy\nreduction of just 6 points, compared to ChatGLM4-9B-1M, which shows a more pronounced decline\nof 11 points, particularly given its lower initial accuracy.\nLV-Eval\nLV-Eval comprises 11 diverse QA datasets that demand comprehension of multiple pieces\nof evidence at once. To rectify the shortcomings of its original metric, which was excessively stringent\nand led to a high rate of false negatives, we adopt the keyword recall as the reported score. As shown\nin Table 12, integrating YARN and DCA substantially bolsters the long-context competencies of\nQwen2 models on LV-Eval. Qwen2-7B-Instruct achieves parity with ChatGLM4-9B-1M, albeit with\na more noticeable decline at extended contexts. Moreover, Qwen2-72B-Instruct demonstrates strong\nperformance across all lengths, confirming its proficiency in handling long-context tasks.\n5.2.4\nMULTILINGUAL EVALUATION\nFor the multilingual evaluation, we implement a comprehensive human evaluation for the assessment\nof multilingual capabilities. Specifically, we design diverse test cases assessing different capabilities\nof large language models, and we have test cases that are in a number of languages. For the annotators,\nwe invite one professional annotator for each language who majors in the language for the evaluation.\nFor each test case, the annotator grades the response from model with a score from 1 to 5.\nWe report the results of our model and the baselines in the evaluation of different languages. From\nTable 13, it can be found that on average Qwen2-72B-Instruct significantly outperforms GPT-3.5-\nTurbo and it is competitive with GPT-4-Turbo and slightly falls behind Claude-3-Opus. This shows\nthat our multilingual pre-training and instruction tuning data contribute to the multilingual capabilities\nof Qwen2-72B-Instruct and it is competitive with most state-of-the-art proprietary LLMs.\n5.2.5\nSAFETY & RESPONSIBILITY\nLLMs with openly accessible weights effectively accelerate the development of the research as well\nas their applications. Moreover, we believe that it is crucial to build safe and responsible LLMs so\nthat the effect of the misuse of AI technologies could be significantly alleviated.\nWe implement a multilingual safety evaluation that tests the LLMs in different languages. Specif-\nically, we assess the safety performance of the models in the topics about illegal behaviors, fraud,\n18\n\nTable 14: Performance of models in safety evaluation. We compare Qwen2-72B-Instruct with\nGPT-4 and Mixtral-8x22B-Instruct. The lower, the better. Qwen2-72B-Instruct rejected more prompts\nwith risks than the competitors.\nRisk Category\nGPT-4\nMixtral-8x22B\nQwen2-72B-Instruct\nIllegal\n0.00\n6.87\n0.00\nFraud\n3.40\n8.49\n2.41\nPornography\n23.63\n33.82\n22.91\nPrivacy\n3.37\n15.03\n2.47\nTable 15: Contamination Analysis. The contaminated samples in this table are identified using a\nstrict criterion: any test sample with a 13-gram overlap with the pre-training or post-training data is\nconsidered contaminated. We report the percentage of contaminated samples as well as the model\nperformance on both the original and non-contaminated test sets.\nTest set\nPercent of\nQwen2-72B-Instruct\nQwen2-7B-Instruct\nContamination Original\nNon-Contam.\n∆\nOriginal\nNon-Contam.\n∆\nMMLU\n11.2%\n82.3\n83.2\n0.9\n70.5\n71.3\n0.8\nMMLU-Pro\n11.6%\n64.4\n65.6\n1.2\n44.1\n46.5\n2.4\nGPQA\n1.0%\n42.4\n41.8\n0.6\n34.3\n34.1\n-0.2\nHumanEval\n75.0%\n86.0\n87.0\n1.0\n79.9\n87.8\n7.9\nMBPP\n29.6%\n80.2\n79.7\n0.5\n67.2\n69.0\n1.8\nMultiPL-E\n37.7%\n69.2\n69.2\n0.0\n59.1\n58.9\n-0.2\nGSM8k\n0.7%\n93.2\n92.8\n-0.4\n85.7\n85.6\n-0.1\nMath\n31.7%\n69.0\n74.6\n5.6\n52.9\n57.6\n4.7\nIFEval\n0.9%\n77.6\n77.4\n-0.2\n54.7\n53.7\n-1.0\npornography, and privacy. We have collected prompts prone to jail-breaking and use them to test\nwhether the models can provide safe responses by rejection.\nThe results are presented in Table 14, where the proportion of harmful responses generated by the\nmodels are shown and the lower, the better. It can be observed that Qwen2-72B-Instruct performs\nbetter than the proprietary model, GPT-4, and significantly outperforms the open-weight model,\nMixtral-8x22B-Instruct. However, we believe that there is still much room for our model to improve to\nbe a safer and more responsible model, especially in terms of pornography, which is a conventionally\ndifficult category to differentiate even for humans.\n5.2.6\nCONTAMINATION ANALYSIS\nFor large language models, what counts as contamination and how to run contamination analysis\nremain an active area of research (Ravaut et al., 2024; Golchin & Surdeanu, 2024; Sainz et al., 2023).\nIn the following, we first introduce how we try to decontaminate the training corpora against the\nevaluation datasets, and then estimate the extent to which benchmark scores are influenced by the\nremaining contamination.\nDuring the construction of the pre-training and post-training datasets, we exclude potentially contam-\ninated data using n-gram matching. However, we found that this approach may lead to a high false\nnegative rate, because there could be commonly used expressions, especially in mathematical and\ncoding data. Therefore, we also applied another constraint based on the longest common subsequence\n(LCS). Specifically, we first remove all symbols and punctuation from both the test and training\nsequences and perform tokenization. For a training sequence st, we remove it if there is a test\nsequence se such that |LCS(st, se)| ≥13 and |LCS(st, se)| ≥0.6 × min(|st|, |se|).\nTo assess the potential effects of leaking data on the test performance, we follow OpenAI (2023) to\nconstruct a strict non-contaminated test set to check if there is a significant performance degradation\nafter strict decontamination. Specifically, we construct the non-contaminated test set by excluding any\n19\n\nsample which has 13-gram overlap with the pre-training or the post-training data (without constraint\non LCS), and then compute the corresponding metric on the test set.\nThe results are presented in Table 15. Although some datasets exhibit a high percentage of contam-\nination under the strict criterion, we noticed that most of the identified contaminated samples are\nfalse positives, primarily stemming from the mathematics and coding datasets. It is likely that certain\ncode snippets and mathematical equations are so common that they do not provide any meaningful\nadvantage in solving the test data. Furthermore, our analysis shows that the performance of the\nQwen2 models remains consistent between the original and non-contaminated test data, suggesting\nthat the potential issue of data contamination does not significantly impact the model’s performance.\n6\nCONCLUSION\nThis technical report has presented the Qwen2 series, a versatile suite of foundational and instruction-\ntuned language models, ranging from 0.5 to 72 billion parameters, including models of dense and\nMixture-of-Experts architecture. Qwen2 outperforms previous open-weight models, notably its\npredecessor Qwen1.5, and displays competitive performance against proprietary models across a\nbroad spectrum of benchmarks in language understanding, generation, multilingual capabilities,\ncoding, mathematics, and reasoning. In this update, we have extra focus on long-context, multi-\nlingual, coding, mathematics capabilities and safety and responsibility. In a commitment to fostering\ninnovation and accessibility within the community, we have made the Qwen2 model weights openly\naccessible, which enables researchers and developers to harness the full potential of Qwen2 in a\nvariety of applications and research projects. Through these efforts, we aim to contribute to the\nadvancement of AI technologies and their positive impact on society.\n20\n\nREFERENCES\nMarah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C´esar Teodoro Mendes, Weizhu Chen, Allie Del\nGiorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann,\nYin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nMichael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp\nWitte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models,\n2024. URL https://www.microsoft.com/en-us/research/blog/phi-2-the-\nsurprising-power-of-small-language-models/.\nAI@Meta. Llama 3 model card, 2024. URL https://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr´on, and Sumit\nSanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints.\nIn EMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nAnthropic.\nThe\nClaude\n3\nmodel\nfamily:\nOpus,\nSonnet,\nHaiku.\nTechnical\nre-\nport,\nAnthropic,\nAI,\n2024.\nURL\nhttps://www-cdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with\nlarge language models. CoRR, abs/2108.07732, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren\nZhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023a.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities.\nCoRR, abs/2308.12966, 2023b.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem´ı Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas\nJoseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from\nAI feedback. CoRR, abs/2212.08073, 2022.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele bench-\nmark: A parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884,\n2023.\nBoxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben\nHe, Xianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of\nLLMs: A survey. CoRR, abs/2406.01252, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.\n21\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and\nTony Xia. TheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889–7901.\nAssociation for Computational Linguistics, 2023a.\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen,\nJunying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang.\nMultilingual-\nSIFT: Multilingual supervised instruction fine-tuning, 2023b. URL https://github.com/\nFreedomIntelligence/MultilingualSIFT.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng\nLi, Hao Zhang, Banghua Zhu, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\narena: An open platform for evaluating LLMs by human preference. CoRR, abs/2403.04132, 2024.\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\nJingren Zhou. Qwen-Audio: Advancing universal audio understanding via unified large-scale\naudio-language models. CoRR, abs/2311.07919, 2023.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,\nand Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. CoRR, abs/2401.06066, 2024.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp.\n933–941. PMLR, 2017.\nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,\nZheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected\nby supervised fine-tuning data composition. CoRR, abs/2310.05492, 2023.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou.\nSelf-play with execution feedback: Improving instruction-following capabilities of large language\nmodels. CoRR, abs/2406.13542, 2024.\nAlena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Al-\nbina Akhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana\nIsaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin,\nPolina Mikhailova, Denis Dimitrov, Alexander Panchenko, and Sergey Markov. MERA: A com-\nprehensive LLM evaluation in russian. CoRR, abs/2401.04531, 2024.\nShahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large\nlanguage models. In ICLR. OpenReview.net, 2024.\n22\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana\nKrishnan, Marc’Aurelio Ranzato, Francisco Guzm´an, and Angela Fan. The Flores-101 evalua-\ntion benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput.\nLinguistics, 10:522–538, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net,\n2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In\nNeurIPS Datasets and Benchmarks, 2021b.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free\nevaluation of large language models for code. CoRR, abs/2403.07974, 2024.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nL´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas\nWang, Timoth´ee Lacroix, and William El Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Th´eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed.\nMixtral of experts. CoRR, abs/2401.04088, 2024.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\nTransformers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nGregory Kamradt. Needle in a haystack - pressure testing LLMs, 2023. URL https://github.\ncom/gkamradt/LLMTest_NeedleInAHaystack.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. In ICLR. OpenReview.net, 2023.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass\nprimary school exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, pp.\n12359–12374. Association for Computational Linguistics, 2023.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: Measuring massive multitask language understanding in Chinese. CoRR,\nabs/2306.09212, 2023.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon-\nzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and\nBenchBuilder pipeline. CoRR, abs/2406.11939, 2024.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid Transformer-Mamba\nlanguage model. CoRR, abs/2403.19887, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL (1), pp. 3214–3252. Association for Computational Linguistics, 2022a.\n23\n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,\nNaman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,\nVishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T.\nDiab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language\nmodels. In EMNLP, pp. 9019–9052. Association for Computational Linguistics, 2022b.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nChatGPT really correct? Rigorous evaluation of large language models for code generation. In\nNeurIPS, 2023a.\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke,\nYifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie\nHuang, Yuxiao Dong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large\nlanguage models. CoRR, abs/2311.18743, 2023b.\nKeming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers\nfor boosting rewards and mitigating tax in alignment. CoRR, abs/2405.17931, 2024a.\nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions\nof all characters: Attaining arbitrary role-play via self-alignment. CoRR, abs/2401.12474, 2024b.\nKeming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language\nmodels. In ICLR. OpenReview.net, 2024c.\nThomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L´eonard Hussenot, Pier Giuseppe\nSessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros,\nAmbrose Slone, Am´elie H´eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai,\nBobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl´ement Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George\nTucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney,\nIvan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine\nLee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej\nMikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar\nChang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De,\nTed Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed,\nZhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl´ement Farabet, Oriol Vinyals, Jeff\nDean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral,\nFernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and\nKathleen Kenealy. Gemma: Open models based on Gemini research and technology. CoRR,\nabs/2403.08295, 2024.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning. In ACL\n(1), pp. 15991–16111. Association for Computational Linguistics, 2023.\nJinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and\nYang You. MixEval: Deriving wisdom of the crowd from LLM benchmark mixtures. CoRR,\nabs/2406.06565, 2024.\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/index/chatgpt/.\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\n24\n\nOpenCompass Contributors. OpenCompass: A universal evaluation platform for foundation models,\n2023. URL https://github.com/open-compass/opencompass.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In EMNLP (1), pp. 2362–2376.\nAssociation for Computational Linguistics, 2020.\nQwen Team.\nIntroducing Qwen1.5, 2024a.\nURL https://qwenlm.github.io/blog/\nqwen1.5/.\nQwen Team. Qwen1.5-110B: The first 100B+ model of the Qwen1.5 series, 2024b. URL https:\n//qwenlm.github.io/blog/qwen1.5-110b/.\nQwen Team. Qwen1.5-MoE: Matching 7B model performance with 1/3 activated parameters, 2024c.\nURL https://qwenlm.github.io/blog/qwen-moe/.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS,\n2023.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-\nmar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts\ninference and training to power next-generation AI scale. In ICML, volume 162 of Proceedings of\nMachine Learning Research, pp. 18332–18346. PMLR, 2022.\nMathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei\nQin, Caiming Xiong, and Shafiq Joty. How much are LLMs contaminated? A comprehensive\nsurvey and the llmsanitize library. CoRR, abs/2404.00699, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A\nbenchmark. CoRR, abs/2311.12022, 2023.\nOscar Sainz, Jon Ander Campos, Iker Garc´ıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and\nEneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamination\nfor each benchmark. In EMNLP (Findings), pp. 10776–10787. Association for Computational\nLinguistics, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nadversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.\nJianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023.\nURL https://spaces.ac.cn/archives/9577.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-\nBench tasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051.\nAssociation for Computational Linguistics, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\n25\n\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task\nlanguage understanding benchmark. CoRR, abs/2406.01574, 2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin,\nRashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar\nMehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike\nLewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR,\nabs/2309.16039, 2023.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial\ndataset for paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685–3690. Association for\nComputational Linguistics, 2019.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\nZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang,\nShiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\nYi: Open foundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nTao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu\nYao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced\nlong-context benchmark with 5 length levels up to 256K. CoRR, abs/2402.05136, 2024.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling re-\nlationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825,\n2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In ACL (1), pp. 4791–4800. Association for Computational Linguistics,\n2019.\nAohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin\nZhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie\nTang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang,\nPeng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang,\nWeng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan\nLiu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao\nYang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du,\nZhenyu Hou, and Zihan Wang. ChatGLM: A family of large language models from GLM-130B to\nGLM-4 all tools. CoRR, abs/2406.12793, 2024.\nYingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, and\nYongbin Li. Tree-Instruct: A preliminary study of the intrinsic relationship between complexity\nand alignment. In LREC/COLING, pp. 16776–16789. ELRA and ICCL, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS, 2023.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\nand Le Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911,\n2023.\n26\n"
    }
  ]
}