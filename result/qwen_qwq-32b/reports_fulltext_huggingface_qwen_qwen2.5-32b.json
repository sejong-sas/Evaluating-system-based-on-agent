{
  "model_id": "Qwen/Qwen2.5-32B",
  "full_texts": [
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen2.5/",
      "full_text": "Qwen2.5: A Party of Foundation Models! | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen2.5: A Party of Foundation Models! September 19, 2024 &nbsp;·&nbsp;9 min&nbsp;·&nbsp;1738 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD Introduction # In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5 . We are announcing what might be the largest opensource release in history! Let&rsquo;s get the party started! Our latest release features the LLMs Qwen2.5 , along with specialized models for coding, Qwen2.5-Coder , and mathematics, Qwen2.5-Math . All open-weight models are dense, decoder-only language models, available in various sizes, including: Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B Qwen2.5-Coder: 1.5B, 7B, and 32B on the way Qwen2.5-Math: 1.5B, 7B, and 72B. All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen-Plus and Qwen-Turbo through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the Qwen2-VL-72B , which features performance enhancements compared to last month&rsquo;s release. For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links: Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math Get ready to unlock a world of possibilities with our extensive lineup of models! We&rsquo;re excited to share these cutting-edge models with you, and we can&rsquo;t wait to see the incredible things you&rsquo;ll achieve with them! Takeaways # In terms of Qwen2.5 , the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages. The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR). Performance # Qwen2.5 # To showcase Qwen2.5&rsquo;s capabilities, we benchmark our largest open-source model, Qwen2.5-72B - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences. Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B. Furthermore, we benchmark the latest version of our API-based model, Qwen-Plus , against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus&rsquo;s competitive standing in the current landscape of large language models. We show that Qwen-Plus significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plus&rsquo;s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models. A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, Qwen2.5-14B and Qwen2.5-32B . These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, Qwen-Turbo , offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service. In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our Qwen2.5-3B stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors. In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities. Qwen2.5-Coder # Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes. We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities. Qwen2.5-Math # In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models. Develop with Qwen2.5 # The simplest way to use is through Hugging Face Transfomer as demonstrated in the model card : from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/Qwen2.5-7B-Instruct&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 512 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service: python - m vllm . entrypoints . openai . api_server \\ -- model Qwen / Qwen2 .5 - 7 B - Instruct or use vllm serve if you use vllm>=0.5.3 . Then you can communicate with Qwen2.5 via curl : curl http://localhost:8000/v1/chat/completions -H &#34;Content-Type: application/json&#34; -d &#39;{ &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;, &#34;messages&#34;: [ {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;} ], &#34;temperature&#34;: 0.7, &#34;top_p&#34;: 0.8, &#34;repetition_penalty&#34;: 1.05, &#34;max_tokens&#34;: 512 }&#39; Furthermore, Qwen2.5 supports vllm&rsquo;s built-in tool calling. This functionality requires vllm>=0.6 . If you want to enable this functionality, please start vllm&rsquo;s OpenAI-compatible service with: vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes You can then use it in the same way you use GPT&rsquo;s tool calling . Qwen2.5 also supports Ollama&rsquo;s tool calling . You can use it by starting Ollama&rsquo;s OpenAI-compatible service and using it in the same way you use GPT&rsquo;s tool calling. Qwen2.5&rsquo;s chat template also includes a tool calling template, meaning that you can use Hugging Face transformers&rsquo; tool calling support . The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by Nous&rsquo; Hermes . Historically, Qwen-Agent provided tool calling support using Qwen2&rsquo;s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2&rsquo;s template and Qwen-Agent as well. Friends of Qwen # 💗 Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends : Hugging Face Transformers Finetuning: Peft , ChatLearn , Llama-Factory , Axolotl , Firefly , Swift , XTuner , Unsloth , Liger Kernel Quantization: AutoGPTQ , AutoAWQ , Neural Compressor Deployment: vLLM , SGL , SkyPilot , TensorRT-LLM , OpenVino , TGI , Xinference API Platforms: Together , Fireworks , OpenRouter , Sillicon Flow Local Run: MLX , Llama.cpp , Ollama , LM Studio , Jan Agent and RAG Frameworks: Dify , LlamaIndex , CrewAI Evaluation: LMSys , OpenCompass , Open LLM Leaderboard Model Training: Arcee AI , Sailor , Dolphin , Openbuddy We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven&rsquo;t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before. What&rsquo;s Next? # While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models&rsquo; reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments! Citation # We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog @misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} } @article{qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } &copy; 2025 Qwen Powered by Hugo"
    }
  ]
}