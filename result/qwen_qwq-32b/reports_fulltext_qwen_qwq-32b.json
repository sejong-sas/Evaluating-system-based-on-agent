{
  "model_id": "qwen/qwq-32b",
  "full_texts": [
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwq-32b/",
      "full_text": " QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen Blog Publication About Try Qwen Chat &nbsp; QwQ-32B: Embracing the Power of Reinforcement Learning March 6, 2025 &nbsp;·&nbsp;4 min&nbsp;·&nbsp;742 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 QWEN CHAT Hugging Face ModelScope DEMO DISCORD Scaling Reinforcement Learning (RL) has the potential to enhance model performance beyond conventional pretraining and post-training methods. Recent studies have demonstrated that RL can significantly improve the reasoning capabilities of models. For instance, DeepSeek R1 has achieved state-of-the-art performance by integrating cold-start data and multi-stage training, enabling deep thinking and complex reasoning. Our research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models. We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated). This remarkable outcome underscores the effectiveness of RL when applied to robust foundation models pretrained on extensive world knowledge. Furthermore, we have integrated agent-related capabilities into the reasoning model, enabling it to think critically while utilizing tools and adapting its reasoning based on environmental feedback. These advancements not only demonstrate the transformative potential of RL but also pave the way for further innovations in the pursuit of artificial general intelligence. QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat . Performance # QwQ-32B is evaluated across a range of benchmarks designed to assess its mathematical reasoning, coding proficiency, and general problem-solving capabilities. The results below highlight QwQ-32B&rsquo;s performance in comparison to other leading models, including DeepSeek-R1-Distilled-Qwen-32B, DeepSeek-R1-Distilled-Llama-70B, o1-mini, and the original DeepSeek-R1. Reinforcement Learning # We began with a cold-start checkpoint and implemented a reinforcement learning (RL) scaling approach driven by outcome-based rewards. In the initial stage, we scale RL specifically for math and coding tasks. Rather than relying on traditional reward models, we utilized an accuracy verifier for math problems to ensure the correctness of final solutions and a code execution server to assess whether the generated codes successfully pass predefined test cases. As training episodes progress, performance in both domains shows continuous improvement. After the first stage, we add another stage of RL for general capabilities. It is trained with rewards from general reward model and some rule-based verifiers. We find that this stage of RL training with a small amount of steps can increase the performance of other general capabilities, such as instruction following, alignment with human preference, and agent performance, without significant performance drop in math and coding. Use QwQ-32B # Below are brief examples demonstrating how to use QwQ-32B via Hugging Face Transformers and Alibaba Cloud DashScope API. from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/QwQ-32B&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;How many r&#39;s are in the word \\&#34; strawberry \\&#34; &#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 32768 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] print ( response ) from openai import OpenAI import os # Initialize OpenAI client client = OpenAI ( # If the environment variable is not configured, replace with your API Key: api_key=&#34;sk-xxx&#34; # How to get an API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key api_key = os . getenv ( &#34;DASHSCOPE_API_KEY&#34; ), base_url = &#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34; ) reasoning_content = &#34;&#34; content = &#34;&#34; is_answering = False completion = client . chat . completions . create ( model = &#34;qwq-32b&#34; , messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : &#34;Which is larger, 9.9 or 9.11?&#34; } ], stream = True , # Uncomment the following line to return token usage in the last chunk # stream_options={ # &#34;include_usage&#34;: True # } ) print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;reasoning content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) for chunk in completion : # If chunk.choices is empty, print usage if not chunk . choices : print ( &#34; \\n Usage:&#34; ) print ( chunk . usage ) else : delta = chunk . choices [ 0 ] . delta # Print reasoning content if hasattr ( delta , &#39;reasoning_content&#39; ) and delta . reasoning_content is not None : print ( delta . reasoning_content , end = &#39;&#39; , flush = True ) reasoning_content += delta . reasoning_content else : if delta . content != &#34;&#34; and is_answering is False : print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) is_answering = True # Print content print ( delta . content , end = &#39;&#39; , flush = True ) content += delta . content Future Work # This marks Qwen&rsquo;s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling. &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/",
      "full_text": " Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Welcome to Qwen! ¶ Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. Qwen3-2507 ¶ With input from the community and insights from further research, Instruct-only and Thinking-only models are coming back! The results are Qwen3-2507: Qwen3-Instruct-2507 has the following features: Significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage . Substantial gains in long-tail knowledge coverage across multiple languages . Markedly better alignment with user preferences in subjective and open-ended tasks , enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding , extensible to 1M. Qwen3-Thinking-2507 has the following features: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving state-of-the-art results among open-source thinking models . Markedly better general capabilities , such as instruction following, tool usage, text generation, and alignment with human preferences. Enhanced 256K long-context understanding capabilities, extensible to 1M. Qwen3 ¶ Qwen3, aka Qwen3-2504, has the following features: Dense and Mixture-of-Experts (MoE) models , available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. Seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose chat) within a single model , ensuring optimal performance across various scenarios. Significantly enhancement in reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Resource &amp; Links ¶ For more information, please visit our: Qwen Home Page Chat with Qwen (with Deep Research and Web Dev) Blog GitHub Hugging Face ModelScope Qwen3 Collection Join our community by joining our Discord and WeChat group. We are looking forward to seeing you there! Next Quickstart Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page Welcome to Qwen! Qwen3-2507 Qwen3 Resource &amp; Links ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2309.00071",
      "full_text": " [2309.00071] YaRN: Efficient Context Window Extension of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2309.00071 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2309.00071 (cs) [Submitted on 31 Aug 2023 ( v1 ), last revised 1 Nov 2023 (this version, v2)] Title: YaRN: Efficient Context Window Extension of Large Language Models Authors: Bowen Peng , Jeffrey Quesnelle , Honglu Fan , Enrico Shippole View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF Abstract: Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2309.00071 [cs.CL] &nbsp; (or arXiv:2309.00071v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2309.00071 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jeffrey Quesnelle [ view email ] [v1] Thu, 31 Aug 2023 18:18:07 UTC (42 KB) [v2] Wed, 1 Nov 2023 17:28:26 UTC (354 KB) Full-text links: Access Paper: View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-09 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/deployment/vllm.html",
      "full_text": " vLLM - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar vLLM ¶ We recommend you trying vLLM for your deployment of Qwen. It is simple to use, and it is fast with state-of-the-art serving throughput, efficient management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc. To learn more about vLLM, please refer to the paper and documentation . Environment Setup ¶ By default, you can install vllm with pip in a clean environment: pip install &quot;vllm&gt;=0.8.5&quot; Please note that the prebuilt vllm has strict dependencies on torch and its CUDA versions. Check the note in the official document for installation ( link ) for more help. API Service ¶ It is easy to build an OpenAI-compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at http://localhost:8000 . You can specify the address with --host and --port arguments. Run the command as shown below: vllm serve Qwen/Qwen3-8B By default, if the model does not point to a valid local directory, it will download the model files from the Hugging Face Hub. To download model from ModelScope, set the following before running the above command: export VLLM_USE_MODELSCOPE = true For distributed inference with tensor parallelism, it is as simple as vllm serve Qwen/Qwen3-8B --tensor-parallel-size 4 The above command will use tensor parallelism on 4 GPUs. You should change the number of GPUs according to your demand. Basic Usage ¶ Then, you can use the create chat interface to communicate with Qwen: curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.6, &quot;top_p&quot;: 0.95, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 32768 }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 32768 , temperature = 0.6 , top_p = 0.95 , extra_body = { &quot;top_k&quot; : 20 , }, ) print ( &quot;Chat response:&quot; , chat_response ) Tip vllm will use the sampling parameters from the generation_config.json in the model files. While the default sampling parameters would work most of the time for thinking mode, it is recommended to adjust the sampling parameters according to your application, and always pass the sampling parameters to the API. Thinking &amp; Non-Thinking Modes ¶ Qwen3 models will think before respond. This behavior could be controlled by either the hard switch, which could disable thinking completely, or the soft switch, where the model follows the instruction of the user on whether it should think. The hard switch is available in vLLM through the following configuration to the API call. To disable thinking, use curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.7, &quot;top_p&quot;: 0.8, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 8192, &quot;presence_penalty&quot;: 1.5, &quot;chat_template_kwargs&quot;: {&quot;enable_thinking&quot;: false} }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 8192 , temperature = 0.7 , top_p = 0.8 , presence_penalty = 1.5 , extra_body = { &quot;top_k&quot; : 20 , &quot;chat_template_kwargs&quot; : { &quot;enable_thinking&quot; : False }, }, ) print ( &quot;Chat response:&quot; , chat_response ) Note Please note that passing enable_thinking is not OpenAI API compatible. The exact method may differ among frameworks. Tip To completely disable thinking, you could use a custom chat template when starting the model: vllm serve Qwen/Qwen3-8B --chat-template ./qwen3_nonthinking.jinja The chat template prevents the model from generating thinking content, even if the user instructs the model to do so with /think . Tip It is recommended to set sampling parameters differently for thinking and non-thinking modes. Parsing Thinking Content ¶ vLLM supports parsing the thinking content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1 Since vLLM 0.9.0, one can also use vllm serve Qwen/Qwen3-8B --reasoning-parser qwen3 The response message will have a field named reasoning_content in addition to content , containing the thinking content generated by the model. Note Please note that this feature is not OpenAI API compatible. Important As of vLLM 0.8.5, enable_thinking=False is not compatible with this feature. If you need to pass enable_thinking=False to the API, you should disable parsing thinking content. This is resolved in vLLM 0.9.0 with the qwen3 reasoning parser. Parsing Tool Calls ¶ vLLM supports parsing the tool calling content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-auto-tool-choice --tool-call-parser hermes For more information, please refer to our guide on Function Calling . Structured/JSON Output ¶ vLLM supports structured/JSON output. Please refer to vLLM’s documentation for the guided_json parameters. Besides, it is also recommended to instruct the model to generate the specific format in the system message or in your prompt. Serving Quantized models ¶ Qwen3 comes with two types of pre-quantized models, FP8 and AWQ. The command serving those models are the same as the original models except for the name change: # For FP8 quantized model vllm serve Qwen/Qwen3-8B-FP8 # For AWQ quantized model vllm serve Qwen/Qwen3-8B-AWQ Note The FP8 models of Qwen3 are block-wise quant, which is supported on NVIDIA GPUs with compute capability &gt; 8.9, that is, Ada Lovelace, Hopper, and later GPUs and runs as w8a8. Since vLLM v0.9.0, FP8 Marlin has supported block-wise quants (running as w8a16) and you can also run Qwen3 FP8 models on Ampere cards. Note If you encountered the following error when deploying the FP8 models, it indicates that the tensor parallel size does not agree with the model weights: File &quot;.../vllm/vllm/model_executor/layers/quantization/fp8.py&quot; , line 477 , in create_weights raise ValueError ( ValueError : The output_size of gate &#39;s and up&#39; s weight = 192 is not divisible by weight quantization block_n = 128. We recommend lowering the degree of tensor parallel, e.g., --tensor-parallel-size 4 or enabling expert parallel, e.g., --tensor-parallel-size 8 --enable-expert-parallel . Context Length ¶ The context length for Qwen3 models in pretraining is up to 32,768 tokens. To handle context length substantially exceeding 32,768 tokens, RoPE scaling techniques should be applied. We have validated the performance of YaRN , a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts. vLLM supports YaRN, which can be configured as vllm serve Qwen/Qwen3-8B --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:4.0,&quot;original_max_position_embeddings&quot;:32768}&#39; --max-model-len 131072 Note vLLM implements static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required. It is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0. Note The default max_position_embeddings in config.json is set to 40,960, which used by vLLM, if --max-model-len is not specified. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing and leave adequate room for model thinking. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance. Python Library ¶ vLLM can also be directly used as a Python library, which is convenient for offline batch inference but lack some API-only features, such as parsing model generation to structure messages. The following shows the basic usage of vLLM as a library: from transformers import AutoTokenizer from vllm import LLM , SamplingParams # Initialize the tokenizer tokenizer = AutoTokenizer . from_pretrained ( &quot;Qwen/Qwen3-8B&quot; ) # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = True , # Set to False to strictly disable thinking ) # Generate outputs outputs = llm . generate ([ text ], sampling_params ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) Since vLLM v0.9.0, you can also use the LLM.chat interface which includes support for chat_template_kwargs : from vllm import LLM , SamplingParams # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] # Generate outputs outputs = llm . chat ( [ messages ], sampling_params , chat_template_kwargs = { &quot;enable_thinking&quot; : True }, # Set to False to strictly disable thinking ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) FAQ ¶ You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix. The first one is --max-model-len . Our provided default max_position_embedding is 40960 and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue. Another argument you can pay attention to is --gpu-memory-utilization . vLLM will pre-allocate this much GPU memory. By default, it is 0.9 . This is also why you find a vLLM service always takes so much memory. If you are in eager mode (by default it is not), you can level it up to tackle the OOM problem. Otherwise, CUDA Graphs are used, which will use GPU memory not controlled by vLLM, and you should try lowering it. If it doesn’t work, you should try --enforce-eager , which may slow down inference, or reduce the --max-model-len . For more usage guide with vLLM, please see vLLM’s Qwen3 Usage Guide . Next TGI Previous SGLang Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page vLLM Environment Setup API Service Basic Usage Thinking &amp; Non-Thinking Modes Parsing Thinking Content Parsing Tool Calls Structured/JSON Output Serving Quantized models Context Length Python Library FAQ ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html",
      "full_text": "Title: Documentation page not found - Read the Docs Community\n\nURL Source: https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html\n\nWarning: Target URL returned error 404: Not Found\nWarning: This page contains shadow DOM that are currently hidden, consider enabling shadow DOM processing.\n\nMarkdown Content:\n404  Documentation page not found\n\n[qwen.readthedocs.io](https://qwen.readthedocs.io/)\n\nThe documentation page you requested does not exist or may have been removed.\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://qwenlm.github.io/blog/qwq-32b/},",
      "full_text": "Title: 404 Page not found\n\nURL Source: https://qwenlm.github.io/blog/qwq-32b/%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 Page not found | Qwen\n\n===============\n\n[![Image 1](https://qwenlm.github.io/img/logo.png)](https://qwenlm.github.io/ \"Qwen (Alt + H)\")\n\n*   [Blog](https://qwenlm.github.io/blog/ \"Blog\")\n*   [Publication](https://qwenlm.github.io/publication \"Publication\")\n*   [About](https://qwenlm.github.io/about \"About\")\n*   [Try Qwen Chat](https://chat.qwen.ai/ \"Try Qwen Chat\")\n\n404\n===\n\nYou will be redirected to home page shortly. If not, please click [here](https://qwenlm.github.io/).\n\n© 2025 [Qwen](https://qwenlm.github.io/)Powered by [Hugo](https://gohugo.io/)[](https://qwenlm.github.io/blog/qwq-32b/%7D,#top \"Go to Top (Alt + G)\")\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwq-32b/",
      "full_text": " QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen Blog Publication About Try Qwen Chat &nbsp; QwQ-32B: Embracing the Power of Reinforcement Learning March 6, 2025 &nbsp;·&nbsp;4 min&nbsp;·&nbsp;742 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 QWEN CHAT Hugging Face ModelScope DEMO DISCORD Scaling Reinforcement Learning (RL) has the potential to enhance model performance beyond conventional pretraining and post-training methods. Recent studies have demonstrated that RL can significantly improve the reasoning capabilities of models. For instance, DeepSeek R1 has achieved state-of-the-art performance by integrating cold-start data and multi-stage training, enabling deep thinking and complex reasoning. Our research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models. We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated). This remarkable outcome underscores the effectiveness of RL when applied to robust foundation models pretrained on extensive world knowledge. Furthermore, we have integrated agent-related capabilities into the reasoning model, enabling it to think critically while utilizing tools and adapting its reasoning based on environmental feedback. These advancements not only demonstrate the transformative potential of RL but also pave the way for further innovations in the pursuit of artificial general intelligence. QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat . Performance # QwQ-32B is evaluated across a range of benchmarks designed to assess its mathematical reasoning, coding proficiency, and general problem-solving capabilities. The results below highlight QwQ-32B&rsquo;s performance in comparison to other leading models, including DeepSeek-R1-Distilled-Qwen-32B, DeepSeek-R1-Distilled-Llama-70B, o1-mini, and the original DeepSeek-R1. Reinforcement Learning # We began with a cold-start checkpoint and implemented a reinforcement learning (RL) scaling approach driven by outcome-based rewards. In the initial stage, we scale RL specifically for math and coding tasks. Rather than relying on traditional reward models, we utilized an accuracy verifier for math problems to ensure the correctness of final solutions and a code execution server to assess whether the generated codes successfully pass predefined test cases. As training episodes progress, performance in both domains shows continuous improvement. After the first stage, we add another stage of RL for general capabilities. It is trained with rewards from general reward model and some rule-based verifiers. We find that this stage of RL training with a small amount of steps can increase the performance of other general capabilities, such as instruction following, alignment with human preference, and agent performance, without significant performance drop in math and coding. Use QwQ-32B # Below are brief examples demonstrating how to use QwQ-32B via Hugging Face Transformers and Alibaba Cloud DashScope API. from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/QwQ-32B&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;How many r&#39;s are in the word \\&#34; strawberry \\&#34; &#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 32768 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] print ( response ) from openai import OpenAI import os # Initialize OpenAI client client = OpenAI ( # If the environment variable is not configured, replace with your API Key: api_key=&#34;sk-xxx&#34; # How to get an API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key api_key = os . getenv ( &#34;DASHSCOPE_API_KEY&#34; ), base_url = &#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34; ) reasoning_content = &#34;&#34; content = &#34;&#34; is_answering = False completion = client . chat . completions . create ( model = &#34;qwq-32b&#34; , messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : &#34;Which is larger, 9.9 or 9.11?&#34; } ], stream = True , # Uncomment the following line to return token usage in the last chunk # stream_options={ # &#34;include_usage&#34;: True # } ) print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;reasoning content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) for chunk in completion : # If chunk.choices is empty, print usage if not chunk . choices : print ( &#34; \\n Usage:&#34; ) print ( chunk . usage ) else : delta = chunk . choices [ 0 ] . delta # Print reasoning content if hasattr ( delta , &#39;reasoning_content&#39; ) and delta . reasoning_content is not None : print ( delta . reasoning_content , end = &#39;&#39; , flush = True ) reasoning_content += delta . reasoning_content else : if delta . content != &#34;&#34; and is_answering is False : print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) is_answering = True # Print content print ( delta . content , end = &#39;&#39; , flush = True ) content += delta . content Future Work # This marks Qwen&rsquo;s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling. &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2309.00071",
      "full_text": " [2309.00071] YaRN: Efficient Context Window Extension of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2309.00071 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2309.00071 (cs) [Submitted on 31 Aug 2023 ( v1 ), last revised 1 Nov 2023 (this version, v2)] Title: YaRN: Efficient Context Window Extension of Large Language Models Authors: Bowen Peng , Jeffrey Quesnelle , Honglu Fan , Enrico Shippole View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF Abstract: Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2309.00071 [cs.CL] &nbsp; (or arXiv:2309.00071v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2309.00071 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jeffrey Quesnelle [ view email ] [v1] Thu, 31 Aug 2023 18:18:07 UTC (42 KB) [v2] Wed, 1 Nov 2023 17:28:26 UTC (354 KB) Full-text links: Access Paper: View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-09 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/deployment/vllm.html",
      "full_text": " vLLM - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar vLLM ¶ We recommend you trying vLLM for your deployment of Qwen. It is simple to use, and it is fast with state-of-the-art serving throughput, efficient management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc. To learn more about vLLM, please refer to the paper and documentation . Environment Setup ¶ By default, you can install vllm with pip in a clean environment: pip install &quot;vllm&gt;=0.8.5&quot; Please note that the prebuilt vllm has strict dependencies on torch and its CUDA versions. Check the note in the official document for installation ( link ) for more help. API Service ¶ It is easy to build an OpenAI-compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at http://localhost:8000 . You can specify the address with --host and --port arguments. Run the command as shown below: vllm serve Qwen/Qwen3-8B By default, if the model does not point to a valid local directory, it will download the model files from the Hugging Face Hub. To download model from ModelScope, set the following before running the above command: export VLLM_USE_MODELSCOPE = true For distributed inference with tensor parallelism, it is as simple as vllm serve Qwen/Qwen3-8B --tensor-parallel-size 4 The above command will use tensor parallelism on 4 GPUs. You should change the number of GPUs according to your demand. Basic Usage ¶ Then, you can use the create chat interface to communicate with Qwen: curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.6, &quot;top_p&quot;: 0.95, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 32768 }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 32768 , temperature = 0.6 , top_p = 0.95 , extra_body = { &quot;top_k&quot; : 20 , }, ) print ( &quot;Chat response:&quot; , chat_response ) Tip vllm will use the sampling parameters from the generation_config.json in the model files. While the default sampling parameters would work most of the time for thinking mode, it is recommended to adjust the sampling parameters according to your application, and always pass the sampling parameters to the API. Thinking &amp; Non-Thinking Modes ¶ Qwen3 models will think before respond. This behavior could be controlled by either the hard switch, which could disable thinking completely, or the soft switch, where the model follows the instruction of the user on whether it should think. The hard switch is available in vLLM through the following configuration to the API call. To disable thinking, use curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.7, &quot;top_p&quot;: 0.8, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 8192, &quot;presence_penalty&quot;: 1.5, &quot;chat_template_kwargs&quot;: {&quot;enable_thinking&quot;: false} }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 8192 , temperature = 0.7 , top_p = 0.8 , presence_penalty = 1.5 , extra_body = { &quot;top_k&quot; : 20 , &quot;chat_template_kwargs&quot; : { &quot;enable_thinking&quot; : False }, }, ) print ( &quot;Chat response:&quot; , chat_response ) Note Please note that passing enable_thinking is not OpenAI API compatible. The exact method may differ among frameworks. Tip To completely disable thinking, you could use a custom chat template when starting the model: vllm serve Qwen/Qwen3-8B --chat-template ./qwen3_nonthinking.jinja The chat template prevents the model from generating thinking content, even if the user instructs the model to do so with /think . Tip It is recommended to set sampling parameters differently for thinking and non-thinking modes. Parsing Thinking Content ¶ vLLM supports parsing the thinking content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1 Since vLLM 0.9.0, one can also use vllm serve Qwen/Qwen3-8B --reasoning-parser qwen3 The response message will have a field named reasoning_content in addition to content , containing the thinking content generated by the model. Note Please note that this feature is not OpenAI API compatible. Important As of vLLM 0.8.5, enable_thinking=False is not compatible with this feature. If you need to pass enable_thinking=False to the API, you should disable parsing thinking content. This is resolved in vLLM 0.9.0 with the qwen3 reasoning parser. Parsing Tool Calls ¶ vLLM supports parsing the tool calling content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-auto-tool-choice --tool-call-parser hermes For more information, please refer to our guide on Function Calling . Structured/JSON Output ¶ vLLM supports structured/JSON output. Please refer to vLLM’s documentation for the guided_json parameters. Besides, it is also recommended to instruct the model to generate the specific format in the system message or in your prompt. Serving Quantized models ¶ Qwen3 comes with two types of pre-quantized models, FP8 and AWQ. The command serving those models are the same as the original models except for the name change: # For FP8 quantized model vllm serve Qwen/Qwen3-8B-FP8 # For AWQ quantized model vllm serve Qwen/Qwen3-8B-AWQ Note The FP8 models of Qwen3 are block-wise quant, which is supported on NVIDIA GPUs with compute capability &gt; 8.9, that is, Ada Lovelace, Hopper, and later GPUs and runs as w8a8. Since vLLM v0.9.0, FP8 Marlin has supported block-wise quants (running as w8a16) and you can also run Qwen3 FP8 models on Ampere cards. Note If you encountered the following error when deploying the FP8 models, it indicates that the tensor parallel size does not agree with the model weights: File &quot;.../vllm/vllm/model_executor/layers/quantization/fp8.py&quot; , line 477 , in create_weights raise ValueError ( ValueError : The output_size of gate &#39;s and up&#39; s weight = 192 is not divisible by weight quantization block_n = 128. We recommend lowering the degree of tensor parallel, e.g., --tensor-parallel-size 4 or enabling expert parallel, e.g., --tensor-parallel-size 8 --enable-expert-parallel . Context Length ¶ The context length for Qwen3 models in pretraining is up to 32,768 tokens. To handle context length substantially exceeding 32,768 tokens, RoPE scaling techniques should be applied. We have validated the performance of YaRN , a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts. vLLM supports YaRN, which can be configured as vllm serve Qwen/Qwen3-8B --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:4.0,&quot;original_max_position_embeddings&quot;:32768}&#39; --max-model-len 131072 Note vLLM implements static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required. It is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0. Note The default max_position_embeddings in config.json is set to 40,960, which used by vLLM, if --max-model-len is not specified. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing and leave adequate room for model thinking. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance. Python Library ¶ vLLM can also be directly used as a Python library, which is convenient for offline batch inference but lack some API-only features, such as parsing model generation to structure messages. The following shows the basic usage of vLLM as a library: from transformers import AutoTokenizer from vllm import LLM , SamplingParams # Initialize the tokenizer tokenizer = AutoTokenizer . from_pretrained ( &quot;Qwen/Qwen3-8B&quot; ) # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = True , # Set to False to strictly disable thinking ) # Generate outputs outputs = llm . generate ([ text ], sampling_params ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) Since vLLM v0.9.0, you can also use the LLM.chat interface which includes support for chat_template_kwargs : from vllm import LLM , SamplingParams # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] # Generate outputs outputs = llm . chat ( [ messages ], sampling_params , chat_template_kwargs = { &quot;enable_thinking&quot; : True }, # Set to False to strictly disable thinking ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) FAQ ¶ You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix. The first one is --max-model-len . Our provided default max_position_embedding is 40960 and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue. Another argument you can pay attention to is --gpu-memory-utilization . vLLM will pre-allocate this much GPU memory. By default, it is 0.9 . This is also why you find a vLLM service always takes so much memory. If you are in eager mode (by default it is not), you can level it up to tackle the OOM problem. Otherwise, CUDA Graphs are used, which will use GPU memory not controlled by vLLM, and you should try lowering it. If it doesn’t work, you should try --enforce-eager , which may slow down inference, or reduce the --max-model-len . For more usage guide with vLLM, please see vLLM’s Qwen3 Usage Guide . Next TGI Previous SGLang Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page vLLM Environment Setup API Service Basic Usage Thinking &amp; Non-Thinking Modes Parsing Thinking Content Parsing Tool Calls Structured/JSON Output Serving Quantized models Context Length Python Library FAQ ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively",
      "full_text": " QwQ-32B: How to Run effectively | Unsloth Documentation OpenAI gpt-oss &amp; all model types now supported! Ctrl K Reddit page Discord Newsletter Latest news More Get Started 🦥 Unsloth Docs ⭐ Beginner? Start here! 🛠️ Unsloth Requirements 🤔 FAQ + Is Fine-tuning Right For Me? 📒 Unsloth Notebooks 🔮 All Our Models 📥 Installing + Updating Updating Pip Install Windows Installation Conda Install Google Colab 🧬 Fine-tuning LLMs Guide ❓ What Model Should I Use? 🧠 LoRA Hyperparameters Guide Basics 🦥 Unsloth Dynamic GGUFs on Aider Polyglot Memory Efficient RL Long Context gpt-oss Training 🐋 DeepSeek-V3.1: How to Run Locally gpt-oss: How to Run &amp; Fine-tune ⚡ Tutorial: How to Fine-tune gpt-oss Grok 2 ✨ Gemma 3: How to Run &amp; Fine-tune ✨ Gemma 3n: How to Run &amp; Fine-tune 🌠 Qwen3-Coder: How to Run Locally 🌠 Qwen3: How to Run &amp; Fine-tune 🌠 Qwen3-2507 🌙 Kimi K2: How to Run Locally 🐋 DeepSeek-R1-0528: How to Run Locally 💡 Reinforcement Learning (RL) Guide ⚡ Tutorial: Train your own Reasoning model with GRPO 🏆 Reinforcement Learning - DPO, ORPO &amp; KTO 🔊 Text-to-Speech (TTS) Fine-tuning 📈 Datasets Guide 🚀 Tutorials: How To Fine-tune &amp; Run LLMs 💥 Magistral: How to Run &amp; Fine-tune 📙 Devstral: How to Run &amp; Fine-tune 🦙 Tutorial: How to Finetune Llama-3 and Use In Ollama 🐳 DeepSeek-V3-0324: How to Run Locally 🐋 DeepSeek-R1: How to Run Locally 🐳 DeepSeek-R1 Dynamic 1.58-bit 🌠 QwQ-32B: How to Run effectively Phi-4 Reasoning: How to Run &amp; Fine-tune Cogito v2: How to Run Locally 🦙 Llama 4: How to Run &amp; Fine-tune 🦥 Unsloth Dynamic 2.0 GGUFs 🖥️ Running &amp; Saving Models Saving to GGUF Saving to Ollama Saving to VLLM Troubleshooting Inference ♻️ Continued Pretraining 💬 Chat Templates 👁️ Vision Fine-tuning 🏁 Finetuning from Last Checkpoint ⚠️ Troubleshooting &amp; FAQs 🛠️ Unsloth Environment Flags Training LLMs with Blackwell, RTX 50 series &amp; Unsloth 📊 Unsloth Benchmarks Multi-GPU Training with Unsloth Powered by GitBook On this page Official Recommended Settings Recommended settings for llama.cpp Dry Repetition Penalty Tutorial: How to Run QwQ-32B in Ollama 📖 Tutorial: How to Run QwQ-32B in llama.cpp Still doesn&#x27;t work? Try Min_p = 0.1, Temperature = 1.5 &lt;think&gt; token not shown? Extra Notes Tokenizer Bug Fixes Dynamic 4-bit Quants Was this helpful? Copy Basics 🚀 Tutorials: How To Fine-tune &amp; Run LLMs 🌠 QwQ-32B: How to Run effectively How to run QwQ-32B effectively with our bug fixes and without endless generations + GGUFs. Qwen released QwQ-32B - a reasoning model with performance comparable to DeepSeek-R1 on many benchmarks . However, people have been experiencing infinite generations , many repetitions , &lt;think&gt; token issues and finetuning issues. We hope this guide will help debug and fix most issues! Our model uploads with our bug fixes work great for fine-tuning, vLLM and Transformers. If you&#x27;re using llama.cpp and engines that use llama.cpp as backend, follow our instructions here to fix endless generations. Unsloth QwQ-32B uploads with our bug fixes: GGUF Dynamic 4-bit BnB 4-bit 16-bit ⚙️ Official Recommended Settings According to Qwen , these are the recommended settings for inference: Temperature of 0.6 Top_K of 40 (or 20 to 40) Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1) Top_P of 0.95 Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers) Chat template: &lt;|im_start|&gt;user\\nCreate a Flappy Bird game in Python.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n llama.cpp uses min_p = 0.1 by default, which might cause issues. Force it to 0.0. 👍 Recommended settings for llama.cpp We noticed many people use a Repetition Penalty greater than 1.0. For example 1.1 to 1.5. This actually interferes with llama.cpp&#x27;s sampling mechanisms. The goal of a repetition penalty is to penalize repeated generations, but we found this doesn&#x27;t work as expected. Turning off Repetition Penalty also works (ie setting it to 1.0), but we found using it to be useful to penalize endless generations. To use it, we found you must also edit the ordering of samplers in llama.cpp to before applying Repetition Penalty , otherwise there will be endless generations. So add this: Copy --samplers &quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&quot; By default, llama.cpp uses this ordering: Copy --samplers &quot;dry;top_k;typ_p;top_p;min_p;xtc;temperature&quot; We reorder essentially temperature and dry, and move min_p forward. This means we apply samplers in this order: Copy top_k=40 top_p=0.95 min_p=0.0 temperature=0.6 dry typ_p xtc If you still encounter issues, you can increase the --repeat-penalty 1.0 to 1.2 or 1.3. Courtesy to @krist486 for bringing llama.cpp sampling directions to my attention. ☀️ Dry Repetition Penalty We investigated usage of dry penalty as suggested in https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md using a value of 0.8, but we actually found this to rather cause syntax issues especially for coding . If you still encounter issues, you can increase the dry penalty to 0.8. Utilizing our swapped sampling ordering can also help if you decide to use dry penalty . 🦙 Tutorial: How to Run QwQ-32B in Ollama Install ollama if you haven&#x27;t already! Copy apt-get update apt-get install pciutils -y curl -fsSL https://ollama.com/install.sh | sh Run run the model! Note you can call ollama serve in another terminal if it fails! We include all our fixes and suggested parameters (temperature, min_p etc) in param in our Hugging Face upload! Copy ollama run hf.co/unsloth/QwQ-32B-GGUF:Q4_K_M 📖 Tutorial: How to Run QwQ-32B in llama.cpp Obtain the latest llama.cpp on GitHub here . You can follow the build instructions below as well. Change -DGGML_CUDA=ON to -DGGML_CUDA=OFF if you don&#x27;t have a GPU or just want CPU inference. Copy apt-get update apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y git clone https://github.com/ggerganov/llama.cpp cmake llama.cpp -B llama.cpp/build \\ -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose Q4_K_M, or other quantized versions (like BF16 full precision). More versions at: https://huggingface.co/unsloth/QwQ-32B-GGUF Copy # !pip install huggingface_hub hf_transfer import os os.environ[&quot;HF_HUB_ENABLE_HF_TRANSFER&quot;] = &quot;1&quot; from huggingface_hub import snapshot_download snapshot_download( repo_id = &quot;unsloth/QwQ-32B-GGUF&quot;, local_dir = &quot;unsloth-QwQ-32B-GGUF&quot;, allow_patterns = [&quot;*Q4_K_M*&quot;], # For Q4_K_M ) Run Unsloth&#x27;s Flappy Bird test, which will save the output to Q4_K_M_yes_samplers.txt Edit --threads 32 for the number of CPU threads, --ctx-size 16384 for context length, --n-gpu-layers 99 for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference. We use --repeat-penalty 1.1 and --dry-multiplier 0.5 which you can adjust. Copy ./llama.cpp/llama-cli \\ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --seed 3407 \\ --prio 2 \\ --temp 0.6 \\ --repeat-penalty 1.1 \\ --dry-multiplier 0.5 \\ --min-p 0.01 \\ --top-k 40 \\ --top-p 0.95 \\ -no-cnv \\ --samplers &quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&quot; \\ --prompt &quot;&lt;|im_start|&gt;user\\nCreate a Flappy Bird game in Python. You must include these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird&#x27;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don&#x27;t hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&quot; \\ 2&gt;&amp;1 | tee Q4_K_M_yes_samplers.txt The full input from our https://unsloth.ai/blog/deepseekr1-dynamic 1.58bit blog is: Copy &lt;|im_start|&gt;user Create a Flappy Bird game in Python. You must include these things: 1. You must use pygame. 2. The background color should be randomly chosen and is a light shade. Start with a light blue color. 3. Pressing SPACE multiple times will accelerate the bird. 4. The bird&#x27;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color. 5. Place on the bottom some land colored as dark brown or yellow chosen randomly. 6. Make a score shown on the top right side. Increment if you pass pipes and don&#x27;t hit them. 7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade. 8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt; &lt;|im_start|&gt;assistant &lt;think&gt; The beginning and the end of the final Python output after removing the thinking parts: Copy import pygame import random import sys pygame.init() ### Continues class Bird: def __init__(self): ### Continues def main(): best_score = 0 current_score = 0 game_over = False pipes = [] first_time = True # Track first game play # Initial setup background_color = (173, 216, 230) # Light blue initially land_color = random.choice(land_colors) bird = Bird() while True: for event in pygame.event.get(): ### Continues if not game_over: # Update bird and pipes bird.update() ### Continues # Drawing ### Continues pygame.display.flip() clock.tick(60) if __name__ == &quot;__main__&quot;: main() Full final Python output (removed thinking parts): Copy import pygame import random import sys pygame.init() WIDTH, HEIGHT = 800, 600 screen = pygame.display.set_mode((WIDTH, HEIGHT)) pygame.display.set_caption(&quot;Flappy Bird Clone&quot;) clock = pygame.time.Clock() # Colors pipe_colors = [(0, 100, 0), (210, 180, 140), (50, 50, 50)] land_colors = [(139, 69, 19), (255, 255, 0)] # Game constants GRAVITY = 0.5 PIPE_SPEED = 5 BIRD_SIZE = 30 LAND_HEIGHT = 50 PIPE_WIDTH = 50 PIPE_GAP = 150 class Bird: def __init__(self): self.x = WIDTH // 2 self.y = HEIGHT // 2 self.velocity = 0 self.shape = random.choice([&#x27;square&#x27;, &#x27;circle&#x27;, &#x27;triangle&#x27;]) self.color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)) self.rect = pygame.Rect(self.x - BIRD_SIZE//2, self.y - BIRD_SIZE//2, BIRD_SIZE, BIRD_SIZE) def update(self): self.velocity += GRAVITY self.y += self.velocity self.rect.y = self.y - BIRD_SIZE//2 self.rect.x = self.x - BIRD_SIZE//2 # Keep x centered def draw(self): if self.shape == &#x27;square&#x27;: pygame.draw.rect(screen, self.color, self.rect) elif self.shape == &#x27;circle&#x27;: pygame.draw.circle(screen, self.color, (self.rect.centerx, self.rect.centery), BIRD_SIZE//2) elif self.shape == &#x27;triangle&#x27;: points = [ (self.rect.centerx, self.rect.top), (self.rect.left, self.rect.bottom), (self.rect.right, self.rect.bottom) ] pygame.draw.polygon(screen, self.color, points) def spawn_pipe(): pipe_x = WIDTH top_height = random.randint(50, HEIGHT - PIPE_GAP - LAND_HEIGHT) rect_top = pygame.Rect(pipe_x, 0, PIPE_WIDTH, top_height) bottom_y = top_height + PIPE_GAP bottom_height = (HEIGHT - LAND_HEIGHT) - bottom_y rect_bottom = pygame.Rect(pipe_x, bottom_y, PIPE_WIDTH, bottom_height) color = random.choice(pipe_colors) return { &#x27;rect_top&#x27;: rect_top, &#x27;rect_bottom&#x27;: rect_bottom, &#x27;color&#x27;: color, &#x27;scored&#x27;: False } def main(): best_score = 0 current_score = 0 game_over = False pipes = [] first_time = True # Track first game play # Initial setup background_color = (173, 216, 230) # Light blue initially land_color = random.choice(land_colors) bird = Bird() while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.KEYDOWN: if event.key == pygame.K_ESCAPE or event.key == pygame.K_q: pygame.quit() sys.exit() if event.key == pygame.K_SPACE: if game_over: # Reset the game bird = Bird() pipes.clear() current_score = 0 if first_time: # First restart after initial game over background_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255)) first_time = False else: background_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255)) land_color = random.choice(land_colors) game_over = False else: # Jump the bird bird.velocity = -15 # Initial upward velocity if not game_over: # Update bird and pipes bird.update() # Move pipes left remove_pipes = [] for pipe in pipes: pipe[&#x27;rect_top&#x27;].x -= PIPE_SPEED pipe[&#x27;rect_bottom&#x27;].x -= PIPE_SPEED # Check if bird passed the pipe if not pipe[&#x27;scored&#x27;] and bird.rect.x &gt; pipe[&#x27;rect_top&#x27;].right: current_score += 1 pipe[&#x27;scored&#x27;] = True # Check if pipe is offscreen if pipe[&#x27;rect_top&#x27;].right &lt; 0: remove_pipes.append(pipe) # Remove offscreen pipes for p in remove_pipes: pipes.remove(p) # Spawn new pipe if needed if not pipes or pipes[-1][&#x27;rect_top&#x27;].x &lt; WIDTH - 200: pipes.append(spawn_pipe()) # Check collisions land_rect = pygame.Rect(0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT) bird_rect = bird.rect # Check pipes for pipe in pipes: if bird_rect.colliderect(pipe[&#x27;rect_top&#x27;]) or bird_rect.colliderect(pipe[&#x27;rect_bottom&#x27;]): game_over = True break # Check land and top if bird_rect.bottom &gt;= land_rect.top or bird_rect.top &lt;= 0: game_over = True if game_over: if current_score &gt; best_score: best_score = current_score # Drawing screen.fill(background_color) # Draw pipes for pipe in pipes: pygame.draw.rect(screen, pipe[&#x27;color&#x27;], pipe[&#x27;rect_top&#x27;]) pygame.draw.rect(screen, pipe[&#x27;color&#x27;], pipe[&#x27;rect_bottom&#x27;]) # Draw land pygame.draw.rect(screen, land_color, (0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT)) # Draw bird bird.draw() # Draw score font = pygame.font.SysFont(None, 36) score_text = font.render(f&#x27;Score: {current_score}&#x27;, True, (0, 0, 0)) screen.blit(score_text, (WIDTH - 150, 10)) # Game over screen if game_over: over_text = font.render(&#x27;Game Over!&#x27;, True, (255, 0, 0)) best_text = font.render(f&#x27;Best: {best_score}&#x27;, True, (255, 0, 0)) restart_text = font.render(&#x27;Press SPACE to restart&#x27;, True, (255, 0, 0)) screen.blit(over_text, (WIDTH//2 - 70, HEIGHT//2 - 30)) screen.blit(best_text, (WIDTH//2 - 50, HEIGHT//2 + 10)) screen.blit(restart_text, (WIDTH//2 - 100, HEIGHT//2 + 50)) pygame.display.flip() clock.tick(60) if __name__ == &quot;__main__&quot;: main() When running it, we get a runnable game! Now try the same without our fixes! So remove --samplers &quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&quot; This will save the output to Q4_K_M_no_samplers.txt Copy ./llama.cpp/llama-cli \\ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --seed 3407 \\ --prio 2 \\ --temp 0.6 \\ --repeat-penalty 1.1 \\ --dry-multiplier 0.5 \\ --min-p 0.01 \\ --top-k 40 \\ --top-p 0.95 \\ -no-cnv \\ --prompt &quot;&lt;|im_start|&gt;user\\nCreate a Flappy Bird game in Python. You must include these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird&#x27;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don&#x27;t hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&quot; \\ 2&gt;&amp;1 | tee Q4_K_M_no_samplers.txt You will get some looping, but problematically incorrect Python syntax and many other issues. For example the below looks correct, but is wrong! Ie line 39 pipes.clear() ### &lt;&lt;&lt; NameError: name &#x27;pipes&#x27; is not defined. Did you forget to import &#x27;pipes&#x27;? Copy import pygame import random pygame.init() # Constants WIDTH, HEIGHT = 800, 600 GROUND_HEIGHT = 20 GRAVITY = 0.7 PIPE_SPEED = -3 BIRD_SIZE = 45 MIN_GAP = 130 MAX_GAP = 200 PIPE_COLORS = [(0, 96, 0), (205, 133, 63), (89, 97, 107)] DARK_BROWN = (94, 72, 4) YELLOW = (252, 228, 6) screen = pygame.display.set_mode((WIDTH, HEIGHT)) clock = pygame.time.Clock() def random_light_color(): return ( random.randint(180, 230), random.randint(190, 300), random.randint(250, 255) ) def reset_game(): global bird_x, bird_y global pipes, score global background_color, land_color global bird_shape, bird_color # Bird properties bird_x = WIDTH * 0.3 bird_y = HEIGHT // 2 bird_vel = -5 # Initial upward thrust pipes.clear() ### &lt;&lt;&lt; NameError: name &#x27;pipes&#x27; is not defined. Did you forget to import &#x27;pipes&#x27;? If you use --repeat-penalty 1.5 , it gets even worse and more obvious, with actually totally incorrect syntax. Copy import pygame from random import randint # For generating colors/shapes/positions randomly pygame.init() # Constants: WIDTH, HEIGHT =456 ,702 # BACKGROUND_COLOR_LIGHTS=[&#x27;lightskyblue&#x27;] GAP_SIZE=189 # BIRD_RADIUS=3. PIPE_SPEED=- ( ) ? class Game(): def __init__(self): self.screen_size=( ) def reset_game_vars(): global current_scor e # set to zero and other initial states. # Main game loop: while running : for event in pygame.event.get() : if quit ... etc pygame.quit() print(&quot;Code is simplified. Due time constraints, full working version requires further implementation.&quot;) You might be wondering maybe it&#x27;s Q4_K_M? B16 ie full precision should work fine right? Incorrect - the outputs again fail if we do not use our fix of - -samplers &quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&quot; when using a Repetition Penalty. 🌄 Still doesn&#x27;t work? Try Min_p = 0.1, Temperature = 1.5 According to the Min_p paper https://arxiv.org/pdf/2407.01082 , for more creative and diverse outputs, and if you still see repetitions, try disabling top_p and top_k! Copy ./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\ --threads 32 --n-gpu-layers 99 \\ --ctx-size 16384 \\ --temp 1.5 \\ --min-p 0.1 \\ --top-k 0 \\ --top-p 1.0 \\ -no-cnv \\ --prompt &quot;&lt;|im_start|&gt;user\\nCreate a Flappy Bird game in Python. You must include these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird&#x27;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don&#x27;t hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&quot; Another approach is to disable min_p directly, since llama.cpp by default uses min_p = 0.1 ! Copy ./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\ --threads 32 --n-gpu-layers 99 \\ --ctx-size 16384 \\ --temp 0.6 \\ --min-p 0.0 \\ --top-k 40 \\ --top-p 0.95 \\ -no-cnv \\ --prompt &quot;&lt;|im_start|&gt;user\\nCreate a Flappy Bird game in Python. You must include these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird&#x27;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don&#x27;t hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&quot; 🤔 &lt;think&gt; token not shown? Some people are reporting that because &lt;think&gt; is default added in the chat template, some systems are not outputting the thinking traces correctly. You will have to manually edit the Jinja template from: Copy {%- if tools %} {{- &#x27;&lt;|im_start|&gt;system\\n&#x27; }} {%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %} {{- messages[0][&#x27;content&#x27;] }} {%- else %} {{- &#x27;&#x27; }} {%- endif %} {{- &quot;\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;&quot; }} {%- for tool in tools %} {{- &quot;\\n&quot; }} {{- tool | tojson }} {%- endfor %} {{- &quot;\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\&quot;name\\&quot;: &lt;function-name&gt;, \\&quot;arguments\\&quot;: &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n&quot; }} {%- else %} {%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %} {{- &#x27;&lt;|im_start|&gt;system\\n&#x27; + messages[0][&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == &quot;user&quot;) or (message.role == &quot;system&quot; and not loop.first) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role + &#x27;\\n&#x27; + message.content + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\\n&#x27; }} {%- elif message.role == &quot;assistant&quot; and not message.tool_calls %} {%- set content = message.content.split(&#x27;&lt;/think&gt;&#x27;)[-1].lstrip(&#x27;\\n&#x27;) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role + &#x27;\\n&#x27; + content + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\\n&#x27; }} {%- elif message.role == &quot;assistant&quot; %} {%- set content = message.content.split(&#x27;&lt;/think&gt;&#x27;)[-1].lstrip(&#x27;\\n&#x27;) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role }} {%- if message.content %} {{- &#x27;\\n&#x27; + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- &#x27;\\n&lt;tool_call&gt;\\n{&quot;name&quot;: &quot;&#x27; }} {{- tool_call.name }} {{- &#x27;&quot;, &quot;arguments&quot;: &#x27; }} {{- tool_call.arguments | tojson }} {{- &#x27;}\\n&lt;/tool_call&gt;&#x27; }} {%- endfor %} {{- &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- elif message.role == &quot;tool&quot; %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != &quot;tool&quot;) %} {{- &#x27;&lt;|im_start|&gt;user&#x27; }} {%- endif %} {{- &#x27;\\n&lt;tool_response&gt;\\n&#x27; }} {{- message.content }} {{- &#x27;\\n&lt;/tool_response&gt;&#x27; }} {%- if loop.last or (messages[loop.index0 + 1].role != &quot;tool&quot;) %} {{- &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- &#x27;&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&#x27; }} {%- endif %} to another by removing the &lt;think&gt;\\n at the end. The model will now have to manually add &lt;think&gt;\\n during inference, which might not always succeed. DeepSeek also edited all models to default add a &lt;think&gt; token to force the model to go into reasoning model. So change {%- if add_generation_prompt %} {{- &#x27;&lt;|im_start|&gt;assistant\\n&lt;think&gt;\\n&#x27; }} {%- endif %} to {%- if add_generation_prompt %} {{- &#x27;&lt;|im_start|&gt;assistant\\n&#x27; }} {%- endif %} ie remove &lt;think&gt;\\n Full jinja template with removed &lt;think&gt;\\n part Copy {%- if tools %} {{- &#x27;&lt;|im_start|&gt;system\\n&#x27; }} {%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %} {{- messages[0][&#x27;content&#x27;] }} {%- else %} {{- &#x27;&#x27; }} {%- endif %} {{- &quot;\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;&quot; }} {%- for tool in tools %} {{- &quot;\\n&quot; }} {{- tool | tojson }} {%- endfor %} {{- &quot;\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\&quot;name\\&quot;: &lt;function-name&gt;, \\&quot;arguments\\&quot;: &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n&quot; }} {%- else %} {%- if messages[0][&#x27;role&#x27;] == &#x27;system&#x27; %} {{- &#x27;&lt;|im_start|&gt;system\\n&#x27; + messages[0][&#x27;content&#x27;] + &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == &quot;user&quot;) or (message.role == &quot;system&quot; and not loop.first) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role + &#x27;\\n&#x27; + message.content + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\\n&#x27; }} {%- elif message.role == &quot;assistant&quot; and not message.tool_calls %} {%- set content = message.content.split(&#x27;&lt;/think&gt;&#x27;)[-1].lstrip(&#x27;\\n&#x27;) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role + &#x27;\\n&#x27; + content + &#x27;&lt;|im_end|&gt;&#x27; + &#x27;\\n&#x27; }} {%- elif message.role == &quot;assistant&quot; %} {%- set content = message.content.split(&#x27;&lt;/think&gt;&#x27;)[-1].lstrip(&#x27;\\n&#x27;) %} {{- &#x27;&lt;|im_start|&gt;&#x27; + message.role }} {%- if message.content %} {{- &#x27;\\n&#x27; + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- &#x27;\\n&lt;tool_call&gt;\\n{&quot;name&quot;: &quot;&#x27; }} {{- tool_call.name }} {{- &#x27;&quot;, &quot;arguments&quot;: &#x27; }} {{- tool_call.arguments | tojson }} {{- &#x27;}\\n&lt;/tool_call&gt;&#x27; }} {%- endfor %} {{- &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- elif message.role == &quot;tool&quot; %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != &quot;tool&quot;) %} {{- &#x27;&lt;|im_start|&gt;user&#x27; }} {%- endif %} {{- &#x27;\\n&lt;tool_response&gt;\\n&#x27; }} {{- message.content }} {{- &#x27;\\n&lt;/tool_response&gt;&#x27; }} {%- if loop.last or (messages[loop.index0 + 1].role != &quot;tool&quot;) %} {{- &#x27;&lt;|im_end|&gt;\\n&#x27; }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- &#x27;&lt;|im_start|&gt;assistant\\n&#x27; }} {%- endif %} Extra Notes We first thought maybe: QwQ&#x27;s context length was not natively 128K, but rather 32K with YaRN extension. For example in the readme file for https://huggingface.co/Qwen/QwQ-32B , we see: Copy { ..., &quot;rope_scaling&quot;: { &quot;factor&quot;: 4.0, &quot;original_max_position_embeddings&quot;: 32768, &quot;type&quot;: &quot;yarn&quot; } } We tried overriding llama.cpp&#x27;s YaRN handling, but nothing changed. Copy --override-kv qwen2.context_length=int:131072 \\ --override-kv qwen2.rope.scaling.type=str:yarn \\ --override-kv qwen2.rope.scaling.factor=float:4 \\ --override-kv qwen2.rope.scaling.original_context_length=int:32768 \\ --override-kv qwen2.rope.scaling.attn_factor=float:1.13862943649292 \\ We also thought maybe the RMS Layernorm epsilon was wrong - not 1e-5 but maybe 1e-6. For example this has rms_norm_eps=1e-06 , whilst this has rms_norm_eps=1e-05 . We also overrided it, but it did not work: Copy --override-kv qwen2.attention.layer_norm_rms_epsilon=float:0.000001 \\ We also tested if tokenizer IDs matched between llama.cpp and normal Transformers courtesy of @kalomaze . They matched, so this was not the culprit. We provide our experimental results below: 61KB file_BF16_no_samplers.txt BF16 full precision with no sampling fix 55KB file_BF16_yes_samplers.txt BF16 full precision with sampling fix 71KB final_Q4_K_M_no_samplers.txt Q4_K_M precision with no sampling fix 65KB final_Q4_K_M_yes_samplers.txt Q4_K_M precision with sampling fix ✏️ Tokenizer Bug Fixes We found a few issues as well specifically impacting finetuning! The EOS token is correct, but the PAD token should probably rather be &quot;&lt;|vision_pad|&gt; &quot; We updated it in: https://huggingface.co/unsloth/QwQ-32B/blob/main/tokenizer_config.json Copy &quot;eos_token&quot;: &quot;&lt;|im_end|&gt;&quot;, &quot;pad_token&quot;: &quot;&lt;|endoftext|&gt;&quot;, 🛠️ Dynamic 4-bit Quants We also uploaded dynamic 4bit quants which increase accuracy vs naive 4bit quantizations! We attach the QwQ quantization error plot analysis for both activation and weight quantization errors: We uploaded dynamic 4-bit quants to: https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit Since vLLM 0.7.3 (2025 February 20th) https://github.com/vllm-project/vllm/releases/tag/v0.7.3 , vLLM now supports loading Unsloth dynamic 4bit quants! All our GGUFs are at https://huggingface.co/unsloth/QwQ-32B-GGUF ! Previous DeepSeek-R1 Dynamic 1.58-bit Next Phi-4 Reasoning: How to Run &amp; Fine-tune Last updated 1 month ago Was this helpful? Community Reddit r/unsloth Twitter (X) LinkedIn Resources Tutorials Docker Hugging Face Company About Events Contact ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://qwenlm.github.io/blog/qwq-32b/},",
      "full_text": "Title: 404 Page not found\n\nURL Source: https://qwenlm.github.io/blog/qwq-32b/%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 Page not found | Qwen\n\n===============\n\n[![Image 1](https://qwenlm.github.io/img/logo.png)](https://qwenlm.github.io/ \"Qwen (Alt + H)\")\n\n*   [Blog](https://qwenlm.github.io/blog/ \"Blog\")\n*   [Publication](https://qwenlm.github.io/publication \"Publication\")\n*   [About](https://qwenlm.github.io/about \"About\")\n*   [Try Qwen Chat](https://chat.qwen.ai/ \"Try Qwen Chat\")\n\n404\n===\n\nYou will be redirected to home page shortly. If not, please click [here](https://qwenlm.github.io/).\n\n© 2025 [Qwen](https://qwenlm.github.io/)Powered by [Hugo](https://gohugo.io/)[](https://qwenlm.github.io/blog/qwq-32b/%7D,#top \"Go to Top (Alt + G)\")\n",
      "fetch_method": "jina-reader"
    }
  ]
}