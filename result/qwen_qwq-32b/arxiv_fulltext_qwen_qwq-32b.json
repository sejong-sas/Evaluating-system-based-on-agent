{
  "model_id": "qwen/qwq-32b",
  "full_texts": [
    {
      "arxiv_id": "2309.00071",
      "full_text": "YaRN: Efficient Context Window Extension of Large\nLanguage Models\nBowen Peng1∗\nJeffrey Quesnelle1†\nHonglu Fan23\nEnrico Shippole‡\n1Nous Research\n2EleutherAI\n3University of Geneva\nAbstract\nRotary Position Embeddings (RoPE) have been shown to effectively encode posi-\ntional information in transformer-based language models. However, these models\nfail to generalize past the sequence length they were trained on. We present YaRN\n(Yet another RoPE extensioN method), a compute-efficient method to extend the\ncontext window of such models, requiring 10x less tokens and 2.5x less training\nsteps than previous methods. Using YaRN, we show that LLaMA models can\neffectively utilize and extrapolate to context lengths much longer than their original\npre-training would allow, while also surpassing previous the state-of-the-art at\ncontext window extension. In addition, we demonstrate that YaRN exhibits the\ncapability to extrapolate beyond the limited context of a fine-tuning dataset. The\nmodels fine-tuned using YaRN has been made available and reproduced online up\nto 128k context length at https://github.com/jquesnelle/yarn.\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nPerplexity (lower is better)\nCodeLlama-13b-hf\nYarn-Llama-2-13b-64k\nYarn-Llama-2-13b-128k\ntogethercomputer/LLaMA-2-7B-32K\nCodeLlama-7b-hf\nYarn-Llama-2-7b-64k\nYarn-Llama-2-7b-128k\nFigure 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\n∗Reddit: /u/bloc97 GitHub: bloc97\n†Reddit: /u/emozilla X: @theemozilla GitHub: jquesnelle\n‡X: @EnricoShippole GitHub: conceptofmind\nPreprint. Under review.\narXiv:2309.00071v2  [cs.CL]  1 Nov 2023\n\n1\nIntroduction\nTransformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for\nmany natural language processing (NLP) tasks where long-range abilities such as in-context learning\n(ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the\ncontext window) determined by its training processes has been one of the major limits of a pretrained\nLLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or\nwithout fine-tuning) has become more and more desirable. To this end, the position encodings of\ntransformers are the center of the discussions.\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later\nimproved to a learnable absolute position encoding [15]. Since then, relative positional encoding\nschemes [32] have further increased the performance of Transformers. Currently, the most popular\nrelative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].\nOne reoccurring limitation with positional encodings is the inability to generalize past the context\nwindow seen during training. While some methods such as ALiBi are able to do limited generalization,\nnone are able to generalize to sequences significantly longer than their pre-trained length [22].\nSome works have been done to overcome such limitation. [9] and concurrently [21] proposed to\nextend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning\non a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by\ntaking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\"\ninterpolation have been proposed, with different emphasis:\n• the \"Dynamic NTK\" interpolation method [14] for pre-trained models without fine-tuning.\n• the \"NTK-by-parts\" interpolation method [7] which performs the best when fine-tuned on a\nsmall amount of longer-context data.\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their\npresence in the open-source models such as Code Llama [31] (using \"NTK-aware\" interpolation) and\nQwen 7B [2] (using \"Dynamic NTK\").\nIn this paper, in addition to making a complete account of the previous unpublished works on the\n\"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet\nanother RoPE extensioN method), an improved method to efficiently extend the context window\nof models trained with Rotary Position Embeddings (RoPE) including the LLaMA [38], the GPT-\nNeoX [5], and the PaLM [10] families of models.\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less\nthan ∼0.1% of the original pre-training data. In the meantime, by combining with the inference-time\ntechnique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window\nextension without any fine-tuning.\n2\nBackground and Related Work\n2.1\nRotary Position Embeddings\nThe basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]. We work on\na hidden layer where the set of hidden neurons are denoted by D. Given a sequence of vectors\nx1, · · · , xL ∈R|D|, following the notation of [34], the attention layer first converts the vectors into\nthe query vectors and the key vectors:\nqm = fq(xm, m) ∈R|D|, kn = fk(xn, n) ∈R|D|.\n(1)\nNext, the attention weights are calculated as\nsoftmax(qT\nmkn\np\n|D|\n),\n(2)\nwhere qm, kn are considered as column vectors so that qT\nmkn is simply the Euclidean inner product.\nIn RoPE, we first assume that |D| is even and identify the embedding space and the hidden states as\n2\n\ncomplex vector spaces:\nR|D| ∼= C|D|/2\nwhere the inner product qT k becomes the real part of the standard Hermitian inner product Re(q∗k).\nMore specifically, the isomorphisms interleave the real part and the complex part\n\u0000(xm)1, · · · , (xm)|D|\n\u0001\n7→\n\u0000(xm)1 + i(xm)2, · · · , ((xm)|D|−1 + i(xm)|D|)\n\u0001\n,\n(3)\n\u0000(qm)1, · · · , (qm)|D|\n\u0001\n7→\n\u0000(qm)1 + i(qm)2, · · · , ((qm)|D|−1 + i(qm)|D|)\n\u0001\n.\n(4)\nTo convert embeddings xm, xn into query and key vectors, we are first given R-linear operators\nWq, Wk : R|D| →R|D|.\nIn complex coordinates, the functions fq, fk are given by\nfq(xm, m) = eimθWqxm, fk(xn, n) = einθWkxn,\n(5)\nwhere θ = diag(θ1, · · · , θ|D|/2) is the diagonal matrix with θd = b−2d/|D| and b = 10000. This way,\nRoPE associates each (complex-valued) hidden neuron with a separate frequency θd. The benefit\nof doing so is that the dot product between the query vector and the key vector only depends on the\nrelative distance m −n as follows\n⟨fq(xm, m), fk(xn, n)⟩R\n(6)\n=Re(⟨fq(xm, m), fk(xn, n)⟩C)\n(7)\n=Re(x∗\nmW∗\nqWkxneiθ(m−n))\n(8)\n=g(xm, xn, m −n).\n(9)\nIn real coordinates, the RoPE can be written using the following function\nfW(xm, m, θd) =\n\n\n\n\n\n\n\ncosmθ1\n−sinmθ1\n0\n0\n· · ·\n0\n0\nsinmθ1\ncosmθ1\n0\n0\n· · ·\n0\n0\n0\n0\ncosmθ2\n−sinmθ2\n· · ·\n0\n0\n0\n0\nsinmθ2\ncosmθ2\n· · ·\n0\n0\n0\n0\n0\n0\n· · ·\ncosmθl\n−sinmθl\n0\n0\n0\n0\n· · ·\nsinmθl\ncosmθl\n\n\n\n\n\n\n\nWxm,\nso that\nfq = fWq, fk = fWk.\n2.2\nPosition Interpolation\nAs language models are usually pre-trained with a fixed context length, it is natural to ask how to\nextend the context length by fine-tuning on relatively less amount of data. For language models using\nRoPE as the position embedding, Chen et al. [9], and concurrently kaiokendev [21] proposed the\nPosition Interpolation (PI) to extend the context length beyond the pre-trained limit. While a direct\nextrapolation does not perform well on sequences w1, · · · , wL with L larger than the pre-trained\nlimit, they discovered that interpolating the position indicies within the pre-trained limit works well\nwith the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with\nRoPE, they modify the RoPE by\nf ′\nW (xm, m, θd) = fW\n\u0012\nxm, mL\nL′ , θd\n\u0013\n,\n(10)\nwhere L′ > L is a new context window beyond the pre-trained limit. With the original pre-trained\nmodel plus the modified RoPE formula, they fine-tuned the language model further on several orders\nof magnitude fewer tokens (a few billion in Chen et al. [9]) and successfully acheived context window\nextension.\n3\n\n2.3\nAdditional Notation\nThe ratio between the extended context length and the original context length has been of special\nimportance, and we introduce the notation s defined by\ns = L′\nL ,\n(11)\nand we call s the scale factor.\nWe also rewrite and simplify Eq. 10 into the following general form:\nf ′\nW(xm, m, θd) = fW(xm, g(m), h(θd)),\n(12)\nwhere g(m), h(θd) are method-dependent functions. For PI, we have g(m) = m/s, h(θd) = θd. In\nthe subsequent sections, when we introduce a new interpolation method, we sometimes only specify\nthe functions g(m) and h(θd).\nAdditionally, we define λd as the wavelength of the RoPE embedding at d-th hidden dimension:\nλd = 2π\nθd\n= 2πb\n2d\n|D| .\n(13)\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension\nd to perform a full rotation (2π).\nGiven that some interpolation methods (eg. PI) do not care about the wavelength of the dimensions,\nwe will refer to those methods as \"blind\" interpolation methods, while others do (eg. YaRN), which\nwe will classify as \"targeted\" interpolation methods.\n2.4\nRelated work\nReRoPE [33] also aims to extend the context size of existing models pre-trained with RoPE, and claims\n\"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically\ndecreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves\ncontext extension by modifying the attention mechanism and thus is not purely an embedding\ninterpolation method. Since it is currently not compatible with Flash Attention 2 [13] and requires\ntwo attention passes during inference, we do not consider it for comparison.\nConcurrently with our work, LM-Infinite [16] proposes similar ideas to YaRN, but focuses on \"on-the-\nfly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism\nof the models, it is not an embedding interpolation method and is not immediately compatible with\nFlash Attention 2.\n3\nMethodology\nWhereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound\ndescribed by PI [9] is insufficient at predicting the complex dynamics between RoPE and the LLM’s\ninternal embeddings. In the following subsections, we describe the main issues with PI we have\nindividually identified and solved, so as to give the readers the context, origin and justifications of\neach method which we use in concert to obtain the full YaRN method.\n3.1\nLoss of High Frequency information - \"NTK-aware\" interpolation\nIf we look at RoPE only from an information encoding perspective, it was shown in [36], using\nNeural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency\ninformation if the input dimension is low and the corresponding embeddings lack high frequency\ncomponents. Here we can see the similarities: a token’s positional information is one-dimensional,\nand RoPE expands it to an n-dimensional complex vector embedding.\n4\n\nRoPE closely resembles Fourier Features [36] in many aspects, as it is possible to define RoPE as a\nspecial 1D case of a Fourier Feature. Stretching the RoPE embeddings indiscriminately results in the\nloss of important high frequency details which the network needs in order to resolve tokens that are\nboth very similar and very close together (the rotation describing the smallest distance needs to not\nbe too small for the network to be able to detect it).\nWe hypothesise that the slight increase of perplexity for short context sizes after fine-tuning on larger\ncontext sizes seen in PI [9] might be related to this problem. Under ideal circumstances, there is no\nreason that fine-tuning on larger context sizes should degrade the performance of smaller context\nsizes.\nIn order to resolve the problem of losing high frequency information when interpolating the RoPE\nembeddings, the \"NTK-aware\" interpolation was developed in [6]. Instead of scaling every dimension\nof RoPE equally by a factor s, we spread out the interpolation pressure across multiple dimensions\nby scaling high frequencies less and low frequencies more. One can obtain such a transformation in\nmany ways, but the simplest would be to perform a base change on the value of θ.\nMore precisely, following the notations set out in Section 2.3, we define the \"NTK-aware\" interpola-\ntion scheme as follows (see the Appendix A.1 for the details of the deduction).\nDefinition 1 The \"NTK-aware\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions.\ng(m) = m\n(14)\nh(θd) = b′−2d/|D|,\n(15)\nwhere\nb′ = b · s\n|D|\n|D|−2 .\n(16)\nGiven the results from [6], this method performs much better at extending the context size of non-fine-\ntuned models compared to PI [9]. However, one major disadvantage of this method is that given it is\nnot just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values,\nthus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore,\ndue to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true\ncontext extension scale. In practice, the scale value s has to be set higher than the expected scale for\na given context length extension.\nWe note that shortly before the release of this article, Code Llama [31] was released and uses\n\"NTK-aware\" scaling by manually scaling the base b to 1M.\n3.2\nLoss of Relative Local Distances - \"NTK-by-parts\" interpolation\nIn the case of blind interpolation methods like PI and \"NTK-aware\" interpolation, we treat all the\nRoPE hidden dimensions equally (as in they have the same effect on the network). However, there\nare strong clues that point us towards the need for targeted interpolation methods.\nIn this section, we think heavily in terms of the wavelengths λd defined in Eq. 13 in the formula of\nRoPE. For simplicity, we omit the subscript d in λd and the reader is encouraged to think about λ as\nthe wavelength of an arbitrary periodic function.\nOne interesting observation of RoPE embeddings is that given a context size L, there are some\ndimensions d where the wavelength is longer than the maximum context length seen during pretraining\n(λ > L), this suggests that some dimensions’ embeddings might not be distributed evenly in the\nrotational domain. In such cases, we presume having all unique position pairs implies that the\nabsolute positional information remains intact. On the contrary, when the wavelength is short, only\nrelative positional information is accessible to the network.\nMoreover, when we stretch all the RoPE dimensions either by a scale s or using a base change b′,\nall tokens become closer to each other, as the dot product of two vectors rotated by a lesser amount\nis bigger. This scaling severely impairs a LLM’s ability to understand small and local relationships\nbetween its internal embeddings. We hypothesize that such compression leads to the model being\nconfused on the positional order of close-by tokens, and consequently harming the model’s abilities.\n5\n\nIn order to remedy this issue, given the two previous observations that we have found, we choose not\nto interpolate the higher frequency dimensions at all while always interpolating the lower frequency\ndimensions. In particular,\n• if the wavelength λ is much smaller than the context size L, we do not interpolate;\n• if the wavelength λ is equal to or bigger than the context size L, we want to only interpolate\nand avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n• dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\nAs a result, it is more convenient to introduce the ratio r = L\nλ between the original context size L and\nthe wavelength λ. In the d-th hidden state, the ratio r depends on d in the following way:\nr(d) = L\nλd\n=\nL\n2πb′ 2d\n|D| .\n(17)\nIn order to define the boundary of the different interpolation strategies as above, we introduce\ntwo extra parameters α, β. All hidden dimensions d where r(d) < α are those where we linearly\ninterpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r(d) > β are\nthose where we do not interpolate at all. Define the ramp function γ to be\nγ(r) =\n\n\n\n\n\n\n\n0,\nif r < α\n1,\nif r > β\nr −α\nβ −α,\notherwise.\n(18)\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions4.\ng(m) = m\n(19)\nh(θd) =\n\u0010\n1 −γ\n\u0000r(d)\n\u0001\u0011θd\ns + γ\n\u0000r(d)\n\u0001\nθd.\n(20)\nThe values of α and β should be tuned on a case-by-case basis. For example, we have found\nexperimentally that for the Llama family of models, good values for α and β are α = 1 and β = 32.\nUsing the techniques described in this section, a variant of the resulting method was released under\nthe name \"NTK-by-parts\" interpolation [7]. This improved method performs better than the previous\nPI [9] and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with\nfine-tuned models, as shown in [7].\n3.3\nDynamic Scaling - \"Dynamic NTK\" interpolation\nIn a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to\nthe maximal context size. A typical example is the autoregressive generation where the sequence\nlengths increment by 1 after each step. There are two ways of applying an interpolation method that\nuses a scale factor s (including PI, \"NTK-aware\" and \"NTK-by-parts\"):\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale\nfactor s = L′/L where L′ is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max(1, l′/L)\nwhere l′ is the sequence length of the current sequence.\nThe problem of (1) is that the model may experience a performance discount at a length less than\nL and an abrupt degradation when the sequence length is longer than L′. But by doing Dynamic\n4The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over θd/s and\nθd converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of\nimplementation, but both would work.\n6\n\nScaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting\nthe trained context limit L′. We call this inference-time method the Dynamic Scaling method. When\nit is combined with \"NTK-awared\" interpolation, we call it \"Dynamic NTK\" interpolation. It first\nappeared in public as a reddit post in [14].\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pre-\ntrained on L without any finetuning (L′ = L). This is supported by the experiment in Appendix B.3.\nOften in the repeated forward-passes, the kv-caching [8] is applied so that we can reuse the previous\nkey-value vectors and improve the overall efficiency. We point out that in some implementations when\nthe RoPE embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling\nwith kv-caching. The correct implementation should cache the kv-embeddings before applying RoPE,\nas the RoPE embedding of every token changes when s changes.\n3.4\nYaRN\nIn addition to the previous interpolation techniques, we also observe that introducing a temperature t\non the logits before the attention softmax has a uniform impact on perplexity regardless of the data\nsample and the token position over the extended context window (See Appendix A.2). More precisely,\ninstead of Eq. 2, we modify the computation of attention weights into\nsoftmax\n \nqT\nmkn\nt\np\n|D|\n!\n.\n(21)\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of\nthis attention scaling: we can instead use a \"length scaling\" trick which scales both qm and kn by a\nconstant factor\np\n1/t by simply scaling the complex RoPE embeddings by the same amount. With\nthis, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it\nhas zero overhead during both inference and training, as RoPE embeddings are generated in advance\nand are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have\nthe YaRN method.\nDefinition 3 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 21 and\nthe \"NTK-by-parts\" interpolation introduced in Section 3.2.\nFor LLaMA and Llama 2 models, we recommend the following values:\nr\n1\nt = 0.1 ln(s) + 1.\n(22)\nThe equation above is found by fitting\np\n1/t at the lowest perplexity against the scale extension\nby various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and\n65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama\n2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature\nconstant t may have certain degree of \"universality\" and may be generalizable across some models\nand training data.\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned\nand non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with\nlibraries that modify the attention mechanism such as Flash Attention 2 [13].\n4\nExperiments\nWe show that YaRN successfully achieves context window extension of language models using RoPE\nas its position embedding. Moreover, this result is achieved with only 400 training steps, representing\napproximately 0.1% of the model’s original pre-training corpus, a 10x reduction from Rozière et al.\n[31] and 2.5x reduction in training steps from Chen et al. [9], making it highly compute-efficient for\ntraining with no additional inference costs. We calculate the perplexity of long documents and score\n7\n\non established benchmarks to evaluate the resulting models, finding that they surpass all other context\nwindow extension methods.\nWe broadly followed the training and evaluation procedures as outlined in [9].\n4.1\nTraining\nFor training, we extended the Llama 2 [39] 7B and 13B parameter models. No changes were made to\nthe LLaMA model architecture other than the calculation of the embedding frequencies as described\nin 3.4 with s = 16 and s = 32.\nWe used a learning rate of 2 × 10−5 with no weight decay and a linear warmup of 20 steps along with\nAdamW [24] β1 = 0.9 and β2 = 0.95. For s = 16 we fine-tuned for 400 steps with global batch\nsize 64 using PyTorch [26] Fully Sharded Data Parallelism [42] and Flash Attention 2 [13] on the\nPG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token. For s = 32\nwe followed the same procedure, but started from the finished s = 16 checkpoint and trained for an\nadditional 200 steps.\n4.2\nExtrapolation and Transfer Learning\nIn Code Llama [31], a dataset with 16k context was used with a scale factor set to s ≈88.6, which\ncorresponds to a context size of 355k. They show that the network extrapolates up to 100k context\nwithout ever seeing those context sizes during training. Similar to 3.1 and Rozière et al. [31], YaRN\nalso supports training with a higher scale factor s than the length of the dataset. Due to compute\nconstraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same\ndataset with 64k context.\nWe show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k\ncontext during training. Unlike previous \"blind\" interpolation methods, YaRN is much more efficient\nat transfer learning when increasing the scale s. This demonstrates successful transfer learning from\ns = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32\nmodel is equivalent to the s = 16 model across the entire context size, despite only being trained on\ns = 32 for 200 steps.\n4.3\nEvaluation\nThe evaluations focus on three aspects:\n1. the perplexity scores of fine-tuned models with extended context window,\n2. the passkey retrieval task on fine-tuned models,\n3. the common LLM benchmark results of fine-tuned models,\n4.3.1\nLong Sequence Language Modeling\nTo evaluate the long sequence language modeling performances, we use the GovReport [18] and\nProof-pile [4] datasets both of which contain many long sequence samples. For all evaluations, the\ntest splits of both datasets were used exclusively. All perplexity evaluations were calculated using the\nsliding window method from Press et al. [27] with S = 256.\nFirstly, we evaluated how the model performed as the context window increased. We selected 10\nrandom samples from Proof-pile with at least 128k tokens each and evaluated the perplexity of each\nof these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens.\nTable 1 shows a side-by-side comparison of Llama-2 model extended from 4096 to 8192 context\nlength via PI (LLongMA-2 7b5), \"NTK-aware\" and YaRN. Note that PI and \"NTK-aware\" models\nwere trained using the methodology in Chen et al. [9], while YaRN used the same methodology but\n2.5x less training steps and data, as described in 4.\n5LLongMA-2 7b [28] is fine-tuned from Llama-2 7b, trained at 8k context length with PI using the RedPajama\ndataset [12].\n8\n\nExtension\nTrained\nContext\nEvaluation Context Window Size\nMethod\nTokens\nWindow\n2048\n4096\n6144\n8192\n10240\nPI (s = 2)\n1B\n8k\n3.92\n3.51\n3.51\n3.34\n8.07\nNTK (θ = 20k)\n1B\n8k\n4.20\n3.75\n3.74\n3.59\n6.24\nYaRN (s = 2)\n400M\n8k\n3.91\n3.50\n3.51\n3.35\n6.04\nTable 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents over Llama-2 extended via PI,\nNTK and YaRN\nWe further evaluated YaRN at the scale factor s = 16, 32 and compared them against a few open-\nsource models fine-tuned from Llama-2 and extended to more than 32k context window such as\nTogether.ai [37] and \"NTK-aware\" Code Llama [31]. The results are summarized in Table 2 (with a\nmore detailed plot in Figure 1).\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n8192\n32768\n65536\n98304\n131072\n7B\nTogether\n32k\nPI\n3.50\n2.64\n> 102\n> 103\n> 104\n7B\nCode Llama\n100k\nNTK\n3.71\n2.74\n2.55\n2.54\n2.71\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.51\n2.65\n2.42\n> 101\n> 101\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.56\n2.70\n2.45\n2.36\n2.37\n13B\nCode Llama\n100k\nNTK\n3.54\n2.63\n2.41\n2.37\n2.54\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.25\n2.50\n2.29\n> 101\n> 101\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.29\n2.53\n2.31\n2.23\n2.24\nTable 2: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\nWe observe that the model exhibits strong performance across the entire targeted context size, with\nYaRN interpolation being the first method to successfully extend the effective context size of Llama 2\nto 128k. Of particular note are the YaRN (s = 32) models, which show continued declining perplexity\nthrough 128k, despite the fine-tuning data being limited to 64k tokens in length, demonstrating that\nthe model is able to generalize to unseen context lengths.\nFurthermore, in Appendix B.1, we show the results of the average perplexity on 50 untruncated\nGovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal\ncontext window without Dynamic Scaling in Table 4. Similar to the Proof-pile results, the GovReport\nresults show that fine-tuning with YaRN achieves good performance on long sequences.\n4.3.2\nPasskey Retrieval\nThe passkey retrieval task as defined in [25] measures a model’s ability to retrieve a simple passkey\n(i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our\nevaluation of the models, we performed 10 iterations of the passkey retrieval task with the passkey\nplaced at a random location uniformly distributed across the evaluation context window on different\ncontext window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at\n128k context size passes the passkey retrieval task with very high accuracy (> 99%) within the entire\ncontext window size. We show detailed results in Appendix B.2.\n4.3.3\nStandardized Benchmarks\nThe Hugging Face Open LLM Leaderboard [19] compares a multitude of LLMs across a standard-\nized set of four public benchmarks. Specifically, we use 25-shot ARC-Challenge [11], 10-shot\nHellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23].\nTo test the degradation of model performance under context extension, we evaluated our models\nusing this suite and compared it to established scores for the Llama 2 baselines as well as publicly\navailable PI and \"NTK-aware\" models. The results are summarized in Table 3.\n9\n\nModel\nModel\nContext\nExtension\nARC-c\nHellaswag\nMMLU\nTruthfulQA\nSize\nName\nWindow\nMethod\n7B\nLlama 2\n4k\nNone\n53.1\n77.8\n43.8\n39.0\n7B\nTogether\n32k\nPI\n47.6\n76.1\n43.3\n39.2\n7B\nCode Llama\n100k\nNTK\n39.9\n60.8\n31.1\n37.8\n7B\nYaRN (s = 16)\n64k\nYaRN\n52.3\n78.8\n42.5\n38.2\n7B\nYaRN (s = 32)\n128k\nYaRN\n52.1\n78.4\n41.7\n37.3\n13B\nLlama 2\n4k\nNone\n59.4\n82.1\n55.8\n37.4\n13B\nCode Llama\n100k\nNTK\n40.9\n63.4\n32.8\n43.8\n13B\nYaRN (s = 16)\n64k\nYaRN\n58.1\n82.3\n52.8\n37.8\n13B\nYaRN (s = 32)\n128k\nYaRN\n58.0\n82.2\n51.9\n37.3\nTable 3: Performance of context window extensions methods on the Hugging Face Open LLM benchmark suite\ncompared with original Llama 2 baselines\nWe observe that there is minimal performance degradation between the YaRN models and their\nrespective Llama 2 baselines. We also observe that there was on average a 0.49% drop in scores\nbetween the YaRN s = 16 and s = 32 models. From this we conclude that the the iterative extension\nfrom 64k to 128k results in negligible performance loss.\n5\nConclusion\nIn conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods\nand can act as a drop-in replacement to PI, with no downsides and minimal implementation effort.\nThe fine-tuned models preserve their original abilities on multiple benchmarks while being able\nto attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with fine-\ntuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of\nwhich are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of\nextrapolation with YaRN where it is able to \"train short, and test long\".\n10\n\n6\nReproducibility\nTo aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to\ntrain the YaRN models in Table 2, as well as the evaluation code that produced Figure 1 and Tables\n1, 2, 3, 4, and 5. The code also contains implementations of various extension methods referenced\nthroughout the paper. For training YaRN, we used the publicly available PG19 dataset [29] tokenized\nto 64k tokens.\nReferences\n[1] Mistrallite. URL https://huggingface.co/amazon/MistralLite.\n[2] Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts).\nURL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md.\n[3] Long-data collections. URL https://huggingface.co/datasets/togethercomputer/\nLong-Data-Collections.\n[4] Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/\nzhangir-azerbayev/proof-pile.\n[5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,\nK. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang,\nand S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022.\narXiv: 2204.06745.\n[6] bloc97.\nNTK-Aware\nScaled\nRoPE\nallows\nLLaMA\nmodels\nto\nhave\nextended\n(8k+)\ncontext\nsize\nwithout\nany\nfine-tuning\nand\nminimal\nperplexity\ndegradation.,\n2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\n[7] bloc97. Add NTK-Aware interpolation \"by parts\" correction, 2023. URL https://github.\ncom/jquesnelle/scaled-rope/pull/1.\n[8] C. Chen.\nTransformer Inference Arithmetic, 2022.\nURL https://kipp.ly/blog/\ntransformer-inference-arithmetic/.\n[9] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models\nvia positional interpolation, 2023. arXiv: 2306.15595.\n[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,\nS. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,\nD. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,\nA. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck,\nJ. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022.\narXiv: 2204.02311.\n[11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv:\n1803.05457.\n[12] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\n[13] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\narXiv: 2307.08691.\n11\n\n[14] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA\nwith zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/\n14mrgpr/dynamically_scaled_rope_further_increases/.\n[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to\nsequence learning, 2017. arXiv: 1705.03122.\n[16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length\ngeneralization for large language models, 2023. arXiv: 2308.16137.\n[17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n[18] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efficient attentions for long document\nsummarization. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1419–1436.\nAssociation for Computational Linguistics, June 2021.\n[19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\n[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n[21] kaiokendev. Things I’m learning while training superhot., 2023. URL https://kaiokendev.\ngithub.io/til#extending-context-to-8k.\n[22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional\nencoding on length generalization in transformers, 2023. arXiv: 2305.19466.\n[23] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, May 2022.\n[24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference\non Learning Representations, 2019.\n[25] A. Mohtashami and M. Jaggi. Landmark attention: Random-access infinite context length for\ntransformers, 2023. arXiv: 2305.16300.\n[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,\nhigh-performance deep learning library. In NeurIPS, pages 8024–8035, 2019.\n[27] O. Press, N. Smith, and M. Lewis. Train Short, Test Long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022.\n[28] J. Quesnelle, E. Shippole, and \"Kaiokendev\". Llongma: Scaling rotary embeddings through lin-\near positional interpolation. https://huggingface.co/conceptofmind/LLongMA-2-7b/,\n2023.\n[29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning\nRepresentations, 2020.\n[30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y. Zhou.\nExploring the limits of transfer learning with a unified text-to-text transformer. Technical report,\nGoogle, 2019.\n12\n\n[31] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,\nJ. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,\nA. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve.\nCode Llama: Open foundation models for code, 2023. arXiv: 2308.12950.\n[32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\n464–468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n[33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.\n[34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with\nrotary position embedding, 2022. arXiv: 2104.09864.\n[35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A\nlength-extrapolatable transformer, 2022. arXiv: 2212.10554.\n[36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ra-\nmamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions\nin low dimensional domains. In Proceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\nISBN 9781713829546.\n[37] Together.ai.\nLLaMA-2-7B-32K,\n2023.\nURL\nhttps://huggingface.co/\ntogethercomputer/LLaMA-2-7B-32K.\n[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and\nefficient foundation language models, 2023. arXiv: 2302.13971.\n[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.\n[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really\nfinish your sentence?\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\n[42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott,\nS. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch\nFSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277.\n13\n\nA\nAdditional details on interpolation methods\nA.1\nShort notes on the deduction of \"NTK-aware\" interpolation\nIn Section 3.1, we introduce a change of basis from b to b′ in the definition of \"NTK-aware\"\ninterpolation method. Here is a short note on its mathematical deduction.\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a\nbase-change instead of scaling the frequencies by a fixed factor s. The property we want to guarantee\nis that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest\nfrequency to stay constant.\nWe introduce a new base b′ such that the last dimension matches the wavelength of linear interpolation\nwith a scale factor s. Since the original RoPE method skips odd dimensions in order to concatenate\nboth cos( 2πx\nλ ) and sin( 2πx\nλ ) components into a single embedding, the last dimension d ∈D is\n|D| −2.\nThe new base b′ can be chosen so that\nb′\n|D|−2\n|D|\n= s · b\n|D|−2\n|D| .\n(23)\nSolving for b′ yields\nb′ = b · s\n|D|\n|D|−2 .\n(24)\nA.2\nThe impact of pre-softmax scaling of YaRN on perplexity\nIn Section 3.4, we mention the impact of the factor t inside the softmax computation of attention\nweights. Here we fix 896 16k-token documents from RedPajama [12]6, and calculate their perplexity\nscores with different scaling 1/\n√\nt. The result is in Figure 2. For comparison, recall that our\nrecommended factor in this case (s = 8) is given by the following.\nr\n1\nt = 0.1 ln(s) + 1 ≈1.208.\n(25)\n6We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far\nas we are aware of.\n14\n\nTo show the impact of the factor 1/\n√\nt on different token positions, we cut each 16k-token document\ninto chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in\npercentages\nppl(t) −ppl(t = 1)\nppl(t = 1)\n(26)\nof each chunk. The plot is shown in Figure 3.\nTo further demonstrate the best values of t across all samples over different token positions, we plot\nthe sample counts with minimal perplexity at a given 1/\n√\nt for each of the 8 position segments over\nthe 16k-token range in Figure 4.\nWe observe that:\n• for a suitable t, a sample may obtain better perplexity scores across the extended context\nwindow;\n• the best value of t is mostly consistent across different samples and different positions.\nWe remark that this finding is consistent for different values of s and the best value of t follows our\nrecommended formula (Eq. 22) closely.\nB\nAdditional tables and charts\nB.1\nGovReport evaluations\nIn Section 4.3.1, we mention the evaluation on GovReport documents. The evaluation results are\ndetailed in Table 4 below.\nB.2\nPasskey Retrieval\nHere we can observe that the lowest perplexity point alone does not provide a comprehensive depiction\non the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits\nincreasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at\na context length of 128k. This suggest that while the output of Code Llama might start to degrade in\nquality above 100k context size, it is still able to maintain strong retrieval capabilities.\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while\nhaving a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a\ngreat indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine\nlong context performance. This also suggests that the YaRN models with s = 16 might be relatively\nundertrained for the passkey retrieval task.\nB.3\nDynamic scaling on models without any fine-tuning\nWe first recall from Section 3.3 that the Dynamic Scaling technique is an inference-time technique\nthat dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN.\nWe choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding\nwindow of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN. Since the original maximal\ncontext length of Llama 2 is 4096, we observe that Dynamic Scaling effectively extend the inference\nlength and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in\nFigure 5.\nWe see that\n• Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained\ncontext window;\n• Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained\nLlama-2 without any finetuning.\n15\n\nFigure 2: Fix s = 8, compare the LLaMA 7b perplexity on 896 16k-token documents over different scaling\n1/\n√\nt. The shaded area represents 1 standard deviation (68%).\nFigure 3: Fix s = 8, compare the mean of perplexity change percentages ppl(t) −ppl(t = 1)\nppl(t = 1)\nat different\nsegments of token positions on 896 16k-token documents over different scaling 1/\n√\nt.\nModel\nModel\nContext\nExtension\nPerplexity\nSize\nName\nWindow\nMethod\n7B\nTogether\n32k\nPI\n3.67\n7B\nCode Llama\n100k\nNTK\n4.44\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.59\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.64\n13B\nCode Llama\n100k\nNTK\n4.22\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.35\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.39\nTable 4: Sliding window perplexity (S = 256) of 50 long GovReport documents with a fixed context window\nsize of 32k\n16\n\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1/\n√\nt for a given\nsegment of token positions over the 16k-token range.\nModel\nModel\nScaling\nContext\nTraining\nExtension\nPasskey\nPasskey\nSize\nName\nFactor (s)\nWindow\nData Context\nMethod\nContext\nAccuracy\n7B\nTogether\n4\n32k\n32k\nPI\n32k\n100%\n7B\nCode Llama\n88.6\n100k\n16k\nNTK\n112k\n94.3%\n7B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n96.3%\n7B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\n13B\nCode Llama\n88.6\n100k\n16k\nNTK\n128k\n99.4%\n13B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n97.5%\n13B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\nTable 5: Passkey retrieval performance of various models. The passkey context denotes the maximum tested\ncontext window size where the accuracy of passkey retrieval was >= 80%, and the passkey accuracy is the\naverage accuracy of passkey retrieval on all context sizes tested that were smaller or equal than the passkey\ncontext size.\nFigure 5: The comparison between RoPE, Dynamic-PI and Dynamic-YaRN using Llama 2 on a long\nGovReport sample. This model has not been finetuned for long context.\n17\n\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\nPerplexity (lower is better)\nYarn-Mistral-7b-64k\nYarn-Mistral-7b-128k\namazon/MistralLite\nmistralai/Mistral-7B-v0.1\nFigure 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\nB.4\nMistral\nWe additionally extended the Mistral 7B v0.1 model [20], which broadly follows the Llama architec-\nture. For Mistral we trained a 64k context window model (s = 8) for 1000 steps using 16k sequence\nlengths with a constant learning rate of 1 × 10−6. The model’s sliding window attention size was set\nto the context window size, effectively disabling sliding window attention. We then trained for an\nadditional 500 steps at s = 16 to arrive at a 128k context window model. The training data was a mix\nof the pre-train and fine-tune splits of Together Computer’s Long-Data Collections [3].\nWe evaluated the models following the same procedure as described in 4.3.1, comparing against the\nbase v0.1 model and MistralLite [1], an NTK-aware (θ = 1M) version of v0.1. The results (Figure 6\nand Table 6) were consistent with those of the Llama family of models.\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n4096\n8192\n16384\n65536\n131072\n7B\nMistral v0.1\n8k\n-\n3.09\n2.96\n36.8\n> 103\n> 103\n7B\nMistralLite\n16k\nNTK\n3.26\n3.13\n47.3\n> 103\n> 103\n7B\nYaRN (s = 8)\n64k\nYaRN\n3.18\n3.04\n2.65\n2.20\n57.4\n7B\nYaRN (s = 16)\n128k\nYaRN\n3.21\n3.08\n2.68\n2.24\n2.19\nTable 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\n18\n"
    },
    {
      "arxiv_id": "2412.15115",
      "full_text": "2025-01-06\nQwen2.5 Technical Report\nQwen Team\nhttps://huggingface.co/Qwen\nhttps://modelscope.cn/organization/qwen\nhttps://github.com/QwenLM/Qwen2.5\nAbstract\nIn this report, we introduce Qwen2.5, a comprehensive series of large language models\n(LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has\nbeen significantly improved during both the pre-training and post-training stages. In\nterms of pre-training, we have scaled the high-quality pre-training datasets from the\nprevious 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for\ncommon sense, expert knowledge, and reasoning capabilities. In terms of post-training,\nwe implement intricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning, including offline learning DPO and online learning\nGRPO. Post-training techniques significantly enhance human preference, and notably\nimprove long text generation, structural data analysis, and instruction following.\nTo handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich\nconfigurations. The open-weight offerings include base models and instruction-tuned\nmodels in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Quantized versions\nof the instruction-tuned models are also provided. Over 100 models can be accessed\nfrom Hugging Face Hub, ModelScope, and Kaggle. In addition, for hosted solutions, the\nproprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-\nTurbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio.\nQwen2.5 has demonstrated top-tier performance on a wide range of benchmarks eval-\nuating language understanding, reasoning, mathematics, coding, human preference\nalignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms\na number of open and proprietary models and demonstrates competitive performance to\nthe state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times\nlarger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while perform-\ning competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the\nfoundation, Qwen2.5 models have been instrumental in training specialized models such\nas Qwen2.5-Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), QwQ (Qwen\nTeam, 2024d), and multimodal models.\n3T\n7T\n18T\ntokens\nMath\nMBPP\nBBH\nMMLU\nQwen1.5-72B\nQwen2-72B\nQwen2.5-72B\nFigure 1: In the iterative development of the Qwen series, data scaling has played a crucial role. Qwen 2.5,\nwhich leverages 18 trillion tokens for pre-training, has demonstrated the most advanced capabilities\nwithin the Qwen series, especially in terms of domain expertise, underscoring the importance of scale\ntogether with mixture in enhancing the model’s capabilities.\n1\narXiv:2412.15115v2  [cs.CL]  3 Jan 2025\n\n1\nIntroduction\nThe sparks of artificial general intelligence (AGI) are increasingly visible through the fast development\nof large foundation models, notably large language models (LLMs) (Brown et al., 2020; OpenAI, 2023;\n2024a; Gemini Team, 2024; Anthropic, 2023a;b; 2024; Bai et al., 2023; Yang et al., 2024a; Touvron et al.,\n2023a;b; Dubey et al., 2024). The continuous advancement in model and data scaling, combined with\nthe paradigm of large-scale pre-training followed by high-quality supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), has enabled large language\nmodels (LLMs) to develop emergent capabilities in language understanding, generation, and reasoning.\nBuilding on this foundation, recent breakthroughs in inference time scaling, particularly demonstrated\nby o1 (OpenAI, 2024b), have enhanced LLMs’ capacity for deep thinking through step-by-step reasoning\nand reflection. These developments have elevated the potential of language models, suggesting they may\nachieve significant breakthroughs in scientific exploration as they continue to demonstrate emergent\ncapabilities indicative of more general artificial intelligence.\nBesides the fast development of model capabilities, the recent two years have witnessed a burst of open\n(open-weight) large language models in the LLM community, for example, the Llama series (Touvron\net al., 2023a;b; Dubey et al., 2024), Mistral series (Jiang et al., 2023a; 2024a), and our Qwen series (Bai\net al., 2023; Yang et al., 2024a; Qwen Team, 2024a; Hui et al., 2024; Qwen Team, 2024c; Yang et al.,\n2024b). The open-weight models have democratized the access of large language models to common\nusers and developers, enabling broader research participation, fostering innovation through community\ncollaboration, and accelerating the development of AI applications across diverse domains.\nRecently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the open-\nweight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B,\n14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized\nmodels in different precisions. Specifically, the flagship model Qwen2.5-72B-Instruct demonstrates\ncompetitive performance against the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is\naround 5 times larger. Additionally, we also release the proprietary models of Mixture-of-Experts (MoE,\nLepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022), namely Qwen2.5-Turbo and Qwen2.5-Plus1,\nwhich performs competitively against GPT-4o-mini and GPT-4o respectively.\nIn this technical report, we introduce Qwen2.5, the result of our continuous endeavor to create better\nLLMs. Below, we show the key features of the latest version of Qwen:\n• Better in Size: Compared with Qwen2, in addition to 0.5B, 1.5B, 7B, and 72B models, Qwen2.5\nbrings back the 3B, 14B, and 32B models, which are more cost-effective for resource-limited\nscenarios and are under-represented in the current field of open foundation models. Qwen2.5-\nTurbo and Qwen2.5-Plus offer a great balance among accuracy, latency, and cost.\n• Better in Data: The pre-training and post-training data have been improved significantly. The\npre-training data increased from 7 trillion tokens to 18 trillion tokens, with focus on knowledge,\ncoding, and mathematics. The pre-training is staged to allow transitions among different mixtures.\nThe post-training data amounts to 1 million examples, across the stage of supervised finetuning\n(SFT, Ouyang et al., 2022), direct preference optimization (DPO, Rafailov et al., 2023), and group\nrelative policy optimization (GRPO, Shao et al., 2024).\n• Better in Use: Several key limitations of Qwen2 in use have been eliminated, including larger\ngeneration length (from 2K tokens to 8K tokens), better support for structured input and output,\n(e.g., tables and JSON), and easier tool use. In addition, Qwen2.5-Turbo supports a context length\nof up to 1 million tokens.\n2\nArchitecture & Tokenizer\nBasically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B\n/ 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus.\nBelow, we provide details about the architecture of models.\nFor dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford\net al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components:\nGrouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation\nfunction (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su\n1Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx\n(to be released) in the API.\n2\n\nTable 1: Model architecture and license of Qwen2.5 open-weight models.\nModels\nLayers\nHeads (Q / KV)\nTie Embedding\nContext / Generation Length\nLicense\n0.5B\n24\n14 / 2\nYes\n32K / 8K\nApache 2.0\n1.5B\n28\n12 / 2\nYes\n32K / 8K\nApache 2.0\n3B\n36\n16 / 2\nYes\n32K / 8K\nQwen Research\n7B\n28\n28 / 4\nNo\n128K / 8K\nApache 2.0\n14B\n48\n40 / 8\nNo\n128K / 8K\nApache 2.0\n32B\n64\n40 / 8\nNo\n128K / 8K\nApache 2.0\n72B\n80\n64 / 8\nNo\n128K / 8K\nQwen\net al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and\nRMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training.\nBuilding upon the dense model architectures, we extend it to MoE model architectures. This is achieved\nby replacing standard feed-forward network (FFN) layers with specialized MoE layers, where each layer\ncomprises multiple FFN experts and a routing mechanism that dispatches tokens to the top-K experts.\nFollowing the approaches demonstrated in Qwen1.5-MoE (Yang et al., 2024a), we implement fine-grained\nexpert segmentation (Dai et al., 2024) and shared experts routing (Rajbhandari et al., 2022; Dai et al., 2024).\nThese architectural innovations have yielded substantial improvements in model performance across\ndownstream tasks.\nFor tokenization, we utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair\nencoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary of 151,643\nregular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen\nversions, adding two new tokens for tool functionality and allocating the remainder for other model\ncapabilities. This expansion establishes a unified vocabulary across all Qwen2.5 models, enhancing\nconsistency and reducing potential compatibility issues.\n3\nPre-training\nOur language model pre-training process consists of several key components. First, we carefully curate\nhigh-quality training data through sophisticated filtering and scoring mechanisms, combined with\nstrategic data mixture. Second, we conduct extensive research on hyperparameter optimization to\neffectively train models at various scales. Finally, we incorporate specialized long-context pre-training\nto enhance the model’s ability to process and understand extended sequences. Below, we detail our\napproaches to data preparation, hyperparameter selection, and long-context training.\n3.1\nPre-training Data\nQwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor\nQwen2. These improvements stem from several key aspects:\n(1) Better data filtering. High-quality pre-training data is crucial for model performance, mak-\ning data quality assessment and filtering a critical component of our pipeline. We leverage\nQwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional\nanalysis to evaluate and score training samples. The filtering method represents a significant\nadvancement over our previous approach used for Qwen2, as it benefits from Qwen2’s expanded\npre-training on a larger multilingual corpus. The enhanced capabilities enable more nuanced\nquality assessment, resulting in both improved retention of high-quality training data and more\neffective filtering of low-quality samples across multiple languages.\n(2) Better math and code data. During the pre-training phase of Qwen2.5, we incorporate training\ndata from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024). This data\nintegration strategy proves highly effective, as these specialized datasets are instrumental in\nachieving state-of-the-art performance on mathematical and coding tasks. By leveraging these\nhigh-quality domain-specific datasets during pre-training, Qwen2.5 inherits strong capabilities\nin both mathematical reasoning and code generation.\n(3) Better synthetic data. To generate high-quality synthetic data, particularly in mathematics, code,\nand knowledge domains, we leverage both Qwen2-72B-Instruct (Yang et al., 2024a) and Qwen2-\nMath-72B-Instruct (Qwen Team, 2024c). The quality of this synthesized data is further enhanced\nthrough rigorous filtering using our proprietary general reward model and the specialized\nQwen2-Math-RM-72B (Qwen Team, 2024c) model.\n3\n\n(4) Better data mixture. To optimize the pre-training data distribution, we employ Qwen2-Instruct\nmodels to classify and balance content across different domains. Our analysis revealed that\ndomains like e-commerce, social media, and entertainment are significantly overrepresented\nin web-scale data, often containing repetitive, template-based, or machine-generated content.\nConversely, domains such as technology, science, and academic research, while containing higher-\nquality information, are traditionally underrepresented. Through strategic down-sampling of\noverrepresented domains and up-sampling of high-value domains, we ensure a more balanced\nand information-rich training dataset that better serves our model’s learning objectives.\nBuilding on these techniques, we have developed a larger and higher-quality pre-training dataset,\nexpanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens.\n3.2\nScaling Law for Hyper-parameters\nWe develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al.,\n2022; Kaplan et al., 2020). While previous studies (Dubey et al., 2024; Almazrouei et al., 2023; Hoffmann\net al., 2022) primarily used scaling laws to determine optimal model sizes given compute budgets, we\nleverage them to identify optimal hyperparameters across model architectures. Specifically, our scaling\nlaws help determine key training parameters like batch size B and learning rate µ for both dense models\nand MoE models of varying sizes.\nThrough extensive experimentation, we systematically study the relationship between model architecture\nand optimal training hyper-parameters. Specifically, we analyze how the optimal learning rate µopt\nand batch size Bopt vary with model size N and pre-training data size D. Our experiments cover a\ncomprehensive range of architectures, including dense models with 44M to 14B parameters and MoE\nmodels with 44M to 1B activated parameters, trained on datasets ranging from 0.8B to 600B tokens.\nUsing these optimal hyper-parameter predictions, we then model the final loss as a function of model\narchitecture and training data scale.\nAdditionally, we leverage scaling laws to predict and compare the performance of MoE models with\nvarying parameter counts against their dense counterparts. This analysis guides our hyper-parameter\nconfiguration for MoE models, enabling us to achieve performance parity with specific dense model\nvariants (such as Qwen2.5-72B and Qwen2.5-14B) through careful tuning of both activated and total\nparameters.\n3.3\nLong-context Pre-training\nFor optimal training efficiency, Qwen2.5 employs a two-phase pre-training approach: an initial phase\nwith a 4,096-token context length, followed by an extension phase for longer sequences. Following\nthe strategy used in Qwen2, we extend the context length from 4,096 to 32,768 tokens during the final\npre-training stage for all model variants except Qwen2.5-Turbo. Concurrently, we increase the base\nfrequency of RoPEfrom 10,000 to 1,000,000 using the ABF technique (Xiong et al., 2023).\nFor Qwen2.5-Turbo, we implement a progressive context length expansion strategy during training,\nadvancing through four stages: 32,768 tokens, 65,536 tokens, 131,072 tokens, and ultimately 262,144\ntokens, with a RoPE base frequency of 10,000,000. At each stage, we carefully curate the training data\nto include 40% sequences at the current maximum length and 60% shorter sequences. This progressive\ntraining methodology enables smooth adaptation to increasing context lengths while maintaining the\nmodel’s ability to effectively process and generalize across sequences of varying lengths.\nTo enhance our models’ ability to process longer sequences during inference, we implement two key\nstrategies: YARN (Peng et al., 2023) and Dual Chunk Attention (DCA, An et al., 2024). Through these\ninnovations, we achieve a four-fold increase in sequence length capacity, enabling Qwen2.5-Turbo to\nhandle up to 1 million tokens and other models to process up to 131,072 tokens. Notably, these approaches\nnot only improve the modeling of long sequences by reducing perplexity but also maintain the models’\nstrong performance on shorter sequences, ensuring consistent quality across varying input lengths.\n4\nPost-training\nQwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2:\n(1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages\na massive dataset comprising millions of high-quality examples. This expansion specifically\naddresses key areas where the previous model showed limitations, such as long-sequence\n4\n\ngeneration, mathematical problem-solving, coding, instruction-following, structured data under-\nstanding, logical reasoning, cross-lingual transfer, and robust system instruction.\n(2) Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is\ndivided into two distinct stages: Offline RL and Online RL.\n• Offline RL: This stage focuses on developing capabilities that are challenging for the reward\nmodel to evaluate, such as reasoning, factuality, and instruction-following. Through meticu-\nlous construction and validation of training data, we ensure that the Offline RL signals are\nboth learnable and reliable (Xiang et al., 2024), enabling the model to acquire those complex\nskills effectively.\n• Online RL: The Online RL phase leverages the reward model’s ability to detect nuances in\noutput quality, including truthfulness, helpfulness, conciseness, relevance, harmlessness\nand debiasing. It enables the model to generate responses that are precise, coherent, and\nwell-structured while maintaining safety and readability. As a result, the model’s outputs\nconsistently meet human quality standards and expectations.\n4.1\nSupervised Fine-tuning\nIn this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on\nseveral critical areas:\n(1) Long-sequence Generation: Qwen2.5 is capable of generating high-quality content with an\noutput context length of up to 8,192 tokens, a significant advancement over the typical post-\ntraining response length, which often remains under 2,000 tokens. To address this gap, we\ndevelop long-response datasets (Quan et al., 2024). We employ back-translation techniques to\ngenerate queries for long-text data from pre-training corpora, impose output length constraints,\nand use Qwen2 to filter out low-quality paired data.\n(2) Mathematics: We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b),\nwhich encompasses a diverse range of query sources, including public datasets, K-12 problem\ncollections, and synthetic problems. To ensure high-quality reasoning, we employ rejection\nsampling (Yuan et al., 2023) along with reward modeling and annotated answers for guidance,\nproducing step-by-step reasoning process.\n(3) Coding: To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-\nCoder (Hui et al., 2024). We use multiple language-specific agents into a collaborative framework,\ngenerating diverse and high-quality instruction pairs across nearly 40 programming languages.\nWe expand our instruction dataset by synthesizing new examples from code-related Q&A\nwebsites and gathering algorithmic code snippets from GitHub. A comprehensive multilingual\nsandbox is used to perform static code checking and validate code snippets through automated\nunit testing, ensuring code quality and correctness (Dou et al., 2024; Yang et al., 2024c).\n(4) Instruction-following:\nTo ensure high-quality instruction-following data, we implement a\nrigorous code-based validation framework. In this approach, LLMs generate both instructions\nand corresponding verification code, along with comprehensive unit tests for cross-validation.\nThrough execution feedback-based rejection sampling, we carefully curate the training data used\nfor Supervised Fine-Tuning, thereby guaranteeing the model’s faithful adherence to intended\ninstructions (Dong et al., 2024).\n(5) Structured Data Understanding: We develop a comprehensive structured understanding dataset\nthat encompasses both traditional tasks, such as tabular question-answering, fact verification,\nerror correction, and structural understanding, as well as complex tasks involving structured\nand semi-structured data. By incorporating reasoning chains into the model’s responses, we\nsignificantly enhance its ability to infer information from structured data, thereby improving its\nperformance across these diverse tasks. This approach not only broadens the scope of the dataset\nbut also deepens the model’s capacity to reason and derive meaningful insights from complex\ndata structures.\n(6) Logical Reasoning: To enhance the model’s logical reasoning capabilities, we introduce a diverse\nset of 70,000 new queries spanning various domains. These queries encompass multiple-choice\nquestions, true / false questions, and open-ended questions. The model is trained to approach\nproblems systematically, employing a range of reasoning methods such as deductive reason-\ning, inductive generalization, analogical reasoning, causal reasoning, and statistical reasoning.\nThrough iterative refinement, we systematically filter out data containing incorrect answers or\nflawed reasoning processes. This process progressively strengthens the model’s ability to reason\nlogically and accurately, ensuring robust performance across different types of reasoning tasks.\n5\n\n(7) Cross-Lingual Transfer: To facilitate the transfer of the model’s general capabilities across lan-\nguages, we employ a translation model to convert instructions from high-resource languages\ninto various low-resource languages, thereby generating corresponding response candidates. To\nensure the accuracy and consistency of these responses, we evaluate the semantic alignment be-\ntween each multilingual response and its original counterpart. This process preserves the logical\nstructure and stylistic nuances of the original responses, thereby maintaining their integrity and\ncoherence across different languages.\n(8) Robust System Instruction: We construct hundreds of general system prompts to improve the\ndiversity of system prompts in post-training, ensuring consistency between system prompts and\nconversations. Evaluations with different system prompts show that the model maintains good\nperformance (Lu et al., 2024b) and reduced variance, indicating improved robustness.\n(9) Response Filtering: To evaluate the quality of responses, we employ multiple automatic an-\nnotation methods, including a dedicated critic model and a multi-agent collaborative scoring\nsystem. Responses are subjected to rigorous assessment, and only those deem flawless by all\nscoring systems are retained. This comprehensive approach ensures that our outputs maintain\nthe highest quality standards.\nUltimately, we construct a dataset of over 1 million SFT examples. The model is fine-tuned for two epochs\nwith a sequence length of 32,768 tokens. To optimize learning, the learning rate is gradually decreased\nfrom 7 × 10−6 to 7 × 10−7. To address overfitting, we apply a weight decay of 0.1, and gradient norms\nare clipped at a maximum value of 1.0.\n4.2\nOffline Reinforcement Learning\nCompared to Online Reinforcement Learning (RL), Offline RL enables the pre-preparation of training\nsignals, which is particularly advantageous for tasks where standard answers exist but are challenging to\nevaluate using reward models. In this study, we focus on objective query domains such as mathematics,\ncoding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex.\nIn the previous phase, we extensively employ strategies like execution feedback and answer matching to\nensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model\nto resample responses for a new set of queries. Responses that pass our quality checks are used as positive\nexamples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO)\ntraining (Rafailov et al., 2023). To further enhance the reliability and accuracy of the training signals, we\nmake use of both human and automated review processes (Cao et al., 2024). This dual approach ensures\nthat the training data is not only learnable but also aligned with human expectations. Ultimately, we\nconstruct a dataset consisting of approximately 150,000 training pairs. The model is then trained for one\nepoch using the Online Merging Optimizer (Lu et al., 2024a), with a learning rate of 7 × 10−7.\n4.3\nOnline Reinforcement Learning\nTo develop a robust reward model for online RL, we adhere to a set of carefully defined labeling criteria.\nThose criteria ensure that the responses generated by the model are not only high-quality but also aligned\nwith ethical and user-centric standards (Wang et al., 2024a). The specific guidelines for data labeling are\nas follows:\n• Truthfulness: Responses must be grounded in factual accuracy, faithfully reflecting the pro-\nvided context and instructions. The model should avoid generating information that is false or\nunsupported by the given data.\n• Helpfulness: The model’s output should be genuinely useful, addressing the user’s query\neffectively while providing content that is positive, engaging, educational, and relevant. It\nshould follow the given instructions precisely and offer value to the user.\n• Conciseness: Responses should be succinct and to the point, avoiding unnecessary verbosity.\nThe goal is to convey information clearly and efficiently without overwhelming the user with\nexcessive detail.\n• Relevance: All parts of the response should be directly related to the user’s query, dialogue\nhistory, and the assistant’s context. The model should tailor its output to ensure it is perfectly\naligned with the user’s needs and expectations.\n• Harmlessness: The model must prioritize user safety by avoiding any content that could lead\nto illegal, immoral, or harmful behavior. It should promote ethical conduct and responsible\ncommunication at all times.\n6\n\n• Debiasing: The model should produce responses that are free from bias, including but not\nlimited to gender, race, nationality, and politics. It should treat all topics equally and fairly,\nadhering to widely accepted moral and ethical standards.\nThe queries utilized to train the reward model are drawn from two distinct datasets: publicly available\nopen-source data and a proprietary query set characterized by higher complexity. Responses are gener-\nated from checkpoints of the Qwen models, which have been fine-tuned using different methods—SFT,\nDPO, and RL—at various stages of training. To introduce diversity, those responses are sampled at\ndifferent temperature settings. Preference pairs are created through both human and automated labeling\nprocesses, and the training data for DPO is also integrated into this dataset.\nIn our online reinforcement learning (RL) framework, we employ Group Relative Policy Optimization\n(GRPO, Shao et al., 2024). The query set utilized for training the reward model is identical to the one used\nin the RL training phase. The sequence in which queries are processed during training is determined by\nthe variance of their response scores, as evaluated by the reward model. Specifically, queries with higher\nvariance in response scores are prioritized to ensure more effective learning. We sample 8 responses\nfor each query. All models are trained with a 2048 global batch size and 2048 samples in each episode,\nconsidering a pair of queries and responses as a sample.\n4.4\nLong Context Fine-tuning\nTo further extend the context length of Qwen2.5-Turbo, we introduce longer SFT examples during\npost-training, enabling it to better align with human preference in long queries.\nIn the SFT phase, we employ a two-stage approach. In the first stage, the model is fine-tuned exclusively\nusing short instructions, each containing up to 32,768 tokens. This stage uses the same data and training\nsteps as those employed for the other Qwen2.5 models, ensuring strong performance on short tasks.\nIn the second stage, the fine-tuning process combines both short instructions (up to 32,768 tokens)\nand long instructions (up to 262,144 tokens). This hybrid approach effectively enhances the model’s\ninstruction-following ability in long context tasks while maintaining its performance on short tasks.\nDuring the RL stage, we use a training strategy similar to that used for the other Qwen2.5 models,\nfocusing solely on short instructions. This design choice is driven by two primary considerations: first,\nRL training is computationally expensive for long context tasks; second, there is currently a scarcity of\nreward models that provide suitable reward signals for long context tasks. Additionally, we find that\nadopting RL on short instructions alone can still significantly enhance the model’s alignment with human\npreferences in long context tasks.\n5\nEvaluation\nThe base models produced by pre-training and the instruction-tuned models produced by post-training\nare evaluated accordingly with a comprehensive evaluation suite, including both commonly-used open\nbenchmarks and skill-oriented in-house datasets. The evaluation suite is designed to be primarily\nautomatic with minimal human interaction.\nTo prevent test data leakage, we exclude potentially contaminated data using n-gram matching when\nconstructing the pre-training and post-training datasets. Following the criteria used in Qwen2, a training\nsequence st is removed from the training data if there exists a test sequence se such that the length of the\nlongest common subsequence (LCS) between tokenized st and se satisfies both |LCS(st, se)| ≥13 and\n|LCS(st, se)| ≥0.6 × min(|st|, |se|).\n5.1\nBase Models\nWe conduct comprehensive evaluations of the base language models of the Qwen2.5 series. The evaluation\nof base models primarily emphasizes their performance in natural language understanding, general\nquestion answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.\nThe evaluation datasets include:\nGeneral Tasks\nMMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024b) (5-shot),\nMMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot), ARC-C (Clark et al.,\n2018) (25-shot), TruthfulQA (Lin et al., 2022a) (0-shot), Winogrande (Sakaguchi et al., 2021) (5-shot),\nHellaSwag (Zellers et al., 2019) (10-shot).\n7\n\nTable 2: Performance of the 70B+ base models and Qwen2.5-Plus.\nDatasets\nLlama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus\nGeneral Tasks\nMMLU\n79.5\n77.8\n85.2\n84.2\n86.1\n85.4\nMMLU-Pro\n52.8\n51.6\n61.6\n55.7\n58.1\n64.0\nMMLU-redux\n75.0\n72.9\n-\n80.5\n83.9\n82.8\nBBH\n81.0\n78.9\n85.9\n82.4\n86.3\n85.8\nARC-C\n68.8\n70.7\n-\n68.9\n72.4\n70.9\nTruthfulQA\n45.6\n51.0\n-\n54.8\n60.4\n55.3\nWindoGrande\n85.3\n85.0\n86.7\n85.1\n83.9\n85.5\nHellaSwag\n88.0\n88.7\n-\n87.3\n87.6\n89.2\nMathematics & Science Tasks\nGPQA\n36.3\n34.3\n-\n37.4\n45.9\n43.9\nTheoremQA\n32.3\n35.9\n-\n42.8\n42.4\n48.5\nMATH\n42.5\n41.7\n53.8\n50.9\n62.1\n64.4\nMMLU-stem\n73.7\n71.7\n-\n79.6\n82.7\n81.2\nGSM8K\n77.6\n83.7\n89.0\n89.0\n91.5\n93.0\nCoding Tasks\nHumanEval\n48.2\n46.3\n61.0\n64.6\n59.1\n59.1\nHumanEval+\n42.1\n40.2\n-\n56.1\n51.2\n52.4\nMBPP\n70.4\n71.7\n73.0\n76.9\n84.7\n79.7\nMBPP+\n58.4\n58.1\n-\n63.9\n69.2\n66.9\nMultiPL-E\n46.3\n46.7\n-\n59.6\n60.5\n61.0\nMultilingual Tasks\nMulti-Exam\n70.0\n63.5\n-\n76.6\n78.7\n78.5\nMulti-Understanding\n79.9\n77.7\n-\n80.7\n89.6\n89.2\nMulti-Mathematics\n67.1\n62.9\n-\n76.0\n76.7\n82.4\nMulti-Translation\n38.0\n23.3\n-\n37.8\n39.0\n40.4\nMathematics & Science Tasks\nGPQA (Rein et al., 2023) (5-shot), Theorem QA (Chen et al., 2023a)\n(5-shot), GSM8K (Cobbe et al., 2021) (4-shot), MATH (Hendrycks et al., 2021b) (4-shot).\nCoding Tasks\nHumanEval (Chen et al., 2021) (0-shot), HumanEval+ (Liu et al., 2023)(0-shot), MBPP (Austin\net al., 2021) (0-shot), MBPP+ (Liu et al., 2023) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot) (Python,\nC++, JAVA, PHP, TypeScript, C#, Bash, JavaScript).\nMultilingual Tasks\nWe group them into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova et al.,\n2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French, Portuguese,\nGerman, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar et al., 2023) (5-shot),\nXCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023) (5-shot), XStoryCloze (Lin\net al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c) Mathematics: MGSM (Goyal et al., 2022)\n(8-shot CoT); and (d) Translation: Flores-101 (Goyal et al., 2022) (5-shot).\nFor base models, we compare Qwen2.5 models with Qwen2 models and other leading open-weight\nmodels in terms of scales of parameters.\nQwen2.5-72B & Qwen2.5-Plus\nWe compare the base models of Qwen2.5-72B and Qwen2.5-Plus to other\nleading open-weight base models: Llama3-70B (Dubey et al., 2024), Llama3-405B (Dubey et al., 2024),\nMixtrail-8x22B (Jiang et al., 2024a), and our previous 72B version, the Qwen2-72B (Yang et al., 2024a).\nThe Qwen2.5-72B base model significantly outperforms its peers in the same category across a wide\nrange of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the pa-\nrameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked\nimprovements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics,\nand coding challenges. With significantly lower training and inference costs, Qwen2.5-Plus achieves\nvery competitive performance results compared to Qwen2.5-72B and Llama3-405B, outperforming other\nbaseline models on the Hellaswag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics, and\nMulti-Translation. Moreover, Qwen2.5-Plus achieves 64.0 on MMLU-Pro, which is 5.9 points higher than\nQwen2.5-72B.\nQwen2.5-14B/32B & Qwen2.5-Turbo\nThe evaluation of the Qwen2.5-Turbo, Qwen2.5-14B, and 32B\nmodels is compared against baselines of similar sizes. These baselines include Yi-1.5-34B (Young et al.,\n8\n\nTable 3: Performance of the 14B-30B+ base models and Qwen2.5-Turbo.\nDatasets\nQwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B\nGeneral Tasks\nMMLU\n74.3\n75.2\n77.2\n79.5\n79.7\n83.3\nMMLU-pro\n44.1\n49.1\n48.3\n55.6\n51.2\n55.1\nMMLU-redux\n69.0\n-\n74.1\n77.1\n76.6\n82.0\nBBH\n66.8\n74.9\n76.4\n76.1\n78.2\n84.5\nARC-C\n63.6\n71.4\n65.6\n67.8\n67.3\n70.4\nTruthfulQA\n57.4\n40.1\n53.9\n56.3\n58.4\n57.8\nWinogrande\n81.5\n59.7\n84.9\n81.1\n81.0\n82.0\nHellaswag\n85.0\n86.4\n85.9\n85.0\n84.3\n85.2\nMathematics & Science Tasks\nGPQA\n30.8\n34.9\n37.4\n41.4\n32.8\n48.0\nTheoremqa\n28.8\n35.8\n40.0\n42.1\n43.0\n44.1\nMATH\n36.1\n42.7\n41.7\n55.6\n55.6\n57.7\nMMLU-stem\n66.5\n71.0\n72.6\n77.0\n76.4\n80.9\nGSM8K\n78.5\n81.1\n81.7\n88.3\n90.2\n92.9\nCoding Tasks\nHumanEval\n43.3\n54.9\n46.3\n57.3\n56.7\n58.5\nHumanEval+\n40.2\n46.3\n40.2\n51.2\n51.2\n52.4\nMBPP\n64.2\n75.7\n65.5\n76.2\n76.7\n84.5\nMBPP+\n53.9\n60.2\n55.4\n63.0\n63.2\n67.2\nMultiPL-E\n38.5\n48.0\n39.5\n53.9\n53.5\n59.4\nMultilingual Tasks\nMulti-Exam\n61.6\n65.8\n58.3\n70.3\n70.6\n75.4\nMulti-Understanding\n76.5\n82.2\n73.9\n85.3\n85.9\n88.4\nMulti-Mathematics\n56.1\n61.6\n49.3\n71.3\n68.5\n73.7\nMulti-Translation\n33.5\n38.7\n30.0\n36.8\n36.2\n37.3\nTable 4: Performance of the 7B+ base models.\nDatasets\nMistral-7B\nLlama3-8B\nGemma2-9B\nQwen2-7B\nQwen2.5-7B\nGeneral Tasks\nMMLU\n64.2\n66.6\n71.3\n70.3\n74.2\nMMLU-pro\n30.9\n35.4\n44.7\n40.1\n45.0\nMMLU-redux\n58.1\n61.6\n67.9\n68.1\n71.1\nBBH\n56.1\n57.7\n68.2\n62.3\n70.4\nARC-C\n60.0\n59.3\n68.2\n60.6\n63.7\nTruthfulQA\n42.2\n44.0\n45.3\n54.2\n56.4\nWinogrande\n78.4\n77.4\n79.5\n77.0\n75.9\nHellaSwag\n83.3\n82.1\n81.9\n80.7\n80.2\nMathematics & Science Tasks\nGPQA\n24.7\n25.8\n32.8\n30.8\n36.4\nTheoremQA\n19.2\n22.1\n28.9\n29.6\n36.0\nMATH\n10.2\n20.5\n37.7\n43.5\n49.8\nMMLU-stem\n50.1\n55.3\n65.1\n64.2\n72.3\nGSM8K\n36.2\n55.3\n70.7\n80.2\n85.4\nCoding Tasks\nHumanEval\n29.3\n33.5\n37.8\n51.2\n57.9\nHumanEval+\n24.4\n29.3\n30.5\n43.3\n50.6\nMBPP\n51.1\n53.9\n62.2\n64.2\n74.9\nMBPP+\n40.9\n44.4\n50.6\n51.9\n62.9\nMultiPL-E\n29.4\n22.6\n34.9\n41.0\n50.3\nMultilingual Tasks\nMulti-Exam\n47.1\n52.3\n61.2\n59.2\n59.4\nMulti-Understanding\n63.3\n68.6\n78.3\n72.0\n79.3\nMulti-Mathematics\n26.3\n36.3\n53.0\n57.5\n57.8\nMulti-Translation\n23.3\n31.9\n36.5\n31.5\n32.4\n9\n\nTable 5: Performance of the smaller base models.\nDatasets\nQwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B\nGeneral Tasks\nMMLU\n44.3\n47.5\n55.9\n60.9\n52.2\n65.6\nMMLU-pro\n14.7\n15.7\n21.6\n28.5\n23.0\n34.6\nMMLU-redux\n40.7\n45.1\n51.8\n58.5\n50.9\n63.7\nBBH\n18.2\n20.3\n36.5\n45.1\n41.9\n56.3\nARC-C\n31.0\n35.6\n43.7\n54.7\n55.7\n56.5\nTruthfulQA\n39.7\n40.2\n45.9\n46.6\n36.2\n48.9\nWinogrande\n56.9\n56.3\n65.0\n65.0\n71.5\n71.1\nHellaswag\n49.1\n52.1\n67.0\n67.9\n74.6\n74.6\nMathematics & Science Tasks\nGPQA\n29.8\n24.8\n20.7\n24.2\n25.3\n26.3\nTheoremQA\n9.6\n16.0\n14.8\n22.1\n15.9\n27.4\nMATH\n11.2\n19.5\n21.6\n35.0\n18.3\n42.6\nMMLU-STEM\n27.5\n39.8\n42.7\n54.8\n45.8\n62.5\nGSM8K\n36.4\n41.6\n46.9\n68.5\n30.3\n79.1\nCoding Tasks\nHumanEval\n22.6\n30.5\n34.8\n37.2\n19.5\n42.1\nHumanEval+\n18.9\n26.8\n29.9\n32.9\n15.9\n36.0\nMBPP\n33.1\n39.3\n46.9\n60.2\n42.1\n57.1\nMBPP+\n27.6\n33.8\n37.6\n49.6\n33.6\n49.4\nMultiPL-E\n16.3\n18.9\n27.9\n33.1\n17.6\n41.2\nMultilingual Tasks\nMulti-Exam\n29.4\n30.8\n43.1\n47.9\n38.1\n54.6\nMulti-Understanding\n40.4\n41.0\n50.7\n65.1\n46.8\n76.6\nMulti-Mathematics\n7.8\n13.5\n21.3\n37.5\n18.2\n48.9\nMulti-Translation\n14.1\n15.3\n23.8\n25.0\n26.9\n29.3\n2024), Gemma2-27B (Gemma Team et al., 2024), and Qwen1.5-32B (Qwen Team, 2024b). The results\nare shown in Table 3. The Qwen2.5-14B model demonstrates a solid performance across various tasks,\nparticularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2,\noutcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional\ncapabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor\nQwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable\nscores of 57.7 in MATH and 84.5 in MBPP. For Qwen2.5-Turbo, although its training cost and inference cost\nare significantly smaller than those of Qwen2.5-14B, it achieves comparable results, where its MMLU-Pro\nscore is even better than that of Qwen2.5-32B.\nQwen2.5-7B\nFor 7B-level models, we focus on comparing Qwen2.5-7B with other leading 7B+ models,\nincluding Mistral-7B (Jiang et al., 2023a), Llama3-8B (Dubey et al., 2024), Gemma2-9B (Gemma Team et al.,\n2024), and our predecessor, Qwen2-7B (Yang et al., 2024a). The results can be found in Table 4. Note that\nthe non-embedding parameters of Qwen2-7B and Qwen2.5-7B are only 6.5B, while that of Gemma2-9B\nis 8.2B. The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks,\ndespite having fewer non-embedding parameters. It demonstrates significant improvements across\nvarious tasks, achieving 74.2 on general benchmarks like MMLU (Hendrycks et al., 2021a), 49.8 on math\nchallenges such as MATH (Hendrycks et al., 2021b), and 57.9 on coding tasks like HumanEval (Chen\net al., 2021).\nQwen2.5-0.5B/1.5B/3B\nFor edge-side models, we compare Qwen2.5-0.5B, 1.5B, and 3B against estab-\nlished baselines: Qwen2-0.5B/1.5B (Yang et al., 2024a) and Gemma2-2.6B (Gemma Team et al., 2024). The\nresults are given in Table 5. Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across\nnearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math\nand coding tasks.\n5.2\nInstruction-tuned Model\nTo critically evaluate instruction-tuned models, we adopt a multifaceted approach. Foundational skills\nand human preferences are assessed using open datasets and benchmarks. Additionally, our detailed\nin-house evaluations delve deeper into the models’competencies in key areas and multilingualism. A\nparticular focus is placed on assessing long-context capability. The subsequent sections outline the\nevaluation methods and present the results.\n10\n\nTable 6: Performance of the 70B+ Instruct models and Qwen2.5-Plus.\nDatasets\nLlama-3.1-70B Llama-3.1-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus\nGeneral Tasks\nMMLU-Pro\n66.4\n73.3\n64.4\n71.1\n72.5\nMMLU-redux\n83.0\n86.2\n81.6\n86.8\n86.3\nLiveBench 0831\n46.6\n53.2\n41.5\n52.3\n54.6\nMathematics & Science Tasks\nGPQA\n46.7\n51.1\n42.4\n49.0\n49.7\nMATH\n68.0\n73.8\n69.0\n83.1\n84.7\nGSM8K\n95.1\n96.8\n93.2\n95.8\n96.0\nCoding Tasks\nHumanEval\n80.5\n89.0\n86.0\n86.6\n87.8\nMBPP\n84.2\n84.5\n80.2\n88.2\n85.5\nMultiPL-E\n68.2\n73.5\n69.2\n75.1\n77.0\nLiveCodeBench\n32.1\n41.6\n32.2\n55.5\n51.4\nAlignment Tasks\nIFEval\n83.6\n86.0\n77.6\n84.1\n86.3\nArena-Hard\n55.7\n69.3\n48.1\n81.2\n81.4\nMTbench\n8.79\n9.08\n9.12\n9.35\n9.30\nTable 7: Performance of the 14B-30B+ instruction-tuned models and Qwen2.5-Turbo.\nDatasets\nQwen2-57BA14B Gemma2-27B GPT4o-mini Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B\nGeneral Tasks\nMMLU-Pro\n52.8\n55.5\n63.1\n64.5\n63.7\n69.0\nMMLU-redux\n72.6\n75.7\n81.5\n81.7\n80.0\n83.9\nLiveBench 0831\n31.1\n39.6\n43.3\n42.3\n44.4\n50.7\nMathematics & Science Tasks\nGPQA\n34.3\n38.4\n40.2\n42.3\n45.5\n49.5\nMATH\n49.1\n54.4\n70.2\n81.1\n80.0\n83.1\nGSM8K\n85.3\n90.4\n93.2\n93.8\n94.8\n95.9\nCoding Tasks\nHumanEval\n79.9\n78.7\n88.4\n86.6\n83.5\n88.4\nMBPP\n70.9\n81.0\n85.7\n82.8\n82.0\n84.0\nMultiPL-E\n66.4\n67.4\n75.0\n73.7\n72.8\n75.4\nLiveCodeBench\n22.5\n-\n40.7\n37.8\n42.6\n51.2\nAlignment Tasks\nIFEval\n59.9\n77.1\n80.4\n76.3\n81.0\n79.5\nArena-Hard\n17.8\n57.5\n74.9\n67.1\n68.3\n74.5\nMTbench\n8.55\n9.10\n-\n8.81\n8.88\n9.20\n5.2.1\nOpen Benchmark Evaluation\nTo comprehensively evaluate the quality of instruction-tuned models, we compile automatic and human\nevaluation to assess the capabilities and human preference. For the evaluation of basic capabilities, we\napply similar datasets in the pre-trained model evaluation, which target on natural language understand-\ning, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU-Pro, MMLU-redux and\nLiveBench 0831 (White et al., 2024) for general evaluation, GPQA, GSM8K and MATH for science and\nmathematics, HumanEval, MBPP, MultiPL-E and LiveCodeBench 2305-2409 (Jain et al., 2024) for coding,\nIFEval (Zhou et al., 2023)2 for instruction following. Additionally, we assess the performance of human\npreference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng\net al., 2023) and Arena-Hard (Li et al., 2024).\nQwen2.5-72B-Instruct & Qwen2.5-Plus\nAs shown in Table 6, we compare Qwen2.5-72B-Instruct and\nQwen2.5-Plus to other leading open-weight instrution-tuned models: Llama3.1-70B-Instruct (Dubey\n2For simplicity, we report the results of the subset strict-prompt.\n11\n\nTable 8: Performance of the 7B+ instruction-tuned models.\nDatasets\nGemma2-9B Llama3.1-8B Qwen2-7B Qwen2.5-7B\nGeneral Tasks\nMMLU-Pro\n52.1\n48.3\n44.1\n56.3\nMMLU-redux\n72.8\n67.2\n67.3\n75.4\nLiveBench 0831\n30.6\n26.7\n29.2\n35.9\nMathematics & Science Tasks\nGPQA\n32.8\n32.8\n34.3\n36.4\nMATH\n44.3\n51.9\n52.9\n75.5\nGSM8K\n76.7\n84.5\n85.7\n91.6\nCoding Tasks\nHumanEval\n68.9\n72.6\n79.9\n84.8\nMBPP\n74.9\n69.6\n67.2\n79.2\nMultiPL-E\n53.4\n50.7\n59.1\n70.4\nLiveCodeBench\n18.9\n8.3\n23.9\n28.7\nAlignment Tasks\nIFEval\n70.1\n75.9\n54.7\n71.2\nArena-Hard\n41.6\n27.8\n25.0\n52.0\nMTbench\n8.49\n8.23\n8.26\n8.75\nTable 9: Performance comparison of 2B-4B instruction-tuned models.\nDatasets\nGemma2-2B Phi3.5-Mini MiniCPM3-4B Qwen2.5-3B\nNon-Emb Params\n2.0B\n3.6B\n4.0B\n2.8B\nGeneral Tasks\nMMLU-Pro\n26.7\n47.5\n43.0\n43.7\nMMLU-redux\n51.9\n67.7\n59.9\n64.4\nLiveBench 0831\n20.1\n27.4\n27.6\n26.8\nMathematics & Science Tasks\nGPQA\n29.3\n27.2\n31.3\n30.3\nMATH\n26.6\n48.5\n46.6\n65.9\nGSM8K\n63.2\n86.2\n81.1\n86.7\nCoding Tasks\nHumanEval\n68.9\n72.6\n74.4\n74.4\nMBPP\n74.9\n63.2\n72.5\n72.7\nMultiPL-E\n30.5\n47.2\n49.1\n60.2\nLiveCodeBench\n5.8\n15.8\n23.8\n19.9\nAlignment Tasks\nIFEval\n51.0\n52.1\n68.4\n58.2\net al., 2024), Llama3.1-405B-Instruct (Dubey et al., 2024), and our previous 72B version, Qwen2-72B-\nInstruct (Yang et al., 2024a). The Qwen2.5-72B-Instruct model delivers exceptional performance, even\nsurpassing the larger Llama-3.1-405B-Instruct in several critical benchmarks including MMLU-redux,\nMATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard and MTBench. Moreover, Qwen2.5-Plus outper-\nforms Qwen2.5-72B-Instruct on 9 out of 13 benchmarks.\nQwen2.5-14B/32B-Instruct & Qwen2.5-Turbo\nThe performance of the Qwen2.5-Turbo, Qwen2.5-14B-\nInstruct, and Qwen2.5-32B-Instruct models is evaluated and compared against baselines of similar sizes.\nThe baselines include GPT4o-mini, Gemma2-27B-IT (Gemma Team et al., 2024), and Qwen2-57BA14B-\nInstruct (Yang et al., 2024a). The results are summarized in Table 7. The Qwen2.5-32B-Instruct model\nexhibits superior performance across most tasks when compared to other models of similar size. Notably,\nour open-weight Qwen2.5-14B-Instruct model delivers competitive results across all benchmarks, rivaling\nthose of GPT-4o-mini. Despite its significantly lower training and inference costs, the Qwen2.5-Turbo\nmodel outperforms Qwen2.5-14B-Instruct on eight out of ten benchmarks. This demonstrates that\nQwen2.5-Turbo achieves remarkable efficiency and effectiveness, making it a compelling choice for\nresource-constrained environments.\n12\n\nTable 10: Performance comparison of 0.5B-1.5B instruction-tuned models.\nDatasets\nQwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B\nGeneral Tasks\nMMLU-Pro\n14.4\n15.0\n22.9\n32.4\nMMLU-redux\n12.9\n24.1\n41.2\n50.7\nLiveBench\n7.4\n12.6\n12.4\n18.8\nMathematics & Science Tasks\nGPQA\n23.7\n29.8\n21.2\n29.8\nMATH\n13.9\n34.4\n25.3\n55.2\nGSM8K\n40.1\n49.6\n61.6\n73.2\nCoding Tasks\nHumanEval\n31.1\n35.4\n42.1\n61.6\nMBPP\n39.7\n49.6\n44.2\n63.2\nMultiPL-E\n20.8\n28.5\n38.5\n50.4\nLiveCodeBench\n1.6\n5.1\n4.5\n14.8\nAlignment Tasks\nIFEval\n14.6\n27.9\n29.0\n42.5\nTable 11: Performance Comparison on our in-house English automatic evaluation benchmark.\nModels\nIF\nKnowledge\nComprehension\nCoding\nMath\nReasoning\nProprietary LLMs\nGPT-4o-2024-08-06\n83.28\n68.08\n76.51\n58.05\n52.36\n66.45\nGPT-4o-2024-11-20\n80.06\n65.25\n79.07\n60.19\n49.74\n67.07\nClaude3.5-sonnet-2024-10-22\n84.22\n74.61\n79.02\n67.17\n48.67\n70.20\nQwen2 Series\nQwen2-0.5B-Instruct\n18.33\n18.59\n30.64\n5.42\n13.16\n32.03\nQwen2-1.5B-Instruct\n29.42\n29.23\n45.81\n17.02\n20.34\n38.86\nQwen2-7B-Instruct\n50.47\n44.79\n58.04\n43.04\n38.31\n50.25\nQwen2-72B-Instruct\n76.08\n59.49\n72.19\n48.95\n48.07\n60.33\nLlama-3.1 Series\nLlama-3.1-70B-Instruct\n81.33\n63.42\n69.29\n55.96\n48.00\n63.18\nLlama-3.1-405B-Instruct\n83.33\n67.10\n75.55\n58.14\n47.09\n64.74\nQwen2.5 Series\nQwen2.5-0.5B-Instruct\n33.35\n30.29\n29.78\n15.41\n26.29\n36.13\nQwen2.5-1.5B-Instruct\n40.25\n41.19\n47.69\n26.19\n40.99\n42.23\nQwen2.5-3B-Instruct\n60.60\n46.11\n57.98\n41.43\n49.38\n49.80\nQwen2.5-7B-Instruct\n70.01\n52.74\n62.69\n48.41\n56.93\n54.69\nQwen2.5-14B-Instruct\n74.17\n59.78\n69.11\n52.68\n59.68\n62.51\nQwen2.5-Turbo\n72.76\n58.56\n68.70\n54.48\n57.77\n61.06\nQwen2.5-32B-Instruct\n76.79\n64.08\n71.28\n58.90\n60.97\n65.49\nQwen2.5-72B-Instruct\n82.65\n66.09\n74.43\n60.41\n59.73\n65.90\nQwen2.5-Plus\n83.18\n68.41\n79.35\n59.58\n62.52\n66.92\nOther Instruction-tuned Models\nAs illustrated in Table 8, the Qwen2.5-7B-Instruct model significantly\noutperforms its competitors, Gemma2-9B-IT and Llama3.1-8B-Instruct, across all tasks except IFEval.\nNotably, Qwen2.5-7B-Instruct exhibits clear advantages in mathematics (MATH: 75.5) and coding (Hu-\nmanEval: 84.8). For the edge-side instruction models, the Qwen2.5-3B-Instruct model, despite having\nfewer parameters than both the Phi3.5-mini-instruct (Abdin et al., 2024) and MiniCPM3-4B-Instruct (Hu\net al., 2024) models, surpasses them in mathematics and coding tasks, as shown in Table 9. Additionally,\nit delivers competitive results in language understanding. The Qwen2.5-1.5B-Instruct and Qwen2.5-0.5B-\nInstruct models have also seen substantial performance improvements over their previous versions, as\ndetailed in Table 10. These enhancements make them particularly well-suited for edge-side applications\nin highly resource-constrained environments.\n13\n\nTable 12: Performance Comparison on our in-house Chinese automatic evaluation benchmark.\nModels\nIF\nKnowledge\nComprehension\nCoding\nMath\nReasoning\nProprietary LLMs\nGPT-4o-2024-08-06\n42.50\n68.55\n80.11\n61.53\n61.74\n56.88\nGPT-4o-2024-11-20\n42.71\n71.29\n83.04\n62.39\n66.04\n62.04\nClaude3.5-sonnet-2024-10-22\n49.25\n72.09\n82.16\n66.00\n63.71\n66.60\nQwen2 Series\nQwen2-0.5B-Instruct\n4.69\n40.43\n39.13\n9.85\n14.07\n32.73\nQwen2-1.5B-Instruct\n6.81\n51.54\n46.89\n14.14\n24.57\n35.19\nQwen2-7B-Instruct\n16.83\n65.95\n60.30\n37.05\n50.52\n44.96\nQwen2-72B-Instruct\n31.98\n74.96\n75.49\n41.57\n65.55\n58.19\nLlama-3.1 Series\nLlama-3.1-70B-Instruct\n28.96\n57.41\n67.24\n54.82\n41.18\n52.42\nLlama-3.1-405B-Instruct\n30.39\n63.79\n72.27\n60.73\n46.05\n55.88\nQwen2.5 Series\nQwen2.5-0.5B-Instruct\n6.12\n39.13\n42.97\n9.60\n24.03\n33.72\nQwen2.5-1.5B-Instruct\n7.38\n48.68\n49.69\n22.96\n37.30\n39.17\nQwen2.5-3B-Instruct\n16.50\n57.18\n62.55\n29.88\n51.64\n39.57\nQwen2.5-7B-Instruct\n26.64\n65.77\n67.55\n39.56\n61.06\n49.70\nQwen2.5-14B-Instruct\n26.87\n70.28\n76.96\n49.78\n67.01\n56.41\nQwen2.5-Turbo\n32.94\n72.93\n74.37\n51.92\n66.08\n53.30\nQwen2.5-32B-Instruct\n32.64\n74.70\n79.46\n54.45\n67.86\n60.19\nQwen2.5-72B-Instruct\n37.22\n75.86\n78.85\n56.71\n68.39\n63.02\nQwen2.5-Plus\n46.15\n72.07\n82.64\n58.48\n69.96\n62.98\n5.2.2\nIn-house Automatic Evaluation\nDespite the availability of several open benchmark datasets for evaluation, we believe that these are\ninsufficient to fully capture the capabilities of LLMs. To address this, we have developed a series\nof in-house datasets designed to assess various aspects of model performance, including knowledge\nunderstanding, text generation, coding, and more. These evaluations are conducted in both Chinese\nand English. In addition, we have specifically evaluated the multilingual performance of instruction-\ntuned models. The results are summarized in Table 11 for English, Table 12 for Chinese, Table 13 for\nmultilingualism of 70B+ Instruct models, and Table 14 for 7B-14B models, respectively.\nEnglish & Chinese Evaluation\nWe compare the performance of Qwen2.5-Instruct models against\nseveral leading language models, including GPT-4, Claude3.5-sonnet, Qwen2, and Llama-3.1, across both\nEnglish and Chinese languages. Our analysis focuses on model size and its impact on performance, as\nwell as how our latest Qwen2.5 series compares to previous iterations and competing models. For smaller\nmodels, we observe that the Qwen2.5-0.5B model achieves performance that is on par with or even\nsurpasses the Qwen2-1.5B model. This indicates that the Qwen2.5 series has optimized parameter usage,\nenabling mid-sized models to achieve similar performance levels to larger models from the previous\ngeneration. The Qwen2.5-3B model demonstrates performance that is comparable to the Qwen2-7B\nmodel. Notably, the Qwen2.5-32B model exhibits a remarkable improvement over the Qwen2-72B model.\nOur flagship model, Qwen2.5-72B, further narrows the gap between Qwen and state-of-the-art models\nlike GPT-4 and Claude3.5-sonnet. In particular, Qwen2.5-72B matches or exceeds the performance\nof Llama-3.1-405B in all metrics except for instruction following. This achievement underscores the\ncompetitiveness of Qwen2.5-72B in a wide range of language processing tasks, while also identifying\nareas for future improvement. Qwen2.5-Plus addresses the previous shortcomings in Chinese instruction\nfollowing and further enhances its advantages in other areas.\nMultilingual Evaluation\nTo comprehensively evaluate the multilingual capabilities of instruction-tuned\nmodels, we followed P-MMEval (Zhang et al., 2024) and extended several benchmarks as follows: (1)\nIFEval (Multilingual): We expanded the IFEval benchmark, originally in English, to include multilingual\nexamples. To ensure language neutrality, we removed instances that contained language-specific content\n(e.g., ”start with letter A”). (2) Knowledge Utilization: to assess the knowledge utilization abilities\nof the Qwen2.5 series models across multiple languages, we employed five MMLU-like benchmarks\n(multiple-choice format). These benchmarks include: AMMLU (Arabic), JMMLU (Japanese), KMMLU\n(Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Additionally, we evaluated the models’\nperformance on the translated version of the MMLU benchmark (okapi MMLU), which has been adapted\n14\n\nTable 13: Performance of the 70B+ Instruct models on Multilingual Tasks.\nDatasets\nQwen2-72B Llama3.1-70B Qwen2.5-32B Mistral-Large GPT4o-mini Qwen2.5-72B\nInstruction Following\nIFEval (multilingual)\n79.69\n80.47\n82.68\n82.69\n85.03\n86.98\nKnowledge\nAMMLU (Arabic)\n68.85\n70.08\n70.44\n69.24\n69.73\n72.44\nJMMLU (Japanese)\n77.37\n73.89\n76.55\n75.77\n73.74\n80.56\nKMMLU (Korean)\n57.04\n53.23\n60.75\n56.42\n56.77\n61.96\nIndoMMLU (Indonesian)\n66.31\n67.50\n66.42\n63.21\n67.75\n69.25\nTurkishMMLU (Turkish)\n69.22\n66.89\n72.41\n64.78\n71.19\n76.12\nokapi MMLU (translated)\n77.84\n76.49\n77.16\n78.37\n73.44\n79.97\nMath Reasoning\nMGSM8K (extended)\n82.72\n73.31\n87.15\n89.01\n87.36\n88.16\nCultural Nuances\nBLEnD\n25.90\n30.49\n27.88\n33.47\n35.91\n32.48\nTable 14: Performance of the 7B-14B Instruct models on Multilingual Tasks.\nDatasets\nQwen2-7B Llama3.1-8B Qwen2.5-7B Gemma2-9B Qwen2.5-14B\nInstruction Following\nIFEval (multilingual)\n51.43\n60.68\n74.87\n77.47\n77.08\nKnowledge\nAMMLU (Arabic)\n54.87\n54.28\n59.78\n60.26\n66.81\nJMMLU (Japanese)\n57.71\n53.26\n61.88\n64.59\n72.78\nKMMLU (Korean)\n43.96\n42.28\n46.59\n46.24\n59.71\nIndoMMLU (Indonesian)\n54.05\n53.92\n56.42\n61.73\n65.09\nTurkishMMLU (Turkish)\n49.27\n45.61\n54.28\n55.44\n66.85\nokapi MMLU (translated)\n60.47\n55.18\n66.98\n46.72\n72.12\nMath Reasoning\nMGSM8K (extended)\n56.13\n66.05\n66.11\n78.37\n82.27\nCultural Nuances\nBLEnD\n22.49\n19.47\n23.66\n28.31\n26.99\ninto multiple languages from its original English form. (3) MGSM8K (Extended): Building upon the\noriginal MGSM8K benchmark, we extended the language support to include Arabic (ar), Korean (ko),\nPortuguese (pt), and Vietnamese (vi). (4) Cultural Nuances: To evaluate the models’ ability to capture\ncultural nuances, we utilized the BLEnD benchmark (Myung et al., 2024). This benchmark is specifically\ndesigned to test LLMs on their understanding of cultural subtleties.\nQwen2.5 exhibits competitive performance in instruction following, multilingual knowledge, and mathe-\nmatical reasoning, aligning well with models of comparable size. Although it shows notable improve-\nments in capturing cultural nuances relative to its predecessor, Qwen2, there remains potential for further\nrefinement in this domain.\n5.2.3\nReward Model\nThe reward model serves as the cornerstone for guiding RL processes, and thus we conduct a separate\nevaluation of the reward model used in the Qwen2.5 series. Our assessment benchmarks encompass\nReward Bench (Lambert et al., 2024), RMB (Zhou et al., 2024), PPE (Frick et al., 2024b), and an internally\ncollected out-of-domain Chinese human preference benchmark (Human-Preference-Chinese) to provide\na comprehensive analysis. For comparison, we included baseline models such as Nemotron-4-340B-\nReward (Adler et al., 2024), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024c), and Athene-RM-\n70B (Frick et al., 2024a). The results are shown in Table 15. Overall, our findings indicate that Llama-3.1-\nNemotron-70B-Reward excels on the Reward Bench, while Athene-RM-70B performs best on the RMB\nbenchmark. The Qwen2.5-RM-72B, leads in both the PPE and Human-Preference-Chinese evaluations,\nranking second only to Athene-RM-70B on the RMB and achieving a performance level comparable to\n15\n\nTable 15: Performance comparison across multiple RM benchmarks.\nMetric\nNemotron-4-340B-\nReward\nLlama-3.1-Nemotron-\n70B-Reward\nAthene-RM\n-70B\nQwen2.5-RM\n-72B\nReward Bench\nChat\n95.80\n97.50\n98.32\n97.21\nChat Hard\n87.10\n85.70\n70.61\n78.73\nSafety\n91.50\n95.10\n92.10\n92.71\nReasoning\n93.60\n98.10\n92.19\n97.65\nScore\n92.00\n94.10\n88.32\n91.59\nRMB\nHelpfulness (BoN)\n48.85\n61.02\n67.24\n65.72\nHelpfulness (Pairwise)\n68.70\n75.28\n80.82\n78.83\nHarmlessness (BoN)\n50.92\n52.00\n67.02\n56.35\nHarmlessness (Pairwise)\n70.84\n69.96\n80.83\n73.94\nOverall\n59.83\n64.57\n73.98\n68.71\nPPE\nHuman Preference\n59.28\n64.32\n66.48\n64.80\nIFEval\n62.66\n63.40\n62.15\n67.97\nGPQA\n56.56\n59.14\n59.26\n59.80\nMATH\n65.12\n69.73\n79.14\n81.48\nMBPP-Plus\n49.15\n55.62\n67.97\n64.34\nMMLU-Pro\n69.69\n70.20\n76.95\n75.66\nObjective-Avg\n60.64\n63.62\n69.09\n69.85\nHuman-Preference-Chinese\nAccuracy\n50.46\n59.95\n61.11\n61.27\nNemotron-4-340B-Reward on the Reward Bench, albeit slightly behind Llama-3.1-Nemotron-70B-Reward.\nDue to the lack of evaluation methods for reward models, current reward models are typically evaluated\nusing Reward Bench. However, our evaluation results from multiple RM benchmarks suggest that over-\noptimization on a specific benchmark may trigger Goodhart’s law (Hoskin, 1996), resulting in degraded\nperformance on other benchmarks and potentially impacting downstream alignment performance. This\nhighlights the need for comprehensive evaluation of reward models across diverse benchmarks rather\nthan relying solely on a single benchmark.\nMore importantly, through iterative experimentation, we have also come to recognize a critical limitation:\ncurrent reward model evaluation benchmarks do not accurately predict the performance of the RL models\ntrained under their guidance. In other words, a higher score on RM benchmarks does not necessarily\ncorrelate with superior performance of the resulting RL model. This insight underscores the need for\nfurther research into more predictive evaluation methods for reward models.\n5.2.4\nLong Context Capabilities\nWe utilize three benchmarks to evaluate long context capabilities of Qwen2.5 models: RULER (Hsieh\net al., 2024), LV-Eval (Yuan et al., 2024), and Longbench-Chat (Bai et al., 2024). In LV-Eval, we adopt\nkeyword recall as the reported score to mitigate the high rate of false negatives present in the original\nmetrics.\nThe results are shown in Table 16 and Table 17. We can observe that the Qwen2.5 models, after equipping\nlength extrapolation techniques (i.e., DCA + YARN), have demonstrated strong long context processing\ncapabilities on the three datasets. Among them, Qwen2.5-72B-Instruct has shown the strongest perfor-\nmance across all context lengths, significantly outperforming existing open-weight long-context models\nas well as the proprietary models like GPT-4o-mini and GPT-4.\nFurthermore, as shown in Figure 2, Qwen2.5-Turbo achieves 100% accuracy in the 1M-token passkey\nretrieval task, demonstrating its exceptional ability to capture detailed information from ultra-long\ncontexts. We develop a sparse attention mechanism based on Minference (Jiang et al., 2024b) to signifi-\ncantly enhance inference speed, which is critical for user experience when processing long contexts. For\nsequences of 1M tokens, this approach reduces the computational load of the attention mechanism by\n12.5 times. Figure 3 illustrates the time to first token (TTFT) of Qwen2.5-Turbo across various hardware\nconfigurations, where our method achieves a 3.2 to 4.3 times speedup.\n16\n\nTable 16: Performance of Qwen2.5 Models on RULER. YARN+DCA does not change the model behavior\nwithin 32K tokens.\nModel\nClaimed\nLength\nRULER\nAvg.\n4K\n8K\n16K\n32K\n64K\n128K\nGLM4-9b-Chat-1M\n1M\n89.9\n94.7\n92.8\n92.1\n89.9\n86.7\n83.1\nLlama-3-8B-Instruct-Gradient-1048k\n1M\n88.3\n95.5\n93.8\n91.6\n87.4\n84.7\n77.0\nLlama-3.1-70B-Instruct\n128K\n89.6\n96.5\n95.8\n95.4\n94.8\n88.4\n66.6\nGPT-4o-mini\n128K\n87.3\n95.0\n92.9\n92.7\n90.2\n87.6\n65.8\nGPT-4\n128K\n91.6\n96.6\n96.3\n95.2\n93.2\n87.0\n81.2\nQwen2.5-7B-Instruct\n128K\n85.4\n96.7\n95.1\n93.7\n89.4\n82.3\n55.1\nw/o DCA + YARN\n80.1\n96.7\n95.1\n93.7\n89.4\n74.5\n31.4\nQwen2.5-14B-Instruct\n128K\n91.4\n97.7\n96.8\n95.9\n93.4\n86.7\n78.1\nw/o DCA + YARN\n86.5\n97.7\n96.8\n95.9\n93.4\n82.3\n53.0\nQwen2.5-32B-Instruct\n128K\n92.9\n96.9\n97.1\n95.5\n95.5\n90.3\n82.0\nw/o DCA + YARN\n88.0\n96.9\n97.1\n95.5\n95.5\n85.3\n57.7\nQwen2.5-72B-Instruct\n128K\n95.1\n97.7\n97.2\n97.7\n96.5\n93.0\n88.4\nw/o DCA + YARN\n90.8\n97.7\n97.2\n97.7\n96.5\n88.5\n67.0\nQwen2.5-Turbo\n1M\n93.1\n97.5\n95.7\n95.5\n94.8\n90.8\n84.5\nTable 17: Performance of Qwen2.5 Models on LV-Eval and LongBench-Chat. YARN+DCA does not\nchange the model behavior within 32k tokens.\nModel\nClaimed\nLength\nLV-Eval\nLongBench-\nChat\n16k\n32k\n64k\n128k\n256k\nGLM4-9B-Chat-1M\n1M\n46.4\n43.2\n42.9\n40.4\n37.0\n7.82\nLlama-3-8B-Instruct-Gradient-1048k\n1M\n31.7\n31.8\n28.8\n26.3\n21.1\n6.20\nLlama-3.1-70B-Instruct\n128k\n48.6\n47.4\n42.9\n26.2\nN/A\n6.80\nGPT-4o-mini\n128k\n52.9\n48.1\n46.0\n40.7\nN/A\n8.48\nQwen2.5-7B-Instruct\n128k\n55.9\n49.7\n48.0\n41.1\n36.9\n7.42\nw/o DCA + YARN\n55.9\n49.7\n33.1\n13.6\n0.5\n-\nQwen2.5-14B-Instruct\n128k\n53.0\n50.8\n46.8\n43.6\n39.4\n8.04\nw/o DCA + YARN\n53.0\n50.8\n37.0\n18.4\n0.8\n-\nQwen2.5-32B-Instruct\n128k\n56.0\n53.6\n48.8\n45.3\n41.0\n8.70\nw/o DCA + YARN\n56.0\n53.6\n40.1\n20.5\n0.7\n-\nQwen2.5-72B-Instruct\n128k\n60.4\n57.5\n53.9\n50.9\n45.2\n8.72\nw/o DCA + YARN\n60.4\n57.5\n47.4\n27.0\n2.4\n-\nQwen2.5-Turbo\n1M\n53.4\n50.0\n45.4\n43.9\n38.0\n8.34\nContext Length (# Tokens)\nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nTesting Qwen2.5-Turbo via “Passkey Retrieval”\nRetrieve Hidden Number from Irrelevant Sentences across Context Lengths and Document Depth\n100%\nAccuracy of \nRetrieval\n50%\nAccuracy of \nRetrieval\n0%\nAccuracy of \nRetrieval\nFigure 2: Performance of Qwen2.5-Turbo on Passkey Retrieval Task with 1M Token Lengths.\n17\n\n4.3x\n3.1x\n1.7x\n3.2x\n2.4x\n1.4x\n5.6x\n4.1x\n2.3x\n4x\n2.9x\n1.7x\nFigure 3: TTFT (Time To First Token) of Qwen2.5-Turbo and Qwen2.5-7B with Full Attention and Our\nMethod.\n6\nConclusion\nQwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-\ntraining on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning\nand multi-stage reinforcement learning. These improvements boost human preference alignment, long\ntext generation, and structural data analysis, making Qwen2.5 highly effective for instruction-following\ntasks. Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters\nand proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus.\nEmpirical evaluations show that Qwen2.5-72B-Instruct matches the performance of the state-of-the-art\nLlama-3-405B-Instruct, despite being six times smaller. Qwen2.5 also serves as a foundation for specialized\nmodels, demonstrating its versatility for domain-specific applications. We believe that Qwen2.5’s robust\nperformance, flexible architecture, and broad availability make it a valuable resource for both academic\nresearch and industrial applications, positioning it as a key player of future innovations.\nIn the future, we will focus on advancing robust foundational models. First, we will iteratively refine both\nbase and instruction-tuned large language models (LLMs) by incorporating broader, more diverse, higher-\nquality data. Second, we will also continue to develop multimodal models. Our goal is to integrate\nvarious modalities into a unified framework. This will facilitate seamless, end-to-end information\nprocessing across textual, visual, and auditory domains. Third, we are committed to enhancing the\nreasoning capabilities of our models. This will be achieved through strategic scaling of inference compute\nresources. These efforts aim to push the boundaries of current technological limitations and contribute to\nthe broader field of artificial intelligence.\n7\nAuthors\nCore Contributors: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le\nYu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,\n18\n\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui,\nZhenru Zhang, Zihan Qiu\nContributors: Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Chaojie Yang, Chang Si, Cheng Chen,\nChengpeng Li, Chujie Zheng, Fan Hong, Guanting Dong, Guobin Zhao, Hangrui Hu, Hanyu Zhao, Hao\nLin, Hao Xiang, Haoyan Huang, Humen Zhong, Jialin Wang, Jialong Tang, Jiandong Jiang, Jianqiang\nWan, Jianxin Ma, Jianyuan Zeng, Jie Zhang, Jin Xu, Jinkai Wang, Jinzheng He, Jun Tang, Ke Yi, Keqin\nChen, Langshi Chen, Le Jiang, Lei Zhang, Liang Chen, Man Yuan, Mingkun Yang, Minmin Sun, Na Ni,\nNuo Chen, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu, Rong Zhang,\nRu Peng, Ruize Gao, Shanghaoran Quan, Shen Huang, Shuai Bai, Shuang Luo, Sibo Song, Song Chen,\nTao He, Ting He, Wei Ding, Wei Liao, Weijia Xu, Wenbin Ge, Wenbiao Yin, Wenyuan Yu, Xianyan Jia,\nXianzhong Shi, Xiaodong Deng, Xiaoming Huang, Ximing Zhou, Xinyu Wang, Xipin Wei, Xuejing Liu,\nYang Liu, Yang Yao, Yang Zhang, Yibo Miao, Yidan Zhang, Yikai Zhu, Yinger Zhang, Yong Jiang, Yong Li,\nYongan Yue, Yuanzhi Zhu, Yunfei Chu, Zekun Wang, Zhaohai Li, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang\nGuo, Zhipeng Zhang, Zhiying Xu, Zile Qiao, Ziye Meng\nReferences\nMarah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan\nBjorck, S´ebastien Bubeck, Martin Cai, Caio C´esar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary,\nParul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit\nGarg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie\nHuynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud\nKhademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric\nLin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun\nPatra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset,\nSambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital\nShah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua\nWang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan\nYu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your\nphone. CoRR, abs/2404.14219, 2024.\nBo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared\nCasper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier\nDelalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman,\nShaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu\nJawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick\nLeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Peters Long, Ameya Mahabaleshwarkar, Somshubra\nMajumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan,\nSean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti,\nChristopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai\nPrabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak\nScowcroft, Jason D. Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha\nSmelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang\nSun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang,\nVivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340B technical report. CoRR, abs/2406.11704,\n2024.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr´on, and Sumit\nSanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. In\nEMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM´erouane Debbah, ´Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Maz-\nzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The Falcon series of open language\nmodels. CoRR, abs/2311.16867, 2023.\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nAnthropic. Introducing Claude, 2023a. URL https://www.anthropic.com/index/introducing-claude.\nAnthropic. Claude 2. Technical report, Anthropic, 2023b. URL https://www-files.anthropic.com/pro\nduction/images/Model-Card-Claude-2.pdf.\n19\n\nAnthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, AI, 2024. URL\nhttps://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model Card Claud\ne 3.pdf.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large\nlanguage models. CoRR, abs/2108.07732, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang\nZhu. Qwen technical report. CoRR, abs/2309.16609, 2023.\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign:\nA recipe for long context alignment of large language models. In EMNLP (Findings), pp. 1376–1395.\nAssociation for Computational Linguistics, 2024.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark:\nA parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In NeurIPS, 2020.\nBoxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He,\nXianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of LLMs: A\nsurvey. CoRR, abs/2406.01252, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg,\nand Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code\ngeneration. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor\nBabuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,\nJan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR,\nabs/2107.03374, 2021.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.\nTheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889–7901. Association for\nComputational Linguistics, 2023a.\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying\nChen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. MultilingualSIFT: Multilingual\nsupervised instruction fine-tuning, 2023b. URL https://github.com/FreedomIntelligence/Multili\nngualSIFT.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. CoRR,\nabs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\n20\n\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language\nmodels. CoRR, abs/2401.06066, 2024.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 933–941.\nPMLR, 2017.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou.\nSelf-play with execution feedback: Improving instruction-following capabilities of large language\nmodels. CoRR, abs/2406.13542, 2024.\nShihan Dou, Jiazheng Zhang, Jianxiang Zang, Yunbo Tao, Haoxiang Jia, Shichun Liu, Yuming Yang,\nShenxi Wu, Shaoqing Zhang, Muling Wu, et al. Multi-programming language sandbox for llms. CoRR,\nabs/2410.23074, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aur´elien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh\nTang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus\nNikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,\nEhab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr´egoire Mialon, Guan\nPang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1–120:39, 2022.\nAlena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina\nAkhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Ka-\nterina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova,\nDenis Dimitrov, Alexander Panchenko, and Sergey Markov. MERA: A comprehensive LLM evaluation\nin russian. CoRR, abs/2401.04531, 2024.\nEvan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene-70b:\nRedefining the boundaries of post-training for open models, July 2024a. URL https://nexusflow.ai/b\nlogs/athene.\nEvan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Nikolas Angelopoulos, Jiantao Jiao,\nBanghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for RLHF. CoRR,\nabs/2410.14872, 2024b.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,\nRohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we\ndone with mmlu? CoRR, abs/2406.04127, 2024.\nGemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\nTechnical report, Google, 2024. URL https://storage.googleapis.com/deepmind-media/gemini/gemi\nni v1 5 report.pdf.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\nL´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, et al. Gemma 2: Improving\nopen language models at a practical size. CoRR, abs/2408.00118, 2024.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana\nKrishnan, Marc’Aurelio Ranzato, Francisco Guzm´an, and Angela Fan. The Flores-101 evaluation\nbenchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:\n522–538, 2022.\n21\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS\nDatasets and Benchmarks, 2021b.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-\noptimal large language models. CoRR, abs/2203.15556, 2022.\nKeith Hoskin. The “awful idea of accountability”: Inscribing people into the measurement of objects.\nAccountability: Power, ethos and the technologies of managing, 1996.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\nand Boris Ginsburg. RULER: What’s the real context size of your long-context language models? CoRR,\nabs/2404.06654, 2024.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang\nHuang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang\nZhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training\nstrategies. CoRR, abs/2404.06395, 2024.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen\nYu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of\nlarge language models for code. CoRR, abs/2403.07974, 2024.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ee\nLacroix, and William El Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,\nDevendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\nGuillaume Bour, Guillaume Lample, L´elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´eophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mixtral of experts. CoRR,\nabs/2401.04088, 2024a.\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han,\nAmir H Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating\npre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024b.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transform-\ners: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR,\nabs/2001.08361, 2020.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass primary\nschool exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, pp. 12359–12374.\nAssociation for Computational Linguistics, 2023.\nNathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad Miranda, Bill Yuchen\nLin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith,\nand Hanna Hajishirzi. RewardBench: Evaluating reward models for language modeling. CoRR,\nabs/2403.13787, 2024.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation\nand automatic sharding. CoRR, abs/2006.16668, 2020.\n22\n\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez,\nand Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder\npipeline. CoRR, abs/2406.11939, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL (1), pp. 3214–3252. Association for Computational Linguistics, 2022a.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,\nNaman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,\nVishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab,\nVeselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In\nEMNLP, pp. 9019–9052. Association for Computational Linguistics, 2022b.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT\nreally correct? Rigorous evaluation of large language models for code generation. In NeurIPS, 2023.\nKeming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers for\nboosting rewards and mitigating tax in alignment. CoRR, abs/2405.17931, 2024a.\nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all\ncharacters: Attaining arbitrary role-play via self-alignment. CoRR, abs/2401.12474, 2024b.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. Crosslingual generalization through multitask finetuning. In ACL (1), pp. 15991–16111.\nAssociation for Computational Linguistics, 2023.\nJunho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty,\nEunsu Kim, Carla P´erez-Almendros, Abinew Ali Ayele, V´ıctor Guti´errez-Basulto, Yazm´ın Ib´a˜nez-\nGarc´ıa, Hwaran Lee, Shamsuddeen Hassan Muhammad, Ki-Woong Park, Anar Sabuhi Rzayev, Nina\nWhite, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jos´e Camacho-Collados,\nand Alice Oh. Blend: A benchmark for llms on everyday knowledge in diverse cultures and languages.\nCoRR, abs/2406.09948, 2024.\nOpenAI. GPT4 technical report. CoRR, abs/2303.08774, 2023.\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Learning to reason with LLMs, 2024b. URL https://openai.com/index/learning-to-reaso\nn-with-llms/.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instructions with human feedback. In NeurIPS, 2022.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In EMNLP (1), pp. 2362–2376.\nAssociation for Computational Linguistics, 2020.\nShanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang\nZhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate long texts.\nCoRR, abs/2410.23933, 2024.\nQwen Team. Code with CodeQwen1.5, 2024a. URL https://qwenlm.github.io/blog/codeqwen1.5/.\nQwen Team. Introducing Qwen1.5, 2024b. URL https://qwenlm.github.io/blog/qwen1.5/.\nQwen Team. Introducing Qwen2-Math, 2024c. URL https://qwenlm.github.io/blog/qwen2-math/.\nQwen Team. QwQ: Reflect deeply on the boundaries of the unknown, 2024d. URL https://qwenlm.git\nhub.io/blog/qwq-32b-preview/.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understand-\ning by generative pre-training. Technical report, OpenAI, 2018.\n23\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad\nAwan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts inference and\ntraining to power next-generation AI scale. In ICML, volume 162 of Proceedings of Machine Learning\nResearch, pp. 18332–18346. PMLR, 2022.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A benchmark.\nCoRR, abs/2311.12022, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial\nwinograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In ACL (1). The Association for Computer Linguistics, 2016.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and\nDaya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\nCoRR, abs/2402.03300, 2024.\nJianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023. URL\nhttps://spaces.ac.cn/archives/9577.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench\ntasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051. Association\nfor Computational Linguistics, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models.\nCoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian\nFuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui\nHou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,\nPunit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,\nYuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aur´elien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nfoundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\nBinghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu\nZhou, Chenyu Shi, et al. Secrets of RLHF in large language models part II: Reward modeling. CoRR,\nabs/2401.06080, 2024a.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords.\nIn AAAI, pp. 9154–9160. AAAI Press, 2020.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang\nYue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task language understanding\nbenchmark. CoRR, abs/2406.01574, 2024b.\nZhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii\nKuchaiev, and Yi Dong. HelpSteer2-Preference: Complementing ratings with preferences. CoRR,\nabs/2410.01257, 2024c.\n24\n\nColin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-\nZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie\nNeiswanger, and Micah Goldblum. LiveBench: A challenging, contamination-free LLM benchmark.\nCoRR, abs/2406.19314, 2024.\nHao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, and\nJunyang Lin. Aligning large language models via self-steering optimization. CoRR, abs/2410.17131,\n2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad,\nSharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang,\nand Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,\nXipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR,\nabs/2407.10671, 2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,\nJingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model\nvia self-improvement. CoRR, abs/2409.12122, 2024b.\nJian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui,\nand Junyang Lin. Evaluating and aligning codellms on human preference. CoRR, abs/2412.05210,\n2024c.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset\nfor paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685–3690. Association for Computational\nLinguistics, 2019.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\nZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming\nYang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi\nXu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open\nfoundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nTao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao,\nDahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced long-context\nbenchmark with 5 length levels up to 256K. CoRR, abs/2402.05136, 2024.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling\nrelationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825,\n2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine\nreally finish your sentence? In ACL (1), pp. 4791–4800. Association for Computational Linguistics,\n2019.\nYidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, and\nJingren Zhou. P-MMEval: A parallel multilingual multitask benchmark for consistent evaluation of\nLLMs. CoRR, abs/2411.09116, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS, 2023.\nEnyu Zhou, Guodong Zheng, Bing Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong,\nJessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. RMB: Comprehensively\nbenchmarking reward models in LLM alignment. CoRR, abs/2410.09893, 2024.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023.\n25\n\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. ST-MoE: Designing stable and transferable sparse expert models. CoRR, abs/2202.08906, 2022.\n26\n"
    }
  ]
}