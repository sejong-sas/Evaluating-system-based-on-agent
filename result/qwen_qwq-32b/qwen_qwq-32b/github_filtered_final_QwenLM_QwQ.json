{
  "1-1 (Weights)": "\"model_name = \\\"Qwen/QwQ-32B\\\"\"\n\"To run the Qwen/QwQ-32B-GGUF model with Ollama, use the following command.\"\n\"ollama run hf.co/Qwen/QwQ-32B-GGUF:Q4_K_M # select one from Q8_0; Q6_K; Q5_K_M; Q5_0; Q4_K_M; Q4_0; Q3_K_M; Q2_K.\"\n\"ðŸ¤— <a href=\\\"https://huggingface.co/Qwen/QwQ-32B\\\">Hugging Face</a>\"",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "model_name = \"Qwen/QwQ-32B\""
    },
    {
      "source": "[readme]",
      "quote": "To run the Qwen/QwQ-32B-GGUF model with Ollama, use the following command."
    },
    {
      "source": "[readme]",
      "quote": "ollama run hf.co/Qwen/QwQ-32B-GGUF:Q4_K_M # select one from Q8_0; Q6_K; Q5_K_M; Q5_0; Q4_K_M; Q4_0; Q3_K_M; Q2_K."
    },
    {
      "source": "[readme]",
      "quote": "ðŸ¤— <a href=\"https://huggingface.co/Qwen/QwQ-32B\">Hugging Face</a>"
    }
  ],
  "1-2 (Code)": "\"parser.add_argument(\\\"--model_name\\\", type=str, default='Qwen/QwQ-32B', help=\\\"model name of vllm server\\\")\"",
  "1-2 (Code)__evidence": [
    {
      "source": "[py_files/eval/generate_api_answers/infer_multithread.py]",
      "quote": "parser.add_argument(\"--model_name\", type=str, default='Qwen/QwQ-32B', help=\"model name of vllm server\")"
    }
  ],
  "1-3 (License)": "\"Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\"\n\"# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\"\n\"# You may obtain a copy of the License at\"\n\"#     http://www.apache.org/licenses/LICENSE-2.0\"\n\"# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\"",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/eval/eval/ifeval_utils/instructions_util.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/eval/eval/ifeval_utils/instructions_util.py]",
      "quote": "# You may obtain a copy of the License at"
    },
    {
      "source": "[py_files/eval/eval/ifeval_utils/instructions_util.py]",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    },
    {
      "source": "[py_files/eval/eval/ifeval_utils/instructions_util.py]",
      "quote": "# distributed under the License is distributed on an \"AS IS\" BASIS,"
    }
  ],
  "1-4 (Paper)": "\"ðŸ“‘ <a href=\\\"https://qwenlm.github.io/blog/qwq-32b/\\\">Blog</a>\"\n\"title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\"\n\"url = {https://qwenlm.github.io/blog/qwq-32b/},\"",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "ðŸ“‘ <a href=\"https://qwenlm.github.io/blog/qwq-32b/\">Blog</a>"
    },
    {
      "source": "[readme]",
      "quote": "title = {QwQ-32B: Embracing the Power of Reinforcement Learning},"
    },
    {
      "source": "[readme]",
      "quote": "url = {https://qwenlm.github.io/blog/qwq-32b/},"
    }
  ],
  "1-5 (Architecture)": "\"Our latest release, QwQ-32B, is a mid-sized model that competes effectively with top-tier reasoning models like DeepSeek-R1 and o1-mini, delivering robust and competitive results.\"  This sentence provides the sole architectural detail available: it names the variant (\"QwQ-32B\"), categorizes it as \"mid-sized,\" and highlights its competitive reasoning capability relative to other models.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Our latest release, QwQ-32B, is a mid-sized model that competes effectively with top-tier reasoning models like DeepSeek-R1 and o1-mini, delivering robust and competitive results."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The material explicitly opens with \"## Try QwQ with API\", signalling official support for remote access. Users are told, \"If you face issues in deploying QwQ, we encourage you to test our API service provided by [Alibaba Cloud Model Studio](https://www.alibabacloud.com/help/en/model-studio/developer-reference/what-is-qwen-llm).\"  Concrete invocation guidance is supplied via a code snippet: \"parser.add_argument(\"--model_name\", type=str, default='Qwen/QwQ-32B', help=\"model name of vllm server\")\", which designates the served endpointâ€™s model identifier. A further call example demonstrates the request path and model tag: \"res = get_content(user_input, \\\"http://10.77.249.36:8030/v1\\\", \\\"Qwen/QwQ\\\")\". Together these quotations establish that a publicly reachable API is offered through Alibaba Cloud Model Studio, and they illustrate parameter and URL formats for issuing inference queries to the QwQ model.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "## Try QwQ with API"
    },
    {
      "source": "[readme]",
      "quote": "If you face issues in deploying QwQ, we encourage you to test our API service provided by [Alibaba Cloud Model Studio](https://www.alibabacloud.com/help/en/model-studio/developer-reference/what-is-qwen-llm)."
    },
    {
      "source": "[py_files/eval/generate_api_answers/infer_multithread.py]",
      "quote": "parser.add_argument(\"--model_name\", type=str, default='Qwen/QwQ-32B', help=\"model name of vllm server\")"
    },
    {
      "source": "[py_files/eval/generate_api_answers/utils_vllm.py]",
      "quote": "res = get_content(user_input, \"http://10.77.249.36:8030/v1\", \"Qwen/QwQ\")"
    }
  ],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "The line \"title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\" directly links the QwQ-32B model to a reinforcement-learningâ€“centred methodology, indicating that RL is a prominent component of its training philosophy.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "title = {QwQ-32B: Embracing the Power of Reinforcement Learning},"
    }
  ],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "\"return langdetect.detect(value) == self._language\" demonstrates the explicit language-based criterion applied during data filtering: each text sample is passed to a language-detection function, and only those for which the detected language equals the specified target (\"self._language\") are kept. This line of code shows that the pipeline includes an automated language check before further processing.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[py_files/eval/eval/ifeval_utils/instructions.py]",
      "quote": "return langdetect.detect(value) == self._language"
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "used"
  }
}