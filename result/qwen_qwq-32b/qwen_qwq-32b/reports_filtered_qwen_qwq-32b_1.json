{
  "1-1 (Weights)": "The quotes repeatedly emphasize that ‚ÄúQwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat,‚Äù confirming that anyone can obtain the full 32-billion-parameter checkpoint. Multiple concrete download locations are listed: the main repository at ‚Äúhttps://huggingface.co/Qwen/QwQ-32B,‚Äù an Unsloth repository that supplies ‚Äúdynamic 4-bit quants‚Äù at ‚Äúhttps://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit,‚Äù and a GGUF collection‚Äî‚ÄúAll our GGUFs are at https://huggingface.co/unsloth/QwQ-32B-GGUF.‚Äù The maintainers add that ‚ÄúYou can choose Q4_K_M, or other quantized versions (like BF16 full precision),‚Äù showing multiple precision/size options. They also note that the dynamic 4-bit files ‚Äúincrease accuracy vs naive 4bit quantizations‚Äù and provide ‚ÄúQwQ quantization error plot analysis for both activation and weight quantization errors.‚Äù A README excerpt for the main model repository is cited, illustrating configuration details (e.g., a ‚Äòrope_scaling‚Äô block). Broader context is supplied by two further sentences: ‚ÄúBasically, the Qwen2.5 series include dense models ‚Ä¶ 32B ‚Ä¶ parameters,‚Äù and ‚ÄúThe open-weight offerings include base models and instruction-tuned models ‚Ä¶ 32B ‚Ä¶ parameters,‚Äù reinforcing that 32-billion-parameter weights are officially released and indexed alongside more than a hundred other checkpoints across Hugging Face, ModelScope, and Kaggle.",
  "1-2 (Code)": "",
  "1-3 (License)": "Licensing information is consistent across the quotes. Two separate statements explicitly say ‚ÄúQwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license,‚Äù while the tabular snippets (‚Äú32B 64 40 / 8 ‚Ä¶ Apache 2.0‚Äù and ‚ÄúTable 1 ‚Ä¶ 32B ‚Ä¶ Apache 2.0‚Äù) repeat the same license reference. These sentences jointly establish that the model, its weights, and associated artifacts are distributed under the permissive Apache License 2.0, granting rights of use, modification, redistribution, and commercial exploitation without additional restrictions.",
  "1-4 (Paper)": "Several references point to the project‚Äôs technical write-up and blog. The primary source is titled ‚ÄúQwQ-32B: Embracing the Power of Reinforcement Learning | Qwen Blog Publication,‚Äù and another identical title variant appears without the subtitle. A citation line gives the canonical blog URL: ‚ÄúQwen Team. QwQ: Reflect deeply on the boundaries of the unknown, 2024d. URL https://qwenlm.github.io/blog/qwq-32b-preview/.‚Äù The blog claims that ‚ÄúQwQ-32B ‚Ä¶ achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters,‚Äù and another sentence frames it as ‚Äúa reasoning model with performance comparable to DeepSeek-R1 on many benchmarks.‚Äù Finally, broader contextual quotes explain that QwQ is built on the ‚ÄúQwen2.5‚Äù family and mention related specialized derivatives such as ‚ÄúQwen2.5-Math‚Äù and ‚ÄúQwen2.5-Coder,‚Äù situating the paper within a larger research program.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat ."
    },
    {
      "source": "[pdf_text]",
      "quote": "üõ†Ô∏è Dynamic 4-bit Quants We also uploaded dynamic 4bit quants which increase accuracy vs naive 4bit quantizations! We attach the QwQ quantization error plot analysis for both activation and weight quantization errors: We uploaded dynamic 4-bit quants to: https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"
    },
    {
      "source": "[pdf_text]",
      "quote": "All our GGUFs are at https://huggingface.co/unsloth/QwQ-32B-GGUF !"
    },
    {
      "source": "[pdf_text]",
      "quote": "For example in the readme file for https://huggingface.co/Qwen/QwQ-32B , we see: Copy { ..., \"rope_scaling\": { \"factor\": 4.0, \"original_max_position_embeddings\": 32768, \"type\": \"yarn\" } }"
    },
    {
      "source": "[pdf_text]",
      "quote": "Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models.\n32B    64    40 / 8    No    128K / 8K    Apache 2.0"
    },
    {
      "source": "[sections/https://qwenlm.github.io/blog/qwq-32b/]",
      "quote": "QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat ."
    },
    {
      "source": "[sections/https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively]",
      "quote": "All our GGUFs are at https://huggingface.co/unsloth/QwQ-32B-GGUF !"
    },
    {
      "source": "[sections/https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively]",
      "quote": "Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose Q4_K_M, or other quantized versions (like BF16 full precision). More versions at: https://huggingface.co/unsloth/QwQ-32B-GGUF"
    },
    {
      "source": "[sections/https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively]",
      "quote": "We uploaded dynamic 4-bit quants to: https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "The open-weight offerings include base models and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Over 100 models can be accessed from Hugging Face Hub, ModelScope, and Kaggle."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat ."
    },
    {
      "source": "[pdf_text]",
      "quote": "32B 64 40 / 8 No 128K / 8K Apache 2.0"
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models.\n32B    64    40 / 8    No    128K / 8K    Apache 2.0"
    },
    {
      "source": "[sections/https://qwenlm.github.io/blog/qwq-32b/]",
      "quote": "QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat ."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "32B\t64\t40 / 8\tNo\t128K / 8K\tApache 2.0"
    },
    {
      "source": "[web:https://ai.meta.com/llama/license/]",
      "quote": "https://ai.meta.com/llama/license/"
    },
    {
      "source": "[web:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE]",
      "quote": "https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen Blog Publication"
    },
    {
      "source": "[pdf_text]",
      "quote": "We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), QwQ (Qwen Team, 2024d), and multimodal models."
    },
    {
      "source": "[sections/https://qwenlm.github.io/blog/qwq-32b/]",
      "quote": "QwQ-32B: Embracing the Power of Reinforcement Learning"
    },
    {
      "source": "[sections/https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively]",
      "quote": "Qwen released QwQ-32B - a reasoning model with performance comparable to DeepSeek-R1 on many benchmarks ."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), QwQ (Qwen Team, 2024d), and multimodal models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown, 2024d. URL https://qwenlm.github.io/blog/qwq-32b-preview/."
    }
  ]
}