{
  "repo": "QwenLM/QwQ",
  "full_texts": [
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwq-32b/",
      "full_text": " QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen Blog Publication About Try Qwen Chat &nbsp; QwQ-32B: Embracing the Power of Reinforcement Learning March 6, 2025 &nbsp;·&nbsp;4 min&nbsp;·&nbsp;742 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 QWEN CHAT Hugging Face ModelScope DEMO DISCORD Scaling Reinforcement Learning (RL) has the potential to enhance model performance beyond conventional pretraining and post-training methods. Recent studies have demonstrated that RL can significantly improve the reasoning capabilities of models. For instance, DeepSeek R1 has achieved state-of-the-art performance by integrating cold-start data and multi-stage training, enabling deep thinking and complex reasoning. Our research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models. We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated). This remarkable outcome underscores the effectiveness of RL when applied to robust foundation models pretrained on extensive world knowledge. Furthermore, we have integrated agent-related capabilities into the reasoning model, enabling it to think critically while utilizing tools and adapting its reasoning based on environmental feedback. These advancements not only demonstrate the transformative potential of RL but also pave the way for further innovations in the pursuit of artificial general intelligence. QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat . Performance # QwQ-32B is evaluated across a range of benchmarks designed to assess its mathematical reasoning, coding proficiency, and general problem-solving capabilities. The results below highlight QwQ-32B&rsquo;s performance in comparison to other leading models, including DeepSeek-R1-Distilled-Qwen-32B, DeepSeek-R1-Distilled-Llama-70B, o1-mini, and the original DeepSeek-R1. Reinforcement Learning # We began with a cold-start checkpoint and implemented a reinforcement learning (RL) scaling approach driven by outcome-based rewards. In the initial stage, we scale RL specifically for math and coding tasks. Rather than relying on traditional reward models, we utilized an accuracy verifier for math problems to ensure the correctness of final solutions and a code execution server to assess whether the generated codes successfully pass predefined test cases. As training episodes progress, performance in both domains shows continuous improvement. After the first stage, we add another stage of RL for general capabilities. It is trained with rewards from general reward model and some rule-based verifiers. We find that this stage of RL training with a small amount of steps can increase the performance of other general capabilities, such as instruction following, alignment with human preference, and agent performance, without significant performance drop in math and coding. Use QwQ-32B # Below are brief examples demonstrating how to use QwQ-32B via Hugging Face Transformers and Alibaba Cloud DashScope API. from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/QwQ-32B&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;How many r&#39;s are in the word \\&#34; strawberry \\&#34; &#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 32768 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] print ( response ) from openai import OpenAI import os # Initialize OpenAI client client = OpenAI ( # If the environment variable is not configured, replace with your API Key: api_key=&#34;sk-xxx&#34; # How to get an API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key api_key = os . getenv ( &#34;DASHSCOPE_API_KEY&#34; ), base_url = &#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34; ) reasoning_content = &#34;&#34; content = &#34;&#34; is_answering = False completion = client . chat . completions . create ( model = &#34;qwq-32b&#34; , messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : &#34;Which is larger, 9.9 or 9.11?&#34; } ], stream = True , # Uncomment the following line to return token usage in the last chunk # stream_options={ # &#34;include_usage&#34;: True # } ) print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;reasoning content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) for chunk in completion : # If chunk.choices is empty, print usage if not chunk . choices : print ( &#34; \\n Usage:&#34; ) print ( chunk . usage ) else : delta = chunk . choices [ 0 ] . delta # Print reasoning content if hasattr ( delta , &#39;reasoning_content&#39; ) and delta . reasoning_content is not None : print ( delta . reasoning_content , end = &#39;&#39; , flush = True ) reasoning_content += delta . reasoning_content else : if delta . content != &#34;&#34; and is_answering is False : print ( &#34; \\n &#34; + &#34;=&#34; * 20 + &#34;content&#34; + &#34;=&#34; * 20 + &#34; \\n &#34; ) is_answering = True # Print content print ( delta . content , end = &#39;&#39; , flush = True ) content += delta . content Future Work # This marks Qwen&rsquo;s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling. &copy; 2025 Qwen Powered by Hugo "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2309.00071",
      "full_text": " [2309.00071] YaRN: Efficient Context Window Extension of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2309.00071 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2309.00071 (cs) [Submitted on 31 Aug 2023 ( v1 ), last revised 1 Nov 2023 (this version, v2)] Title: YaRN: Efficient Context Window Extension of Large Language Models Authors: Bowen Peng , Jeffrey Quesnelle , Honglu Fan , Enrico Shippole View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF Abstract: Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2309.00071 [cs.CL] &nbsp; (or arXiv:2309.00071v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2309.00071 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jeffrey Quesnelle [ view email ] [v1] Thu, 31 Aug 2023 18:18:07 UTC (42 KB) [v2] Wed, 1 Nov 2023 17:28:26 UTC (354 KB) Full-text links: Access Paper: View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-09 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}