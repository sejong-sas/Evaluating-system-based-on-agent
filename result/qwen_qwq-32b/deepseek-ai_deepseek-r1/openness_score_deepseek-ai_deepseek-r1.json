{
  "model": "deepseek-ai/DeepSeek-R1",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The README states the code and weights are released under the MIT licence, which grants use, modification, redistribution and commercial use without additional restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An arXiv technical report specifically titled “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning” (arXiv:2501.12948) is cited."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quotation gives the type or quantity of hardware used to train DeepSeek-R1."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "There is no information about the training software stack (framework versions, libraries, etc.)."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "API available (strong evidence): You can chat with DeepSeek-R1 on DeepSeek's official website: https://chat.deepseek.com, and switch on the button \"DeepThink\". We also provide OpenAI-Compatible API at DeepSeek Platform: https://platform.deepseek.com/"
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.0,
      "reason": "No quotes provide sources, sizes or licensing of the pre-training corpus for DeepSeek-R1. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "It is stated that a ‘small amount of long CoT data’ was constructed for cold-start fine-tuning, but no dataset name, size or access link is supplied."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "Only a remark that RL data is currently limited; no disclosure of its composition or source."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No concrete filtering details in quotes."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The README states the code and weights are released under the MIT licence, which grants use, modification, redistribution and commercial use without additional restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An arXiv technical report specifically titled “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning” (arXiv:2501.12948) is cited."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quotation gives the type or quantity of hardware used to train DeepSeek-R1."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "There is no information about the training software stack (framework versions, libraries, etc.)."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "API available (strong evidence): You can chat with DeepSeek-R1 on DeepSeek's official website: https://chat.deepseek.com, and switch on the button \"DeepThink\". We also provide OpenAI-Compatible API at DeepSeek Platform: https://platform.deepseek.com/"
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.0,
      "reason": "No quotes provide sources, sizes or licensing of the pre-training corpus for DeepSeek-R1. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "It is stated that a ‘small amount of long CoT data’ was constructed for cold-start fine-tuning, but no dataset name, size or access link is supplied."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "Only a remark that RL data is currently limited; no disclosure of its composition or source."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No concrete filtering details in quotes."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 4.688,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 7.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": true
  }
}