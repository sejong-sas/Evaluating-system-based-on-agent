{
  "1-1 (Weights)": "The quotes explicitly mention that DeepSeek-R1 and DeepSeek-R1-Zero, along with six additional dense models (sized 1.5B, 7B, 8B, 14B, 32B, and 70B), have been open-sourced to support the research community. These models are distilled from DeepSeek-R1 using methodologies based on Qwen and Llama, highlighting the availability of model weights for research and further exploration.",
  "1-2 (Code)": "No information regarding public training code, inference pipelines, or other code-based resources for DeepSeek-R1 is provided in the quotes.",
  "1-3 (License)": "There is no license-related information provided in the quotes, so details about usage rights, modifications, redistribution, or commercial use are not available.",
  "1-4 (Paper)": "The quotes consistently refer to an official paper titled 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'. This paper appears to be the primary technical resource associated with DeepSeek-R1, focusing on the reinforcement learning techniques used to enhance the model's reasoning capabilities.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    },
    {
      "source": "[abstract]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
    },
    {
      "source": "[title]",
      "quote": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
    }
  ],
  "1-5 (Architecture)": "",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The quotes indicate that the DeepSeek-R1 model provides an accessible API that is open source. This API is highlighted as a resource for the research community, with the potential to aid in distilling smaller, improved models in the future.",
  "3-1 (Pre-training)": "",
  "3-2 (Fine-tuning)": "The provided quotes explain that DeepSeek-R1 is integrated into a multi-stage training pipeline that includes fine-tuning with a small amount of cold-start data. They further detail the use of reasoning data generated by DeepSeek-R1 to fine-tune several dense models, and note that DeepSeek-R1 itself functions as a teacher model generating 800K training samples for the fine-tuning process of smaller dense models.",
  "3-3 (Reinforcement Learning)": "The quotes reveal that reinforcement learning (RL) plays a significant role in the development of DeepSeek-R1 and its variant, DeepSeek-R1-Zero. DeepSeek-R1-Zero is described as undergoing thousands of RL steps and achieving strong reasoning performance without supervised fine-tuning, emphasizing a pure RL approach without reliance on cold-start data. Additionally, one quote points out that DeepSeek-R1, when compared to a later version (DeepSeek-V3), suffers from reduced performance on specific benchmarks, partially due to its tendency to refuse answering certain queries after safety-oriented RL adjustments.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/deepseek-ai/DeepSeek-R1/main/DeepSeek_R1.pdf]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks."
    }
  ],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The provided quote explains that for DeepSeek-R1, a small amount of long chain-of-thought (CoT) data is carefully constructed and collected to fine-tune the model as the initial reinforcement learning actor. This targeted fine-tuning strategy is introduced to avoid the early unstable cold start phase experienced by the base model and is positioned in contrast to methods like those used in DeepSeek-R1-Zero.",
  "4-3 (Reinforcement Learning Data)": "The quote indicates that the current availability of reinforcement learning training data for DeepSeek-R1 is very limited. It also suggests an anticipation that with future iterations or the next version, the engineering performance of DeepSeek-R1 may improve as more RL training data becomes available, thereby positively impacting the model's capabilities.",
  "4-4 (Data Filtering)": "The quote outlines a data filtering approach specific to the creation of cold-start data for DeepSeek-R1. A readable pattern is designed that incorporates a summary at the end of each response and actively filters out responses that are not reader-friendly. This method focuses on ensuring that the final outputs are both coherent and easily accessible for readers.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/https://raw.githubusercontent.com/deepseek-ai/DeepSeek-R1/main/DeepSeek_R1.pdf]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/2501.12948]",
      "quote": "In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}