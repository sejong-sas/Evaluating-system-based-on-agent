{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The fine-tuning data for DeepSeek-R1 is centered around the use of a small, carefully curated collection of long chain-of-thought (CoT) examples, purposefully constructed and collected to overcome the early instability issues encountered during reinforcement learning (RL) training. In contrast to the DeepSeek-R1-Zero approach, this dataset is designed to serve as a stabilizing initial point by fine-tuning either the DeepSeek-V3-Base model or the model directly as the initial RL actor. This method aims at mitigating the cold start phase instability by providing a more robust foundation for subsequent RL training processes.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/2.3.1 Cold Start]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "sections/2.3.1 Cold Start",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}