{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "According to the provided material, Gemma’s 2 B parameter variant is pre-trained alongside 9 B and 27 B models, but with several procedures that are called out explicitly. First, the 2 B and 9 B checkpoints are optimized with knowledge-distillation rather than pure next-token prediction (“We also train the 2B and 9B models with knowledge distillation … instead of next token prediction.”). Token budgets are large: the 27 B model ingests about 13 trillion tokens, the 9 B variant 8 trillion, and the 2 B model 2 trillion (“We train Gemma 2 27B on 13 trillion tokens … and the 2B on 2 trillion tokens.”).  \nHardware and parallelism details are reported for the smallest model: “For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding,” indicating data-parallel training with no model partitioning.  \nThe developers state that “We provide a brief overview of the parts of our pre-training that differs from Gemma 1,” signaling changes relative to the earlier generation. One recurring theme is a formal “three-pillar approach,” repeated twice, that emphasizes (1) safety mitigation at training time, (2) robust and transparent evaluations, and (3) continued tooling via the Responsible Generative AI Toolkit.  \nEmpirical behavior is also mentioned: “We find that Gemma 2 memorizes significantly less than prior models at a similar size, with memorization rates below 0.1 %.” Finally, scale-rationalized over-training is acknowledged: a teacher LLM distills knowledge into the 2 B and 9 B students on token counts “more than 50× the compute-optimal quantity.” Altogether, the pre-training regime couples large-scale data, knowledge-distillation, TPUv5e hardware, safety-oriented procedures, and memorization analysis for the 2 B Gemma checkpoint.",
  "3-2 (Fine-tuning)": "The fine-tuning stage for Gemma 2 expressly reuses the control-token set from Gemma 1: “Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models … but a different formatting schema,” showing continuity in controllability while updating data layout. Safety alignment remains central: “A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies,” indicating that policy compliance is a guiding objective during this phase, parallel to practices used for Gemini systems. Distribution terms encourage community participation: “Gemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications.” In sum, Gemma 2’s fine-tuning involves inherited control tokens, reformatted data, explicit safety-policy alignment, and an open-weight license that enables external supervised or instruction-tuning workflows.",
  "3-3 (Reinforcement Learning)": "For post-training alignment, the authors employ Reinforcement Learning from Human Feedback (RLHF). They say twice that they “use a similar RLHF algorithm as Gemma 1.1 … but a different reward model, which is an order of magnitude larger than the policy.” Thus, the methodology mirrors the previous Gemma generation in algorithmic structure while scaling up the reward-model capacity substantially. No other hyperparameters are noted, but the key distinction is this enlarged reward model, intended to improve feedback fidelity while keeping the core RLHF loop unchanged.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction."
    },
    {
      "source": "[pdf_text]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[pdf_text]",
      "quote": "We provide a brief overview of the parts of our pre-training that differs from Gemma 1."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to the inaugural Gemma release, we have followed a three pillar approach which focuses on safety mitigation at training time, robust and transparent model evaluations, and further development of the Responsible Generative AI Toolkit."
    },
    {
      "source": "[pdf_text]",
      "quote": "We find that Gemma 2 memorizes significantly less than prior models at a similar size, with memorization rates below 0.1% (note the log y-axis)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to the inaugural Gemma release, we have followed a three pillar approach which focuses on safety mitigation at training time, robust and transparent model evaluations, and further development of the Responsible Generative AI Toolkit, a series of models and tools to help developers implement responsibility and safety best practices for their applications."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023)."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/base]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[pdf_text]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF) We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    }
  ]
}