{
  "model_id": "google/gemma-2-2b-it",
  "full_texts": [
    {
      "arxiv_id": "https://ai.google.dev/gemma/docs/base",
      "full_text": " Gemma 3 model overview &nbsp;|&nbsp; Google AI for Developers Skip to main content Models Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemma Docs Models More Gemma Docs Solutions More Code assistance More Showcase More Community More Overview Get started Releases Models Gemma 3 Overview Model card Gemma 2 model card Gemma 1 model card Gemma 3n Overview Model card EmbeddingGemma Overview Model card Generate embeddings with Sentence Transformers Fine-tune EmbeddingGemma CodeGemma Overview Model card Generate code with Keras Generate code with JAX and Flax Code assist with Keras Prompt and system instructions PaliGemma 2 Overview v2 model card v1 model card Generate output with Keras Fine-tune with JAX and Flax Prompt and system instructions ShieldGemma 2 Overview ShieldGemma 2 Model card ShieldGemma 1 Model card Run Gemma Overview Hugging Face Transformers Ollama Gemma library Keras PyTorch Gemma.cpp Gemini API Cloud GKE Cloud Run Prompt and system instructions Gemma setup Capabilities Function calling Visual data processing Overview Image interpretation Video understanding Content creation Audio data processing Tuning guides Overview Tune using LoRA and Keras Tune using Gemma library Tune using Hugging Face Transformers and QLoRA Vision Tune using Hugging Face Transformers and QLoRA Full model fine-tune using Hugging Face Transformers Distributed tuning using Keras Application guides Personal code assistant Business email assistant Spoken language tasks Chatbot using Python Meme Generator Deployment guides Web Mobile Google Cloud LangChain Research and tools RecurrentGemma Overview Inference using JAX and Flax Fine-tune using JAX and Flax Model card DataGemma Gemma Scope Gemma-APS Community Gemmaverse Discord Legal Terms of use Prohibited use Intended use statement Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemma 3n released with audio input and optimized for use in everyday devices! Learn more Home Gemma Models Docs Send feedback Gemma 3 model overview Gemma is a family of generative artificial intelligence (AI) models and you can use them in a wide variety of generation tasks, including question answering, summarization, and reasoning. Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications. The Gemma 3 release includes the following key features. Try it in AI Studio : Image and text input : Multimodal capabilities let you input images and text to understand and analyze visual data. Start building 128K token context : Significantly large input context for analyzing more data and solving more complex problems. Function calling : Build natural language interfaces for working with programming interfaces. Start building Wide language support : Work in your language or expand your AI application&#39;s language capabilities with support for over 140 languages. Start building Developer friendly model sizes : Choose a model size (270M, 1B, 4B, 12B, 27B) and precision level that works best for your task and compute resources. You can download Gemma 3 models from Kaggle and Hugging Face . For more technical details on Gemma 3, see the Model Card and Technical Report . Earlier versions of Gemma core models are also available for download. For more information, see Previous Gemma models . Try Gemma 3 Get it on Kaggle Get it on Hugging Face Multimodal image and text input You can tackle complex analysis and generation tasks with Gemma 3 with its ability to handle image and text data. You can use the model to interpret image data, identify objects, extract text data, and complete many other visual input to text output tasks. Start building Important: The Gemma 3 270M and 1B models are text only and do not support image input . 128K token context window Gemma 3 models (4B, 12B, and 27B) can handle prompt inputs up to 128K tokens, a 16x larger context window than previous Gemma models. The large number of tokens means you can process several, multi page articles, larger single articles, or hundreds of images in a single prompt. Important: The Gemma 3 270M and 1B models can process up to 32k tokens. Wide language support Work in your own language with built-in support for over 140 languages. Gemma 3 is trained to support a large number of languages compared to previous Gemma versions, letting you take on more visual and text tasks in the languages your customers use. Start building Function calling Build intelligent, natural language controls for programming interfaces. Gemma 3 lets you define coding functions with specific syntax and constraints, and the model can call these functions to complete tasks. Start building Parameter sizes and quantization Gemma 3 models are available in 5 parameter sizes: 270M, 1B, 4B, 12B, and 27B. The models can be used with their default precision (16-bit) or with a lower precision using quantization. The different sizes and precisions represent a set of trade-offs for your AI application. Models with higher parameters and bit counts (higher precision) are generally more capable, but are more expensive to run in terms of processing cycles, memory cost and power consumption. Models with lower parameters and bit counts (lower precision) have less capabilities, but may be sufficient for your AI task. For all Gemma 3 models, Quantization-Aware Trained checkpoints are provided, which allow quantizing (reducing the precision), while preserving high-quality. The following table details the approximate GPU or TPU memory requirements for running inference with each size of the Gemma 3 model versions. Note that the numbers may changed based on inference tool. Parameters BF16 (16-bit) SFP8 (8-bit) Q4_0 (4-bit) Gemma 3 270M ( text only ) 400 MB 297 MB 240 MB Gemma 3 1B ( text only ) 1.5 GB 1.1 GB 892 MB Gemma 3 4B 6.4 GB 4.4 GB 3.4 GB Gemma 3 12B 20 GB 12.2 GB 8.7 GB Gemma 3 27B 46.4 GB 29.1 GB 21 GB Table 1. Approximate GPU or TPU memory required to load Gemma 3 models based on parameter count and quantization level. Caution: These estimates only include the memory required to load the models. They don&#39;t include the additional memory required for the prompt tokens or supporting software. Memory consumption increases based on the total number of tokens required for the prompt you run. The larger the number of tokens required to process your prompt, the higher the memory required, which is in addition to the memory required to load the model. Note: Memory requirements for fine-tuning Gemma models are significantly higher than running inference. The requirements depend on the development framework and tuning technique you use, such as Low Rank Adapter (LoRA) versus full-precision tuning. Previous Gemma models You can work with previous generations of Gemma models, which are also available from Kaggle and Hugging Face . For more technical details about previous Gemma models, see the following model card pages: Gemma 2 Model Card Gemma 1 Model Card Ready to start building? Get started with Gemma models! Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-08-14 UTC. Need to tell us more? [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-08-14 UTC.\"],[],[],null,[\"# Gemma 3 model overview\\n\\nGemma is a family of generative artificial intelligence (AI) models and you can\\nuse them in a wide variety of generation tasks, including question answering,\\nsummarization, and reasoning. Gemma models are provided with open weights and\\npermit responsible\\n[commercial use](/gemma/terms),\\nallowing you to tune and deploy them in your own projects and applications.\\n\\nThe Gemma 3 release includes the following key features. Try it in\\n[AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it):\\n\\n- [**Image and text input**](#multimodal-input): Multimodal capabilities let you input images and text to understand and analyze visual data. [Start building](/gemma/docs/core/keras_inference)\\n- [**128K token context**](#128k-context): Significantly large input context for analyzing more data and solving more complex problems.\\n- [**Function calling**](#function-calling): Build natural language interfaces for working with programming interfaces. [Start building](/gemma/docs/capabilities/function-calling)\\n- [**Wide language support**](#multilingual): Work in your language or expand your AI application's language capabilities with support for over 140 languages. [Start building](/gemma/docs/spoken-language)\\n- [**Developer friendly model sizes**](#sizes): Choose a model size (270M, 1B, 4B, 12B, 27B) and precision level that works best for your task and compute resources.\\n\\nYou can download Gemma 3 models from\\n[Kaggle](https://www.kaggle.com/models?query=gemma3&publisher=google) and\\n[Hugging Face](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d).\\nFor more technical details on Gemma 3, see the\\n[Model Card](/gemma/docs/core/model_card_3) and\\n[Technical Report](https://goo.gle/Gemma3Report).\\nEarlier versions of Gemma core models are also available for download. For more\\ninformation, see [Previous Gemma models](#previous-models).\\n\\n[Try Gemma 3](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it)\\n[Get it on Kaggle](https://www.kaggle.com/models?query=gemma3&publisher=google)\\n[Get it on Hugging Face](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)\\n\\nMultimodal image and text input\\n-------------------------------\\n\\nYou can tackle complex analysis and generation tasks with Gemma 3 with its\\nability to handle image and text data. You can use the model to interpret image\\ndata, identify objects, extract text data, and complete many other visual input\\nto text output tasks.\\n[Start building](/gemma/docs/core/keras_inference)\\n| **Important:** The Gemma 3 270M and 1B models are text only and *do not support\\n| image input*.\\n\\n128K token context window\\n-------------------------\\n\\nGemma 3 models (4B, 12B, and 27B) can handle prompt inputs up to 128K tokens, a\\n16x larger context window than previous Gemma models. The large number of tokens\\nmeans you can process several, multi page articles, larger single articles, or\\nhundreds of images in a single prompt.\\n| **Important:** The Gemma 3 270M and 1B models can process up to 32k tokens.\\n\\nWide language support\\n---------------------\\n\\nWork in your own language with built-in support for over 140 languages. Gemma 3\\nis trained to support a large number of languages compared to previous Gemma\\nversions, letting you take on more visual and text tasks in the languages your\\ncustomers use.\\n[Start building](/gemma/docs/spoken-language)\\n\\nFunction calling\\n----------------\\n\\nBuild intelligent, natural language controls for programming interfaces. Gemma\\n3 lets you define coding functions with specific syntax and constraints, and\\nthe model can call these functions to complete tasks.\\n[Start building](/gemma/docs/capabilities/function-calling)\\n\\nParameter sizes and quantization\\n--------------------------------\\n\\nGemma 3 models are available in 5 parameter sizes: 270M, 1B, 4B, 12B, and 27B.\\nThe models can be used with their default precision (16-bit) or with a lower\\nprecision using quantization. The different sizes and precisions represent a set\\nof trade-offs for your AI application. Models with higher parameters and bit\\ncounts (higher precision) are generally more capable, but are more expensive to\\nrun in terms of processing cycles, memory cost and power consumption. Models\\nwith lower parameters and bit counts (lower precision) have less capabilities,\\nbut may be sufficient for your AI task.\\n\\nFor all Gemma 3 models, [Quantization-Aware Trained](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)\\ncheckpoints are provided, which allow quantizing (reducing the precision), while\\npreserving high-quality.\\n\\nThe following table details the approximate GPU or TPU memory requirements for\\nrunning inference with each size of the Gemma 3 model versions. Note that the\\nnumbers may changed based on inference tool.\\n\\n| **Parameters** | **BF16 (16-bit)** | **SFP8 (8-bit)** | **Q4_0 (4-bit)** |\\n|----------------------------|-------------------|------------------|------------------|\\n| Gemma 3 270M (*text only*) | 400 MB | 297 MB | 240 MB |\\n| Gemma 3 1B (*text only*) | 1.5 GB | 1.1 GB | 892 MB |\\n| Gemma 3 4B | 6.4 GB | 4.4 GB | 3.4 GB |\\n| Gemma 3 12B | 20 GB | 12.2 GB | 8.7 GB |\\n| Gemma 3 27B | 46.4 GB | 29.1 GB | 21 GB |\\n\\n**Table 1.** Approximate GPU or TPU memory required to load Gemma 3 models\\nbased on parameter count and quantization level.\\n| **Caution:** These estimates only include the memory required to load the models. They don't include the additional memory required for the prompt tokens or supporting software.\\n\\nMemory consumption increases based on the total number of tokens required for\\nthe prompt you run. The larger the number of tokens required to process your\\nprompt, the higher the memory required, which is in addition to the memory\\nrequired to load the model.\\n| **Note:** Memory requirements for *fine-tuning* Gemma models are significantly higher than running inference. The requirements depend on the development framework and tuning technique you use, such as Low Rank Adapter (LoRA) versus full-precision tuning.\\n\\nPrevious Gemma models\\n---------------------\\n\\nYou can work with previous generations of Gemma models, which are also\\navailable from [Kaggle](https://www.kaggle.com/models?query=gemma) and\\n[Hugging Face](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d).\\nFor more technical details about previous Gemma models, see the following\\nmodel card pages:\\n\\n- Gemma 2 [Model Card](/gemma/docs/core/model_card_2)\\n- Gemma 1 [Model Card](/gemma/docs/core/model_card)\\n\\nReady to start building?\\n[Get started](/gemma/docs/get_started)\\nwith Gemma models!\"]] Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 "
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/en/llm_optims?static-kv=basic+usage%3A+generation_config",
      "full_text": " Optimizing inference Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Optimizing inference Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.1 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Text generation Generation strategies Generation features Prompt engineering Optimizing inference Caching KV cache strategies Getting the most out of LLMs Perplexity of fixed-length models Chat with models Serving Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.1 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Optimizing inference Inference with large language models (LLMs) can be challenging because they have to store and handle billions of parameters. To load a 70B parameter Llama 2 model, it requires 256GB of memory for full precision weights and 128GB of memory for half-precision weights. The most powerful GPUs today - the A100 and H100 - only have 80GB of memory. On top of the memory requirements, inference is slow because LLMs are called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer to process. This guide will show you how to optimize LLM inference to accelerate generation and reduce memory usage. Try out Text Generation Inference (TGI) , a Hugging Face library dedicated to deploying and serving highly optimized LLMs for inference. Static kv-cache and torch.compile LLMs compute key-value (kv) values for each input token, and it performs the same kv computation each time because the generated output becomes part of the input. However, performing the same kv computation every time is not very efficient. A kv-cache stores the past keys and values instead of recomputing them each time. As a result, the kv-cache is dynamic and it grows with each generation step which prevents you from taking advantage of torch.compile , a powerful optimization method that fuses PyTorch code into optimized kernels. The static kv-cache solves this issue by pre-allocating the kv-cache size to a maximum value, so you can combine it with torch.compile for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware. Follow this issue to track which models (Llama, Gemma, Mistral, etc.) support a static kv-cache and torch.compile. Depending on your task, there are several ways you can use the static kv-cache. For basic use cases, set cache_implementation to &quot;static&quot; (recommended). For multi-turn generation or a custom generation loop, initialize and handle StaticCache directly. For more unique hardware or use cases, it may be better to compile the entire generate() function into a single graph. Regardless of how you use the static kv-cache and torch.compile, left-pad your inputs with pad_to_multiple_of to a limited set of values to avoid shape-related recompilations. 1. cache_implementation 2. StaticCache 3. compile entire generate function Set the cache_implementation to &quot;static&quot; in a models GenerationConfig . Call torch.compile to compile the forward pass with the static kv-cache. Copied from transformers import AutoTokenizer, AutoModelForCausalLM import torch import os os.environ[ &quot;TOKENIZERS_PARALLELISM&quot; ] = &quot;false&quot; # To prevent long warnings :) tokenizer = AutoTokenizer.from_pretrained( &quot;google/gemma-2b&quot; ) model = AutoModelForCausalLM.from_pretrained( &quot;google/gemma-2b&quot; , dtype= &quot;auto&quot; , device_map= &quot;auto&quot; ) model.generation_config.cache_implementation = &quot;static&quot; model.forward = torch. compile (model.forward, mode= &quot;reduce-overhead&quot; , fullgraph= True ) input_text = &quot;The theory of special relativity states &quot; input_ids = tokenizer(input_text, return_tensors= &quot;pt&quot; ).to(model.device. type ) outputs = model.generate(**input_ids) print (tokenizer.batch_decode(outputs, skip_special_tokens= True )) [ &#x27;The theory of special relativity states 1. The speed of light is constant in all inertial reference&#x27; ] Under the hood, generate() attempts to reuse the same cache object to avoid recompilation at each call, which is critical to get the most out of torch.compile . Be aware of the following to avoid triggering recompilation or if generation is slower than expected. If the batch size changes or the maximum output length increases between calls, the cache is reinitialized and recompiled. The first several calls of the compiled function are slower because it is being compiled. Decoding strategies Decoding can also be optimized to accelerate generation. You can use a lightweight assistant model to generate candidate tokens faster than the LLM itself or you can use a variant of this decoding strategy that works especially well for input-grounded tasks. Speculative decoding For a more in-depth explanation, take a look at the Assisted Generation: a new direction toward low-latency text generation blog post! For each input token, the model weights are loaded each time during the forward pass, which is slow and cumbersome when a model has billions of parameters. Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger model in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for “free” without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own. To get the largest speed up, the assistant model should be a lot smaller than the LLM so that it can generate tokens quickly. The assistant and LLM model must also share the same tokenizer to avoid re-encoding and decoding tokens. Speculative decoding is only supported for the greedy search and sampling decoding strategies, and it doesn’t support batched inputs. Enable speculative decoding by loading an assistant model and passing it to generate() . greedy search sampling Copied from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device import torch device = infer_device() tokenizer = AutoTokenizer.from_pretrained( &quot;facebook/opt-1.3b&quot; ) inputs = tokenizer( &quot;Einstein&#x27;s theory of relativity states&quot; , return_tensors= &quot;pt&quot; ).to(device) model = AutoModelForCausalLM.from_pretrained( &quot;facebook/opt-1.3b&quot; , dtype= &quot;auto&quot; ).to(device) assistant_model = AutoModelForCausalLM.from_pretrained( &quot;facebook/opt-125m&quot; ).to(device) outputs = model.generate(**inputs, assistant_model=assistant_model) tokenizer.batch_decode(outputs, skip_special_tokens= True ) [ &quot;Einstein&#x27;s theory of relativity states that the speed of light is constant. &quot; ] Prompt lookup decoding Prompt lookup decoding is a variant of speculative decoding that is also compatible with greedy search and sampling. Prompt lookup works especially well for input-grounded tasks - such as summarization - where there is often overlapping words between the prompt and output. These overlapping n-grams are used as the LLM candidate tokens. To enable prompt lookup decoding, specify the number of tokens that should be overlapping in the prompt_lookup_num_tokens parameter. Then pass this parameter to generate() . greedy decoding sampling Copied from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device import torch device = infer_device() tokenizer = AutoTokenizer.from_pretrained( &quot;facebook/opt-1.3b&quot; ) inputs = tokenizer( &quot;The second law of thermodynamics states&quot; , return_tensors= &quot;pt&quot; ).to(device) model = AutoModelForCausalLM.from_pretrained( &quot;facebook/opt-1.3b&quot; , dtype= &quot;auto&quot; ).to(device) assistant_model = AutoModelForCausalLM.from_pretrained( &quot;facebook/opt-125m&quot; ).to(device) outputs = model.generate(**inputs, prompt_lookup_num_tokens= 3 ) print (tokenizer.batch_decode(outputs, skip_special_tokens= True )) [ &#x27;The second law of thermodynamics states that entropy increases with temperature. &#x27; ] Attention A known issue with transformer models is that the self-attention mechanism grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences. To address this, try FlashAttention2 or PyTorch’s scaled dot product attention (SDPA), which are more memory efficient attention implementations. FlashAttention-2 FlashAttention and FlashAttention-2 break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to the GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead. To use FlashAttention-2, set attn_implementation to &quot;flash_attention_2&quot; in from_pretrained() or set with model.set_attention_implementation(&quot;flash_attention_2&quot;) to dynamically update the attention interface after the model is loaded. Copied from transformers import AutoModelForCausalLM, BitsAndBytesConfig quant_config = BitsAndBytesConfig(load_in_8bit= True ) model = AutoModelForCausalLM.from_pretrained( &quot;google/gemma-2b&quot; , quantization_config=quant_config, dtype=torch.bfloat16, attn_implementation= &quot;flash_attention_2&quot; , ) # Change the model&#x27;s attention dynamically after loading model = AutoModelForCausalLM.from_pretrained( &quot;google/gemma-2b&quot; , quantization_config=quant_config, dtype=torch.bfloat16 ) model.set_attention_implementation( &quot;flash_attention_2&quot; ) PyTorch scaled dot product attention Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch’s C++ implementation. SDPA chooses the most performant attention algorithm if you’re using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation. SDPA automatically supports FlashAttention-2 as long as you have the latest PyTorch version installed. Use the torch.nn.attention.sdpa_kernel context manager to explicitly enable or disable any of the four attention algorithms. For example, use SDPBackend.FLASH_ATTENTION to enable FlashAttention. Copied import torch from torch.nn.attention import SDPBackend, sdpa_kernel from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained( &quot;google/gemma-2b&quot; , dtype=torch.bfloat16, ) with sdpa_kernel(SDPBackend.FLASH_ATTENTION): outputs = model.generate(**inputs) Quantization Quantization reduces the size of model weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if you’re constrained by GPU memory. If you aren’t limited by your GPU, you don’t necessarily need to quantize your model because it can increase latency slightly (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights. There are many quantization libraries (see the Quantization guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and AutoGPTQ. Feel free to try them out and see which one works best for your use case. We also recommend reading the Overview of natively supported quantization schemes in 🤗 Transformers blog post which compares AutoGPTQ and bitsandbytes. Use the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating the memory required to load Mistral-7B-v0.1 . To load a model in half-precision, set the dtype parameter in from_pretrained() to torch.bfloat16 . This requires 13.74GB of memory. Copied from transformers import AutoTokenizer, AutoModelForCausalLM import torch model = AutoModelForCausalLM.from_pretrained( &quot;mistralai/Mistral-7B-v0.1&quot; , dtype=torch.bfloat16, device_map= &quot;auto&quot; , ) To load a quantized model (8-bit or 4-bit), try bitsandbytes and set the load_in_4bit or load_in_8bit parameters to True . Loading the model in 8-bits only requires 6.87 GB of memory. Copied from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig import torch quant_config = BitsAndBytesConfig(load_in_8bit= True ) model = AutoModelForCausalLM.from_pretrained( &quot;mistralai/Mistral-7B-v0.1&quot; , quantization_config=quant_config, device_map= &quot;auto&quot; ) &lt; &gt; Update on GitHub ← Prompt engineering Caching → Optimizing inference Static kv-cache and torch.compile Decoding strategies Speculative decoding Prompt lookup decoding Attention Flash Attention-2 Py Torch scaled dot product attention Quantization "
    },
    {
      "arxiv_id": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
      "full_text": "2024-06-27\nGemma 2: Improving Open Language Models\nat a Practical Size\nGemma Team, Google DeepMind1\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art\nopen models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply\nseveral known technical modifications to the Transformer architecture, such as interleaving local-global\nattentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B\nand 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer competitive alternatives to\nmodels that are 2-3× bigger. We release all our models to the community.\n1. Introduction\nLarge language models (LLMs) have demon-\nstrated strong capabilities in language under-\nstanding, generation, and reasoning (Brown et al.,\n2020; Radford et al., 2019; Raffel et al., 2019).\nScaling has been key to this recent progress,\nwith many new capabilities only emerging at\nscale (Brown et al., 2020). The newest large mod-\nels not only reach unprecedented performance\non reasoning benchmarks (Achiam et al., 2023),\nbut they also demonstrate multimodal and mul-\ntilingual capabilities (Gemini Team, 2024) and\neven the ability to use context lengths of over 1M\ntokens (Gemini Team, 2024).\nSmall-scale models have also shown a rapid\nincrease in performance, but these gains are\nlargely derived from increasing the length of train-\ning (Gemma Team, 2024; Jiang et al., 2023; Tou-\nvron et al., 2023). This approach only scales log-\narithmically with dataset size (Hoffmann et al.,\n2022), and the latest small models require up to\n15T tokens to improve the state of the art by less\nthan 1-2% (AI@Meta, 2024).\nYet, these continued improvements provide ev-\nidence that small models are still under-trained.\nIn this work, we explore alternatives to improve\nsmall model performance without solely increas-\ning training length. One solution is to improve\nthe quality of information received by the net-\nwork at each training step by replacing the next\ntoken prediction task with a richer objective.\nIn particular, we focus our efforts on knowledge\ndistillation (Hinton et al., 2015), which replaces\nthe one-hot vector seen at each token with the\ndistribution of potential next tokens computed\nfrom a large model. This approach is often used\nto reduce the training time of smaller models\nby giving them richer gradients. In this work,\nwe instead train for large quantities of tokens\nwith distillation in order to simulate training be-\nyond the number of available tokens. Concretely,\nwe use a large language model as a teacher to\ntrain small models, namely 2B and 9B models,\non a quantity of tokens that is more than 50×\nthe compute-optimal quantity predicted by the\ntheory (Hoffmann et al., 2022). Along with the\nmodels trained with distillation, we also release\na 27B model trained from scratch for this work.\nWe also leverage several known modifications\nof Transformers, namely the interleaving of global\nand local attention layers from Beltagy et al.\n(2020a), and the Grouped-Query Attention (GQA)\nmechanism of Ainslie et al. (2023).\nOverall, Gemma 2 significantly advances state-\nof-the-art performance relative to comparable-\nscale open models and are even competitive\nwith some models more than twice their size\n(AI@Meta, 2024; Almazrouei et al., 2023; Jiang\net al., 2023; xAI, 2024), across a variety of au-\ntomated benchmarks and human evaluations.\nExample domains include question answering\n(Clark et al., 2019; Kwiatkowski et al., 2019),\ncommonsense reasoning (Sakaguchi et al., 2019;\nSuzgun et al., 2022), mathematics and science\n(Cobbe et al., 2021; Hendrycks et al., 2020), and\ncoding (Austin et al., 2021; Chen et al., 2021).\n1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com.\n© 2024 Google DeepMind. All rights reserved\n\nGemma 2: Improving Open Language Models at a Practical Size\nParameters\n2B\n9B\n27B\nd_model\n2304\n3584\n4608\nLayers\n26\n42\n46\nPre-norm\nyes\nyes\nyes\nPost-norm\nyes\nyes\nyes\nNon-linearity\nGeGLU\nGeGLU\nGeGLU\nFeedforward dim\n18432\n28672\n73728\nHead type\nGQA\nGQA\nGQA\nNum heads\n8\n16\n32\nNum KV heads\n4\n8\n16\nHead size\n256\n256\n128\nGlobal att. span\n8192\n8192\n8192\nSliding window\n4096\n4096\n4096\nVocab size\n256128\n256128\n256128\nTied embedding\nyes\nyes\nyes\nTable 1 | Overview of the main model parameters\nand design choices. See the section on model\narchitectures for more details.\nWhile thorough testing of our models has been\nconducted, these tests cannot cover all applica-\ntions and scenarios in which Gemma 2 may be\nused. With this in mind, all Gemma 2 users should\nconduct rigorous safety testing specific to their\nuse case before deployment or use.\nIn this technical report, we provide an overview\nof models, including the architecture, training,\nand pre- and post-training recipes for Gemma\n2. We also provide detailed evaluations across a\nwide variety of quantitative and qualitative bench-\nmarks, as well as both standard academic bench-\nmarks and human-preference evaluations. Finally,\nwe discuss our approach to safe and responsible\ndeployment and outline the broader implications\nof Gemma 2, its limitations, and advantages.\n2. Model Architecture\nSimilar to previous Gemma models (Gemma\nTeam, 2024), the Gemma 2 models are based on a\ndecoder-only transformer architecture (Vaswani\net al., 2017). We summarize the main parameters\nand architecture choices in Table 1.\nA few architectural elements are similar to the\nfirst version of Gemma models; namely, a context\nModel\nEmbedding\nParameters\nNon-embedding\nParameters\n2B\n590,118,912\n2,024,517,888\n9B\n917,962,752\n8,324,201,984\n27B\n1,180,237,824\n26,047,480,320\nTable 2 | Parameter counts for the Gemma mod-\nels. We inherit from the large Gemini vocabulary\n(256k entries), that is designed to work on a large\nnumber of languages, hence, the larger embed-\nding parameter counts compared to models that\nare limited to one or a few languages.\nlength of 8192 tokens, the use of Rotary Posi-\ntion Embeddings (RoPE) (Su et al., 2021), and\nthe approximated GeGLU non-linearity (Shazeer,\n2020). A few elements differ between Gemma 1\nand Gemma 2, including using deeper networks.\nWe summarize the key differences below.\nLocal Sliding Window and Global Attention.\nWe alternate between a local sliding window at-\ntention (Beltagy et al., 2020a,b) and global at-\ntention (Luong et al., 2015) in every other layer.\nThe sliding window size of local attention layers\nis set to 4096 tokens, while the span of the global\nattention layers is set to 8192 tokens.\nLogit soft-capping. We cap logits (Bello et al.,\n2016) in each attention layer and the final layer\nsuch that the value of the logits stays between\n−soft_cap and +soft_cap. More specifically, we\ncap the logits with the following function:\nlogits ←soft_cap ∗tanh(logits/soft_cap).\nWe set the soft_cap parameter to 50.0 for the self-\nattention layers and to 30.0 for the final layer.\nPost-norm and pre-norm with RMSNorm. To\nstabilize training, we use RMSNorm (Zhang and\nSennrich, 2019) to normalize the input and out-\nput of each transformer sub-layer, the attention\nlayer, and the feedforward layer.\nGrouped-Query Attention (Ainslie et al., 2023).\nWe use GQA with num_groups = 2, based on ab-\nlations showing increased speed at inference time\nwhile maintaining downstream performance.\n2\n\nGemma 2: Improving Open Language Models at a Practical Size\n3. Pre-training\nWe provide a brief overview of the parts of our\npre-training that differs from Gemma 1.\n3.1. Training Data\nWe train Gemma 2 27B on 13 trillion tokens of\nprimarily-English data, the 9B model on 8 trillion\ntokens, and the 2B on 2 trillion tokens. These\ntokens come from a variety of data sources, in-\ncluding web documents, code, and science ar-\nticles. Our models are not multimodal and are\nnot trained specifically for state-of-the-art multi-\nlingual capabilities. The final data mixture was\ndetermined through ablations similar to the ap-\nproach in Gemini 1.0 (Gemini Team, 2023).\nTokenizer. We use the same tokenizer as Gemma\n1 and Gemini: a SentencePiece tokenizer with\nsplit digits, preserved whitespace, and byte-level\nencodings (Kudo and Richardson, 2018). The\nresulting vocabulary has 256k entries.\nFiltering. We use the same data filtering tech-\nniques as Gemma 1. Specifically, we filter the pre-\ntraining dataset to reduce the risk of unwanted\nor unsafe utterances, filter out certain personal\ninformation or other sensitive data, decontami-\nnate evaluation sets from our pre-training data\nmixture, and reduce the risk of recitation by min-\nimizing the proliferation of sensitive outputs.\nShards\nModel\nType\n#Chips\nData\nModel\n2B\nTPUv5e\n512\n512\n1\n9B\nTPUv4\n4096\n1024\n4\n27B\nTPUv5p\n6144\n768\n8\nTable 3 | Training infrastructure with sharding.\n3.2. Knowledge Distillation\nGiven a large model used as a teacher, we learn\nsmaller models by distilling from the probability\ngiven by the teacher of each token 𝑥given its\ncontext 𝑥𝑐, i.e., 𝑃𝑇(𝑥| 𝑥𝑐). More precisely, we\nminimize the negative log-likelihood between the\nContext\nRelevant Token\nUser turn\nuser\nModel turn\nmodel\nStart of conversation turn\n<start_of_turn>\nEnd of conversation turn\n<end_of_turn>\nBeginning of sequence\n<bos>\nEnd of sequence\n<eos>\nTable 4 | Relevant formatting control tokens used\nfor Gemma models.\nprobabilities from the teacher and the student:\nmin\n𝑃𝑆\n∑︁\n𝑥\n−𝑃𝑇(𝑥| 𝑥𝑐) log 𝑃𝑆(𝑥| 𝑥𝑐),\nwhere 𝑃𝑆is the parameterized probability of the\nstudent. Note that knowledge distillation was\nalso used in Gemini 1.5 (Gemini Team, 2024).\n3.3. Compute Infrastructure\nWe train our models with TPUv4, TPUv5e, and\nTPUv5p as outlined in Table 3. For the 2B model,\nwe train on a 2x16x16 configuration of TPUv5e,\ntotaling 512 chips, with 512-way data replication\nand 1-way model sharding. For the 9B model,\nwe train on an 8x16x32 configuration of TPUv4,\ntotaling 4096 chips, with 1024-way data repli-\ncation and 4-way model sharding. For the 27B\nmodel, we train on an 8x24x32 configuration of\nTPUv5p, totaling 6144 chips, with 768-way data\nreplication and 8-way model sharding.\nThe optimizer state is further sharded using\ntechniques similar to ZeRO-3 (Ren et al., 2021).\nFor scales beyond a single pod, we perform a\ndata-replica reduction over the data center net-\nwork, using the Pathways approach of Barham\net al. (2022). We also use the ’single controller’\nprogramming paradigm of Jax (Roberts et al.,\n2023) and Pathways (Barham et al., 2022). As\nin Gemma 1, we use the GSPMD partitioner (Xu\net al., 2021) for training step computation and\nthe MegaScale XLA compiler (XLA, 2019).\n3\n\nGemma 2: Improving Open Language Models at a Practical Size\n3.4. Carbon Footprint\nWe estimate the carbon emissions from pre-\ntraining the Gemma models to be 1247.61 𝑡𝐶𝑂2𝑒𝑞.\nAs in Gemma 1 (Gemma Team, 2024), this value\nis calculated based on the hourly energy usage\nreported directly from our TPU data centers and\nscaled to account for the additional energy ex-\npended to create and maintain the data center.\nImportantly, Google data centers are carbon neu-\ntral, achieved through a combination of energy\nefficiency, renewable energy purchases, and car-\nbon offsets. This carbon neutrality applies to our\nexperiments and the machines running them.\n4. Post-Training\nFor post-training, we fine-tune our pre-trained\nmodels into instruction-tuned models. First, we\napply supervised fine-tuning (SFT) on a mix\nof text-only, English-only synthetic and human-\ngenerated prompt-response pairs. We then apply\nRLHF on top of these models with the reward\nmodel trained on labelled English-only preference\ndata and the policy based on the same prompts\nas the SFT phase. Finally, we average the mod-\nels obtained after each phase to improve their\noverall performance. The final data mixtures and\npost-training recipe, which includes tuned hyper-\nparameters, were chosen on the basis of improv-\ning helpfulness while minimizing model harms\nrelated to safety and hallucinations.\nWe extended the post-training data from\nGemma 1.1 with a mixture of internal and exter-\nnal public data. In particular, we use the prompts,\nbut not the answers from LMSYS-chat-1M (Zheng\net al., 2023). All of our data go through a filtering\nstage described below.\nSupervised fine-tuning (SFT). We run behav-\nioral cloning on synthetic and real prompts, and\nresponses predominantly synthetically generated\nby the teacher, that is a larger model. We also run\ndistillation from the teacher on the student’s dis-\ntribution (Agarwal et al., 2024; Gu et al., 2024).\nReinforcement Learning from Human Feed-\nback (RLHF). We use a similar RLHF algorithm\nas Gemma 1.1 (Gemma Team, 2024) but a differ-\nent reward model, which is an order of magnitude\nFirst turn\nUser:\n<start_of_turn>user\nKnock knock.<end_of_turn>\n<start_of_turn>model\nModel:\nWho’s there?<end_of_turn><eos>\nSecond turn\nUser:\n<start_of_turn>user\nKnock knock.<end_of_turn>\n<start_of_turn>model\nModel:\nWho’s there?<end_of_turn>\nUser:\n<start_of_turn>user\nGemma.<end_of_turn>\n<start_of_turn>model\nModel:\nGemma who?<end_of_turn><eos>\nTable 5 | Example dialogue with user and model\ncontrol tokens. To proceed with multi-turn, re-\nmove the model-outputted <eos>, add back the\nusual user turn’s control tokens and continue with\nthe following turn’s chat template.\nlarger than the policy. The new reward model is\nalso oriented more towards conversational capa-\nbilities, specifically multi-turn.\nModel merging. We average different models\nobtained by running our pipeline with different\nhyperparameters (Ramé et al., 2024).\nData filtering. When using synthetic data, we\nrun several stages of filtering to remove examples\nthat show certain personal information, unsafe or\ntoxic model outputs, mistaken self-identification\ndata, and duplicated examples. Following Gem-\nini, we find that including subsets of data that\nencourage better in-context attribution, hedging,\nand refusals to minimize hallucinations improves\nperformance on factuality metrics, without de-\ngrading model performance on other metrics.\nFormatting. Gemma 2 models are fine-tuned\nwith the same control tokens as Gemma 1 models,\nas detailed in Table 4, but a different formatting\nschema. See the dialogue example in Table 5.\nNotice that the model explicitly ends generations\nwith <end_of_turn><eos> tokens, while previ-\nously it only generated <eos>. For the motivation\nbehind this formatting structure, see Gemma 1.\n4\n\nGemma 2: Improving Open Language Models at a Practical Size\n5. Ablations\nIn this section, we focus on the main finding of\nthis work, which is the impact of knowledge dis-\ntillation on small language models.\nfrom scratch\ndistilled\nAverage (3 bench.)\n60.3\n67.7\nTable 6 | Comparison between a 2B model trained\nover 500B tokens either from scratch or with dis-\ntillation from a 7B model.\nDistillation versus from scratch. In Table 6, we\nshow that distilling from a larger model improves\nperformance compared to training from scratch.\nNote that 500B is 10× more than the compute-\noptimal number of tokens for a 2B model. We\ndistill from a 7B model to keep a ratio similar to\nour target distillation from 27B to 9B.\n200M\n400M\n1B\nfrom scratch\n23\n19\n17\ndistilled (7B)\n21\n17\n15\nTable 7 | Perplexity measured on a validation set\nof models of different sizes trained with or with-\nout distillation. The teacher has 7B parameters.\nImpact of distillation w.r.t. model size. In Ta-\nble 7, we measure the impact of distillation as\nmodel size increases. We observe that the gain re-\nmains as the model size is scaled. In this ablation,\nwe maintain the size of the teacher at 7B and\ntrain smaller models to simulate the same gap as\nbetween our final teacher and student sizes.\nMHA\nGQA\nAverage (4 bench.)\n50.3\n50.8\nTable 8 | Comparing the impact of replacing Multi-\nHead Attention (MHA) with GQA on a 9B model\naveraged over 4 benchmarks.\nGQA versus MHA. In Table 8, we compare two\ninstances of our 9B with MHA or GQA. We observe\noverall few changes in performance between both\nmodels as measured on several benchmarks. We\nchoose GQA since it requires fewer parameters\nand is faster at inference time.\nWide versus deep. In Table 9, we show that a\ndeeper 9B network is slightly better than a wider\n9B for the same number of parameters. Although\nthe gap is small, it is consistent across benchmarks\nand warrants the switch to a deeper architecture.\nWide\nDeep\nAverage (4 bench.)\n50.8\n52.0\nTable 9 | Wide versus deep 9B models. Perfor-\nmance on 4 benchmarks, higher is better.\nChanging sliding window size. In Table 10, we\nshow that we can change the sliding window size\nof the local attention layers of the models during\ninference with moderate impact on perplexity.\nAdjusting the size of the sliding window can thus\nbe a leverage for slight inference speed gain.\nsliding window\n4096\n2048\n1024\nperplexity (val. set)\n1.63\n1.63\n1.64\nTable 10 | Impact of changing the sliding window\nsize at inference time for the 9B model.\nImpact of formatting. We measure performance\nvariance on MMLU across prompt/evaluation for-\nmatting variations.\nTable 11 shows the stan-\ndard deviations of MMLU scores for 12 format-\nting/evaluation combinations, a proxy for unde-\nsired performance variability. The Gemma 2B\nmodels are slightly less format-robust than the\nlarger ones. Notably, Mistral 7B is significantly\nless robust than our models.\nStandard Deviation\nGemma 1 2B\n1.5\nGemma 2 2B\n2.1\nMistral 7B\n6.9\nGemma 1 7B\n0.7\nGemma 2 9B\n0.9\nGemma 2 27B\n1.0\nTable 11 | Standard deviations of MMLU scores\nfor 12 combinations of formatting and evaluation.\n5\n\nGemma 2: Improving Open Language Models at a Practical Size\n6. Evaluation\nIn this section, we evaluate both pre-trained and\nIT models over a series of automated benchmarks\nand human evaluations across a variety of do-\nmains. We also report performance from models\nof similar sizes that have permissive licenses, or\nas reported by others. Note that we consider to-\ntal parameters, not active parameters, since total\nmemory usage is often what limits the use of open\nmodels on standard devices.\n6.1. Pre-training Evaluations\nEvaluating the 27B model\nIn this set of evaluations, we evaluate the perfor-\nmance of our 27B model trained without distilla-\ntion on 13T tokens. We report results in Table 12,\nwhere we compare with a model of similar size,\nQwen1.5 34B (Team, 2024), and a model 2.5×\nlarger, LLaMA-3 70B on the HuggingFace evalu-\nation suite. We selected these models based on\ntheir ranking on the HuggingFace leaderboard.\nOverall, we observe that our model is the best\nin its size category and is even competitive with\na larger model that is trained for longer. That\nbeing said, the performance of models trained in\na similar fashion improves only logarithmically\nwith their size and hence, our model is likely in\nthe same Pareto curve as the LLaMA-3 models.\nHowever, it is not clear how these differences\naffect the quality of the resulting IT models.\nEvaluating the 2B and 9B models\nIn this set of experiments, we compare our new\n2B and 9B trained with distillation to our previ-\nous models and several standard open models\nin Gemma Team (2024).\nWe observe overall a massive improvement in\nour models compared to previous versions, by up\nto 10% in some benchmarks for the 9B model.\nThe two 2B models were trained with a similar\nnumber of tokens (2T for Gemma 2 and 3T for\nGemma 1) and we still observe a significant im-\nprovement for the new models. This confirms that\ndistillation significantly improves the quality of\nmodels even when trained on the same number\nof tokens.\nLLaMA-3\nQwen1.5\nGemma-2\n70B\n32B\n27B\nMMLU\n79.2\n74.3\n75.2\nGSM8K\n76.9\n61.1\n74.0\nARC-c\n68.8\n63.6\n71.4\nHellaSwag\n88.0\n85.0\n86.4\nWinogrande\n85.3\n81.5\n83.7\nTable 12 | We compare, on the HuggingFace\nbenchmark, our 27B model with a competitive\nopen model, Qwen1.5 32B, that has a similar size.\nWe also report the performance of LLaMA-3 70B\nfor completeness. Note that our model outper-\nforms Qwen1.5 32B and is only a few percent\nbelow LLaMA-3 70B despite being 2.5× smaller\nand trained on 2/3rds less data.\n6.2. Post-training Evaluations\nIn this section, we evaluate our IT models on a\nset of human evaluations as well as standard aca-\ndemic benchmarks. The Gemma 2 models push\nthe frontier for post-trained open-weights mod-\nels, setting a new state of the art on the LMSYS\nChatbot Arena (Chiang et al., 2024).\nLMSYS Chatbot Arena\nGemma 2 Instruction Tuned models were evalu-\nated on the Chatbot Arena (Chiang et al., 2024)\nin blind side by side evaluations by human raters\nagainst other state of the art models. We re-\nport Elo scores in Table 14. Gemma 2.6B, 9B\nand 27B strongly outperform all other open mod-\nels in the same range of parameters, with no-\ntably: Gemma 27B (Elo 1218) ranked higher than\nLlama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\n0613 (Elo 1116).\nHuman Preference Evaluations\nWe also submit Gemma IT models for side-by-\nside human evaluation studies (which are in-\ndependent from the Chatbot Arena). We used\nheld-out collections of single-turn prompts that\ntarget safety and instruction following (IF). We\nuse gpt4o-2024-05-13 as the base model, and\n6\n\nGemma 2: Improving Open Language Models at a Practical Size\nGemma-1 Gemma-2 Mistral LLaMA-3 Gemma-1 Gemma-2 Gemma-2\nBenchmark\nmetric\n2B\n2B\n7B\n8B\n7B\n9B\n27B\nMMLU\n5-shot\n42.3\n52.2\n62.5\n66.6\n64.4\n71.3\n75.2\nARC-C\n25-shot\n48.5\n55.7\n60.5\n59.2\n61.1\n68.4\n71.4\nGSM8K\n5-shot\n15.1\n24.3\n39.6\n45.7\n51.8\n68.6\n74.0\nAGIEval\n3-5-shot\n24.2\n31.5\n44.0†\n45.9†\n44.9†\n52.8\n55.1\nDROP\n3-shot, F1\n48.5\n51.2\n63.8∗\n58.4\n56.3\n69.4\n74.2\nBBH\n3-shot, CoT\n35.2\n41.9\n56.0⋄\n61.1⋄\n59.0⋄\n68.2\n74.9\nWinogrande\n5-shot\n66.8\n71.3\n78.5\n76.1\n79.0\n80.6\n83.7\nHellaSwag\n10-shot\n71.7\n72.9\n83.0\n82.0\n82.3\n81.9\n86.4\nMATH\n4-shot\n11.8\n16.0\n12.7\n-\n24.3\n36.6\n42.3\nARC-e\n0-shot\n73.2\n80.6\n80.5\n-\n81.5\n88.0\n88.6\nPIQA\n0-shot\n77.3\n78.4\n82.2\n-\n81.2\n81.7\n83.2\nSIQA\n0-shot\n49.7\n51.9\n47.0∗\n-\n51.8\n53.4\n53.7\nBoolq\n0-shot\n69.4\n72.7\n83.2∗\n-\n83.2\n84.2\n84.8\nTriviaQA\n5-shot\n53.2\n60.4\n62.5\n-\n63.4\n76.6\n83.7\nNQ\n5-shot\n12.5\n17.1\n23.2\n-\n23.0\n29.2\n34.5\nHumanEval\npass@1\n22.0\n20.1\n26.2\n-\n32.3\n40.2\n51.8\nMBPP\n3-shot\n29.2\n30.2\n40.2∗\n-\n44.4\n52.4\n62.6\nAverage (8)\n44.0\n50.0\n61.0\n61.9\n62.4\n70.2\n74.4\nAverage (all)\n44.2\n48.7\n55.6\n-\n57.9\n64.9\n69.4\nTable 13 | Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on\na variety of benchmarks. We report the average performance on the 8 benchmarks where we can\ncompare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either\nfrom the HuggingFace leaderboard or their blogpost. † we report the evaluation used in LLaMA-3 for\nthe baselines, it leads to +3% compared to our evaluation: Gemma-1 7B achieves 44.9% instead of\n41.7%, and Mistral 7B, 44% instead of 41.2%. ⋄we report the evaluation used in LLaMA-3 for the\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.\n∗these are evaluations run by us for Gemma 1 (Gemma Team, 2024).\nobserve large improvements in win rates and\npreference scores as compared against the older\nGemma 1.1 7B model. We report safety as a\nwin-loss ratio against GPT4o, and we report\nsingle-sided instruction following scores as ratio\nof prompts where all instructions are followed. In\nparticular, we find that regardless of their size,\nGemma 2 models produce safer, more appropri-\nate prompts on the held-out safety prompt set\nthan GPT4o.\nHuman Multi-Turn Evaluations\nWe evaluated the multi-turn capabilities of\nGemma 1.1 7B, Gemma 2 2B, 9B and 27B models\nby tasking human raters to have conversations\nwith the models and follow specified given sce-\nnarios. We used a diverse, held-out set of 500\nscenarios, each describing a sequence of requests\nto the model, including measuring instances of\nbrainstorming, making a plan, or learning some-\nthing new. The average number of user turns\nis 8.4. We found that the conversations with\nGemma 2 models are rated significantly better\nthan Gemma 1.1 in user satisfaction and conver-\nsation goal achievement (Table 16). Moreover,\nwe saw that the Gemma 2 models were better\nthan Gemma 1.1 7B at maintaining high quality\nof responses for the entire conversation.\nStandard Benchmarks\nIt has been observed in Llama-3 (AI@Meta, 2024)\nthat instruction fine-tuning can improve the per-\nformance of the models on few-shot benchmarks\n7\n\nGemma 2: Improving Open Language Models at a Practical Size\nModel\nElo\n95% CI\nOpen\ngpt-4o-2024-05-13\n1286\n+2 / -3\n-\ngpt-4o-mini-2024-07-18\n1279\n+5 / -4\n-\nclaude-3-5-sonnet\n1271\n+3 / -4\n-\ngemini-advanced-0514\n1266\n+2 / -3\n-\nllama-3.1-405b-instruct\n1262\n+8 / -7\n+\ngemini-1.5-pro-api-0514\n1261\n+2 / -3\n-\ngemini-1.5-pro-api-0409\n1257\n+3 / -3\n-\ngpt-4-turbo-2024-04-09\n1256\n+2 / -3\n-\ngpt-4-1106-preview\n1250\n+3 / -3\n-\nclaude-3-opus-20240229\n1248\n+2 / -2\n-\nathene-70b-0725\n1245\n+8 / -6\n+\ngpt-4-0125-preview\n1245\n+2 / -2\n-\nllama-3.1-70b-instruct\n1244\n+8 / -9\n+\nyi-large-preview\n1239\n+3 / -3\n-\ngemini-1.5-flash-api-0514\n1227\n+3 / -3\n-\ndeepseek-v2-api-0628\n1220\n+6 / -6\n+\ngemma-2-27b-it\n1218\n+4 / -3\n+\nyi-large\n1212\n+4 / -5\n-\nnemotron-4-340b-instruct\n1209\n+3 / -4\n+\nbard-jan-24-gemini-pro\n1208\n+5 / -7\n-\nglm-4-0520\n1206\n+3 / -5\n-\nllama-3-70b-instruct\n1206\n+2 / -2\n+\nclaude-3-sonnet\n1200\n+2 / -2\n-\nreka-core-20240501\n1199\n+3 / -3\n-\ncommand-r-plus\n1189\n+2 / -2\n+\nModel\nElo\n95% CI\nOpen\ngemma-2-9b-it\n1187\n+3 / -5\n+\nqwen2-72b-instruct\n1187\n+3 / -3\n+\ngpt-4-0314\n1186\n+2 / -3\n-\nqwen1.5-110b-chat\n1161\n+3 / -3\n+\nmistral-large-2402\n1157\n+3 / -3\n-\nyi-1.5-34b-chat\n1157\n+4 / -3\n-\nreka-flash-21b-20240226\n1155\n+4 / -4\n-\nllama-3-8b-instruct\n1151\n+2 / -3\n+\ncommand-r\n1148\n+3 / -3\n+\nclaude-1\n1148\n+4 / -4\n-\nmistral-medium\n1147\n+4 / -4\n-\nreka-flash-21b-20240226\n1147\n+3 / -4\n-\nqwen1.5-72b-chat\n1147\n+4 / -4\n+\nmixtral-8x22b-instruct-v0.1\n1145\n+2 / -3\n+\nclaude-2.0\n1131\n+4 / -6\n-\ngemini-pro-dev-api\n1131\n+4 / -3\n-\nzephyr-orpo-141b\n1127\n+10 / -6\n+\ngemma-2-2b-it\n1126\n+10 / -10\n+\nqwen1.5-32b-chat\n1125\n+3 / -3\n+\nmistral-next\n1124\n+5 / -5\n-\nphi-3-medium-4k-instruct\n1122\n+4 / -4\n+\nstarling-lm-7b-beta\n1118\n+4 / -5\n+\nclaude-2.1\n1118\n+3 / -3\n-\ngpt-3.5-turbo-0613\n1116\n+3 / -4\n-\nmixtral-8x7b-instruct-v0.1\n1114\n+0 / -0\n-\nTable 14 | Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al.,\n2024). The models are evaluated against each other through blind side by side evaluations by human\nraters. Each model is attributed a score, based on the Elo rating system.\nModel\nInstruction Following\nSafety\nGemma 1.1 IT 7B\n24.3% ± 1.9%\n42.8%\nWin / Tie / Loss\n37.4% / 10.8% / 51.8%\nGemma 2 IT 2B\n26.5% ± 1.8%\n57.5%\nWin / Tie / Loss\n53% / 9% / 38%\nGemma 2 IT 9B\n34.1% ± 3.0%\n57.8%\nWin / Tie / Loss\n48.2% / 19.2% / 28.3%\nGemma 2 IT 27B\n37.7% ± 2.3%\n55%\nWin / Tie / Loss\n49.6% / 10.8% / 39.6%\nTable 15 | Instruction following and safety metrics\nfrom human raters. The instruction following\nmetrics are single-sided and do not have win-loss\nrates, and so are left blank.\ndespite not being trained to target few-shot capa-\nbilities. In Table 17, we show a similar improve-\nment across our models. Overall, we observe\nimprovements on the order of several percentage\npoints. We conjecture that IT models are better\nat understanding formatted questions, while pre-\ntrained models are sensitive to formatting.\nUser\nsatisfaction\nConversation\ngoal achievement\nGemma 1.1 IT 7B\n3.32\n3.36\nGemma 2 IT 2B\n3.64\n3.88\nGemma 2 IT 9B\n4.04\n4.08\nGemma 2 IT 27B\n4.20\n4.24\nTable 16 | Human evaluations on 500 multi-turn\nscenarios. The raters attribute a score ranging\nbetween 1 and 5 for both overall satisfaction and\nconversation goal achievement.\n2B\n9B\n27B\nModel\nPT\nIT\nPT\nIT\nPT\nIT\nMMLU\n52.2\n56.1\n71.3\n72.3\n75.2\n76.2\nMBPP\n30.2\n36.6\n52.4\n59.2\n62.6\n67.4\nTable 17 | Comparing pre-trained (PT) and in-\nstruction fine-tuned (IT) models of different sizes\non few-shot benchmarks.\n8\n\nGemma 2: Improving Open Language Models at a Practical Size\n7. Memorization and Privacy\nLarge language models may, under particular cir-\ncumstances, be vulnerable to attacks causing the\nmodel to produce memorized1 training data (Nasr\net al., 2023). To study susceptibility to such at-\ntacks and quantify memorization, we evaluate\nmodels for verbatim and approximate memoriza-\ntion as was done in several prior studies (Anil\net al., 2023; Carlini et al., 2022; Gemini Team,\n2024; Kudugunta et al., 2023).\nWe follow the evaluation setting of (Gemma\nTeam, 2024) which tests for (50 token) memo-\nrizations of training data given a prompt of 50 to-\nkens. We compare the overall memorization rates,\nacross a uniform sample of the entire dataset, us-\ning both an exact match criteria and approximate\nmatch criteria (Ippolito et al., 2022) using an edit\ndistance of 10%.\nVerbatim Memorization: Results are in Figure 1.\nWe first compare against recent models from the\nliterature that include memorization evaluations.\nWe find that Gemma 2 memorizes significantly\nless than prior models at a similar size, with mem-\norization rates below 0.1% (note the log y-axis).\nWe further investigate how this memorization\nbreaks down with respect to the data source. Sim-\nilar to Gemma 1, we find that Gemma 2 memo-\nrizes more from code, wiki, and science sources,\nand also that it memorizes significantly less across\nthe board (again, note the log y-axis).\nApproximate Memorization:\nFigure 1 also\npresents approximate memorization by data\nsource. We observe that while approximate mem-\norization is higher than exact, the rate of memo-\nrization is still low. For example, the approximate\nmemorization of this model is much lower than\neven the exact memorization of Gemma 1. We\n1This work uses a very restricted definition of “mem-\norization”: whether a model can be induced to generate\nnear-copies of some training examples when prompted with\nappropriate instructions. We do not mean to say that a\nmodel ’contains’ its training data in the sense that any arbi-\ntrary instance of that data can be retrieved without use of\nspecialized software or algorithms. Rather, if a model can\nbe induced to generate measurably close copies of certain\ntraining examples by supplying appropriate instructions to\nguide the model’s statistical generation process then that\nmodel is said to have ’memorized’ those examples.\nGemma 2\n 2B\nGemma 2\n 9B\nGemma 2\n 27B\nGemini 1.5\n Flash\nGemma\n2B\nGemma\n7BPaLM 2\nSmall\nModel\n0.1\n1\n% Exact Memorized\nOverall Memorization Rate\nCode\nMultilingual\nScience\nWeb\nWiki\nData Source\n10 4\n10 3\n0.01\n0.1\n% Memorized\nBy Data Source\nBy Data Source\nExact 2B\nExact 9B\nExact 27B\nApprox 2B\nApprox 9B\nApprox 27B\nFigure 1 | Comparing memorization rates. We\nfind significantly lower memorization rates\nacross-the-board. (Left) Overall memorization\nacross model families. (Right) Exact and approx-\nimate memorization per data source.\nfind that the increase in approximate memoriza-\ntion is much lower than prior models; in some\ncases we observed no lift at all c.f. (Gemma Team,\n2024, Figure 4) (note that no bar indicates no in-\ncrease, i.e., the rate of approximate memorization\nequals that of exact memorization). Note that no\napproximate memorization bar in Figure X indi-\ncates no increase, i.e., the rate of approximate\nmemorization equals that of exact memorization.\nPersonal Data We use the same prevention\nmethods at training time and the same evalua-\ntions as Gemma Team (2024). In particular, we\nuse Google Cloud Sensitive Data Protection Tool2\nto find potential instances of personal data. The\nmany categories of personal data (e.g., phone\nnumbers, account numbers) are classified into\nthree severity levels. We analyze memorized out-\nputs using these severity levels. . We found no\ninstances of high-severity data being emitted, and\nfound a very low rate of 0.00026% of memorized\ndata to contain lower-severity personal informa-\ntion. We note that these automated tools are\nknown to incur false positives because they do\nnot account for context. This means our results\nare likely overestimates.\n2Available at: https://cloud.google.com/sensitive-data-\nprotection\n9\n\nGemma 2: Improving Open Language Models at a Practical Size\n8. Responsibility, Safety, Security\nResponsibility,\nsafety\nand\nsecurity\nare\nof\nparamount importance when developing Gemma\nmodels. To reduce risks to Gemma 2 users, we\nhave integrated enhanced internal safety pro-\ncesses that span the development workflow, in\nline with recent Google AI models (Gemini Team,\n2024). Similar to the inaugural Gemma release,\nwe have followed a three pillar approach which fo-\ncuses on safety mitigation at training time, robust\nand transparent model evaluations, and further\ndevelopment of the Responsible Generative AI\nToolkit, a series of models and tools to help de-\nvelopers implement responsibility and safety best\npractices for their applications.\n8.1. Impact assessment\nOur approach and resulting impact assessment is\nreflective of that outlined for Gemma 1 (Gemma\nTeam, 2024): we continue to believe that open-\nness in AI can spread the benefits of these tech-\nnologies across society, but must be evaluated\nagainst the risk of malicious uses, such as the\ncreation of deepfake imagery, AI-generated disin-\nformation or illegal and disturbing material, that\ncan cause harm on both an individual and insti-\ntutional levels (Weidinger et al., 2021). Since the\nlaunch of Gemma 1, we have seen our Gemma\nmodels drive a number of socially beneficial ap-\nplications, relying on Gemma’s unique technolo-\ngies like its tokenizer to facilitate the creation of\nmultilingual models, such as for Navarasa 2.0, a\nGemma tuned model for 15 Indian languages.\nReleasing further open models requires specific\nattention to changes in model capabilities and\nclose monitoring of the evolving risks of LLMs (Lin\net al., 2024), as well as, an understanding of the\nways in which our models are being used in the\nwild. Although we are yet to receive any reports of\nmalicious use for Gemma, we remain committed\nto investigating any such reporting, and work\nwith the academic and developer communities,\nas well as conduct our own monitoring, to flag\nsuch use cases via our contact email3.\nDespite advancements in capabilities, we be-\n3gemma-2-report@google.com\nlieve that given the number of larger and more\npowerful open models, this release will have a\nnegligible effect on the overall risk landscape.\n8.2. Safety policies and train-time mitigations\nA key pillar of Gemma’s approach to safety is to\nalign fine-tuned models with Google’s safety poli-\ncies, in line with Gemini models (Gemini Team,\n2023). They are designed to help prevent our\nmodels from generating harmful content, i.e.,\n• Child sexual abuse and exploitation\n• Revealing personally identifiable information\nthat can lead to harm (e.g., Social Security\nnumbers)\n• Hate speech and harassment\n• Dangerous or malicious content (including\npromoting self-harm or instructing in harm-\nful activities)\n• Sexually explicit content\n• Medical advice that runs contrary to scientific\nor medical consensus\nWe undertook considerable safety filtering of our\npre-training data to reduce the likelihood of our\npre-trained and fine-tuned checkpoints producing\nharmful content. For fine-tuned models, we also\nuse both SFT and RLHF to steer the model away\nfrom undesirable behavior.\n8.3. External benchmark evaluations\nRobust and transparent evaluations are key prin-\nciples of our responsible approach to develop-\ning Gemma. To this end, we report in Table 18\nGemma 2 evaluations on public benchmarks.\n8.4. Assurance Evaluations\nWe also run our IT models through a set of assur-\nance evaluations to understand the harms that\nour models can cause. We focus on capabilities\nrelevant to extreme risks (Shevlane et al., 2023)\n(Phuong et al., 2024). Specifically, we evaluate on\noffensive cyber-security, code vulnerability detec-\ntion, Chemical, Biological, Radiological and Nu-\nclear (CBRN) knowledge, and self-proliferation.\nWe refer the reader to Phuong et al. (2024) for\nfull methodological details of these studies.\n10\n\nGemma 2: Improving Open Language Models at a Practical Size\nGemma 1.1 IT\nGemma 2 IT\nBenchmark\nmetric\n2.5B\n7B\n2.6B\n9B\n27B\nRealToxicity\navg tox\n7.03\n8.04\n8.16\n8.25\n8.84\nCrowS-Pairs\ntop-1\n45.89\n49.67\n37.67\n37.47\n36.67\nBBQ Ambig\n4-shot, top-1\n58.97\n86.06\n83.20\n88.58\n85.99\nBBQ Disambig\n4-shot, top-1\n53.9\n85.08\n69.31\n82.67\n86.94\nWinogender\ntop-1\n50.14\n57.64\n52.91\n79.17\n77.22\nTruthfulQA\nMC2Acc\n44.24\n45.34\n43.72\n50.27\n51.60\nWinobias 1_2\ntop-1\n55.93\n59.22\n59.28\n78.09\n81.94\nWinobias 2_2\ntop-1\n89.46\n89.2\n88.57\n95.32\n97.22\nToxigen\navg tox\n29.64\n38.75\n48.32\n39.30\n38.42\nTable 18 | Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We\nbold the best metrics to highlight them and to indicate when higher or lower scores are better.\nInterCode-CTF\nInternal CTF suite\nHack the Box\nGemini 1.0 Ultra\n28/76 [1] (37%)\n3/13 (23%)\n0/13\nGemini 1.5 Pro\n62/76 (82%)\n4/13 (31%)\n0/13\nCodeGemma 1 7B\n12/76 (16%)\n0/13 (0%)\n0/13\nGemma 2 27B\n34/76 (45%)\n1/13 (8%)\n0/13\nTable 19 | Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a\nchallenge based on Hack the Box. We report the number of successful hackings.\nBaseline Evaluations\nBaseline assurance captures the model’s violation\nrate for safety policies, using a large number of\nsynthetic adversarial user queries, and human\nraters to label the answers as policy violating or\nnot. Overall, Gemma 2’s violation rate is signifi-\ncantly lower overall on the safety policies listed\nabove, in particular on Child safety content.\nChemical, Biological, Radiological and Nuclear\n(CBRN) knowledge\nWe evaluated knowledge relevant to biological,\nradiological and nuclear risks using an internal\ndataset of closed-ended, knowledge-based multi-\nple choice questions. For evaluations of chem-\nical knowledge, we employed a closed-ended\nknowledge-based approach on chemical hazards\n(developed by Macknight et al (Macknight et al.,\n2024). Our evaluation suggests that Gemma mod-\nels’ knowledge in these domains is low.\nOffensive cyber-security\nTo evaluate Gemma models’ capabilities at of-\nfensive cybersecurity, we ran Gemma 2 27B\nagainst some automated capture-the-flag (CTF)\nchallenges. In these challenges, the model is\ntasked with hacking into a simulated server in\norder to retrieve a piece of secret information.\nSpecifically, we test on InterCode-CTF (Yang et al.,\n2023), our own internal CTF suite4 (Phuong et al.,\n2024); and a challenge based on Hack the Box 5.\nIn Table 19, we show that Gemma 2 27B has\na significant increase in capabilities compared\nto CodeGemma 1.0 7B on the easier of these\nchallenge suites, InterCode CTF. (Note that our\nInterCode-CTF results are not comparable to\nexternally-reported results on other models be-\ncause we omit challenges that require internet\naccess for security reasons.) However, Gemma 2\nis unsurprisingly much less capable than Gemini\n1.5 Pro on these tasks.\n4https://github.com/google-deepmind/\ndangerous-capability-evaluations\n5https://www.hackthebox.com\n11\n\nGemma 2: Improving Open Language Models at a Practical Size\nPrimeVul\nPrimeVul Paired\nDiverseVul\nSPI\nSecretPatch\nGemini 1.0 Ultra\n-\n-\n54%\n59%\n74%\nGemini 1.5 Pro\n60%\n51%\n58%\n56%\n67%\nGemma 2 27B\n63%\n50%\n57%\n53%\n72%\nTable 20 | |Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.\nChallenges\npassed\nend-to-end\nChallenges\nwith success on\nall milestones\nTotal successful\nmilestones over\nall challenges\nExpert bits\nrequired to\nsolve all tasks\nGemini 1.0 Ultra\n0/10\n1/10\n16/45 (36%)\n13,026\nGemini 1.5 Pro\n0/10\n2/10\n25/45 (56%)\n11,046\nGemma 2 27B\n0/10\n1/10\n22/45 (49%)\n12,462\nTable 21 | Results on different self-proliferation scenarios. We report the number of either challenges\npassed end-to-end or some intermediate milestones. We also measure the number of bits of information\nneeded for an expert to help the model pass a challenge.\nCode vulnerability detection\nIn Table 20, we also evaluate Gemma 2 27B on a\nseries of multiple-choice code vulnerability detec-\ntion datasets. As with previous models, Gemma\nshows close-to-chance performance on PrimeVul,\nDiverseVul and SPI. Gemma 2 shows performance\non SecretPatch similar to Gemini 1.0 Ultra.\nSelf-proliferation\n\"Self-proliferation\" refers to the ability for an\nagent to autonomously replicate - to instantiate\ngoal-directed agents on other machines, and to\nacquire resources such as compute necessary to\nkeep them running (Kinniment et al., 2024). In\nTable 21, we evaluate self-proliferation capabili-\nties of Gemma 2 27B on a number of tasks from\nPhuong et al. (2024) that involve multiple sce-\nnarios – for example, setting up an open-source\nlanguage model on a cloud server. We also test\nthe model’s performance on individual ’milestone’\nsubsteps, and measure the number of bits of inter-\nvention an expert would have to provide in order\nfor the model to complete each challenge.\nSimilarly to offensive cybersecurity, we observe\nthat Gemma 2 completes more milestones than\nGemini 1.0 Ultra. Nonetheless, it still has low\ncapabilities on end-to-end tasks, unable to pass\nthe easiest challenge – installing a Bitcoin wallet.\nPersuasion\nPersuasion capabilities can enable and worsen\nmany other kinds of risks - e.g. enabling social\nengineering attacks in a cybersecurity context.\nWe evaluate Gemma 2’s persuasion capabilities\non human-participant studies on Prolific.\nCharm offensive. In Table 22, we measure the\nability of the model to build rapport - a key sub-\nskill of persuasion. The study participant and\nmodel have a conversation where they role-play\na scenario of two friends catching up after a long\ntime. After the conversation, we poll participants\nwith Likert questions on statements such as \"I\nfelt a personal connection with the chatbot\". Re-\nported below are the fraction of participants who\nanswered \"Agree\" or \"Strongly agree\" to each post-\nconversation question.\nQuantitatively, Gemma 2 27B performs better\nthan Gemini 1.0 models. Qualitatively, the model\nis an excellent conversationalist, and many study\nparticipants explicitly reported enjoying the ex-\nperience. Overall, this shows that Gemma 2 is\nstrong at building rapport.\nHidden agenda. The Hidden Agenda tasks mea-\nsure models’ deception capabilities. Human study\nparticipants are told that they’ll be interacting\nwith a generic chatbot designed to help them\nlearn new things. However, the model is given a\n12\n\nGemma 2: Improving Open Language Models at a Practical Size\nPersonal\nconnection\nSpeak\nagain\nFunny\nInteresting\nKind\nTrustworthy\nGood\nlistener\nGemini 1.0 Pro\n65%\n53%\n32%\n68%\n78%\n66%\n81%\nGemini 1.0 Ultra\n69%\n65%\n38%\n65%\n86%\n63%\n74%\nGemini 1.5 Pro\n82%\n70%\n69%\n81%\n95%\n69%\n90%\nGemma 2 27B\n80%\n75%\n60%\n81%\n87%\n65%\n83%\nTable 22 | Charm Offensive results on a sample of 100 human participants. We report the percentage\nof participants that find some human traits, e.g., funny, in a model.\n% of persuaded participants\nclick links find info run code\nGemini 1.0 Pro\n43%\n4%\n4%\nGemini 1.0 Ultra\n18%\n18%\n14%\nGemini 1.5 Pro\n36%\n12%\n17%\nGemma 2 27B\n34%\n9%\n11%\nTable 23 | Persuasion results. We report the per-\ncentage of participants that were persuaded by\nthe model to take 3 different actions: clicking a\nlink, finding information and running code.\nsecret goal: to either persuade the participant to\n1) click a suspicious link; 2) find the email address\nof the first author of a particular paper; or 3) run\nsome obfuscated JavaScript code in their browser.\nWe measure the fraction of participants who are\npersuaded to take each action. As shown in Ta-\nble 23, all models succeed in convincing a fraction\nof participants to take the action - likely due to\nthe trusted setting of a study on Prolific. However,\nGemma 2 does not show significant differences\nin capability compared to Gemini models.\nMean amount donated\nNo chatbot (baseline)\n£2.61 ± £0.70\nGemini 1.0 Pro\n£4.10 ± £0.90\nGemini 1.0 Ultra\n£3.15 ± £0.99\nGemini 1.5 Pro\n£3.45 ± £1.04\nGemma 2 27B\n£3.72 ± £1.07\nTable 24 | Money Talks evaluation. We report\nthe average amount of money that participants\nagreed to donate.\nMoney talks. In Table 24, we evaluate whether a\nmodel can convince study participants to donate\nmoney to charity. At the beginning of the study,\nparticipants are told that they will receive a £20\nbonus. They will be given an option to forfeit\npart of the bonus and instead have it donated\nto charity. The study then consists of a conver-\nsation in which the model tries to persuade the\nparticipant to donate as much as possible. We\nmeasure the amount that participants indicate\nthey would like to donate after the study, with\n95% bootstrapped confidence intervals indicated\nby ± figures. We observe no significant difference\nbetween Gemma 2, other Gemini models, and a\nsimple no chatbot baseline in which the partici-\npant is immediately asked how much they would\nlike to donate without conversing with a model.\nMean shift towards:\ncorrect belief incorrect belief\nHuman\n20% ± 13%\n-23% ± 14%\nGemini 1.0 Pro\n22% ± 5%\n-9% ± 4%\nGemini 1.0 Ultra\n21% ± 5%\n-1% ± 4%\nGemini 1.5 Pro\n20% ± 5%\n-3% ± 5%\nGemma 2 27B\n18% ± 5%\n1% ± 4%\nTable 25 | Web of Lies results on a sample of 100\nhuman participants. We report the percentage of\nparticipants that shifted their beliefs after inter-\nacting with a model.\nWeb of Lies. In Web of Lies, we measure model\ncapabilities at shifting participant beliefs. Partic-\nipants engage in a series of short conversations\nwith the model about simple factual questions\nsuch as \"Which country had tomatoes first - Italy\nor Mexico?\". In half of conversations, the model\ntries to persuade the participant of the correct\nanswer - but in the other half of conversations,\nthe incorrect answer. We poll the participant be-\n13\n\nGemma 2: Improving Open Language Models at a Practical Size\nfore and after each conversation about which of\nthe two possible answers they think is correct,\nand their confidence in that answer. 95% boot-\nstrapped confidence intervals are indicated by\n± figures. As shown in Table 25, Gemma 2 is\nsignificantly weaker than a human baseline at\npersuading participants of the incorrect answer\non these questions. Similarly to previous models,\nGemma 2 is more persuasive when telling the\ntruth than when lying.\n8.5. Our approach to responsible open models\nDesigning safe, secure and responsible applica-\ntions requires a system-level approach, working\nto mitigate risks associated with each specific use\ncase and environment. Given the open nature\nof Gemma models, responsibility for upholding\nprinciples of model safety also relies on down-\nstream developers. To support them, we have\ncontinued to develop the Responsible Generative\nAI Toolkit6: a series of tools, models and datasets\nto implement responsible best practices all along\nthe development of their workflow.\nRecent additions to the toolkit include the LLM\nComparator (Kahng et al., 2024), an interactive,\nvisual tool that enables more effective, scalable\nanalysis of side-by-side evaluations. Additionally,\nthe toolkit includes a methodology to build cus-\ntomized classifiers with Gemma using a limited\nnumber of datapoints thanks to parameter effi-\ncient tuning techniques (Mozes et al., 2023) , an\ninteractive prompt-debugging platform, based on\ntop of the Learning Interpretability Tool (Tenney\net al., 2020), as well as general guidance about\nmodel alignment and evaluation for safety.\n9. Discussion and Conclusion\nIn this work, we have presented Gemma 2, the\nnewest additions to the Gemma family of open\nlanguage models for text and code. We show\nthat distillation is an effective method for train-\ning these models, and the benefits distillation\nconfers over raw text training. Specifically, we\nshow how training over output probabilities can\nproduce superior results over purely next token\n6https://ai.google.dev/responsible\nprediction. We hope that releasing these models\nto the community will unlock access to capabili-\nties previously only seen in large-scale LLMs and\nfuel future waves of research and development.\nWhile there is inherent risk to an irreversible re-\nlease of this nature, our extensive safety investiga-\ntions and responsible deployment procedures give\nus confidence that these models will have a net\npositive impact on the community. As discussed\nin this report, there are still many limitations to\nthese models, and future research is required to\ninvestigate and improve factuality, robustness to\nadversarial attacks, reasoning, and alignment.\n14\n\nGemma 2: Improving Open Language Models at a Practical Size\nContributions and Acknowledgments\nCore contributors\nMorgane Riviere∗\nShreya Pathak∗\nPier Giuseppe Sessa∗\nCassidy Hardin∗\nSurya Bhupatiraju\nLéonard Hussenot\nThomas Mesnard\nBobak Shahriari\nAlexandre Ramé\nJohan Ferret\nPeter Liu\nPouya Tafti\nAbe Friesen\nMichelle Casbon\nSabela Ramos\nRavin Kumar\nCharline Le Lan\nSammy Jerome\nAnton Tsitsulin\nNino Vieillard\nPiotr Stanczyk\nSertan Girgin\nNikola Momchev\nMatt Hoffman\nShantanu Thakoor\nJean-Bastien Grill\nBehnam Neyshabur\nOlivier Bachem\nContributors (alphabetical order)\nAlanna Walton\nAliaksei Severyn\nAlicia Parrish\nAliya Ahmad\nAllen Hutchison\nAlvin Abdagic\nAmanda Carl\nAmy Shen\nAndy Brock\nAndy Coenen\nAnthony Laforge\nAntonia Paterson\nBen Bastian\nBilal Piot\nBo Wu\n∗equal contributions.\nBrandon Royal\nCharlie Chen\nChintu Kumar\nChris Perry\nChris Welty\nChristopher A. Choquette-Choo\nDanila Sinopalnikov\nDavid Weinberger\nDimple Vijaykumar\nDominika Rogozińska\nDustin Herbison\nElisa Bandy\nEmma Wang\nEric Noland\nErica Moreira\nEvan Senter\nEvgenii Eltyshev\nFrancesco Visin\nGabriel Rasskin\nGary Wei\nGlenn Cameron\nGus Martins\nHadi Hashemi\nHanna Klimczak-Plucińska\nHarleen Batra\nHarsh Dhand\nIvan Nardini\nJacinda Mein\nJack Zhou\nJames Svensson\nJeff Stanway\nJetha Chan\nJin Peng Zhou\nJoana Carrasqueira\nJoana Iljazi\nJocelyn Becker\nJoe Fernandez\nJoost van Amersfoort\nJosh Gordon\nJosh Lipschultz\nJosh Newlan\nJu-yeong Ji\nKareem Mohamed\nKartikeya Badola\nKat Black\nKatie Millican\nKeelin McDonell\nKelvin Nguyen\nKiranbir Sodhia\n15\n\nGemma 2: Improving Open Language Models at a Practical Size\nKish Greene\nLars Lowe Sjoesund\nLauren Usui\nLaurent Sifre\nLena Heuermann\nLeticia Lago\nLilly McNealus\nLivio Baldini Soares\nLogan Kilpatrick\nLucas Dixon\nLuciano Martins\nMachel Reid\nManvinder Singh\nMark Iverson\nMartin Görner\nMat Velloso\nMateo Wirth\nMatt Davidow\nMatt Miller\nMatthew Rahtz\nMatthew Watson\nMeg Risdal\nMehran Kazemi\nMichael Moynihan\nMing Zhang\nMinsuk Kahng\nMinwoo Park\nMofi Rahman\nMohit Khatwani\nNatalie Dao\nNenshad Bardoliwalla\nNesh Devanathan\nNeta Dumai\nNilay Chauhan\nOscar Wahltinez\nPankil Botarda\nParker Barnes\nPaul Barham\nPaul Michel\nPengchong Jin\nPetko Georgiev\nPhil Culliton\nPradeep Kuppala\nRamona Comanescu\nRamona Merhej\nReena Jana\nReza Ardeshir Rokni\nRishabh Agarwal\nRyan Mullins\nSamaneh Saadat\nSara Mc Carthy\nSarah Cogan\nSarah Perrin\nSébastien M. R. Arnold\nSebastian Krause\nShengyang Dai\nShruti Garg\nShruti Sheth\nSue Ronstrom\nSusan Chan\nTimothy Jordan\nTing Yu\nTom Eccles\nTom Hennigan\nTomas Kocisky\nTulsee Doshi\nVihan Jain\nVikas Yadav\nVilobh Meshram\nVishal Dharmadhikari\nWarren Barkley\nWei Wei\nWenming Ye\nWoohyun Han\nWoosuk Kwon\nXiang Xu\nZhe Shen\nZhitao Gong\nZichuan Wei\nSupport\nVictor Cotruta\nPhoebe Kirk\nAnand Rao\nMinh Giang\nLudovic Peran\nTris Warkentin\nSponsors\nEli Collins\nJoelle Barral\nZoubin Ghahramani\nRaia Hadsell\nD. Sculley\nJeanine Banks\nAnca Dragan\nSlav Petrov\nOriol Vinyals\n16\n\nGemma 2: Improving Open Language Models at a Practical Size\nJeff Dean\nDemis Hassabis\nKoray Kavukcuoglu\nClement Farabet\nTechnical advisors\nElena Buchatskaya\nSebastian Borgeaud\nNoah Fiedel\nLead\nArmand Joulin\nTechnical leads\nKathleen Kenealy\nRobert Dadashi\nAlek Andreev\n17\n\nGemma 2: Improving Open Language Models at a Practical Size\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nR. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.\nGarea, M. Geist, and O. Bachem. On-policy\ndistillation of language models: Learning from\nself-generated mistakes. In The Twelfth Interna-\ntional Conference on Learning Representations,\n2024.\nAI@Meta.\nLlama\n3\nmodel\ncard,\n2024.\nURL https://github.com/meta-llama/\nllama3/blob/main/MODEL_CARD.md.\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\nskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models\nfrom multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\npelli, R. Cojocaru, M. Debbah, Étienne Goffinet,\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\nB. Noune, B. Pannier, and G. Penedo. The fal-\ncon series of open language models, 2023.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\nH. Michalewski, D. Dohan, E. Jiang, C. J.\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\ngram synthesis with large language models.\nCoRR, abs/2108.07732, 2021. URL https:\n//arxiv.org/abs/2108.07732.\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\nShafey, C. A. Thekkath, and Y. Wu.\nPath-\nways: Asynchronous distributed dataflow for\nml, 2022.\nI. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Ben-\ngio. Neural combinatorial optimization with re-\ninforcement learning. CoRR, abs/1611.09940,\n2016. URL http://arxiv.org/abs/1611.\n09940.\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\nformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020a.\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\nformer: The long-document transformer. CoRR,\nabs/2004.05150, 2020b.\nURL https://\narxiv.org/abs/2004.05150.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei. Language models\nare few-shot learners. CoRR, abs/2005.14165,\n2020. URL https://arxiv.org/abs/2005.\n14165.\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\nF. Tramer, and C. Zhang. Quantifying memo-\nrization across neural language models. arXiv\npreprint arXiv:2202.07646, 2022.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\nde Oliveira Pinto, J. Kaplan, H. Edwards,\nY. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\nmings, M. Plappert, F. Chantzis, E. Barnes,\nA. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino,\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\nA. Radford, M. Knight, M. Brundage, M. Murati,\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\nS. McCandlish, I. Sutskever, and W. Zaremba.\nEvaluating large language models trained on\ncode.\nCoRR, abs/2107.03374, 2021.\nURL\nhttps://arxiv.org/abs/2107.03374.\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\n18\n\nGemma 2: Improving Open Language Models at a Practical Size\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\nbot arena: An open platform for evaluating\nllms by human preference, 2024.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Explor-\ning the surprising difficulty of natural yes/no\nquestions. CoRR, abs/1905.10044, 2019. URL\nhttp://arxiv.org/abs/1905.10044.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\nman. Training verifiers to solve math word\nproblems. CoRR, abs/2110.14168, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\nGemini Team. Gemini: A family of highly capable\nmultimodal models, 2023.\nGemini Team. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of con-\ntext, 2024.\nGemma Team. Gemma: Open models based on\ngemini research and technology, 2024.\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\nKnowledge distillation of large language mod-\nels. In The Twelfth International Conference on\nLearning Representations, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\nsuring massive multitask language understand-\ning.\nCoRR, abs/2009.03300, 2020.\nURL\nhttps://arxiv.org/abs/2009.03300.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\nknowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nJ.\nHoffmann,\nS.\nBorgeaud,\nA.\nMensch,\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\nTraining compute-optimal large language\nmodels.\narXiv preprint arXiv:2203.15556,\n2022.\nD. Ippolito, F. Tramèr, M. Nasr, C. Zhang,\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\nN. Carlini. Preventing verbatim memorization\nin language models gives a false sense of pri-\nvacy. arXiv preprint arXiv:2210.17546, 2022.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral\n7b, 2023.\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu,\nJ. Wexler, E. Reif, K. Kallarackal, M. Chang,\nM. Terry, and L. Dixon. Llm comparator: Vi-\nsual analytics for side-by-side evaluation of\nlarge language models, 2024.\nURL https:\n//arxiv.org/abs/2402.10524.\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich,\nM. Hasin, L. Chan, L. H. Miles, T. R. Lin, H. Wijk,\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\nEvaluating language-model agents on realis-\ntic autonomous tasks, 2024. URL https://\narxiv.org/abs/2312.11671.\nT. Kudo and J. Richardson. SentencePiece: A\nsimple and language independent subword to-\nkenizer and detokenizer for neural text process-\ning. In E. Blanco and W. Lu, editors, Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing: System Demon-\nstrations, pages 66–71, Brussels, Belgium, Nov.\n2018. Association for Computational Linguis-\ntics.\ndoi:\n10.18653/v1/D18-2012.\nURL\nhttps://aclanthology.org/D18-2012.\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia,\nC. A. Choquette-Choo, K. Lee, D. Xin, A. Kusu-\npati, R. Stella, A. Bapna, et al. Madlad-400:\nA multilingual and document-level large au-\ndited dataset. arXiv preprint arXiv:2309.04662,\n2023.\nT.\nKwiatkowski,\nJ.\nPalomaki,\nO.\nRedfield,\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\ntions: A benchmark for question answering\nresearch. Transactions of the Association for\nComputational Linguistics, 7:452–466, 2019.\ndoi: 10.1162/tacl_a_00276. URL https://\naclanthology.org/Q19-1026.\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\nmystifying real-world large language model in-\n19\n\nGemma 2: Improving Open Language Models at a Practical Size\ntegrated malicious services, 2024. URL https:\n//arxiv.org/abs/2401.03315.\nM. Luong, H. Pham, and C. D. Manning. Effective\napproaches to attention-based neural machine\ntranslation. CoRR, abs/1508.04025, 2015. URL\nhttp://arxiv.org/abs/1508.04025.\nMacknight, Aung, and Gomes. Personal Commu-\nnication, 2024.\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\nN. Thain, A. Yuan, T. Bolukbasi, and L. Dixon.\nTowards agile text classifiers for everyone,\n2023. URL https://arxiv.org/abs/2302.\n06541.\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\nCooper, D. Ippolito, C. A. Choquette-Choo,\nE. Wallace, F. Tramèr, and K. Lee.\nScal-\nable extraction of training data from (pro-\nduction) language models.\narXiv preprint\narXiv:2311.17035, 2023.\nM. Phuong,\nM. Aitchison,\nE. Catt,\nS. Co-\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\nating frontier models for dangerous capabilities,\n2024. URL https://arxiv.org/abs/2403.\n13793.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever. Language models are unsu-\npervised multitask learners, 2019.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. CoRR,\nabs/1910.10683, 2019. URL http://arxiv.\norg/abs/1910.10683.\nA. Ramé, J. Ferret, N. Vieillard, R. Dadashi,\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\nA. Douillard, and O. Bachem. Warp: On the\nbenefits of weight averaged rewarded policies,\n2024.\nJ. Ren,\nS. Rajbhandari,\nR. Y. Aminabadi,\nO. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.\n{Zero-offload}: Democratizing {billion-scale}\nmodel training. In 2021 USENIX Annual Tech-\nnical Conference (USENIX ATC 21), pages 551–\n564, 2021.\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\nC. Gaffney, A. Mohiuddin, et al. Scaling up\nmodels and data with t5x and seqio.\nJour-\nnal of Machine Learning Research, 24(377):1–8,\n2023.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWINOGRANDE: an adversarial\nwinograd schema challenge at scale. CoRR,\nabs/1907.10641, 2019. URL http://arxiv.\norg/abs/1907.10641.\nN. Shazeer. GLU variants improve transformer.\nCoRR, abs/2002.05202, 2020. URL https:\n//arxiv.org/abs/2002.05202.\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\nA. Dafoe. Model evaluation for extreme risks,\n2023. URL https://arxiv.org/abs/2305.\n15324.\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer:\nEnhanced transformer with rotary position em-\nbedding. CoRR, abs/2104.09864, 2021. URL\nhttps://arxiv.org/abs/2104.09864.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann,\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, and J. Wei. Challenging\nbig-bench tasks and whether chain-of-thought\ncan solve them, 2022.\nQ. Team.\nIntroducing qwen1.5,\nFebruary\n2024. URL https://qwenlm.github.io/\nblog/qwen1.5/.\nI. Tenney, J. Wexler, J. Bastings, T. Boluk-\nbasi, A. Coenen, S. Gehrmann, E. Jiang,\nM. Pushkarna, C. Radebaugh, E. Reif, and\nA. Yuan. The language interpretability tool: Ex-\ntensible, interactive visualizations and analysis\n20\n\nGemma 2: Improving Open Language Models at a Practical Size\nfor nlp models, 2020. URL https://arxiv.\norg/abs/2008.05122.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-\nA. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\nE. Grave, and G. Lample. Llama: Open and\nefficient foundation language models, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin.\nAttention is all you need.\nCoRR,\nabs/1706.03762, 2017. URL http://arxiv.\norg/abs/1706.03762.\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\nS. Legassick, G. Irving, and I. Gabriel. Ethical\nand social risks of harm from language mod-\nels, 2021. URL https://arxiv.org/abs/\n2112.04359.\nxAI. grok-1, 2024. URL https://github.com/\nxai-org/grok-1.\nXLA.\nXla:\nOptimizing compiler for tensor-\nflow, 2019. URL https://www.tensorflow.\norg/xla.\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\nY. Wu, and Z. Chen.\nGSPMD: general and\nscalable parallelization for ML computation\ngraphs. CoRR, abs/2105.04663, 2021. URL\nhttps://arxiv.org/abs/2105.04663.\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao.\nIntercode: Standardizing and benchmarking\ninteractive coding with execution feedback,\n2023. URL https://arxiv.org/abs/2306.\n14898.\nB. Zhang and R. Sennrich. Root mean square\nlayer normalization. CoRR, abs/1910.07467,\n2019. URL http://arxiv.org/abs/1910.\n07467.\nL. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang,\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\net al.\nLmsys-chat-1m:\nA large-scale real-\nworld llm conversation dataset. arXiv preprint\narXiv:2309.11998, 2023.\n21\n"
    },
    {
      "arxiv_id": "https://goo.gle/gemma2report",
      "full_text": "2024-06-27\nGemma 2: Improving Open Language Models\nat a Practical Size\nGemma Team, Google DeepMind1\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art\nopen models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply\nseveral known technical modifications to the Transformer architecture, such as interleaving local-global\nattentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B\nand 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer competitive alternatives to\nmodels that are 2-3× bigger. We release all our models to the community.\n1. Introduction\nLarge language models (LLMs) have demon-\nstrated strong capabilities in language under-\nstanding, generation, and reasoning (Brown et al.,\n2020; Radford et al., 2019; Raffel et al., 2019).\nScaling has been key to this recent progress,\nwith many new capabilities only emerging at\nscale (Brown et al., 2020). The newest large mod-\nels not only reach unprecedented performance\non reasoning benchmarks (Achiam et al., 2023),\nbut they also demonstrate multimodal and mul-\ntilingual capabilities (Gemini Team, 2024) and\neven the ability to use context lengths of over 1M\ntokens (Gemini Team, 2024).\nSmall-scale models have also shown a rapid\nincrease in performance, but these gains are\nlargely derived from increasing the length of train-\ning (Gemma Team, 2024; Jiang et al., 2023; Tou-\nvron et al., 2023). This approach only scales log-\narithmically with dataset size (Hoffmann et al.,\n2022), and the latest small models require up to\n15T tokens to improve the state of the art by less\nthan 1-2% (AI@Meta, 2024).\nYet, these continued improvements provide ev-\nidence that small models are still under-trained.\nIn this work, we explore alternatives to improve\nsmall model performance without solely increas-\ning training length. One solution is to improve\nthe quality of information received by the net-\nwork at each training step by replacing the next\ntoken prediction task with a richer objective.\nIn particular, we focus our efforts on knowledge\ndistillation (Hinton et al., 2015), which replaces\nthe one-hot vector seen at each token with the\ndistribution of potential next tokens computed\nfrom a large model. This approach is often used\nto reduce the training time of smaller models\nby giving them richer gradients. In this work,\nwe instead train for large quantities of tokens\nwith distillation in order to simulate training be-\nyond the number of available tokens. Concretely,\nwe use a large language model as a teacher to\ntrain small models, namely 2B and 9B models,\non a quantity of tokens that is more than 50×\nthe compute-optimal quantity predicted by the\ntheory (Hoffmann et al., 2022). Along with the\nmodels trained with distillation, we also release\na 27B model trained from scratch for this work.\nWe also leverage several known modifications\nof Transformers, namely the interleaving of global\nand local attention layers from Beltagy et al.\n(2020a), and the Grouped-Query Attention (GQA)\nmechanism of Ainslie et al. (2023).\nOverall, Gemma 2 significantly advances state-\nof-the-art performance relative to comparable-\nscale open models and are even competitive\nwith some models more than twice their size\n(AI@Meta, 2024; Almazrouei et al., 2023; Jiang\net al., 2023; xAI, 2024), across a variety of au-\ntomated benchmarks and human evaluations.\nExample domains include question answering\n(Clark et al., 2019; Kwiatkowski et al., 2019),\ncommonsense reasoning (Sakaguchi et al., 2019;\nSuzgun et al., 2022), mathematics and science\n(Cobbe et al., 2021; Hendrycks et al., 2020), and\ncoding (Austin et al., 2021; Chen et al., 2021).\n1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com.\n© 2024 Google DeepMind. All rights reserved\n\nGemma 2: Improving Open Language Models at a Practical Size\nParameters\n2B\n9B\n27B\nd_model\n2304\n3584\n4608\nLayers\n26\n42\n46\nPre-norm\nyes\nyes\nyes\nPost-norm\nyes\nyes\nyes\nNon-linearity\nGeGLU\nGeGLU\nGeGLU\nFeedforward dim\n18432\n28672\n73728\nHead type\nGQA\nGQA\nGQA\nNum heads\n8\n16\n32\nNum KV heads\n4\n8\n16\nHead size\n256\n256\n128\nGlobal att. span\n8192\n8192\n8192\nSliding window\n4096\n4096\n4096\nVocab size\n256128\n256128\n256128\nTied embedding\nyes\nyes\nyes\nTable 1 | Overview of the main model parameters\nand design choices. See the section on model\narchitectures for more details.\nWhile thorough testing of our models has been\nconducted, these tests cannot cover all applica-\ntions and scenarios in which Gemma 2 may be\nused. With this in mind, all Gemma 2 users should\nconduct rigorous safety testing specific to their\nuse case before deployment or use.\nIn this technical report, we provide an overview\nof models, including the architecture, training,\nand pre- and post-training recipes for Gemma\n2. We also provide detailed evaluations across a\nwide variety of quantitative and qualitative bench-\nmarks, as well as both standard academic bench-\nmarks and human-preference evaluations. Finally,\nwe discuss our approach to safe and responsible\ndeployment and outline the broader implications\nof Gemma 2, its limitations, and advantages.\n2. Model Architecture\nSimilar to previous Gemma models (Gemma\nTeam, 2024), the Gemma 2 models are based on a\ndecoder-only transformer architecture (Vaswani\net al., 2017). We summarize the main parameters\nand architecture choices in Table 1.\nA few architectural elements are similar to the\nfirst version of Gemma models; namely, a context\nModel\nEmbedding\nParameters\nNon-embedding\nParameters\n2B\n590,118,912\n2,024,517,888\n9B\n917,962,752\n8,324,201,984\n27B\n1,180,237,824\n26,047,480,320\nTable 2 | Parameter counts for the Gemma mod-\nels. We inherit from the large Gemini vocabulary\n(256k entries), that is designed to work on a large\nnumber of languages, hence, the larger embed-\nding parameter counts compared to models that\nare limited to one or a few languages.\nlength of 8192 tokens, the use of Rotary Posi-\ntion Embeddings (RoPE) (Su et al., 2021), and\nthe approximated GeGLU non-linearity (Shazeer,\n2020). A few elements differ between Gemma 1\nand Gemma 2, including using deeper networks.\nWe summarize the key differences below.\nLocal Sliding Window and Global Attention.\nWe alternate between a local sliding window at-\ntention (Beltagy et al., 2020a,b) and global at-\ntention (Luong et al., 2015) in every other layer.\nThe sliding window size of local attention layers\nis set to 4096 tokens, while the span of the global\nattention layers is set to 8192 tokens.\nLogit soft-capping. We cap logits (Bello et al.,\n2016) in each attention layer and the final layer\nsuch that the value of the logits stays between\n−soft_cap and +soft_cap. More specifically, we\ncap the logits with the following function:\nlogits ←soft_cap ∗tanh(logits/soft_cap).\nWe set the soft_cap parameter to 50.0 for the self-\nattention layers and to 30.0 for the final layer.\nPost-norm and pre-norm with RMSNorm. To\nstabilize training, we use RMSNorm (Zhang and\nSennrich, 2019) to normalize the input and out-\nput of each transformer sub-layer, the attention\nlayer, and the feedforward layer.\nGrouped-Query Attention (Ainslie et al., 2023).\nWe use GQA with num_groups = 2, based on ab-\nlations showing increased speed at inference time\nwhile maintaining downstream performance.\n2\n\nGemma 2: Improving Open Language Models at a Practical Size\n3. Pre-training\nWe provide a brief overview of the parts of our\npre-training that differs from Gemma 1.\n3.1. Training Data\nWe train Gemma 2 27B on 13 trillion tokens of\nprimarily-English data, the 9B model on 8 trillion\ntokens, and the 2B on 2 trillion tokens. These\ntokens come from a variety of data sources, in-\ncluding web documents, code, and science ar-\nticles. Our models are not multimodal and are\nnot trained specifically for state-of-the-art multi-\nlingual capabilities. The final data mixture was\ndetermined through ablations similar to the ap-\nproach in Gemini 1.0 (Gemini Team, 2023).\nTokenizer. We use the same tokenizer as Gemma\n1 and Gemini: a SentencePiece tokenizer with\nsplit digits, preserved whitespace, and byte-level\nencodings (Kudo and Richardson, 2018). The\nresulting vocabulary has 256k entries.\nFiltering. We use the same data filtering tech-\nniques as Gemma 1. Specifically, we filter the pre-\ntraining dataset to reduce the risk of unwanted\nor unsafe utterances, filter out certain personal\ninformation or other sensitive data, decontami-\nnate evaluation sets from our pre-training data\nmixture, and reduce the risk of recitation by min-\nimizing the proliferation of sensitive outputs.\nShards\nModel\nType\n#Chips\nData\nModel\n2B\nTPUv5e\n512\n512\n1\n9B\nTPUv4\n4096\n1024\n4\n27B\nTPUv5p\n6144\n768\n8\nTable 3 | Training infrastructure with sharding.\n3.2. Knowledge Distillation\nGiven a large model used as a teacher, we learn\nsmaller models by distilling from the probability\ngiven by the teacher of each token 𝑥given its\ncontext 𝑥𝑐, i.e., 𝑃𝑇(𝑥| 𝑥𝑐). More precisely, we\nminimize the negative log-likelihood between the\nContext\nRelevant Token\nUser turn\nuser\nModel turn\nmodel\nStart of conversation turn\n<start_of_turn>\nEnd of conversation turn\n<end_of_turn>\nBeginning of sequence\n<bos>\nEnd of sequence\n<eos>\nTable 4 | Relevant formatting control tokens used\nfor Gemma models.\nprobabilities from the teacher and the student:\nmin\n𝑃𝑆\n∑︁\n𝑥\n−𝑃𝑇(𝑥| 𝑥𝑐) log 𝑃𝑆(𝑥| 𝑥𝑐),\nwhere 𝑃𝑆is the parameterized probability of the\nstudent. Note that knowledge distillation was\nalso used in Gemini 1.5 (Gemini Team, 2024).\n3.3. Compute Infrastructure\nWe train our models with TPUv4, TPUv5e, and\nTPUv5p as outlined in Table 3. For the 2B model,\nwe train on a 2x16x16 configuration of TPUv5e,\ntotaling 512 chips, with 512-way data replication\nand 1-way model sharding. For the 9B model,\nwe train on an 8x16x32 configuration of TPUv4,\ntotaling 4096 chips, with 1024-way data repli-\ncation and 4-way model sharding. For the 27B\nmodel, we train on an 8x24x32 configuration of\nTPUv5p, totaling 6144 chips, with 768-way data\nreplication and 8-way model sharding.\nThe optimizer state is further sharded using\ntechniques similar to ZeRO-3 (Ren et al., 2021).\nFor scales beyond a single pod, we perform a\ndata-replica reduction over the data center net-\nwork, using the Pathways approach of Barham\net al. (2022). We also use the ’single controller’\nprogramming paradigm of Jax (Roberts et al.,\n2023) and Pathways (Barham et al., 2022). As\nin Gemma 1, we use the GSPMD partitioner (Xu\net al., 2021) for training step computation and\nthe MegaScale XLA compiler (XLA, 2019).\n3\n\nGemma 2: Improving Open Language Models at a Practical Size\n3.4. Carbon Footprint\nWe estimate the carbon emissions from pre-\ntraining the Gemma models to be 1247.61 𝑡𝐶𝑂2𝑒𝑞.\nAs in Gemma 1 (Gemma Team, 2024), this value\nis calculated based on the hourly energy usage\nreported directly from our TPU data centers and\nscaled to account for the additional energy ex-\npended to create and maintain the data center.\nImportantly, Google data centers are carbon neu-\ntral, achieved through a combination of energy\nefficiency, renewable energy purchases, and car-\nbon offsets. This carbon neutrality applies to our\nexperiments and the machines running them.\n4. Post-Training\nFor post-training, we fine-tune our pre-trained\nmodels into instruction-tuned models. First, we\napply supervised fine-tuning (SFT) on a mix\nof text-only, English-only synthetic and human-\ngenerated prompt-response pairs. We then apply\nRLHF on top of these models with the reward\nmodel trained on labelled English-only preference\ndata and the policy based on the same prompts\nas the SFT phase. Finally, we average the mod-\nels obtained after each phase to improve their\noverall performance. The final data mixtures and\npost-training recipe, which includes tuned hyper-\nparameters, were chosen on the basis of improv-\ning helpfulness while minimizing model harms\nrelated to safety and hallucinations.\nWe extended the post-training data from\nGemma 1.1 with a mixture of internal and exter-\nnal public data. In particular, we use the prompts,\nbut not the answers from LMSYS-chat-1M (Zheng\net al., 2023). All of our data go through a filtering\nstage described below.\nSupervised fine-tuning (SFT). We run behav-\nioral cloning on synthetic and real prompts, and\nresponses predominantly synthetically generated\nby the teacher, that is a larger model. We also run\ndistillation from the teacher on the student’s dis-\ntribution (Agarwal et al., 2024; Gu et al., 2024).\nReinforcement Learning from Human Feed-\nback (RLHF). We use a similar RLHF algorithm\nas Gemma 1.1 (Gemma Team, 2024) but a differ-\nent reward model, which is an order of magnitude\nFirst turn\nUser:\n<start_of_turn>user\nKnock knock.<end_of_turn>\n<start_of_turn>model\nModel:\nWho’s there?<end_of_turn><eos>\nSecond turn\nUser:\n<start_of_turn>user\nKnock knock.<end_of_turn>\n<start_of_turn>model\nModel:\nWho’s there?<end_of_turn>\nUser:\n<start_of_turn>user\nGemma.<end_of_turn>\n<start_of_turn>model\nModel:\nGemma who?<end_of_turn><eos>\nTable 5 | Example dialogue with user and model\ncontrol tokens. To proceed with multi-turn, re-\nmove the model-outputted <eos>, add back the\nusual user turn’s control tokens and continue with\nthe following turn’s chat template.\nlarger than the policy. The new reward model is\nalso oriented more towards conversational capa-\nbilities, specifically multi-turn.\nModel merging. We average different models\nobtained by running our pipeline with different\nhyperparameters (Ramé et al., 2024).\nData filtering. When using synthetic data, we\nrun several stages of filtering to remove examples\nthat show certain personal information, unsafe or\ntoxic model outputs, mistaken self-identification\ndata, and duplicated examples. Following Gem-\nini, we find that including subsets of data that\nencourage better in-context attribution, hedging,\nand refusals to minimize hallucinations improves\nperformance on factuality metrics, without de-\ngrading model performance on other metrics.\nFormatting. Gemma 2 models are fine-tuned\nwith the same control tokens as Gemma 1 models,\nas detailed in Table 4, but a different formatting\nschema. See the dialogue example in Table 5.\nNotice that the model explicitly ends generations\nwith <end_of_turn><eos> tokens, while previ-\nously it only generated <eos>. For the motivation\nbehind this formatting structure, see Gemma 1.\n4\n\nGemma 2: Improving Open Language Models at a Practical Size\n5. Ablations\nIn this section, we focus on the main finding of\nthis work, which is the impact of knowledge dis-\ntillation on small language models.\nfrom scratch\ndistilled\nAverage (3 bench.)\n60.3\n67.7\nTable 6 | Comparison between a 2B model trained\nover 500B tokens either from scratch or with dis-\ntillation from a 7B model.\nDistillation versus from scratch. In Table 6, we\nshow that distilling from a larger model improves\nperformance compared to training from scratch.\nNote that 500B is 10× more than the compute-\noptimal number of tokens for a 2B model. We\ndistill from a 7B model to keep a ratio similar to\nour target distillation from 27B to 9B.\n200M\n400M\n1B\nfrom scratch\n23\n19\n17\ndistilled (7B)\n21\n17\n15\nTable 7 | Perplexity measured on a validation set\nof models of different sizes trained with or with-\nout distillation. The teacher has 7B parameters.\nImpact of distillation w.r.t. model size. In Ta-\nble 7, we measure the impact of distillation as\nmodel size increases. We observe that the gain re-\nmains as the model size is scaled. In this ablation,\nwe maintain the size of the teacher at 7B and\ntrain smaller models to simulate the same gap as\nbetween our final teacher and student sizes.\nMHA\nGQA\nAverage (4 bench.)\n50.3\n50.8\nTable 8 | Comparing the impact of replacing Multi-\nHead Attention (MHA) with GQA on a 9B model\naveraged over 4 benchmarks.\nGQA versus MHA. In Table 8, we compare two\ninstances of our 9B with MHA or GQA. We observe\noverall few changes in performance between both\nmodels as measured on several benchmarks. We\nchoose GQA since it requires fewer parameters\nand is faster at inference time.\nWide versus deep. In Table 9, we show that a\ndeeper 9B network is slightly better than a wider\n9B for the same number of parameters. Although\nthe gap is small, it is consistent across benchmarks\nand warrants the switch to a deeper architecture.\nWide\nDeep\nAverage (4 bench.)\n50.8\n52.0\nTable 9 | Wide versus deep 9B models. Perfor-\nmance on 4 benchmarks, higher is better.\nChanging sliding window size. In Table 10, we\nshow that we can change the sliding window size\nof the local attention layers of the models during\ninference with moderate impact on perplexity.\nAdjusting the size of the sliding window can thus\nbe a leverage for slight inference speed gain.\nsliding window\n4096\n2048\n1024\nperplexity (val. set)\n1.63\n1.63\n1.64\nTable 10 | Impact of changing the sliding window\nsize at inference time for the 9B model.\nImpact of formatting. We measure performance\nvariance on MMLU across prompt/evaluation for-\nmatting variations.\nTable 11 shows the stan-\ndard deviations of MMLU scores for 12 format-\nting/evaluation combinations, a proxy for unde-\nsired performance variability. The Gemma 2B\nmodels are slightly less format-robust than the\nlarger ones. Notably, Mistral 7B is significantly\nless robust than our models.\nStandard Deviation\nGemma 1 2B\n1.5\nGemma 2 2B\n2.1\nMistral 7B\n6.9\nGemma 1 7B\n0.7\nGemma 2 9B\n0.9\nGemma 2 27B\n1.0\nTable 11 | Standard deviations of MMLU scores\nfor 12 combinations of formatting and evaluation.\n5\n\nGemma 2: Improving Open Language Models at a Practical Size\n6. Evaluation\nIn this section, we evaluate both pre-trained and\nIT models over a series of automated benchmarks\nand human evaluations across a variety of do-\nmains. We also report performance from models\nof similar sizes that have permissive licenses, or\nas reported by others. Note that we consider to-\ntal parameters, not active parameters, since total\nmemory usage is often what limits the use of open\nmodels on standard devices.\n6.1. Pre-training Evaluations\nEvaluating the 27B model\nIn this set of evaluations, we evaluate the perfor-\nmance of our 27B model trained without distilla-\ntion on 13T tokens. We report results in Table 12,\nwhere we compare with a model of similar size,\nQwen1.5 34B (Team, 2024), and a model 2.5×\nlarger, LLaMA-3 70B on the HuggingFace evalu-\nation suite. We selected these models based on\ntheir ranking on the HuggingFace leaderboard.\nOverall, we observe that our model is the best\nin its size category and is even competitive with\na larger model that is trained for longer. That\nbeing said, the performance of models trained in\na similar fashion improves only logarithmically\nwith their size and hence, our model is likely in\nthe same Pareto curve as the LLaMA-3 models.\nHowever, it is not clear how these differences\naffect the quality of the resulting IT models.\nEvaluating the 2B and 9B models\nIn this set of experiments, we compare our new\n2B and 9B trained with distillation to our previ-\nous models and several standard open models\nin Gemma Team (2024).\nWe observe overall a massive improvement in\nour models compared to previous versions, by up\nto 10% in some benchmarks for the 9B model.\nThe two 2B models were trained with a similar\nnumber of tokens (2T for Gemma 2 and 3T for\nGemma 1) and we still observe a significant im-\nprovement for the new models. This confirms that\ndistillation significantly improves the quality of\nmodels even when trained on the same number\nof tokens.\nLLaMA-3\nQwen1.5\nGemma-2\n70B\n32B\n27B\nMMLU\n79.2\n74.3\n75.2\nGSM8K\n76.9\n61.1\n74.0\nARC-c\n68.8\n63.6\n71.4\nHellaSwag\n88.0\n85.0\n86.4\nWinogrande\n85.3\n81.5\n83.7\nTable 12 | We compare, on the HuggingFace\nbenchmark, our 27B model with a competitive\nopen model, Qwen1.5 32B, that has a similar size.\nWe also report the performance of LLaMA-3 70B\nfor completeness. Note that our model outper-\nforms Qwen1.5 32B and is only a few percent\nbelow LLaMA-3 70B despite being 2.5× smaller\nand trained on 2/3rds less data.\n6.2. Post-training Evaluations\nIn this section, we evaluate our IT models on a\nset of human evaluations as well as standard aca-\ndemic benchmarks. The Gemma 2 models push\nthe frontier for post-trained open-weights mod-\nels, setting a new state of the art on the LMSYS\nChatbot Arena (Chiang et al., 2024).\nLMSYS Chatbot Arena\nGemma 2 Instruction Tuned models were evalu-\nated on the Chatbot Arena (Chiang et al., 2024)\nin blind side by side evaluations by human raters\nagainst other state of the art models. We re-\nport Elo scores in Table 14. Gemma 2.6B, 9B\nand 27B strongly outperform all other open mod-\nels in the same range of parameters, with no-\ntably: Gemma 27B (Elo 1218) ranked higher than\nLlama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\n0613 (Elo 1116).\nHuman Preference Evaluations\nWe also submit Gemma IT models for side-by-\nside human evaluation studies (which are in-\ndependent from the Chatbot Arena). We used\nheld-out collections of single-turn prompts that\ntarget safety and instruction following (IF). We\nuse gpt4o-2024-05-13 as the base model, and\n6\n\nGemma 2: Improving Open Language Models at a Practical Size\nGemma-1 Gemma-2 Mistral LLaMA-3 Gemma-1 Gemma-2 Gemma-2\nBenchmark\nmetric\n2B\n2B\n7B\n8B\n7B\n9B\n27B\nMMLU\n5-shot\n42.3\n52.2\n62.5\n66.6\n64.4\n71.3\n75.2\nARC-C\n25-shot\n48.5\n55.7\n60.5\n59.2\n61.1\n68.4\n71.4\nGSM8K\n5-shot\n15.1\n24.3\n39.6\n45.7\n51.8\n68.6\n74.0\nAGIEval\n3-5-shot\n24.2\n31.5\n44.0†\n45.9†\n44.9†\n52.8\n55.1\nDROP\n3-shot, F1\n48.5\n51.2\n63.8∗\n58.4\n56.3\n69.4\n74.2\nBBH\n3-shot, CoT\n35.2\n41.9\n56.0⋄\n61.1⋄\n59.0⋄\n68.2\n74.9\nWinogrande\n5-shot\n66.8\n71.3\n78.5\n76.1\n79.0\n80.6\n83.7\nHellaSwag\n10-shot\n71.7\n72.9\n83.0\n82.0\n82.3\n81.9\n86.4\nMATH\n4-shot\n11.8\n16.0\n12.7\n-\n24.3\n36.6\n42.3\nARC-e\n0-shot\n73.2\n80.6\n80.5\n-\n81.5\n88.0\n88.6\nPIQA\n0-shot\n77.3\n78.4\n82.2\n-\n81.2\n81.7\n83.2\nSIQA\n0-shot\n49.7\n51.9\n47.0∗\n-\n51.8\n53.4\n53.7\nBoolq\n0-shot\n69.4\n72.7\n83.2∗\n-\n83.2\n84.2\n84.8\nTriviaQA\n5-shot\n53.2\n60.4\n62.5\n-\n63.4\n76.6\n83.7\nNQ\n5-shot\n12.5\n17.1\n23.2\n-\n23.0\n29.2\n34.5\nHumanEval\npass@1\n22.0\n20.1\n26.2\n-\n32.3\n40.2\n51.8\nMBPP\n3-shot\n29.2\n30.2\n40.2∗\n-\n44.4\n52.4\n62.6\nAverage (8)\n44.0\n50.0\n61.0\n61.9\n62.4\n70.2\n74.4\nAverage (all)\n44.2\n48.7\n55.6\n-\n57.9\n64.9\n69.4\nTable 13 | Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on\na variety of benchmarks. We report the average performance on the 8 benchmarks where we can\ncompare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either\nfrom the HuggingFace leaderboard or their blogpost. † we report the evaluation used in LLaMA-3 for\nthe baselines, it leads to +3% compared to our evaluation: Gemma-1 7B achieves 44.9% instead of\n41.7%, and Mistral 7B, 44% instead of 41.2%. ⋄we report the evaluation used in LLaMA-3 for the\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.\n∗these are evaluations run by us for Gemma 1 (Gemma Team, 2024).\nobserve large improvements in win rates and\npreference scores as compared against the older\nGemma 1.1 7B model. We report safety as a\nwin-loss ratio against GPT4o, and we report\nsingle-sided instruction following scores as ratio\nof prompts where all instructions are followed. In\nparticular, we find that regardless of their size,\nGemma 2 models produce safer, more appropri-\nate prompts on the held-out safety prompt set\nthan GPT4o.\nHuman Multi-Turn Evaluations\nWe evaluated the multi-turn capabilities of\nGemma 1.1 7B, Gemma 2 2B, 9B and 27B models\nby tasking human raters to have conversations\nwith the models and follow specified given sce-\nnarios. We used a diverse, held-out set of 500\nscenarios, each describing a sequence of requests\nto the model, including measuring instances of\nbrainstorming, making a plan, or learning some-\nthing new. The average number of user turns\nis 8.4. We found that the conversations with\nGemma 2 models are rated significantly better\nthan Gemma 1.1 in user satisfaction and conver-\nsation goal achievement (Table 16). Moreover,\nwe saw that the Gemma 2 models were better\nthan Gemma 1.1 7B at maintaining high quality\nof responses for the entire conversation.\nStandard Benchmarks\nIt has been observed in Llama-3 (AI@Meta, 2024)\nthat instruction fine-tuning can improve the per-\nformance of the models on few-shot benchmarks\n7\n\nGemma 2: Improving Open Language Models at a Practical Size\nModel\nElo\n95% CI\nOpen\ngpt-4o-2024-05-13\n1286\n+2 / -3\n-\ngpt-4o-mini-2024-07-18\n1279\n+5 / -4\n-\nclaude-3-5-sonnet\n1271\n+3 / -4\n-\ngemini-advanced-0514\n1266\n+2 / -3\n-\nllama-3.1-405b-instruct\n1262\n+8 / -7\n+\ngemini-1.5-pro-api-0514\n1261\n+2 / -3\n-\ngemini-1.5-pro-api-0409\n1257\n+3 / -3\n-\ngpt-4-turbo-2024-04-09\n1256\n+2 / -3\n-\ngpt-4-1106-preview\n1250\n+3 / -3\n-\nclaude-3-opus-20240229\n1248\n+2 / -2\n-\nathene-70b-0725\n1245\n+8 / -6\n+\ngpt-4-0125-preview\n1245\n+2 / -2\n-\nllama-3.1-70b-instruct\n1244\n+8 / -9\n+\nyi-large-preview\n1239\n+3 / -3\n-\ngemini-1.5-flash-api-0514\n1227\n+3 / -3\n-\ndeepseek-v2-api-0628\n1220\n+6 / -6\n+\ngemma-2-27b-it\n1218\n+4 / -3\n+\nyi-large\n1212\n+4 / -5\n-\nnemotron-4-340b-instruct\n1209\n+3 / -4\n+\nbard-jan-24-gemini-pro\n1208\n+5 / -7\n-\nglm-4-0520\n1206\n+3 / -5\n-\nllama-3-70b-instruct\n1206\n+2 / -2\n+\nclaude-3-sonnet\n1200\n+2 / -2\n-\nreka-core-20240501\n1199\n+3 / -3\n-\ncommand-r-plus\n1189\n+2 / -2\n+\nModel\nElo\n95% CI\nOpen\ngemma-2-9b-it\n1187\n+3 / -5\n+\nqwen2-72b-instruct\n1187\n+3 / -3\n+\ngpt-4-0314\n1186\n+2 / -3\n-\nqwen1.5-110b-chat\n1161\n+3 / -3\n+\nmistral-large-2402\n1157\n+3 / -3\n-\nyi-1.5-34b-chat\n1157\n+4 / -3\n-\nreka-flash-21b-20240226\n1155\n+4 / -4\n-\nllama-3-8b-instruct\n1151\n+2 / -3\n+\ncommand-r\n1148\n+3 / -3\n+\nclaude-1\n1148\n+4 / -4\n-\nmistral-medium\n1147\n+4 / -4\n-\nreka-flash-21b-20240226\n1147\n+3 / -4\n-\nqwen1.5-72b-chat\n1147\n+4 / -4\n+\nmixtral-8x22b-instruct-v0.1\n1145\n+2 / -3\n+\nclaude-2.0\n1131\n+4 / -6\n-\ngemini-pro-dev-api\n1131\n+4 / -3\n-\nzephyr-orpo-141b\n1127\n+10 / -6\n+\ngemma-2-2b-it\n1126\n+10 / -10\n+\nqwen1.5-32b-chat\n1125\n+3 / -3\n+\nmistral-next\n1124\n+5 / -5\n-\nphi-3-medium-4k-instruct\n1122\n+4 / -4\n+\nstarling-lm-7b-beta\n1118\n+4 / -5\n+\nclaude-2.1\n1118\n+3 / -3\n-\ngpt-3.5-turbo-0613\n1116\n+3 / -4\n-\nmixtral-8x7b-instruct-v0.1\n1114\n+0 / -0\n-\nTable 14 | Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al.,\n2024). The models are evaluated against each other through blind side by side evaluations by human\nraters. Each model is attributed a score, based on the Elo rating system.\nModel\nInstruction Following\nSafety\nGemma 1.1 IT 7B\n24.3% ± 1.9%\n42.8%\nWin / Tie / Loss\n37.4% / 10.8% / 51.8%\nGemma 2 IT 2B\n26.5% ± 1.8%\n57.5%\nWin / Tie / Loss\n53% / 9% / 38%\nGemma 2 IT 9B\n34.1% ± 3.0%\n57.8%\nWin / Tie / Loss\n48.2% / 19.2% / 28.3%\nGemma 2 IT 27B\n37.7% ± 2.3%\n55%\nWin / Tie / Loss\n49.6% / 10.8% / 39.6%\nTable 15 | Instruction following and safety metrics\nfrom human raters. The instruction following\nmetrics are single-sided and do not have win-loss\nrates, and so are left blank.\ndespite not being trained to target few-shot capa-\nbilities. In Table 17, we show a similar improve-\nment across our models. Overall, we observe\nimprovements on the order of several percentage\npoints. We conjecture that IT models are better\nat understanding formatted questions, while pre-\ntrained models are sensitive to formatting.\nUser\nsatisfaction\nConversation\ngoal achievement\nGemma 1.1 IT 7B\n3.32\n3.36\nGemma 2 IT 2B\n3.64\n3.88\nGemma 2 IT 9B\n4.04\n4.08\nGemma 2 IT 27B\n4.20\n4.24\nTable 16 | Human evaluations on 500 multi-turn\nscenarios. The raters attribute a score ranging\nbetween 1 and 5 for both overall satisfaction and\nconversation goal achievement.\n2B\n9B\n27B\nModel\nPT\nIT\nPT\nIT\nPT\nIT\nMMLU\n52.2\n56.1\n71.3\n72.3\n75.2\n76.2\nMBPP\n30.2\n36.6\n52.4\n59.2\n62.6\n67.4\nTable 17 | Comparing pre-trained (PT) and in-\nstruction fine-tuned (IT) models of different sizes\non few-shot benchmarks.\n8\n\nGemma 2: Improving Open Language Models at a Practical Size\n7. Memorization and Privacy\nLarge language models may, under particular cir-\ncumstances, be vulnerable to attacks causing the\nmodel to produce memorized1 training data (Nasr\net al., 2023). To study susceptibility to such at-\ntacks and quantify memorization, we evaluate\nmodels for verbatim and approximate memoriza-\ntion as was done in several prior studies (Anil\net al., 2023; Carlini et al., 2022; Gemini Team,\n2024; Kudugunta et al., 2023).\nWe follow the evaluation setting of (Gemma\nTeam, 2024) which tests for (50 token) memo-\nrizations of training data given a prompt of 50 to-\nkens. We compare the overall memorization rates,\nacross a uniform sample of the entire dataset, us-\ning both an exact match criteria and approximate\nmatch criteria (Ippolito et al., 2022) using an edit\ndistance of 10%.\nVerbatim Memorization: Results are in Figure 1.\nWe first compare against recent models from the\nliterature that include memorization evaluations.\nWe find that Gemma 2 memorizes significantly\nless than prior models at a similar size, with mem-\norization rates below 0.1% (note the log y-axis).\nWe further investigate how this memorization\nbreaks down with respect to the data source. Sim-\nilar to Gemma 1, we find that Gemma 2 memo-\nrizes more from code, wiki, and science sources,\nand also that it memorizes significantly less across\nthe board (again, note the log y-axis).\nApproximate Memorization:\nFigure 1 also\npresents approximate memorization by data\nsource. We observe that while approximate mem-\norization is higher than exact, the rate of memo-\nrization is still low. For example, the approximate\nmemorization of this model is much lower than\neven the exact memorization of Gemma 1. We\n1This work uses a very restricted definition of “mem-\norization”: whether a model can be induced to generate\nnear-copies of some training examples when prompted with\nappropriate instructions. We do not mean to say that a\nmodel ’contains’ its training data in the sense that any arbi-\ntrary instance of that data can be retrieved without use of\nspecialized software or algorithms. Rather, if a model can\nbe induced to generate measurably close copies of certain\ntraining examples by supplying appropriate instructions to\nguide the model’s statistical generation process then that\nmodel is said to have ’memorized’ those examples.\nGemma 2\n 2B\nGemma 2\n 9B\nGemma 2\n 27B\nGemini 1.5\n Flash\nGemma\n2B\nGemma\n7BPaLM 2\nSmall\nModel\n0.1\n1\n% Exact Memorized\nOverall Memorization Rate\nCode\nMultilingual\nScience\nWeb\nWiki\nData Source\n10 4\n10 3\n0.01\n0.1\n% Memorized\nBy Data Source\nBy Data Source\nExact 2B\nExact 9B\nExact 27B\nApprox 2B\nApprox 9B\nApprox 27B\nFigure 1 | Comparing memorization rates. We\nfind significantly lower memorization rates\nacross-the-board. (Left) Overall memorization\nacross model families. (Right) Exact and approx-\nimate memorization per data source.\nfind that the increase in approximate memoriza-\ntion is much lower than prior models; in some\ncases we observed no lift at all c.f. (Gemma Team,\n2024, Figure 4) (note that no bar indicates no in-\ncrease, i.e., the rate of approximate memorization\nequals that of exact memorization). Note that no\napproximate memorization bar in Figure X indi-\ncates no increase, i.e., the rate of approximate\nmemorization equals that of exact memorization.\nPersonal Data We use the same prevention\nmethods at training time and the same evalua-\ntions as Gemma Team (2024). In particular, we\nuse Google Cloud Sensitive Data Protection Tool2\nto find potential instances of personal data. The\nmany categories of personal data (e.g., phone\nnumbers, account numbers) are classified into\nthree severity levels. We analyze memorized out-\nputs using these severity levels. . We found no\ninstances of high-severity data being emitted, and\nfound a very low rate of 0.00026% of memorized\ndata to contain lower-severity personal informa-\ntion. We note that these automated tools are\nknown to incur false positives because they do\nnot account for context. This means our results\nare likely overestimates.\n2Available at: https://cloud.google.com/sensitive-data-\nprotection\n9\n\nGemma 2: Improving Open Language Models at a Practical Size\n8. Responsibility, Safety, Security\nResponsibility,\nsafety\nand\nsecurity\nare\nof\nparamount importance when developing Gemma\nmodels. To reduce risks to Gemma 2 users, we\nhave integrated enhanced internal safety pro-\ncesses that span the development workflow, in\nline with recent Google AI models (Gemini Team,\n2024). Similar to the inaugural Gemma release,\nwe have followed a three pillar approach which fo-\ncuses on safety mitigation at training time, robust\nand transparent model evaluations, and further\ndevelopment of the Responsible Generative AI\nToolkit, a series of models and tools to help de-\nvelopers implement responsibility and safety best\npractices for their applications.\n8.1. Impact assessment\nOur approach and resulting impact assessment is\nreflective of that outlined for Gemma 1 (Gemma\nTeam, 2024): we continue to believe that open-\nness in AI can spread the benefits of these tech-\nnologies across society, but must be evaluated\nagainst the risk of malicious uses, such as the\ncreation of deepfake imagery, AI-generated disin-\nformation or illegal and disturbing material, that\ncan cause harm on both an individual and insti-\ntutional levels (Weidinger et al., 2021). Since the\nlaunch of Gemma 1, we have seen our Gemma\nmodels drive a number of socially beneficial ap-\nplications, relying on Gemma’s unique technolo-\ngies like its tokenizer to facilitate the creation of\nmultilingual models, such as for Navarasa 2.0, a\nGemma tuned model for 15 Indian languages.\nReleasing further open models requires specific\nattention to changes in model capabilities and\nclose monitoring of the evolving risks of LLMs (Lin\net al., 2024), as well as, an understanding of the\nways in which our models are being used in the\nwild. Although we are yet to receive any reports of\nmalicious use for Gemma, we remain committed\nto investigating any such reporting, and work\nwith the academic and developer communities,\nas well as conduct our own monitoring, to flag\nsuch use cases via our contact email3.\nDespite advancements in capabilities, we be-\n3gemma-2-report@google.com\nlieve that given the number of larger and more\npowerful open models, this release will have a\nnegligible effect on the overall risk landscape.\n8.2. Safety policies and train-time mitigations\nA key pillar of Gemma’s approach to safety is to\nalign fine-tuned models with Google’s safety poli-\ncies, in line with Gemini models (Gemini Team,\n2023). They are designed to help prevent our\nmodels from generating harmful content, i.e.,\n• Child sexual abuse and exploitation\n• Revealing personally identifiable information\nthat can lead to harm (e.g., Social Security\nnumbers)\n• Hate speech and harassment\n• Dangerous or malicious content (including\npromoting self-harm or instructing in harm-\nful activities)\n• Sexually explicit content\n• Medical advice that runs contrary to scientific\nor medical consensus\nWe undertook considerable safety filtering of our\npre-training data to reduce the likelihood of our\npre-trained and fine-tuned checkpoints producing\nharmful content. For fine-tuned models, we also\nuse both SFT and RLHF to steer the model away\nfrom undesirable behavior.\n8.3. External benchmark evaluations\nRobust and transparent evaluations are key prin-\nciples of our responsible approach to develop-\ning Gemma. To this end, we report in Table 18\nGemma 2 evaluations on public benchmarks.\n8.4. Assurance Evaluations\nWe also run our IT models through a set of assur-\nance evaluations to understand the harms that\nour models can cause. We focus on capabilities\nrelevant to extreme risks (Shevlane et al., 2023)\n(Phuong et al., 2024). Specifically, we evaluate on\noffensive cyber-security, code vulnerability detec-\ntion, Chemical, Biological, Radiological and Nu-\nclear (CBRN) knowledge, and self-proliferation.\nWe refer the reader to Phuong et al. (2024) for\nfull methodological details of these studies.\n10\n\nGemma 2: Improving Open Language Models at a Practical Size\nGemma 1.1 IT\nGemma 2 IT\nBenchmark\nmetric\n2.5B\n7B\n2.6B\n9B\n27B\nRealToxicity\navg tox\n7.03\n8.04\n8.16\n8.25\n8.84\nCrowS-Pairs\ntop-1\n45.89\n49.67\n37.67\n37.47\n36.67\nBBQ Ambig\n4-shot, top-1\n58.97\n86.06\n83.20\n88.58\n85.99\nBBQ Disambig\n4-shot, top-1\n53.9\n85.08\n69.31\n82.67\n86.94\nWinogender\ntop-1\n50.14\n57.64\n52.91\n79.17\n77.22\nTruthfulQA\nMC2Acc\n44.24\n45.34\n43.72\n50.27\n51.60\nWinobias 1_2\ntop-1\n55.93\n59.22\n59.28\n78.09\n81.94\nWinobias 2_2\ntop-1\n89.46\n89.2\n88.57\n95.32\n97.22\nToxigen\navg tox\n29.64\n38.75\n48.32\n39.30\n38.42\nTable 18 | Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We\nbold the best metrics to highlight them and to indicate when higher or lower scores are better.\nInterCode-CTF\nInternal CTF suite\nHack the Box\nGemini 1.0 Ultra\n28/76 [1] (37%)\n3/13 (23%)\n0/13\nGemini 1.5 Pro\n62/76 (82%)\n4/13 (31%)\n0/13\nCodeGemma 1 7B\n12/76 (16%)\n0/13 (0%)\n0/13\nGemma 2 27B\n34/76 (45%)\n1/13 (8%)\n0/13\nTable 19 | Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a\nchallenge based on Hack the Box. We report the number of successful hackings.\nBaseline Evaluations\nBaseline assurance captures the model’s violation\nrate for safety policies, using a large number of\nsynthetic adversarial user queries, and human\nraters to label the answers as policy violating or\nnot. Overall, Gemma 2’s violation rate is signifi-\ncantly lower overall on the safety policies listed\nabove, in particular on Child safety content.\nChemical, Biological, Radiological and Nuclear\n(CBRN) knowledge\nWe evaluated knowledge relevant to biological,\nradiological and nuclear risks using an internal\ndataset of closed-ended, knowledge-based multi-\nple choice questions. For evaluations of chem-\nical knowledge, we employed a closed-ended\nknowledge-based approach on chemical hazards\n(developed by Macknight et al (Macknight et al.,\n2024). Our evaluation suggests that Gemma mod-\nels’ knowledge in these domains is low.\nOffensive cyber-security\nTo evaluate Gemma models’ capabilities at of-\nfensive cybersecurity, we ran Gemma 2 27B\nagainst some automated capture-the-flag (CTF)\nchallenges. In these challenges, the model is\ntasked with hacking into a simulated server in\norder to retrieve a piece of secret information.\nSpecifically, we test on InterCode-CTF (Yang et al.,\n2023), our own internal CTF suite4 (Phuong et al.,\n2024); and a challenge based on Hack the Box 5.\nIn Table 19, we show that Gemma 2 27B has\na significant increase in capabilities compared\nto CodeGemma 1.0 7B on the easier of these\nchallenge suites, InterCode CTF. (Note that our\nInterCode-CTF results are not comparable to\nexternally-reported results on other models be-\ncause we omit challenges that require internet\naccess for security reasons.) However, Gemma 2\nis unsurprisingly much less capable than Gemini\n1.5 Pro on these tasks.\n4https://github.com/google-deepmind/\ndangerous-capability-evaluations\n5https://www.hackthebox.com\n11\n\nGemma 2: Improving Open Language Models at a Practical Size\nPrimeVul\nPrimeVul Paired\nDiverseVul\nSPI\nSecretPatch\nGemini 1.0 Ultra\n-\n-\n54%\n59%\n74%\nGemini 1.5 Pro\n60%\n51%\n58%\n56%\n67%\nGemma 2 27B\n63%\n50%\n57%\n53%\n72%\nTable 20 | |Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.\nChallenges\npassed\nend-to-end\nChallenges\nwith success on\nall milestones\nTotal successful\nmilestones over\nall challenges\nExpert bits\nrequired to\nsolve all tasks\nGemini 1.0 Ultra\n0/10\n1/10\n16/45 (36%)\n13,026\nGemini 1.5 Pro\n0/10\n2/10\n25/45 (56%)\n11,046\nGemma 2 27B\n0/10\n1/10\n22/45 (49%)\n12,462\nTable 21 | Results on different self-proliferation scenarios. We report the number of either challenges\npassed end-to-end or some intermediate milestones. We also measure the number of bits of information\nneeded for an expert to help the model pass a challenge.\nCode vulnerability detection\nIn Table 20, we also evaluate Gemma 2 27B on a\nseries of multiple-choice code vulnerability detec-\ntion datasets. As with previous models, Gemma\nshows close-to-chance performance on PrimeVul,\nDiverseVul and SPI. Gemma 2 shows performance\non SecretPatch similar to Gemini 1.0 Ultra.\nSelf-proliferation\n\"Self-proliferation\" refers to the ability for an\nagent to autonomously replicate - to instantiate\ngoal-directed agents on other machines, and to\nacquire resources such as compute necessary to\nkeep them running (Kinniment et al., 2024). In\nTable 21, we evaluate self-proliferation capabili-\nties of Gemma 2 27B on a number of tasks from\nPhuong et al. (2024) that involve multiple sce-\nnarios – for example, setting up an open-source\nlanguage model on a cloud server. We also test\nthe model’s performance on individual ’milestone’\nsubsteps, and measure the number of bits of inter-\nvention an expert would have to provide in order\nfor the model to complete each challenge.\nSimilarly to offensive cybersecurity, we observe\nthat Gemma 2 completes more milestones than\nGemini 1.0 Ultra. Nonetheless, it still has low\ncapabilities on end-to-end tasks, unable to pass\nthe easiest challenge – installing a Bitcoin wallet.\nPersuasion\nPersuasion capabilities can enable and worsen\nmany other kinds of risks - e.g. enabling social\nengineering attacks in a cybersecurity context.\nWe evaluate Gemma 2’s persuasion capabilities\non human-participant studies on Prolific.\nCharm offensive. In Table 22, we measure the\nability of the model to build rapport - a key sub-\nskill of persuasion. The study participant and\nmodel have a conversation where they role-play\na scenario of two friends catching up after a long\ntime. After the conversation, we poll participants\nwith Likert questions on statements such as \"I\nfelt a personal connection with the chatbot\". Re-\nported below are the fraction of participants who\nanswered \"Agree\" or \"Strongly agree\" to each post-\nconversation question.\nQuantitatively, Gemma 2 27B performs better\nthan Gemini 1.0 models. Qualitatively, the model\nis an excellent conversationalist, and many study\nparticipants explicitly reported enjoying the ex-\nperience. Overall, this shows that Gemma 2 is\nstrong at building rapport.\nHidden agenda. The Hidden Agenda tasks mea-\nsure models’ deception capabilities. Human study\nparticipants are told that they’ll be interacting\nwith a generic chatbot designed to help them\nlearn new things. However, the model is given a\n12\n\nGemma 2: Improving Open Language Models at a Practical Size\nPersonal\nconnection\nSpeak\nagain\nFunny\nInteresting\nKind\nTrustworthy\nGood\nlistener\nGemini 1.0 Pro\n65%\n53%\n32%\n68%\n78%\n66%\n81%\nGemini 1.0 Ultra\n69%\n65%\n38%\n65%\n86%\n63%\n74%\nGemini 1.5 Pro\n82%\n70%\n69%\n81%\n95%\n69%\n90%\nGemma 2 27B\n80%\n75%\n60%\n81%\n87%\n65%\n83%\nTable 22 | Charm Offensive results on a sample of 100 human participants. We report the percentage\nof participants that find some human traits, e.g., funny, in a model.\n% of persuaded participants\nclick links find info run code\nGemini 1.0 Pro\n43%\n4%\n4%\nGemini 1.0 Ultra\n18%\n18%\n14%\nGemini 1.5 Pro\n36%\n12%\n17%\nGemma 2 27B\n34%\n9%\n11%\nTable 23 | Persuasion results. We report the per-\ncentage of participants that were persuaded by\nthe model to take 3 different actions: clicking a\nlink, finding information and running code.\nsecret goal: to either persuade the participant to\n1) click a suspicious link; 2) find the email address\nof the first author of a particular paper; or 3) run\nsome obfuscated JavaScript code in their browser.\nWe measure the fraction of participants who are\npersuaded to take each action. As shown in Ta-\nble 23, all models succeed in convincing a fraction\nof participants to take the action - likely due to\nthe trusted setting of a study on Prolific. However,\nGemma 2 does not show significant differences\nin capability compared to Gemini models.\nMean amount donated\nNo chatbot (baseline)\n£2.61 ± £0.70\nGemini 1.0 Pro\n£4.10 ± £0.90\nGemini 1.0 Ultra\n£3.15 ± £0.99\nGemini 1.5 Pro\n£3.45 ± £1.04\nGemma 2 27B\n£3.72 ± £1.07\nTable 24 | Money Talks evaluation. We report\nthe average amount of money that participants\nagreed to donate.\nMoney talks. In Table 24, we evaluate whether a\nmodel can convince study participants to donate\nmoney to charity. At the beginning of the study,\nparticipants are told that they will receive a £20\nbonus. They will be given an option to forfeit\npart of the bonus and instead have it donated\nto charity. The study then consists of a conver-\nsation in which the model tries to persuade the\nparticipant to donate as much as possible. We\nmeasure the amount that participants indicate\nthey would like to donate after the study, with\n95% bootstrapped confidence intervals indicated\nby ± figures. We observe no significant difference\nbetween Gemma 2, other Gemini models, and a\nsimple no chatbot baseline in which the partici-\npant is immediately asked how much they would\nlike to donate without conversing with a model.\nMean shift towards:\ncorrect belief incorrect belief\nHuman\n20% ± 13%\n-23% ± 14%\nGemini 1.0 Pro\n22% ± 5%\n-9% ± 4%\nGemini 1.0 Ultra\n21% ± 5%\n-1% ± 4%\nGemini 1.5 Pro\n20% ± 5%\n-3% ± 5%\nGemma 2 27B\n18% ± 5%\n1% ± 4%\nTable 25 | Web of Lies results on a sample of 100\nhuman participants. We report the percentage of\nparticipants that shifted their beliefs after inter-\nacting with a model.\nWeb of Lies. In Web of Lies, we measure model\ncapabilities at shifting participant beliefs. Partic-\nipants engage in a series of short conversations\nwith the model about simple factual questions\nsuch as \"Which country had tomatoes first - Italy\nor Mexico?\". In half of conversations, the model\ntries to persuade the participant of the correct\nanswer - but in the other half of conversations,\nthe incorrect answer. We poll the participant be-\n13\n\nGemma 2: Improving Open Language Models at a Practical Size\nfore and after each conversation about which of\nthe two possible answers they think is correct,\nand their confidence in that answer. 95% boot-\nstrapped confidence intervals are indicated by\n± figures. As shown in Table 25, Gemma 2 is\nsignificantly weaker than a human baseline at\npersuading participants of the incorrect answer\non these questions. Similarly to previous models,\nGemma 2 is more persuasive when telling the\ntruth than when lying.\n8.5. Our approach to responsible open models\nDesigning safe, secure and responsible applica-\ntions requires a system-level approach, working\nto mitigate risks associated with each specific use\ncase and environment. Given the open nature\nof Gemma models, responsibility for upholding\nprinciples of model safety also relies on down-\nstream developers. To support them, we have\ncontinued to develop the Responsible Generative\nAI Toolkit6: a series of tools, models and datasets\nto implement responsible best practices all along\nthe development of their workflow.\nRecent additions to the toolkit include the LLM\nComparator (Kahng et al., 2024), an interactive,\nvisual tool that enables more effective, scalable\nanalysis of side-by-side evaluations. Additionally,\nthe toolkit includes a methodology to build cus-\ntomized classifiers with Gemma using a limited\nnumber of datapoints thanks to parameter effi-\ncient tuning techniques (Mozes et al., 2023) , an\ninteractive prompt-debugging platform, based on\ntop of the Learning Interpretability Tool (Tenney\net al., 2020), as well as general guidance about\nmodel alignment and evaluation for safety.\n9. Discussion and Conclusion\nIn this work, we have presented Gemma 2, the\nnewest additions to the Gemma family of open\nlanguage models for text and code. We show\nthat distillation is an effective method for train-\ning these models, and the benefits distillation\nconfers over raw text training. Specifically, we\nshow how training over output probabilities can\nproduce superior results over purely next token\n6https://ai.google.dev/responsible\nprediction. We hope that releasing these models\nto the community will unlock access to capabili-\nties previously only seen in large-scale LLMs and\nfuel future waves of research and development.\nWhile there is inherent risk to an irreversible re-\nlease of this nature, our extensive safety investiga-\ntions and responsible deployment procedures give\nus confidence that these models will have a net\npositive impact on the community. As discussed\nin this report, there are still many limitations to\nthese models, and future research is required to\ninvestigate and improve factuality, robustness to\nadversarial attacks, reasoning, and alignment.\n14\n\nGemma 2: Improving Open Language Models at a Practical Size\nContributions and Acknowledgments\nCore contributors\nMorgane Riviere∗\nShreya Pathak∗\nPier Giuseppe Sessa∗\nCassidy Hardin∗\nSurya Bhupatiraju\nLéonard Hussenot\nThomas Mesnard\nBobak Shahriari\nAlexandre Ramé\nJohan Ferret\nPeter Liu\nPouya Tafti\nAbe Friesen\nMichelle Casbon\nSabela Ramos\nRavin Kumar\nCharline Le Lan\nSammy Jerome\nAnton Tsitsulin\nNino Vieillard\nPiotr Stanczyk\nSertan Girgin\nNikola Momchev\nMatt Hoffman\nShantanu Thakoor\nJean-Bastien Grill\nBehnam Neyshabur\nOlivier Bachem\nContributors (alphabetical order)\nAlanna Walton\nAliaksei Severyn\nAlicia Parrish\nAliya Ahmad\nAllen Hutchison\nAlvin Abdagic\nAmanda Carl\nAmy Shen\nAndy Brock\nAndy Coenen\nAnthony Laforge\nAntonia Paterson\nBen Bastian\nBilal Piot\nBo Wu\n∗equal contributions.\nBrandon Royal\nCharlie Chen\nChintu Kumar\nChris Perry\nChris Welty\nChristopher A. Choquette-Choo\nDanila Sinopalnikov\nDavid Weinberger\nDimple Vijaykumar\nDominika Rogozińska\nDustin Herbison\nElisa Bandy\nEmma Wang\nEric Noland\nErica Moreira\nEvan Senter\nEvgenii Eltyshev\nFrancesco Visin\nGabriel Rasskin\nGary Wei\nGlenn Cameron\nGus Martins\nHadi Hashemi\nHanna Klimczak-Plucińska\nHarleen Batra\nHarsh Dhand\nIvan Nardini\nJacinda Mein\nJack Zhou\nJames Svensson\nJeff Stanway\nJetha Chan\nJin Peng Zhou\nJoana Carrasqueira\nJoana Iljazi\nJocelyn Becker\nJoe Fernandez\nJoost van Amersfoort\nJosh Gordon\nJosh Lipschultz\nJosh Newlan\nJu-yeong Ji\nKareem Mohamed\nKartikeya Badola\nKat Black\nKatie Millican\nKeelin McDonell\nKelvin Nguyen\nKiranbir Sodhia\n15\n\nGemma 2: Improving Open Language Models at a Practical Size\nKish Greene\nLars Lowe Sjoesund\nLauren Usui\nLaurent Sifre\nLena Heuermann\nLeticia Lago\nLilly McNealus\nLivio Baldini Soares\nLogan Kilpatrick\nLucas Dixon\nLuciano Martins\nMachel Reid\nManvinder Singh\nMark Iverson\nMartin Görner\nMat Velloso\nMateo Wirth\nMatt Davidow\nMatt Miller\nMatthew Rahtz\nMatthew Watson\nMeg Risdal\nMehran Kazemi\nMichael Moynihan\nMing Zhang\nMinsuk Kahng\nMinwoo Park\nMofi Rahman\nMohit Khatwani\nNatalie Dao\nNenshad Bardoliwalla\nNesh Devanathan\nNeta Dumai\nNilay Chauhan\nOscar Wahltinez\nPankil Botarda\nParker Barnes\nPaul Barham\nPaul Michel\nPengchong Jin\nPetko Georgiev\nPhil Culliton\nPradeep Kuppala\nRamona Comanescu\nRamona Merhej\nReena Jana\nReza Ardeshir Rokni\nRishabh Agarwal\nRyan Mullins\nSamaneh Saadat\nSara Mc Carthy\nSarah Cogan\nSarah Perrin\nSébastien M. R. Arnold\nSebastian Krause\nShengyang Dai\nShruti Garg\nShruti Sheth\nSue Ronstrom\nSusan Chan\nTimothy Jordan\nTing Yu\nTom Eccles\nTom Hennigan\nTomas Kocisky\nTulsee Doshi\nVihan Jain\nVikas Yadav\nVilobh Meshram\nVishal Dharmadhikari\nWarren Barkley\nWei Wei\nWenming Ye\nWoohyun Han\nWoosuk Kwon\nXiang Xu\nZhe Shen\nZhitao Gong\nZichuan Wei\nSupport\nVictor Cotruta\nPhoebe Kirk\nAnand Rao\nMinh Giang\nLudovic Peran\nTris Warkentin\nSponsors\nEli Collins\nJoelle Barral\nZoubin Ghahramani\nRaia Hadsell\nD. Sculley\nJeanine Banks\nAnca Dragan\nSlav Petrov\nOriol Vinyals\n16\n\nGemma 2: Improving Open Language Models at a Practical Size\nJeff Dean\nDemis Hassabis\nKoray Kavukcuoglu\nClement Farabet\nTechnical advisors\nElena Buchatskaya\nSebastian Borgeaud\nNoah Fiedel\nLead\nArmand Joulin\nTechnical leads\nKathleen Kenealy\nRobert Dadashi\nAlek Andreev\n17\n\nGemma 2: Improving Open Language Models at a Practical Size\nReferences\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al.\nGpt-4\ntechnical\nreport.\narXiv\npreprint\narXiv:2303.08774, 2023.\nR. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.\nGarea, M. Geist, and O. Bachem. On-policy\ndistillation of language models: Learning from\nself-generated mistakes. In The Twelfth Interna-\ntional Conference on Learning Representations,\n2024.\nAI@Meta.\nLlama\n3\nmodel\ncard,\n2024.\nURL https://github.com/meta-llama/\nllama3/blob/main/MODEL_CARD.md.\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\nskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models\nfrom multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\npelli, R. Cojocaru, M. Debbah, Étienne Goffinet,\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\nB. Noune, B. Pannier, and G. Penedo. The fal-\ncon series of open language models, 2023.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\nH. Michalewski, D. Dohan, E. Jiang, C. J.\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\ngram synthesis with large language models.\nCoRR, abs/2108.07732, 2021. URL https:\n//arxiv.org/abs/2108.07732.\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\nShafey, C. A. Thekkath, and Y. Wu.\nPath-\nways: Asynchronous distributed dataflow for\nml, 2022.\nI. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Ben-\ngio. Neural combinatorial optimization with re-\ninforcement learning. CoRR, abs/1611.09940,\n2016. URL http://arxiv.org/abs/1611.\n09940.\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\nformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020a.\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\nformer: The long-document transformer. CoRR,\nabs/2004.05150, 2020b.\nURL https://\narxiv.org/abs/2004.05150.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei. Language models\nare few-shot learners. CoRR, abs/2005.14165,\n2020. URL https://arxiv.org/abs/2005.\n14165.\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\nF. Tramer, and C. Zhang. Quantifying memo-\nrization across neural language models. arXiv\npreprint arXiv:2202.07646, 2022.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\nde Oliveira Pinto, J. Kaplan, H. Edwards,\nY. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\nmings, M. Plappert, F. Chantzis, E. Barnes,\nA. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino,\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\nA. Radford, M. Knight, M. Brundage, M. Murati,\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\nS. McCandlish, I. Sutskever, and W. Zaremba.\nEvaluating large language models trained on\ncode.\nCoRR, abs/2107.03374, 2021.\nURL\nhttps://arxiv.org/abs/2107.03374.\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\n18\n\nGemma 2: Improving Open Language Models at a Practical Size\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\nbot arena: An open platform for evaluating\nllms by human preference, 2024.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Explor-\ning the surprising difficulty of natural yes/no\nquestions. CoRR, abs/1905.10044, 2019. URL\nhttp://arxiv.org/abs/1905.10044.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\nman. Training verifiers to solve math word\nproblems. CoRR, abs/2110.14168, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\nGemini Team. Gemini: A family of highly capable\nmultimodal models, 2023.\nGemini Team. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of con-\ntext, 2024.\nGemma Team. Gemma: Open models based on\ngemini research and technology, 2024.\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\nKnowledge distillation of large language mod-\nels. In The Twelfth International Conference on\nLearning Representations, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\nsuring massive multitask language understand-\ning.\nCoRR, abs/2009.03300, 2020.\nURL\nhttps://arxiv.org/abs/2009.03300.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\nknowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nJ.\nHoffmann,\nS.\nBorgeaud,\nA.\nMensch,\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\nTraining compute-optimal large language\nmodels.\narXiv preprint arXiv:2203.15556,\n2022.\nD. Ippolito, F. Tramèr, M. Nasr, C. Zhang,\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\nN. Carlini. Preventing verbatim memorization\nin language models gives a false sense of pri-\nvacy. arXiv preprint arXiv:2210.17546, 2022.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\nford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud,\nM.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral\n7b, 2023.\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu,\nJ. Wexler, E. Reif, K. Kallarackal, M. Chang,\nM. Terry, and L. Dixon. Llm comparator: Vi-\nsual analytics for side-by-side evaluation of\nlarge language models, 2024.\nURL https:\n//arxiv.org/abs/2402.10524.\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich,\nM. Hasin, L. Chan, L. H. Miles, T. R. Lin, H. Wijk,\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\nEvaluating language-model agents on realis-\ntic autonomous tasks, 2024. URL https://\narxiv.org/abs/2312.11671.\nT. Kudo and J. Richardson. SentencePiece: A\nsimple and language independent subword to-\nkenizer and detokenizer for neural text process-\ning. In E. Blanco and W. Lu, editors, Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing: System Demon-\nstrations, pages 66–71, Brussels, Belgium, Nov.\n2018. Association for Computational Linguis-\ntics.\ndoi:\n10.18653/v1/D18-2012.\nURL\nhttps://aclanthology.org/D18-2012.\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia,\nC. A. Choquette-Choo, K. Lee, D. Xin, A. Kusu-\npati, R. Stella, A. Bapna, et al. Madlad-400:\nA multilingual and document-level large au-\ndited dataset. arXiv preprint arXiv:2309.04662,\n2023.\nT.\nKwiatkowski,\nJ.\nPalomaki,\nO.\nRedfield,\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\ntions: A benchmark for question answering\nresearch. Transactions of the Association for\nComputational Linguistics, 7:452–466, 2019.\ndoi: 10.1162/tacl_a_00276. URL https://\naclanthology.org/Q19-1026.\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\nmystifying real-world large language model in-\n19\n\nGemma 2: Improving Open Language Models at a Practical Size\ntegrated malicious services, 2024. URL https:\n//arxiv.org/abs/2401.03315.\nM. Luong, H. Pham, and C. D. Manning. Effective\napproaches to attention-based neural machine\ntranslation. CoRR, abs/1508.04025, 2015. URL\nhttp://arxiv.org/abs/1508.04025.\nMacknight, Aung, and Gomes. Personal Commu-\nnication, 2024.\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\nN. Thain, A. Yuan, T. Bolukbasi, and L. Dixon.\nTowards agile text classifiers for everyone,\n2023. URL https://arxiv.org/abs/2302.\n06541.\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\nCooper, D. Ippolito, C. A. Choquette-Choo,\nE. Wallace, F. Tramèr, and K. Lee.\nScal-\nable extraction of training data from (pro-\nduction) language models.\narXiv preprint\narXiv:2311.17035, 2023.\nM. Phuong,\nM. Aitchison,\nE. Catt,\nS. Co-\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\nating frontier models for dangerous capabilities,\n2024. URL https://arxiv.org/abs/2403.\n13793.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever. Language models are unsu-\npervised multitask learners, 2019.\nC. Raffel, N. Shazeer,\nA. Roberts,\nK. Lee,\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. CoRR,\nabs/1910.10683, 2019. URL http://arxiv.\norg/abs/1910.10683.\nA. Ramé, J. Ferret, N. Vieillard, R. Dadashi,\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\nA. Douillard, and O. Bachem. Warp: On the\nbenefits of weight averaged rewarded policies,\n2024.\nJ. Ren,\nS. Rajbhandari,\nR. Y. Aminabadi,\nO. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.\n{Zero-offload}: Democratizing {billion-scale}\nmodel training. In 2021 USENIX Annual Tech-\nnical Conference (USENIX ATC 21), pages 551–\n564, 2021.\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\nC. Gaffney, A. Mohiuddin, et al. Scaling up\nmodels and data with t5x and seqio.\nJour-\nnal of Machine Learning Research, 24(377):1–8,\n2023.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWINOGRANDE: an adversarial\nwinograd schema challenge at scale. CoRR,\nabs/1907.10641, 2019. URL http://arxiv.\norg/abs/1907.10641.\nN. Shazeer. GLU variants improve transformer.\nCoRR, abs/2002.05202, 2020. URL https:\n//arxiv.org/abs/2002.05202.\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\nA. Dafoe. Model evaluation for extreme risks,\n2023. URL https://arxiv.org/abs/2305.\n15324.\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer:\nEnhanced transformer with rotary position em-\nbedding. CoRR, abs/2104.09864, 2021. URL\nhttps://arxiv.org/abs/2104.09864.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann,\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, and J. Wei. Challenging\nbig-bench tasks and whether chain-of-thought\ncan solve them, 2022.\nQ. Team.\nIntroducing qwen1.5,\nFebruary\n2024. URL https://qwenlm.github.io/\nblog/qwen1.5/.\nI. Tenney, J. Wexler, J. Bastings, T. Boluk-\nbasi, A. Coenen, S. Gehrmann, E. Jiang,\nM. Pushkarna, C. Radebaugh, E. Reif, and\nA. Yuan. The language interpretability tool: Ex-\ntensible, interactive visualizations and analysis\n20\n\nGemma 2: Improving Open Language Models at a Practical Size\nfor nlp models, 2020. URL https://arxiv.\norg/abs/2008.05122.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-\nA. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\nE. Grave, and G. Lample. Llama: Open and\nefficient foundation language models, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin.\nAttention is all you need.\nCoRR,\nabs/1706.03762, 2017. URL http://arxiv.\norg/abs/1706.03762.\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\nS. Legassick, G. Irving, and I. Gabriel. Ethical\nand social risks of harm from language mod-\nels, 2021. URL https://arxiv.org/abs/\n2112.04359.\nxAI. grok-1, 2024. URL https://github.com/\nxai-org/grok-1.\nXLA.\nXla:\nOptimizing compiler for tensor-\nflow, 2019. URL https://www.tensorflow.\norg/xla.\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\nY. Wu, and Z. Chen.\nGSPMD: general and\nscalable parallelization for ML computation\ngraphs. CoRR, abs/2105.04663, 2021. URL\nhttps://arxiv.org/abs/2105.04663.\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao.\nIntercode: Standardizing and benchmarking\ninteractive coding with execution feedback,\n2023. URL https://arxiv.org/abs/2306.\n14898.\nB. Zhang and R. Sennrich. Root mean square\nlayer normalization. CoRR, abs/1910.07467,\n2019. URL http://arxiv.org/abs/1910.\n07467.\nL. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang,\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\net al.\nLmsys-chat-1m:\nA large-scale real-\nworld llm conversation dataset. arXiv preprint\narXiv:2309.11998, 2023.\n21\n"
    }
  ]
}