{
    "repo": "google/gemma.cpp",
    "branch": "main",
    "files": [
        ".bazelrc",
        ".bazelversion",
        ".clang-format",
        ".clang-tidy",
        ".gitattributes",
        ".github/workflows/build.yml",
        ".gitignore",
        ".vscode/c_cpp_properties.json",
        ".vscode/settings.json",
        "BUILD.bazel",
        "CMakeLists.txt",
        "CMakePresets.json",
        "DEVELOPERS.md",
        "LICENSE",
        "LICENSE-BSD3",
        "MODULE.bazel",
        "README.md",
        "WORKSPACE",
        "bazel/BUILD.bazel",
        "bazel/sentencepiece.bazel",
        "bazel/sentencepiece.patch",
        "cmake.sh",
        "compression/BUILD.bazel",
        "compression/analyze.h",
        "compression/compress-inl.h",
        "compression/compress.cc",
        "compression/compress.h",
        "compression/compress_test.cc",
        "compression/distortion.h",
        "compression/distortion_test.cc",
        "compression/nuq-inl.h",
        "compression/nuq_test.cc",
        "compression/python/BUILD.bazel",
        "compression/python/compression_clif_aux.cc",
        "compression/python/compression_clif_aux.h",
        "compression/python/compression_extension.cc",
        "compression/python/compression_test.py",
        "compression/python/pytree/PYTREE_README.md",
        "compression/python/pytree/build_model_file_for_cpp_binary.py",
        "compression/python/pytree/cpp_load_log.txt",
        "compression/python/pytree/ml_model_transforms.py",
        "compression/python/pytree/ml_model_transforms_test.py",
        "compression/python/pytree/pytree_transforms.py",
        "compression/python/pytree/pytree_transforms_test.py",
        "compression/python/pytree/requirements.txt",
        "compression/python/requirements.txt",
        "compression/sfp-inl.h",
        "compression/sfp_test.cc",
        "compression/test_util-inl.h",
        "compression/types.h",
        "docs/CONTRIBUTING.md",
        "evals/benchmark.cc",
        "evals/benchmark_helper.cc",
        "evals/benchmark_helper.h",
        "evals/benchmarks.cc",
        "evals/cross_entropy.cc",
        "evals/cross_entropy.h",
        "evals/debug_prompt.cc",
        "evals/gemma_batch_bench.cc",
        "evals/gemma_test.cc",
        "evals/prompts.h",
        "evals/run_mmlu.cc",
        "examples/README.md",
        "examples/hello_world/BUILD.bazel",
        "examples/hello_world/CMakeLists.txt",
        "examples/hello_world/README.md",
        "examples/hello_world/build/.gitignore",
        "examples/hello_world/run.cc",
        "examples/simplified_gemma/BUILD.bazel",
        "examples/simplified_gemma/CMakeLists.txt",
        "examples/simplified_gemma/README.md",
        "examples/simplified_gemma/build/.gitignore",
        "examples/simplified_gemma/gemma.hpp",
        "examples/simplified_gemma/run.cc",
        "experimental/.gitkeep",
        "experimental/README.md",
        "gemma/activations.h",
        "gemma/attention.cc",
        "gemma/attention.h",
        "gemma/bindings/GemmaInterop.cs",
        "gemma/bindings/c_api.cc",
        "gemma/bindings/c_api.h",
        "gemma/bindings/context.cc",
        "gemma/bindings/context.h",
        "gemma/configs.cc",
        "gemma/configs.h",
        "gemma/configs_test.cc",
        "gemma/evals/mmlu.json",
        "gemma/gemma-inl.h",
        "gemma/gemma.cc",
        "gemma/gemma.h",
        "gemma/gemma_args.h",
        "gemma/griffin.cc",
        "gemma/griffin.h",
        "gemma/kv_cache.cc",
        "gemma/kv_cache.h",
        "gemma/model_store.cc",
        "gemma/model_store.h",
        "gemma/run.cc",
        "gemma/tensor_info.cc",
        "gemma/tensor_info.h",
        "gemma/tensor_info_test.cc",
        "gemma/tokenizer.cc",
        "gemma/tokenizer.h",
        "gemma/vit.cc",
        "gemma/vit.h",
        "gemma/weights.cc",
        "gemma/weights.h",
        "goldens/2b-it.txt",
        "goldens/7b-it.txt",
        "io/BUILD.bazel",
        "io/blob_compare.cc",
        "io/blob_store.cc",
        "io/blob_store.h",
        "io/blob_store_test.cc",
        "io/fields.cc",
        "io/fields.h",
        "io/fields_test.cc",
        "io/io.cc",
        "io/io.h",
        "io/io_win.cc",
        "io/migrate_weights.cc",
        "ops/bench_matmul.cc",
        "ops/dot-inl.h",
        "ops/dot_test.cc",
        "ops/fp_arith-inl.h",
        "ops/gemma_matvec_test.cc",
        "ops/matmul-inl.h",
        "ops/matmul.cc",
        "ops/matmul.h",
        "ops/matmul_static-inl.h",
        "ops/matmul_static.h",
        "ops/matmul_static_bf16.cc",
        "ops/matmul_static_f32.cc",
        "ops/matmul_static_nuq.cc",
        "ops/matmul_static_sfp.cc",
        "ops/matmul_test.cc",
        "ops/matvec-inl.h",
        "ops/ops-inl.h",
        "ops/ops.h",
        "ops/ops_test.cc",
        "ops/sum-inl.h",
        "paligemma/BUILD.bazel",
        "paligemma/image.cc",
        "paligemma/image.h",
        "paligemma/image_test.cc",
        "paligemma/paligemma_helper.cc",
        "paligemma/paligemma_helper.h",
        "paligemma/paligemma_test.cc",
        "paligemma/testdata/image.ppm",
        "python/BUILD.bazel",
        "python/configs.cc",
        "python/convert_from_safetensors.py",
        "python/gemma_py.cc",
        "python/requirements.txt",
        "python/run_example.py",
        "testdata/frankenstein.txt",
        "testdata/frankenstein_13.txt",
        "testdata/frankenstein_chap1.txt",
        "testdata/frankenstein_chap4.txt",
        "util/allocator.cc",
        "util/allocator.h",
        "util/args.h",
        "util/basics.h",
        "util/mat.cc",
        "util/mat.h",
        "util/test_util.h",
        "util/threading.cc",
        "util/threading.h",
        "util/threading_context.cc",
        "util/threading_context.h",
        "util/threading_test.cc",
        "util/topology.cc",
        "util/topology.h"
    ],
    "license_files": {
        "LICENSE": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.",
        "LICENSE-BSD3": "Copyright (c) The gemma.cpp Project Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n1.  Redistributions of source code must retain the above copyright notice, this\n    list of conditions and the following disclaimer.\n\n2.  Redistributions in binary form must reproduce the above copyright notice,\n    this list of conditions and the following disclaimer in the documentation\n    and/or other materials provided with the distribution.\n\n3.  Neither the name of the copyright holder nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
    },
    "readme": "# gemma.cpp\n\ngemma.cpp is a lightweight, standalone C++ inference engine for the Gemma\nfoundation models from Google.\n\nFor additional information about Gemma, see\n[ai.google.dev/gemma](https://ai.google.dev/gemma). Model weights, including\ngemma.cpp specific artifacts, are\n[available on kaggle](https://www.kaggle.com/models/google/gemma-2).\n\n## Who is this project for?\n\nModern LLM inference engines are sophisticated systems, often with bespoke\ncapabilities extending beyond traditional neural network runtimes. With this\ncomes opportunities for research and innovation through co-design of high level\nalgorithms and low-level computation. However, there is a gap between\ndeployment-oriented C++ inference runtimes, which are not designed for\nexperimentation, and Python-centric ML research frameworks, which abstract away\nlow-level computation through compilation.\n\ngemma.cpp provides a minimalist implementation of Gemma-2, Gemma-3, and\nPaliGemma-2 models, focusing on simplicity and directness rather than full\ngenerality. This is inspired by vertically-integrated model implementations such\nas [ggml](https://github.com/ggerganov/ggml),\n[llama.c](https://github.com/karpathy/llama2.c), and\n[llama.rs](https://github.com/srush/llama2.rs).\n\ngemma.cpp targets experimentation and research use cases. It is intended to be\nstraightforward to embed in other projects with minimal dependencies and also\neasily modifiable with a small ~2K LoC core implementation (along with ~4K LoC\nof supporting utilities). We use the [Google\nHighway](https://github.com/google/highway) Library to take advantage of\nportable SIMD for CPU inference.\n\nFor production-oriented edge deployments we recommend standard deployment\npathways using Python frameworks like JAX, Keras, PyTorch, and Transformers\n([all model variations here](https://www.kaggle.com/models/google/gemma)).\n\n## Contributing\n\nCommunity contributions large and small are welcome. See\n[DEVELOPERS.md](https://github.com/google/gemma.cpp/blob/main/DEVELOPERS.md)\nfor additional notes contributing developers and [join the discord by following\nthis invite link](https://discord.gg/H5jCBAWxAe). This project follows\n[Google's Open Source Community\nGuidelines](https://opensource.google.com/conduct/).\n\n> [!NOTE] Active development is currently done on the `dev` branch. Please open\n> pull requests targeting `dev` branch instead of `main`, which is intended to\n> be more stable.\n\n## What's inside?\n\n-   LLM\n\n    -   CPU-only inference for: Gemma 2-3, Griffin(SSM), PaliGemma 2.\n    -   Sampling with TopK and temperature.\n    -   Backward pass (VJP) and Adam optimizer for Gemma research.\n\n-   Optimizations\n\n    -   Mixed-precision (fp8, bf16, fp32, fp64 bit) GEMM:\n        -   Designed for BF16 instructions, can efficiently emulate them.\n        -   Automatic runtime autotuning 7 parameters per matrix shape.\n    -   Weight compression integrated directly into GEMM:\n        -   Custom fp8 format with 2..3 mantissa bits; tensor scaling.\n        -   Also bf16, f32 and non-uniform 4-bit (NUQ); easy to add new formats.\n\n-   Infrastructure\n\n    -   SIMD: single implementation via Highway. Chooses ISA at runtime.\n    -   Tensor parallelism: CCX-aware, multi-socket thread pool.\n    -   Disk I/O: memory map or parallel read (heuristic with user override).\n    -   Custom format with forward/backward-compatible metadata serialization.\n    -   Model conversion from Safetensors, not yet open sourced.\n    -   Portability: Linux, Windows/OS X supported. CMake/Bazel. 'Any' CPU.\n\n-   Frontends\n\n    -   C++ APIs with streaming for single query and batched inference.\n    -   Basic interactive command-line app.\n    -   Basic Python bindings (pybind11).\n\n## Quick Start\n\n### System requirements\n\nBefore starting, you should have installed:\n\n- [CMake](https://cmake.org/)\n- [Clang C++ compiler](https://clang.llvm.org/get_started.html), supporting at\n  least C++17.\n- `tar` for extracting archives from Kaggle.\n\nBuilding natively on Windows requires the Visual Studio 2012 Build Tools with the\noptional Clang/LLVM C++ frontend (`clang-cl`). This can be installed from the\ncommand line with\n[`winget`](https://learn.microsoft.com/en-us/windows/package-manager/winget/):\n\n```sh\nwinget install --id Kitware.CMake\nwinget install --id Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;installRecommended --add Microsoft.VisualStudio.Component.VC.Llvm.Clang --add Microsoft.VisualStudio.Component.VC.Llvm.ClangToolset\"\n```\n\n### Step 1: Obtain model weights and tokenizer from Kaggle or Hugging Face Hub\n\nVisit the\n[Kaggle page for Gemma-2](https://www.kaggle.com/models/google/gemma-2/gemmaCpp)\nand select `Model Variations |> Gemma C++`.\n\nOn this tab, the `Variation` dropdown includes the options below. Note bfloat16\nweights are higher fidelity, while 8-bit switched floating point weights enable\nfaster inference. In general, we recommend starting with the `-sfp` checkpoints.\n\n> [!NOTE] **Important**: We strongly recommend starting off with the\n> `gemma2-2b-it-sfp` model to get up and running.\n\nGemma 2 models are named `gemma2-2b-it` for 2B and `9b-it` or `27b-it`. See the\n`ModelPrefix` function in `configs.cc`.\n\n### Step 2: Extract Files\n\nAfter filling out the consent form, the download should proceed to retrieve a\ntar archive file `archive.tar.gz`. Extract files from `archive.tar.gz` (this can\ntake a few minutes):\n\n```\ntar -xf archive.tar.gz\n```\n\nThis should produce a file containing model weights such as `2b-it-sfp.sbs` and\na tokenizer file (`tokenizer.spm`). You may want to move these files to a\nconvenient directory location (e.g. the `build/` directory in this repo).\n\n### Step 3: Build\n\nThe build system uses [CMake](https://cmake.org/). To build the gemma inference\nruntime, create a build directory and generate the build files using `cmake`\nfrom the top-level project directory. Note if you previous ran `cmake` and are\nre-running with a different setting, be sure to delete all files in the `build/`\ndirectory with `rm -rf build/*`.\n\n#### Unix-like Platforms\n```sh\ncmake -B build\n```\n\nAfter running `cmake`, you can enter the `build/` directory and run `make` to\nbuild the `./gemma` executable:\n\n```sh\n# Configure `build` directory\ncmake --preset make\n\n# Build project using make\ncmake --build --preset make -j [number of parallel threads to use]\n```\n\nReplace `[number of parallel threads to use]` with a number - the number of\ncores available on your system is a reasonable heuristic. For example, `make -j4\ngemma` will build using 4 threads. If the `nproc` command is available, you can\nuse `make -j$(nproc) gemma` as a reasonable default for the number of threads.\n\nIf you aren't sure of the right value for the `-j` flag, you can simply run\n`make gemma` instead and it should still build the `./gemma` executable.\n\n> [!NOTE]\n> On Windows Subsystem for Linux (WSL) users should set the number of\n> parallel threads to 1. Using a larger number may result in errors.\n\nIf the build is successful, you should now have a `gemma` executable in the\n`build/` directory.\n\n#### Windows\n\n```sh\n# Configure `build` directory\ncmake --preset windows\n\n# Build project using Visual Studio Build Tools\ncmake --build --preset windows -j [number of parallel threads to use]\n```\n\nIf the build is successful, you should now have a `gemma.exe` executable in the\n`build/` directory.\n\n#### Bazel\n\n```sh\nbazel build -c opt --cxxopt=-std=c++20 :gemma\n```\n\nIf the build is successful, you should now have a `gemma` executable in the\n`bazel-bin/` directory.\n\n#### Make\n\nIf you prefer Makefiles, @jart has made one available here:\n\nhttps://github.com/jart/gemma3/blob/main/Makefile\n\n### Step 4: Run\n\nYou can now run `gemma` from inside the `build/` directory.\n\n`gemma` has the following required arguments:\n\nArgument      | Description                  | Example value\n------------- | ---------------------------- | ---------------\n`--weights`   | The compressed weights file. | `2b-it-sfp.sbs`\n`--tokenizer` | The tokenizer file.          | `tokenizer.spm`\n\nExample invocation for the following configuration:\n\n-   weights file `gemma2-2b-it-sfp.sbs` (Gemma2 2B instruction-tuned model,\n    8-bit switched floating point).\n-   Tokenizer file `tokenizer.spm` (can omit for single-format weights files\n    created after 2025-05-06, or output by migrate_weights.cc).\n\n```sh\n./gemma \\\n--tokenizer tokenizer.spm --weights gemma2-2b-it-sfp.sbs\n```\n\n### RecurrentGemma\n\nThis repository includes a version of Gemma based on Griffin\n([paper](https://arxiv.org/abs/2402.19427),\n[code](https://github.com/google-deepmind/recurrentgemma)). Its architecture\nincludes both recurrent layers and local attention, thus it is more efficient\nfor longer sequences and has a smaller memory footprint than standard Gemma. We\nhere provide a C++ implementation of this model based on the paper.\n\nTo use the recurrent version of Gemma included in this repository, build the\ngemma binary as noted above in Step 3. Download the compressed weights and\ntokenizer from the RecurrentGemma\n[Kaggle](https://www.kaggle.com/models/google/recurrentgemma/gemmaCpp) as in\nStep 1, and run the binary as follows:\n\n`./gemma --tokenizer tokenizer.spm --model gr2b-it --weights 2b-it-sfp.sbs`\n\n### PaliGemma Vision-Language Model\n\nThis repository includes a version of the PaliGemma 2 VLM\n([paper](https://arxiv.org/abs/2412.03555)). We provide a C++ implementation of\nthe PaliGemma 2 model here.\n\nTo use the version of PaliGemma included in this repository, build the gemma\nbinary as noted above in Step 3. Download the compressed weights and tokenizer\nfrom\n[Kaggle](https://www.kaggle.com/models/google/paligemma-2/gemmaCpp/paligemma2-3b-mix-224)\nand run the binary as follows:\n\n```sh\n./gemma \\\n--tokenizer paligemma_tokenizer.model \\\n--weights paligemma2-3b-mix-224-sfp.sbs \\\n--image_file paligemma/testdata/image.ppm\n```\n\nNote that the image reading code is very basic to avoid depending on an image\nprocessing library for now. We currently only support reading binary PPMs (P6).\nSo use a tool like `convert` to first convert your images into that format, e.g.\n\n`convert image.jpeg -resize 224x224^ image.ppm`\n\n(As the image will be resized for processing anyway, we can already resize at\nthis stage for slightly faster loading.)\n\nThe interaction with the image (using the mix-224 checkpoint) may then look\nsomething like this:\n\n```\n> Describe the image briefly\nA large building with two towers in the middle of a city.\n> What type of building is it?\nchurch\n> What color is the church?\ngray\n> caption image\nA large building with two towers stands tall on the water's edge. The building\nhas a brown roof and a window on the side. A tree stands in front of the\nbuilding, and a flag waves proudly from its top. The water is calm and blue,\nreflecting the sky above. A bridge crosses the water, and a red and white boat\nrests on its surface. The building has a window on the side, and a flag on top.\nA tall tree stands in front of the building, and a window on the building is\nvisible from the water. The water is green, and the sky is blue.\n```\n\n### Migrating to single-file format\n\nThere is now a new format for the weights file, which is a single file that\nallows to contain the tokenizer (and the model type) directly. A tool to migrate\nfrom the multi-file format to the single-file format is available.\n\n```sh\nio/migrate_weights \\\n  --tokenizer .../tokenizer.spm --weights .../gemma2-2b-it-sfp.sbs \\\n  --output_weights .../gemma2-2b-it-sfp-single.sbs\n```\n\nAfter migration, you can omit the tokenizer argument like this:\n\n```sh\n./gemma --weights .../gemma2-2b-it-sfp-single.sbs\n```\n\n### Troubleshooting and FAQs\n\n**Problems building in Windows / Visual Studio**\n\nCurrently if you're using Windows, we recommend building in WSL (Windows\nSubsystem for Linux). We are exploring options to enable other build\nconfigurations, see issues for active discussion.\n\n**Model does not respond to instructions and produces strange output**\n\nA common issue is that you are using a pre-trained model, which is not\ninstruction-tuned and thus does not respond to instructions. Make sure you are\nusing an instruction-tuned model (`gemma2-2b-it-sfp`) and not a pre-trained\nmodel (any model with a `-pt` suffix).\n\n**What sequence lengths are supported?**\n\nSee `max_seq_len` in `configs.cc` and `InferenceArgs.seq_len`. For the Gemma 3\nmodels larger than 1B, this is typically 32K but 128K would also work given\nenough RAM. Note that long sequences will be slow due to the quadratic cost of\nattention.\n\n**How do I convert my fine-tune to a `.sbs` compressed model file?**\n\nFor PaliGemma 2 checkpoints, you can use python/convert_from_safetensors.py to\nconvert from safetensors format (tested with building via bazel). For an adapter\nmodel, you will likely need to call merge_and_unload() to convert the adapter\nmodel to a single-file format before converting it.\n\nHere is how to use it using a bazel build of the compression library assuming\nlocally installed (venv) torch, numpy, safetensors, absl-py, etc.:\n\n```sh\nbazel build //compression/python:compression\nBAZEL_OUTPUT_DIR=\"${PWD}/bazel-bin/compression\"\npython3 -c \"import site; print(site.getsitepackages())\"\n# Use your sites-packages file here:\nln -s $BAZEL_OUTPUT_DIR [...]/site-packages/compression\npython3 python/convert_from_safetensors.py --load_path [...].safetensors.index.json\n```\n\n**What are some easy ways to make the model run faster?**\n\n1.  Make sure you are using the 8-bit switched floating point `-sfp` models.\n    These are half the size of bf16 and thus use less memory bandwidth and cache\n    space.\n2.  Due to auto-tuning, the second and especially third query will be faster.\n3.  If you're on a laptop, make sure power mode is set to maximize performance\n    and saving mode is **off**. For most laptops, the power saving modes get\n    activated automatically if the computer is not plugged in.\n4.  Close other unused cpu-intensive applications.\n5.  On macs, anecdotally we observe a \"warm-up\" ramp-up in speed as performance\n    cores get engaged.\n\nWe're also working on algorithmic and optimization approaches for faster\ninference, stay tuned.\n\n## Usage\n\n`gemma` has different usage modes, controlled by the verbosity flag.\n\nAll usage modes are currently interactive, triggering text generation upon\nnewline input.\n\n| Verbosity       | Usage mode | Details                                       |\n| --------------- | ---------- | --------------------------------------------- |\n| `--verbosity 0` | Minimal | Only prints generation output. Suitable as a CLI tool. |\n| `--verbosity 1` | Default | Standard user-facing terminal UI. |\n| `--verbosity 2` | Detailed | Shows additional developer and debug info. |\n\n### Interactive Terminal App\n\nBy default, verbosity is set to 1, bringing up a terminal-based interactive\ninterface when `gemma` is invoked:\n\n```sh\n$ ./gemma [...]\n  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __\n / _` |/ _ \\ '_ ` _ \\| '_ ` _ \\ / _` | / __| '_ \\| '_ \\\n| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |\n \\__, |\\___|_| |_| |_|_| |_| |_|\\__,_(_)___| .__/| .__/\n  __/ |                                    | |   | |\n |___/                                     |_|   |_|\n\n...\n\n*Usage*\n  Enter an instruction and press enter (%C reset conversation, %Q quits).\n\n*Examples*\n  - Write an email to grandma thanking her for the cookies.\n  - What are some historical attractions to visit around Massachusetts?\n  - Compute the nth fibonacci number in javascript.\n  - Write a standup comedy bit about WebGPU programming.\n\n> What are some outdoorsy places to visit around Boston?\n\n[ Reading prompt ] .....................\n\n\n**Boston Harbor and Islands:**\n\n* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.\n* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.\n* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.\n* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.\n\n**Forest and Nature:**\n\n* **Forest Park:** Hike through a scenic forest with diverse wildlife.\n* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.\n* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.\n\n...\n```\n\n### Usage as a Command Line Tool\n\nFor using the `gemma` executable as a command line tool, it may be useful to\ncreate an alias for gemma.cpp with arguments fully specified:\n\n```sh\nalias gemma2b=\"~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --weights ~/gemma.cpp/build/gemma2-2b-it-sfp.sbs --verbosity 0\"\n```\n\nReplace the above paths with your own paths to the model and tokenizer paths\nfrom the download.\n\nHere is an example of prompting `gemma` with a truncated input\nfile (using a `gemma2b` alias like defined above):\n\n```sh\ncat configs.h | tail -n 35 | tr '\\n' ' ' | xargs -0 echo \"What does this C++ code do: \" | gemma2b\n```\n\n> [!NOTE]\n> CLI usage of gemma.cpp is experimental and should take context length\n> limitations into account.\n\nThe output of the above command should look like:\n\n```sh\n[ Reading prompt ] [...]\nThis C++ code snippet defines a set of **constants** used in a large language model (LLM) implementation, likely related to the **attention mechanism**.\n\nLet's break down the code:\n[...]\n```\n\n### Incorporating gemma.cpp as a Library in your Project\n\nThe easiest way to incorporate gemma.cpp in your own project is to pull in\ngemma.cpp and dependencies using `FetchContent`. You can add the following to\nyour CMakeLists.txt:\n\n```\ninclude(FetchContent)\n\nFetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)\nFetchContent_MakeAvailable(sentencepiece)\n\nFetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)\nFetchContent_MakeAvailable(gemma)\n\nFetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG 92d327e841d78e11ae888757a3e16d291951cf64)\nFetchContent_MakeAvailable(highway)\n```\n\nNote for the gemma.cpp `GIT_TAG`, you may replace `origin/main` for a specific\ncommit hash if you would like to pin the library version.\n\nAfter your executable is defined (substitute your executable name for\n`[Executable Name]` below):\n\n```\ntarget_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)\nFetchContent_GetProperties(gemma)\nFetchContent_GetProperties(sentencepiece)\ntarget_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})\ntarget_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})\n```\n\n### Building gemma.cpp as a Library\n\ngemma.cpp can also be used as a library dependency in your own project. The\nshared library artifact can be built by modifying the make invocation to build\nthe `libgemma` target instead of `gemma`.\n\n> [!NOTE]\n> If you are using gemma.cpp in your own project with the `FetchContent` steps\n> in the previous section, building the library is done automatically by `cmake`\n> and this section can be skipped.\n\nFirst, run `cmake`:\n\n```sh\ncmake -B build\n```\n\nThen, run `make` with the `libgemma` target:\n\n```sh\ncd build\nmake -j [number of parallel threads to use] libgemma\n```\n\nIf this is successful, you should now have a `libgemma` library file in the\n`build/` directory. On Unix platforms, the filename is `libgemma.a`.\n\n## Independent Projects Using gemma.cpp\n\nSome independent projects using gemma.cpp:\n\n- [gemma-cpp-python - Python bindings](https://github.com/namtranase/gemma-cpp-python)\n- [lua-cgemma - Lua bindings](https://github.com/ufownl/lua-cgemma)\n- [Godot engine demo project](https://github.com/Rliop913/Gemma-godot-demo-project)\n\nIf you would like to have your project included, feel free to get in touch or\nsubmit a PR with a `README.md` edit.\n\n## Acknowledgements and Contacts\n\ngemma.cpp was started in fall 2023 by\n[Austin Huang](mailto:austinvhuang@google.com) and\n[Jan Wassenberg](mailto:janwas@google.com), and subsequently released February\n2024 thanks to contributions from Phil Culliton, Paul Chang, and Dan Zheng.\n\nGriffin support was implemented in April 2024 thanks to contributions by Andrey\nMikhaylov, Eugene Kliuchnikov, Jan Wassenberg, Jyrki Alakuijala, Lode\nVandevenne, Luca Versari, Martin Bruse, Phil Culliton, Sami Boukortt, Thomas\nFischbacher and Zoltan Szabadka.\n\nGemma-2 support was implemented in June/July 2024 with the help of several\npeople.\n\nPaliGemma support was implemented in September 2024 with contributions from\nDaniel Keysers.\n\n[Jan Wassenberg](mailto:janwas@google.com) has continued to contribute many\nimprovements, including major gains in efficiency, since the initial release.\n\nThis is not an officially supported Google product.\n",
    "py_files": {
        "compression/python/compression_test.py": "# Copyright 2024 Google LLC\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for CLIF wrapped .sbs writer.\"\"\"\n\nimport numpy as np\n\nfrom absl.testing import absltest\nfrom compression.python import compression\nfrom python import configs\n\n\nclass CompressionTest(absltest.TestCase):\n\n  def test_sbs_writer(self):\n    info_192 = configs.TensorInfo()\n    info_192.name = \"ignored_192\"\n    info_192.axes = [0]\n    info_192.shape = [192]\n\n    temp_file = self.create_tempfile(\"test.sbs\")\n    writer = compression.SbsWriter(temp_file.full_path)\n    writer.insert(\n        \"tensor0\",\n        # Large enough to require scaling.\n        np.array([3.0012] * 128 + [4.001] * 64, dtype=np.float32),\n        configs.Type.kSFP,\n        info_192,\n    )\n\n    # 2D tensor.\n    info_2d = configs.TensorInfo()\n    info_2d.name = \"ignored_2d\"\n    info_2d.axes = [0, 1]\n    info_2d.shape = [96, 192]\n    writer.insert(\n        \"tensor_2d\",\n        np.array([i / 1e3 for i in range(96 * 192)], dtype=np.float32),\n        configs.Type.kBF16,\n        info_2d,\n    )\n\n    # 3D collapsed into rows.\n    info_3d = configs.TensorInfo()\n    info_3d.name = \"ignored_3d\"\n    info_3d.axes = [0, 1, 2]\n    info_3d.shape = [10, 12, 192]\n    info_3d.cols_take_extra_dims = False\n    writer.insert(\n        \"tensor_3d\",\n        # Verification of scale below depends on the shape and multiplier here.\n        np.array([i / 1e3 for i in range(10 * 12 * 192)], dtype=np.float32),\n        configs.Type.kSFP,\n        info_3d,\n    )\n\n    # Exercise all types supported by Compress.\n    info_256 = configs.TensorInfo()\n    info_256.name = \"ignored_256\"\n    info_256.axes = [0]\n    info_256.shape = [256]\n    writer.insert(\n        \"tensor_sfp\",\n        np.array([0.000375] * 128 + [0.00009] * 128, dtype=np.float32),\n        configs.Type.kSFP,\n        info_256,\n    )\n    writer.insert(\n        \"tensor_bf\",\n        np.array([0.000375] * 128 + [0.00007] * 128, dtype=np.float32),\n        configs.Type.kBF16,\n        info_256,\n    )\n    writer.insert(\n        \"tensor_f32\",\n        np.array([0.000375] * 128 + [0.00006] * 128, dtype=np.float32),\n        configs.Type.kF32,\n        info_256,\n    )\n\n    config = configs.ModelConfig(\n        configs.Model.GEMMA_TINY,\n        configs.Type.kSFP,\n        configs.PromptWrapping.GEMMA_IT,\n    )\n    tokenizer_path = \"\"  # no tokenizer required for testing\n    writer.write(config, tokenizer_path)\n\n    print(\"Ignore next two warnings; test does not enable model deduction.\")\n    reader = compression.SbsReader(temp_file.full_path)\n\n    self.assertEqual(reader.config.model, configs.Model.GEMMA_TINY)\n    self.assertEqual(reader.config.weight, configs.Type.kSFP)\n\n    mat = reader.find_mat(\"tensor0\")\n    self.assertEqual(mat.cols, 192)\n    self.assertEqual(mat.rows, 1)\n    self.assertEqual(mat.type, configs.Type.kSFP)\n    self.assertAlmostEqual(mat.scale, 4.001 / 1.875, places=5)\n\n    mat = reader.find_mat(\"tensor_2d\")\n    self.assertEqual(mat.cols, 192)\n    self.assertEqual(mat.rows, 96)\n    self.assertEqual(mat.type, configs.Type.kBF16)\n    self.assertAlmostEqual(mat.scale, 1.0)\n\n    mat = reader.find_mat(\"tensor_3d\")\n    self.assertEqual(mat.cols, 192)\n    self.assertEqual(mat.rows, 10 * 12)\n    self.assertEqual(mat.type, configs.Type.kSFP)\n    self.assertAlmostEqual(mat.scale, 192 * 120 / 1e3 / 1.875, places=2)\n\n    mat = reader.find_mat(\"tensor_sfp\")\n    self.assertEqual(mat.cols, 256)\n    self.assertEqual(mat.rows, 1)\n    self.assertEqual(mat.type, configs.Type.kSFP)\n    self.assertAlmostEqual(mat.scale, 1.0)\n\n    mat = reader.find_mat(\"tensor_bf\")\n    self.assertEqual(mat.cols, 256)\n    self.assertEqual(mat.rows, 1)\n    self.assertEqual(mat.type, configs.Type.kBF16)\n    self.assertAlmostEqual(mat.scale, 1.0)\n\n    mat = reader.find_mat(\"tensor_f32\")\n    self.assertEqual(mat.cols, 256)\n    self.assertEqual(mat.rows, 1)\n    self.assertEqual(mat.type, configs.Type.kF32)\n    self.assertAlmostEqual(mat.scale, 1.0)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
        "compression/python/pytree/build_model_file_for_cpp_binary.py": "\"\"\"Ad-hoc glue code for building the griffin model-file for the C++ binary.\n\nUsage:\n\npython3 -m venv $HOME/clients/griffin-venv\n\n. $HOME/clients/griffin-venv/bin/activate\n\npython3 -m pip install -r requirements.txt\n\ntime python3 build_model_file_for_cpp_binary.py \\\n  $HOME/GRIFFIN/model_data \\\n  cpp_load_log.txt /tmp/G2B.data\n\nreal    3m5.821s\nuser    2m9.205s\nsys     2m46.720s\n\n./compress_weights --weights /tmp/G2B.data --model gr2b-it \\\n  --compressed_weights /tmp/G2B.compressed\n./gemma --tokenizer tokenizer.spm --weights /tmp/G2B.compressed \\\n  --model gr2b-it\n\nWeights for the recurrent-gemma model that can be converted with this script\ncan be found at:\n\n  https://www.kaggle.com/models/google/recurrentgemma/flax/2b-it\n\"\"\"\n\nimport pprint\nimport re\nimport sys\n\nfrom typing import Any, Mapping\n\nimport numpy\n\nimport orbax.checkpoint\n\nimport ml_model_transforms\nimport pytree_transforms\n\n\ndef _fn_identity(x): return x\n\n\ndef _fn_transpose(x): return x.T\n\n\ndef _fn_transpose_all_heads(x): return x.transpose(0, 2, 1)\n\n\ndef _fn_scaled_softplus(a):\n  return -8 * numpy.logaddexp(a, 0)\n\n\ndef _fn_attention_moveaxis(a):\n  return a.reshape(10, 256, 2560).transpose(0, 2, 1)\n\n\ndef _aspec(pieces=(), transforms=()):\n  \"\"\"Short-hand array-save-specification.\n\n  Args:\n    pieces: Sequence of key-sequences identifying an array.\n    transforms: Sequence of transformations, indexed in\n      parallel to `pieces`, to apply to data arrays prior to saving.\n      Will be padded with identity-transformations to the length of `pieces`.\n\n  Returns:\n    Specification as for use in _LAYETR_NAME_MAPPING.\n  \"\"\"\n  # `zip` trims to shortest sequence, so this amounts to using\n  # default-transforms.\n  # tuple() since we need a Sequence here, not a stateful-iterator zip_object.\n  return tuple(zip(pieces, list(transforms) + [_fn_identity] * len(pieces)))\n\n\n_LAYER_NAME_MAPPING = pytree_transforms.deep_freeze({\n    # Recurrent Layer\n    'griffin_linear_x_w': _aspec(\n        [('recurrent_block', 'linear_x', 'kernel')],\n        [_fn_transpose]),\n    'griffin_linear_x_biases': _aspec(\n        [('recurrent_block', 'linear_x', 'bias')]),\n    'griffin_linear_y_w': _aspec(\n        [('recurrent_block', 'linear_y', 'kernel')],\n        [_fn_transpose]),\n    'griffin_linear_y_biases': _aspec(\n        [('recurrent_block', 'linear_y', 'bias')]),\n    'griffin_linear_out_w': _aspec(\n        [('recurrent_block', 'linear_out', 'kernel')],\n        [_fn_transpose]),\n    'griffin_linear_out_biases': _aspec(\n        [('recurrent_block' ,'linear_out', 'bias')]),\n    'griffin_conv_w': _aspec(\n        [('recurrent_block', 'conv_1d', 'w')]),\n    'griffin_conv_biases': _aspec(\n        [('recurrent_block', 'conv_1d', 'b')]),\n    'griffin_gate_w': _aspec(\n        [('recurrent_block', 'rg_lru', 'input_gate', 'w'),\n         ('recurrent_block', 'rg_lru', 'a_gate', 'w')],\n        [_fn_transpose_all_heads, _fn_transpose_all_heads]),\n    'griffin_gate_biases': _aspec(\n        [('recurrent_block', 'rg_lru', 'input_gate', 'b'),\n         ('recurrent_block', 'rg_lru', 'a_gate', 'b')]),\n    'griffin_a': _aspec(\n        [('recurrent_block', 'rg_lru', 'a_param')],\n        [_fn_scaled_softplus]),\n    # Attention Layer\n    'qkv_einsum_w': _aspec(\n        [('attention_block', 'proj_q', 'kernel'),\n         ('attention_block', 'proj_k', 'kernel'),\n         ('attention_block', 'proj_v', 'kernel'),\n         ],\n        [_fn_transpose, _fn_transpose, _fn_transpose]),\n    'attn_vec_einsum_w': _aspec(\n        [('attention_block', 'proj_final', 'kernel')],\n        [_fn_attention_moveaxis]),\n    'attention_output_biases': _aspec(\n        [('attention_block', 'proj_final', 'bias')]),\n    # Common\n    'pre_attention_norm_scale': _aspec(\n        [('temporal_pre_norm', 'scale')]),\n    'pre_ffw_norm_scale': _aspec(\n        [('channel_pre_norm', 'scale')]),\n    'gating_einsum_w': _aspec(\n        [('mlp_block', 'ffw_up', 'w')],\n        [_fn_transpose_all_heads]),\n    'ffw_gating_biases': _aspec(\n        [('mlp_block', 'ffw_up', 'b')]),\n    'linear_w': _aspec(\n        [('mlp_block', 'ffw_down', 'kernel')],\n        [_fn_transpose]),\n    'ffw_output_biases': _aspec(\n        [('mlp_block', 'ffw_down', 'bias')]),\n    # Other\n    'embedder_input_embedding': _aspec(\n        [('embedder', 'input_embedding')]),\n    'final_norm_scale': _aspec(\n        [('final_norm', 'scale')]),\n})\n\n\ndef process_param_line(line : str) -> tuple[None | str, int, str]:\n  \"\"\"Processes a \"loading parameters\" log-line from the griffin binary.\"\"\"\n  # This is slightly more permissive than strictly needed, to also handle\n  # some earlier form of the output.\n  matched = re.match(\n      r'(?a)Loading Parameters:? \\('\n      r'(?:layer=(?P<layer>\\d+), )?'\n      r'size (?P<size>\\d+)\\):? '\n      r'(?P<tag>\\S+)',\n      line)\n  if not matched:\n    return None\n  layer = matched['layer']\n  wanted_size = int(matched['size'])\n  cpp_tag = matched['tag']\n  return matched['layer'], int(matched['size']), matched['tag']\n\n\ndef collect_pytree_keys(param_lines):\n  \"\"\"Collects all the pytree keys and transforms for model-serialization.\"\"\"\n  pytree_keys = []\n  array_transforms = []\n  unsatisfied = []\n  for maybe_spec in map(process_param_line, param_lines):\n    if not maybe_spec: continue  # Skip non-parameter lines.\n    layer, wanted_size, cpp_tag = maybe_spec\n    pytree_key_tails_and_transforms = _LAYER_NAME_MAPPING.get(cpp_tag, ())\n    if not pytree_key_tails_and_transforms:\n      unsatisfied.append((layer, cpp_tag))\n    else:\n      for key_tail, array_transform in pytree_key_tails_and_transforms:\n        pytree_keys.append(\n            key_tail if layer is None\n            else (f'blocks.{layer}',) + key_tail)\n        array_transforms.append(array_transform)\n  return pytree_keys, array_transforms, unsatisfied\n\n\nclass UnsatisfiedArrayLoadsError(ValueError):\n  \"\"\"Some array-loads could not be satisfied.\"\"\"\n\n\ndef flatten_model_for_cpp_binary(tree,\n                                 cpp_expectations_logfile_path : str,\n                                 out_path : str,\n                                 unsatisfied_ok : bool = False\n                                 ):\n  \"\"\"Produces a model-parameters file readable by the C++ binary.\n\n  Args:\n    tree: The pytree with model-parameters.\n    cpp_expectations_logfile_path:\n      Path to a logfile produced by the C++ binary that shows\n      the expected array-order.\n    out_path: Path to the model-weights file to be written.\n    unsatisfied_ok: If true, we ignore the presence of unsatisfied\n      array-loads and write a model-parameters file that skips these pieces.\n      This will lead to an unusable model-parameters file which however\n      still might be useful for other analysis.\n\n  Returns:\n    Tuple `(unknown_keys, missing_keys)`, where `unknown_keys`\n    is a sequence of `(layer_or_None, name)` descriptions of the keys\n    in the C++ log that could not be satisfied, and `missing_keys`\n    is a sequence of linearized pytree key-sequences for keys\n    not found in the checkpoint.\n\n  Raises:\n    UnsatisfiedArrayLoadsError: If some of the expected arrays\n      could not be included in the output and `unsatisfied_ok`\n      is false.\n  \"\"\"\n  with open(cpp_expectations_logfile_path, 'rt') as h_log:\n    pytree_keys, array_transforms, unknown_keys = collect_pytree_keys(\n        list(h_log))\n  rank_by_pytree_key = {k: n for n, k in enumerate(pytree_keys)}\n  array_transform_by_pytree_key = dict(zip(pytree_keys, array_transforms))\n  #\n  model_contents = ml_model_transforms.model_contents(tree)\n  missing_keys = set(pytree_keys) - model_contents.keys()\n  if (unknown_keys or missing_keys) and not unsatisfied_ok:\n    raise ValueError(\n      f'Unsatisfied loads: unknown_keys: {unknown_keys!r}, '\n      f'missing keys: {sorted(missing_keys)!r}')\n  ml_model_transforms.model_save(\n    tree,\n    filepath_stem=out_path,\n    data_suffix='',\n    manifest_suffix=None,\n    array_transform_by_pytree_key=array_transform_by_pytree_key,\n    key=rank_by_pytree_key.get,\n    report=lambda line: print(line, file=sys.stderr),\n    byte_align=1)\n  return tuple(unknown_keys), tuple(sorted(missing_keys))\n\n\ndef main(args):\n  \"\"\"Creates the model-file.\n\n  Args:\n    sys.argv[] parameters from command line sans the leading one.\n\n  Returns:\n    The pytree with all the de-serialized variables, such as for convenient\n    `python3 -i` inspection.\n  \"\"\"\n  try:\n    model_dir, cpp_load_log, out_path = args\n  except Exception:\n    sys.exit(f'Usage: {__file__} [model_dir] [cpp_load_log] [output_filename]')\n  pattern = (\"recurrent\", \"recurrent\", \"attention\")\n  orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n  variables = orbax_checkpointer.restore(model_dir)\n  if sorted(variables) == ['params']:\n    print('Warning: Using `variables[\"params\"]` as tree-root.', file=sys.stderr)\n    variables_to_use = variables['params']\n  else:\n    variables_to_use = variables\n  unknown, missing = flatten_model_for_cpp_binary(variables_to_use,\n                                                  cpp_load_log,\n                                                  out_path,\n                                                  unsatisfied_ok=True)\n  print('Model file saved.\\n'\n        f'# unknown:\\n{pprint.pformat(unknown)}\\n'\n        f'# missing:\\n{pprint.pformat(missing)}')\n  return variables\n\n\nif __name__ == '__main__':\n  # Return value assignment is for `python3 -i ...` inspection.\n  pytree = main(sys.argv[1:])\n",
        "compression/python/pytree/ml_model_transforms.py": "\"\"\"Transformations for python-trees representing the parameters of a ML model.\n\nImportant: This module assumes that byte-order is the same on the\nmachine that serializes data and the machine that deserializes\ndata. If, for example, numpy-data gets dumped, respectively loaded,\nwith a dtype-specification of numpy.float32, on-file byte-order\nwill be host byte order.\n\n\"\"\"\n\nimport ast\nimport hashlib\nimport itertools\nimport pprint\nimport sys\nimport time\nfrom typing import Any, Callable, Iterable, Iterator, Mapping, TypeVar\n\nimport numpy\nimport pytree_transforms\n\n\nNT = TypeVar('NT')\n\n\ndef ml_model_leaf_summary(path, x, sep=', '):\n  \"\"\"Produces a textual summary of a leaf-node and its path.\n\n  Args:\n    path: The path-to-root, as a reverse-order recursive\n      pair of path-components, with `()` as root.\n    x: The leaf-object.\n    sep: the separator between description-elements.\n      Default ', ' allows for convenient line-by-line processing\n      (such as via grep, perl -ne, etc.), but using e.g. sep=',\\n  '\n      might be more useful for human consumption.\n\n  Returns:\n    A human-readable string providing information about the node.\n  \"\"\"\n  # Using `repr` for path-components to get a faithful presentation.\n  # (...which still however would be somewat painful to correctly\n  # split into components.)\n  path_str = ','.join(map(repr,\n                          pytree_transforms.linearize_revtuple_path(path)))\n  tx = type(x)\n  mod = tx.__module__  # Either a module or a string like 'builtins'.\n  modname = mod if isinstance(mod, str) else mod.__name__\n  type_str = f'{modname}.{tx.__qualname__}'\n  try:\n    # `numpy.ndarray` instances have a `.data` property that gives access\n    # to a buffer via which we can hashlib-fingerprint the numerical\n    # contents. We here simply try to produce a fingerprint and also look\n    # up the .dtype of the object. Technically, there is a somewhat-unsound\n    # assumption here that if these operations succeed, we are indeed looking\n    # at a ndarray or sufficiently similar object for these operations to\n    # make sense. As the output is declared \"for human consumption\", this\n    # fishiness is not a problem.\n    fp = hashlib.sha256(x.data).hexdigest()\n    start = list(itertools.islice(x.flat, 5))\n    stats_str = (\n        f'min={numpy.min(x):.6g}, max={numpy.max(x):.6g}, '\n        f'mean={numpy.mean(x):.6g}, std={numpy.std(x):.6g}')\n    return (f'{path_str:60s}: <{type_str}{sep}'\n            f'fp=0x{fp}{sep}{stats_str}{sep}shape={x.shape}, '\n            f'dtype={x.dtype}{sep}start={start}>')\n  except (AttributeError, ValueError, TypeError):\n    # Fallback - trying to include information about the data-content\n    # of a likely-numerical-array failed.\n    return f'{path_str:60s}: {type_str}({repr(x)})'\n\n\n# A specialized node-handler.\n# Interface follows node-handler expectations defined in pytree_transforms.\ndef _ml_model_tree_node_handler(path: tuple, node : NT) -> (\n    None | tuple[Callable[[Iterable[tuple[Any, NT]]], NT],\n                 Iterator[tuple[Any, NT]]]):\n  \"\"\"Processes a tree-node as required by pytree-iteration and -mapping.\n\n  Args:\n    path: revtuple path to the current node.\n    node: a tree-node in a ML-model tree that is recursively\n      built out of `numpy.ndarray` leaf-values and dicts mapping\n      node-name string-keys to other such nodes representing subtrees -\n      and nothing else.\n\n  Returns:\n    `None` if the tree-node is to be regarded as a leaf, otherwise\n    a pair `(rebuilder, iterator)`, where `iterator` iterates\n    over the data-content of the node, each item represented as a pair\n    of `(lookup_path_item, value_item)`, and `rebuilder` is a function\n    which, when applied to `iterator` or any iterable with the same\n    elements, returns a node that is equivalent to the original.\n\n  Raises:\n    NotAMLModelTreeNodeError: If the tree contains a node that is neither\n      a `dict` nor a `numpy.ndarray` instance.\n  \"\"\"\n  # The astute reader will notice that we are doing something fishy\n  # here - this code could not be translated to Haskell as-is, since\n  # `NT` cannot actually be a proper type-variable in the sense\n  # of parametric polymorphism.\n  del path  # Unused.\n  if isinstance(node, dict):\n    return dict, iter(node.items())\n  if isinstance(node, numpy.ndarray):\n    return None\n  raise pytree_transforms.NotAMLModelTreeNodeError(\n      f'Type of bad node: {type(node)}')\n\n\ndef _ml_model_extract_leaf_transform(\n        path: pytree_transforms.RevTuplePath,\n        leaf: Any):\n  \"\"\"Maps an array-leaf to a pair `(full_path, lambda: array)`.\n\n  The computation that produces the leaf-value is lazified underneath\n  a `lambda`, since if we e.g. performed a memory-expensive\n  transformation (such as some dtype-changes) directly at this point,\n  then going from an iterator over tree-items for one-by-one\n  consumption to a list of these items would have all the\n  dtype-transformed values around simultaneously. We want to avoid\n  situations where we can do nothing about having multiple variants\n  of the data simultaneously in memory.\n  \"\"\"\n  # Hack: If we are encountering a `bfloat16` numpy-array,\n  # we pretend to have the data as a numpy.float32 array,\n  # since that's about all that contemporary CPUs can process\n  # efficiently here.\n  linearized_path = pytree_transforms.linearize_revtuple_path(path)\n  try:\n    # We have to use some trickery to detect `bfloat16`.\n    if leaf.dtype.descr[-1] == ('', '<V2'):\n      return linearized_path, lambda: leaf.astype(numpy.float32)\n    else:\n      return linearized_path, lambda: leaf\n  except Exception:\n    return linearized_path, lambda: leaf\n\n\n# Here, we cannot properly specify the return-type, since this can\n# either be a leaf-type or something recursively-defined.\ndef revtuple_autovifify_from_linear(\n        keys_and_vals: Iterable[tuple[Any, Any]]) -> Any:\n  \"\"\"Performs perl-style autovivification on a nested-dict tree.\n\n  Args:\n    keys_and_vals: An iterable of pairs `(key_path, value)`, where\n      `key_path` is a sequence of keys to be used to navigate to\n      the result via iterative dict-lookup, left-to-right.\n      Must not have duplicate keys, and must not more than one key if\n      an empty-sequence key is present. If this iterable is an\n      iterator, it will be fully exhausted on successful execution.\n\n  Returns:\n    An object representing a nested-dict structure such that\n    for every `key_path` from `keys_and_vals`, recursive-dict-lookup\n    on the elements of that path starting from this object will\n    produce the corresponding value. An empty `keys_and_vals`\n    set will return `{}`. Every dict in the nested return-value\n    that has been populated by autovivification is newly allocated.\n  \"\"\"\n  # Code structure is a bit gnarly here due to f(keys_and_vals=[((), x)])\n  # having to evaluate to x and not a dict.\n  # There may be ways to prettify/simplify this.\n  result = None\n  empty = {}\n  for linear_path, val in keys_and_vals:\n    if linear_path == ():\n      if result is not None:\n        raise ValueError('Root-value seen alongside other values.')\n      result = val\n    else:\n      if result is None:\n        result = {}\n      elif type(result) is not dict:\n        # We already did encounter a root-value.\n        raise ValueError('Root-value seen alongside other values.')\n      cursor = result\n      for n in range(len(linear_path) - 1):\n        cursor = cursor.setdefault(linear_path[n], empty)\n        if cursor is empty:\n          # Regenerate `empty` if we just used it up.\n          empty = {}\n      cursor[linear_path[-1]] = val\n  return {} if result is None else result\n\n\ndef model_overview(tree, out=None) -> None:\n  \"\"\"Prints a human-readable overview to `(out or sys.stdout)`.\"\"\"\n  actual_out = out or sys.stdout\n  for line in pytree_transforms.pytree_leaf_iter(\n      tree, ml_model_leaf_summary,\n      _ml_model_tree_node_handler):\n    print(line, file=actual_out)\n\n\ndef model_contents(tree) -> Mapping[tuple[str, ...], Any]:\n  \"\"\"Maps a model to a {pytree_keys: data_array} mapping.\n\n  Args:\n    tree: The ML-model parameter-tree, built recursively out of\n      dict-instances with numpy.ndarray instances as leaves.\n\n  Returns:\n    A mapping from linearized pytree-key-sequence tuple to the corresponding\n    leaf-value.\n  \"\"\"\n  def leaf_transform(revtuple_path, leaf):\n    return pytree_transforms.linearize_revtuple_path(revtuple_path), leaf\n  return dict(\n      pytree_transforms.pytree_leaf_iter(\n          tree, leaf_transform, _ml_model_tree_node_handler))\n\n\ndef _fn_identity(x): return x\n\n\ndef model_save(tree,\n               filepath_stem: str,\n               data_suffix: str = '.data',\n               manifest_suffix: str | None = '.manifest',\n               key: Callable[[tuple[str, ...]], Any] | None = None,\n               array_transform_by_pytree_key: (\n                   Mapping[tuple[str, ...],\n                           Callable[[numpy.ndarray], numpy.ndarray]] |\n                   None) = None,\n               report: Callable[[str], None] | None = None,\n               byte_align: int = 8) -> tuple[int, float]:\n  \"\"\"Saves the content of a ML-model parameter-tree to filesystem.\n\n  After successful execution, the file f\"{filepath_stem}.data\"\n  will hold the combined numerical model-parameters, and\n  f\"{filepath_stem}.manifest\" will contain the key for interpreting\n  (and rebuilding) the data.\n\n  Args:\n    tree: The ML-model parameter-tree, built recursively out of\n      dict-instances with numpy.ndarray instances as leaves.\n    filepath_stem: Filesystem location for data.\n    data_suffix: Suffix to use for the data file.\n    manifest_suffix: Either `None`, in which case no manifest-file\n      will get written, or the suffix for the manifest-file.\n    key: `None` or a key-function that will be applied to the linear model-path\n      and used for sorting the data arrays by increasing value of the\n      key-function. If the key-function returns `None` on an item,\n      then this item is not included.\n    array_transform_by_pytree_key: Optional mapping from pytree-key\n      to an array-to-array transformation function to apply to the array\n      prior to serialization.\n    report: Optional callable for logging progress-reports.\n    byte_align: byte-alignment to use for numerical array data.\n      Numerical arrays whose size in bytes is not a multiple of this\n      will get padded to the next full multiple.\n\n  Returns:\n    A pair of `(size, time_sec)`, where `size` is the total byte-size\n    of the `.data` file and `time_sec` is the elapsed time\n    for saving the model, in seconds.\n  \"\"\"\n  time0 = time.monotonic()\n  if array_transform_by_pytree_key is None:\n    array_transform_by_pytree_key = {}\n  model_lazy_items = (\n      pytree_transforms.pytree_leaf_iter(\n          tree, _ml_model_extract_leaf_transform,\n          _ml_model_tree_node_handler))\n  if key is not None:\n    to_write = [\n        nkv[1:] for nkv in sorted(\n            (nkv for nkv in ((key(path), path, v)\n                             for path, v in model_lazy_items)\n             if nkv[0] is not None), key=lambda nkv: nkv[0])]\n  else:\n    to_write = list(model_lazy_items)\n  #\n  def lazy_arr_path_shape_dtype_size(path_and_lazy_arr):\n    path, lazy_arr = path_and_lazy_arr\n    arr = array_transform_by_pytree_key.get(path, _fn_identity)(lazy_arr())\n    return path, arr.shape, arr.dtype, arr.data.nbytes\n  arrs_path_shape_dtype_nbytes = list(\n      map(lazy_arr_path_shape_dtype_size, to_write))\n  # We need to know the total size of all the data.\n  bytesizes = [nbytes for *_, nbytes in arrs_path_shape_dtype_nbytes]\n  padded_bytesizes = [-(-bytesize // byte_align * byte_align)\n                      for bytesize in bytesizes]\n  offsets = numpy.cumsum([0] + padded_bytesizes)\n  membuf = numpy.memmap(filepath_stem + data_suffix,\n                        mode='w+', shape=offsets[-1])\n  try:\n    for (path, shape, dtype, nbytes), offset, (_, lazy_arr) in zip(\n        arrs_path_shape_dtype_nbytes, offsets, to_write):\n      # Note that if getting the array from the lazy lambda involved some\n      # computation, such as a copying dtype-change, that computation would\n      # end up being done multiple times here - including once above, to compute\n      # byte-sizes, and once more here.\n      transformed_arr = array_transform_by_pytree_key.get(\n          path,\n          _fn_identity)(lazy_arr())\n      membuf[offset : offset + nbytes] = numpy.frombuffer(\n          transformed_arr.ravel().data, 'u1')\n      if report is not None:\n        samples = ', '.join(map(str, transformed_arr.ravel()[:5]))\n        report(f'# Adding: {path!r}\\n  bytes: {nbytes:10d}, '\n               f'shape: {shape!r:30},\\n  start: [{samples}, ...]')\n      transformed_arr = None  # Drop memory references to numerical arrays ASAP.\n  finally:\n    if membuf is not None:\n      membuf.flush()\n      # NumPy wart: the memory-buffer is a resource that conceptually\n      # should be .close()able - since mmap()ing holds on to a\n      # file descriptor. However, it looks as if that clean-up were done\n      # in the \"finalizer\", despite that having meanwhile been widely\n      # understood as dubious practice. So, the best we can do here is\n      # to explicitly and clearly remove our reference to the instance.\n      del membuf\n  if manifest_suffix is not None:\n    # We still have to serialize the data that allows us to reconstruct\n    # a tree that is equivalent to the original.\n    manifest_data = [\n        dict(path=path,\n             dtype=dtype.descr[-1][-1],\n             shape=shape,\n             nbytes=nbytes,\n             offset=offset)\n        for (path, shape, dtype, nbytes), offset in zip(\n            arrs_path_shape_dtype_nbytes, offsets)]\n    with open(filepath_stem + '.manifest', 'wt') as h_manifest:\n      pprint.pprint(manifest_data, stream=h_manifest)\n  time_taken = time.monotonic() - time0\n  return offsets[-1], time_taken\n\n\ndef model_load(filepath_stem, mmapped=True):\n  \"\"\"Loads a model saved by `model_save`.\n\n  Tries to load the model from f\"{filepath_stem}.data\"\n  and f\"{filepath_stem}.manifest\".\n\n  Args:\n    filepath_stem: The model location on the filesystem.\n    mmapped: Whether data-arrays will be slices of a\n      `numpy.memmap` mapped buffer, to be paged in\n      on demand only, or in-memory copies of the data.\n  Returns:\n    A dict/numpy.ndarray tree representation of the model,\n    equivalent to the original model.\n  \"\"\"\n  with open(filepath_stem + '.manifest', 'rt') as h_manifest:\n    manifest = ast.literal_eval(h_manifest.read())\n  membuf = numpy.memmap(filepath_stem + '.data', mode='r+')\n  paths_and_arrays = []\n  for item in manifest:\n    path = item['path']\n    dtype = numpy.dtype(item['dtype'])\n    shape = item['shape']\n    nbytes = item['nbytes']\n    offset = item['offset']\n    data_array = numpy.frombuffer(membuf[offset : offset + nbytes].data,\n                                  dtype=dtype).reshape(shape)\n    paths_and_arrays.append(\n        (path,\n         data_array if mmapped else data_array.copy()))\n  # At this point, the memory-buffer is no longer needed. Still, if\n  # data-arrays retain references to the underlying data\n  # (i.e. when mmapped=False), this should keep the mapping\n  # - and hence file descriptor - open. We then are in a somewhat\n  # undesirable situation of clean-up of a resource that happens in a\n  # hard-to-predict way releasing a file descriptor.\n  del membuf\n  return revtuple_autovifify_from_linear(paths_and_arrays)\n",
        "compression/python/pytree/ml_model_transforms_test.py": "\"\"\"Basic tests for 'algebraic data type based pytree' transformations.\"\"\"\n\n\nimport io\nimport os\nimport tempfile\nimport unittest\n\nimport numpy\n\nimport ml_model_transforms\n\n\ndef _get_model(prefix):\n  return {\n    prefix + 'a1': numpy.arange(1000, 1024).reshape(6, 4).astype(numpy.float32),\n    prefix + 'a2': numpy.arange(2000, 2048).reshape(6, 8).astype(numpy.float32),\n    prefix + 'b1': {\n      prefix + 'c1': numpy.arange(100, 127).reshape(3, 3, 3).astype(numpy.int8),\n      prefix + 'c2': numpy.arange(100, 128).reshape(7, 4).astype(numpy.float64)\n    }}\n\n\nclass MLModeltransformsTest(unittest.TestCase):\n  \"\"\"Basic correctness validation tests for ML-model transformations.\"\"\"\n\n  def test_ml_model_leaf_summary(self):\n    \"\"\"Tests guarantees given by `ml_model_leaf_summary`.\"\"\"\n    summary = ml_model_transforms.ml_model_leaf_summary(\n      ('a', ()),\n      numpy.arange(1000, 1024).reshape(6, 4).astype(numpy.int16),\n      sep='##')\n    self.assertIn('##', summary)  # Separator is respected.\n    self.assertIn('(6, 4)', summary)  # Shape is mentioned somewhere.\n    self.assertIn('int16', summary)  # dtype is mentioned somewhere.\n\n  def test_revtuple_autovivify_from_linear(self):\n    \"\"\"Tests guarantees given by `revtuple_autovifify_from_linear`.\"\"\"\n    with self.subTest(guarantee='empty'):\n      self.assertEqual(\n        ml_model_transforms.revtuple_autovifify_from_linear([]),\n        {})\n    with self.subTest(guarantee='generic'):\n      keys_vals = [(('a', 'b1', 'c1'), 1001),\n                   (('a', 'b2'), 1002),\n                   (('a2',), 1003),\n                   ]\n      self.assertEqual(\n        ml_model_transforms.revtuple_autovifify_from_linear(keys_vals),\n        {'a': {'b1': {'c1': 1001}, 'b2': 1002}, 'a2': 1003})\n\n  def test_model_overview(self):\n    \"\"\"Tests guarantees given by `model_overview`.\"\"\"\n    model = _get_model('xyz')\n    out_io = io.StringIO()\n    ml_model_transforms.model_overview(model, out=out_io)\n    overview = out_io.getvalue()\n    self.assertIn('xyz', overview)\n\n  def test_model_contents(self):\n    \"\"\"Tests guarantees given by `model_contents`.\"\"\"\n    model = _get_model('pq_')\n    contents = ml_model_transforms.model_contents(model)\n    fingerprints = {k: (a.shape, a.ravel()[:3].tolist())\n                    for k, a in contents.items()}\n    self.assertEqual(fingerprints,\n                     {('pq_a1',): ((6, 4), [1000.0, 1001.0, 1002.0]),\n                      ('pq_a2',): ((6, 8), [2000.0, 2001.0, 2002.0]),\n                      ('pq_b1', 'pq_c1'): ((3, 3, 3), [100, 101, 102]),\n                      ('pq_b1', 'pq_c2'): ((7, 4), [100.0, 101.0, 102.0])})\n\n  def test_model_save_load_basic(self):\n    \"\"\"Tests basic guarantees given by `model_save` and `model_load`.\"\"\"\n    # What we care about here is that the round trip works - so\n    # it makes more sense to test saving and loading as one unit.\n    model_orig = _get_model('model_')\n    with tempfile.TemporaryDirectory() as tempdir:\n      filepath_stem = os.path.join(tempdir, 'the_model')\n      total_size, total_time = ml_model_transforms.model_save(model_orig,\n                                                              filepath_stem)\n      self.assertGreater(total_size, 0)\n      self.assertGreater(total_time, 0)\n      model_reloaded = ml_model_transforms.model_load(filepath_stem)\n      contents_orig = ml_model_transforms.model_contents(model_orig)\n      contents_reloaded = ml_model_transforms.model_contents(model_reloaded)\n      self.assertEqual(\n        {k: v.tolist() for k, v in contents_orig.items()},\n        {k: v.tolist() for k, v in contents_reloaded.items()})\n\n\nif __name__ == '__main__':\n  unittest.main()\n",
        "compression/python/pytree/pytree_transforms.py": "\"\"\"Tools for transforming \"nested python object\" tree data structures.\n\n# Context\n\nThe motivation for this module came from ML applications that ought to\nbe based on a principled handling of nested Python data structures.\nHaving such principled pytree-transforming code available solves\nsome other problems, such as doing away with a need to abuse\ntree-mapping for-side-effect-only and having to use a hope-and-pray\napproach to processing very deeply nested values which with a recursive\napproach might trigger a RecursionError.\n\nWe specifically want to cover the use case of having ML model\nparameters that are available in a nested Python data structure for\nwhich there \"almost\" is a unique-up-to-unique-isomorphism mapping from\nand to this Algebraic Data Type:\n\n`data ModelParams a = Array a | Node [(String, ModelParams a)]`\n\nIn this correspondence, `a` is some array-type (perhaps\n`numpy.ndarray`, `jax.numpy.ndarray`, `tf.tensor`, etc.), but the\ndata-processing code is effectively entirely agnostic to this, and a\n`Node` is \"almost\" an associative-list of (key, value) pairs\nrepresenting a Python dict. (Note: The \"almost\" here is mostly about\nthe conceptual wart that assoc-lists can in principle have key\nduplicates, but Python dicts can not. This is however not a problem\nsince all we need is the transformation in one direction,\ni.e. whatever data-processing `f` we want to express on the\nmodel-parameters-pytree, we can express by specifying a \"faithful\"\nmapping `m` into the above algebraic data type through which every\nsuch pytree data transform factorizes, i.e. for every `f` we can find\na `g` such that `f(p) = g(m(p))`.)\n\n## Components\n\nThe main workhorse in this module is the `pytree_iter` function that\nmaps a \"PyTree (such as representing `ModelParams`)\" to an iterator\nover values obtained by applying a mapping-function to the \"key-path\"\nand leaf-value for every leaf, where the \"key-path\" contains a\nlinked-list representation of the reversed sequence of keys from the\ntree-root, with list-nodes being represented by pairs\n`(latest_dict_key, rest_path)`, and the empty path being represented\nby `()`.\n\nFor the sake of genericity, `pytree_iter` is built in such a way that\nit actually can handle any kind of traversal of PyTree-trees that do\nrepresent algebraic data types (note however that some some do not) -\nbut for this to make sense, the user must have a way to define how to\ninterpret tree-nodes, in particular identify leaves. This requires\nproviding a function `node_handler` with the same signature and\nbehavior as described below for \"node handlers\".\n\nAdditionally, this module provides mapping-over-pytrees via\n`pytree_map`, which is also built in such a way that it makes the\ncorrespondence between an algebraic data type and its Python\nnested-tree representation explicit. Despite being powerful and\nflexible, this, however, may in general require a bit more effort to\nwire up, since node-rebuilding can be fairly nontrivial.\n\nFurthermore, as a prominent application, this module provides a simple\ndeep-freezing function that translates a nested Python data structure\nto deeply-immutable form.\n\n## Concepts and Conventions\n\n\"revtuple representation\":\n\n  As we iterate over a tree, we will have to keep track of the\n  path-to-tree-root. Naturally, two sibling nodes `n1` and `n2`\n  will share the same parent-path (being siblings), so it makes\n  sense to use a linked-list-with-shared-tail representation.\n  Python does not have a natural notion for that, so we use\n  recursively-constructed tuples `(node_tag, parent_path)`\n  that represent the path-from-root in-reverse-order, i.e.\n  for a non-empty path `p`, `p[0]` is the node-tag at the\n  deepest nesting level. We call this a \"revtuple representation\"\n  of the path.\n\n\"node handler\":\n\n  A node-handler classifies a tree-node as \"leaf or other node\", and\n  for non-leaf nodes provides information about both its children and\n  how to rebuild it. The behavior of a node-handler function must be\n  in alignment with this docstring:\n\n  '''Processes a tree-node as required by pytree-iteration and -mapping.\n\n  Args:\n    revtuple_path: Revtuple-representation of the path-from-root\n      to the current node.\n    node: a tree-node in a ML-model tree that is recursively\n      built out of leaf-values and other nodes.\n\n  Returns:\n    `None` if the tree-node is to be regarded as a leaf, otherwise\n    a pair `(rebuilder, iterator)`, where `iterator` iterates\n    over the data-content of the node, each item represented as a pair\n    of `(lookup_path_item, value_item)`, and `rebuilder` is a function\n    which, when applied to an iterable of the aforementioned value-items\n    (or some transformation thereof) returns a node that is equivalent\n    to the original (or up to a transformation of the contents).\n\n  Raises:\n    InvalidTreeNodeError: If the tree contains a node of a kind\n      that is not expected to show up.\n  '''\n\n  Examples:\n\n    (The behavior of a node-handler is somewhat nontrivial, so covering\n    two very common cases via examples is in order.)\n\n    This node-handler would allow descending into (nested)\n    instances of `list` (but not subclass instances thereof):\n\n    ```def list_node_handler(revtuple_path, obj):\n         ''' ... '''\n         if type(obj) is list:\n           return list, enumerate(obj)\n         else:\n           return None\n    ```\n\n    This node-handler would allow descending into (nested) mappings,\n    which upon rebuilding would get turned into `dict` instances:\n\n    ```def mapping_node_handler(revtuple_path, obj):\n         ''' ... '''\n         if isinstance(obj, collections.abc.Mapping):\n           # For generic mappings, we cannot rely on key- and item-iteration\n           # being guaranteed to use identical iteration-order.\n           items = list(obj.items())\n           keys = [kv[0] for kv in items]\n           return (lambda values: dict(zip(keys, values))), items\n         else:\n           return None\n    ```\n\n    A dict/mapping node-handler can of course rename keys, add or remove\n    entries, make decisions based on the item-path, or map a dict to\n    an associative list, etc.\n\n## Further Design Notes\n\nThe `pytree_map` function requests the leaf-transform and node-handler\nto be side-effect-free functions. This is both required to leave\nimplementation-side flexibility, and also follows the general LISP\nrecommendation to not abuse mapping (which should be a pure\ndata-transformation) for imperative data processing. Overall, if\na need for more general \"nested datastructures\" processing becomes\npressing, it is for the better if this leads to a proper articulation\nof the specific needs, to be addressed with appropriate design, rather\nthan abuse of functional data-transforms becoming \"a bad idiom\nthat turned into established practice\".\n\n\"\"\"\n\nimport collections.abc\nimport immutabledict\n\nimport numpy\n\nfrom typing import Any, Callable, Iterable, Iterator, TypeVar\n\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nKT = TypeVar('KT')\nNT = TypeVar('NT')\n\n\n## Type of the reverse-order-keys-to-root path.\n# (This code actually illustrates why https://xkcd.com/2483/ is very misguided.)\nRevTuplePath = tuple\n\n## Type of the `leaf_transform` function-argument used for tree-iteration.\n#\n# This would be the correct type we would have to specify here but cannot,\n# since the design of Python's static typing at the time of this writing\n# is too broken for that:\n#\n#   type LeafTransformFunc[L, R] = Callable[[RevTuplePath, L], R]\n#\n# Instead, we have to settle for...:\nLeafTransformFunc = Callable[[RevTuplePath, Any], Any]\n\n\n## Type of the `tree_node_handler` function-argument used for\n## tree-iteration and tree-mapping.\n#\n# Again, this is the correct type we would have to put here but cannot:\n#\n# type NodeHandlerFunc[KT] = (\n#   Callable[[NT],\n#            None | tuple[Callable[[Iterable[tuple[KT, NT]]], NT],\n#                         Iterator[tuple[KT, NT]]]])\n#\n# ...so, we have to instead settle for:\nNodeHandlerFunc = (\n  Callable[[RevTuplePath, NT],\n           None | tuple[Callable[[Iterable[tuple[Any, NT]]], NT],\n                        Iterator[tuple[Any, NT]]]])\n\n\nPredicate = Callable[[object], bool]\n\n\nclass InvalidTreeNodeError(ValueError):\n  \"\"\"Encountered a tree-node of invalid type.\"\"\"\n\n\ndef linearize_revtuple_path(\n    revtuple_path: RevTuplePath,\n    present_as: Callable[[Iterator[T]], U] = tuple) -> U:\n  \"\"\"Translates a revtuple path to (typically) linear form.\n\n  With default `present_as`, this will map a path of the form\n  `(key_{N}, (key_{N-1}, ..., (root, ())))` into a tuple\n  (root, ..., key_{N-1}, key_{N}).\n\n  Args:\n    revtuple_path: A linked-list-as-recursive-pairs\n      reverse-order tuple-representation of the path.\n      Path-root is `()`, and node-key `x` relative to\n      earlier path `p` is represented as `(x, p)`.\n    present_as: Callable that consumes an iterator over\n      path-pieces - with the deepest-nesting level coming last -\n      turning it into a linearized path. Defaults to `tuple`.\n\n  Returns:\n    Linearized presentation of all the node-keys in the\n    recursive-path in order, deepest-down path component coming last.\n  \"\"\"\n  pieces = []\n  todo = revtuple_path\n  while todo:\n    node, todo = todo\n    pieces.append(node)\n  return present_as(reversed(pieces))\n\n\n# This function itself has type `NodeHandlerFunc`, but Python does not\n# allow us to here simply type-annotate it like this. We cannot even\n# introduce an abbreviation for the complicated output-type,\n# since that would have to be parametric in node-type `NT` (and `KT`).\ndef everything_is_a_leaf_node_handler(\n    revtuple_path: tuple,\n    node : NT) -> (\n    None | tuple[Callable[[Iterable[tuple[Any, NT]]], NT],\n                 Iterator[tuple[Any, NT]]]):\n  \"\"\"Processes a tree-node as required by pytree-iteration and -mapping.\n\n  Interface and signature are in alignment with the requirements for a\n  \"node handler\" function explained in the module-docstring.\n\n  Args:\n    revtuple_path: the path-to-root for this node.\n    node: a tree-node.\n\n  Returns:\n    `None`, i.e. classifying any kind of node as a leaf-node.\n  \"\"\"\n  del revtuple_path, node  # Unused.\n  return None\n\n\ndef leaf_summary(path: RevTuplePath, x: object):\n  \"\"\"Produces a human-readable summary-string for a leaf-node.\n\n  Args:\n    path: revtuple representation of the path-to-root.\n    x: The leaf-value.\n  \"\"\"\n  del path  # Ignored here.\n  tx = type(x)\n  mod = tx.__module__\n  modname = mod if isinstance(mod, str) else mod.__name__\n  type_str = f'{modname}.{tx.__qualname__}'\n  repr_str = repr(x)\n  repr_abbrev = repr_str if len(repr_str) < 40 else repr_str[:40] + ' ...'\n  # On str, int, float, etc. `{type_str}(repr(x))` would actually still be\n  # a (non-literal) Python-expression that would evaluate to the original value.\n  # However, we make no promises beyond \"human-readable\".\n  return f'{type_str}({repr_abbrev})'\n\n\n# With respect to static type annotations, the limitations of Python's\n# approach to static typing really become prominently visible here.\n#\n# Different arguments have type-parameters, but since there is no way\n# to have parametric abbreviations such as `LeafTransformFunc[L, R]`,\n# the only way we would have available to express relations between\n# type-parameters would be to substitute in the not-abbreviated form of\n# `NodeHandlerFunc` and `LeafTransformFunc`, giving us something monstrous.\n# We instead here settle for \"we cannot express that `tree` must\n# have the same type as the input-type to `tree_node_handler` and use `Any`,\n# and likewise for leaf_transform and the output.\ndef pytree_leaf_iter(\n    tree: Any,\n    leaf_transform: LeafTransformFunc,\n    node_handler: NodeHandlerFunc = everything_is_a_leaf_node_handler,\n  ) -> Iterator[Any]:\n  # ...actual return type would be `Iterator[{what leaf_transform returns}]`.\n  \"\"\"Iterates over the leaves of a tree.\n\n  Args:\n    tree: The tree to iterate over.\n    leaf_transform: A callable `f` that will get applied\n      as `f(revtuple_path, leaf)`, where `revtuple_path`\n      is the revtuple representation of the path to the\n      leaf from the root.\n    node_handler: A \"node handler\" (see module docstring)\n      that processes nodes encountered during iterative traversal.\n\n  Yields:\n    Value of `leaf_transform(p, x)`, where `x` is the current leaf\n    and `p` is its revtuple-path to the root.\n  \"\"\"\n  # Note: Exit points for the code below are in non-obvious places\n  # and hence marked with \" # ***EXIT***\".\n  #\n  # Doing iteration properly is slightly nontrivial.\n  # One may be tempted to go for a very simple recursive implementation\n  # (with an extra pre-final `path` argument to `pytree_iter`):\n  #\n  # maybe_substructure = node_handler(path, tree)\n  # if maybe_substructure is None:\n  #   # We are looking at a leaf-node.\n  #   yield leaf_transform(path, tree)\n  # else:\n  #   _, contents_iter = maybe_substructure\n  #   for k, v in contents_iter:\n  #     yield from pytree_iter(v, leaf_transform, (k, path), node_handler)\n  #\n  # That, however, would be flawed, since there is no a priori reason\n  # why a pytree may not be a very deeply nested structure - such as a\n  # long linked list. That would then risk raising `RecursionError`,\n  # and since Python by design(!) does not perform tail call elimination\n  # or any other kind of advanced CPS transforms, there is no recursive\n  # solution here. So, to do this properly, we have to do this iteratively.\n  #\n  # We are facing an annoying situation here: If `tree` itself is a leaf,\n  # we have two options: (a) wrapping it up in a one-node tree\n  # and processing that, or (b) special-casing \"root is a leaf\".\n  # Option (b) leads to some mild node-processing code-duplication\n  # for a single node (the root).\n  # Option (a) requires having special cases for node-processing that\n  # get looked at for every tree node. We go with option (b) here.\n  maybe_substructure = node_handler((), tree)\n  if maybe_substructure is None:\n    # The tree itself is a leaf.\n    yield leaf_transform((), tree)\n    return  # ***EXIT***\n  # Otherwise, we are looking at a tree.\n  _, contents_iter = maybe_substructure\n  current_revtuple_path = ()\n  work_to_do = [contents_iter]\n  # Otherwise-unreachable sentinel for reliably identifying\n  # iterator-exhaustion without using exceptions:\n  sentinel = object()\n  while True:\n    current_iter = work_to_do[-1]\n    maybe_next_item = next(current_iter, sentinel)\n    if maybe_next_item is sentinel:\n      # We are done at this level.\n      work_to_do.pop()\n      if not work_to_do: return  # ***EXIT***\n      current_revtuple_path = current_revtuple_path[1]\n    else:\n      path_piece, subtree = maybe_next_item\n      extended_revtuple_path = (path_piece, current_revtuple_path)\n      maybe_subtree_substructure = node_handler(extended_revtuple_path, subtree)\n      if maybe_subtree_substructure is None:  # Case: subtree is a leaf.\n        yield leaf_transform(extended_revtuple_path, subtree)\n      else:  # Case: subtree is a tree.\n        current_revtuple_path = (path_piece, current_revtuple_path)\n        _, items_iter = maybe_subtree_substructure\n        work_to_do.append(items_iter)\n\n\n# The current design approach here would be appropriate for\n# applying leaf-transforms while retaining the structure of the tree -\n# which closely corresponds to e.g. a (a -> b) -> (Tree a -> Tree b) functor.\n#\n# It is not entirely clear whether this is the abstraction that we should\n# consider as being appropriately generic to flesh out explicitly - rather\n# than starting from a more general approach of which this then is a special\n# case. Some background: https://ncatlab.org/nlab/show/recursion+scheme\n#\n# On the other hand, there is a lot of flexibility via whatever\n# node-rebuilder a node-handler produces - this can do quite some reshaping\n# of a tree, including dropping or duplicating nodes.\ndef pytree_map(\n    tree: Any,\n    leaf_transform,\n    node_handler: NodeHandlerFunc = everything_is_a_leaf_node_handler,\n  ):\n  \"\"\"Maps a (potentially nested) Python value to another such value.\n\n  Args:\n    tree: The Python-object to be mapped.\n    leaf_transform: A callable `f` that will get applied\n      as `f(revtuple_path, leaf)`, where `revtuple_path`\n      is the revtuple representation of the path to the\n      leaf from the root. Must be side effect free.\n    node_handler: A \"node handler\" (see module docstring)\n      that processes nodes encountered during iterative traversal.\n      Must be side effect free.\n\n  Returns:\n    The outcome of translating `tree`.\n  \"\"\"\n  # Note: Exit points for the code below are in non-obvious places\n  # and hence marked with \" # ***EXIT***\".\n  #\n  # Otherwise-inaccessible sentinel object, for reliably identifying\n  # missing-values via identity-check against sentinel lookup-default.\n  sentinel = object()\n  # Code structure mostly follows pytree_leaf_iter.\n  maybe_substructure = node_handler((), tree)\n  if maybe_substructure is None:\n    return leaf_transform((), tree)  # ***EXIT***\n  rebuilder, items_iter = maybe_substructure\n  current_revtuple_path = ()\n  # Per-level, we have a triplet of:\n  # (rebuilder, remaining_items_to_iterate_over, processed).\n  parts_for_assembly = [(rebuilder, items_iter, [])]\n  while True:\n    this_rebuilder, this_items_iter, this_done_pieces = parts_for_assembly[-1]\n    maybe_next_item = next(this_items_iter, sentinel)\n    if maybe_next_item is sentinel:\n      # We are done with all the items for this level.\n      parts_for_assembly.pop()\n      built_iter = this_rebuilder(this_done_pieces)\n      if not parts_for_assembly:  # No outer structure, so at-top-level.\n        return built_iter  # ***EXIT***\n      else:  # We have outer structure.\n        parts_for_assembly[-1][-1].append(built_iter)\n        current_revtuple_path = current_revtuple_path[1]\n        continue  # ...with next is-the-final-item-complete-check.\n    else:\n      # More constituents of the current item.\n      path_piece, subtree_item = maybe_next_item\n      extended_revtuple_path = (path_piece, current_revtuple_path)\n      maybe_subtree_substructure = node_handler(\n          extended_revtuple_path,\n          subtree_item)\n      if maybe_subtree_substructure is None:\n        this_done_pieces.append(\n            leaf_transform(extended_revtuple_path, subtree_item))\n      else:\n        # We have a subtree.\n        subtree_rebuilder, subtree_items_iter = maybe_subtree_substructure\n        current_revtuple_path = (path_piece,\n                                 current_revtuple_path)\n        parts_for_assembly.append(\n          (subtree_rebuilder, subtree_items_iter, []))\n\n\ndef deep_freeze(\n    tree,\n    *,\n    is_mapping : Predicate = lambda x: isinstance(x, collections.abc.Mapping),\n    is_set : Predicate = lambda x: isinstance(x, collections.abc.Set),\n    is_sequence : Predicate = lambda x: isinstance(x, (list, tuple)),\n    leaf_fn: Callable[[Any], Any] = lambda x: x,\n    ):\n  \"\"\"Recursively freezes Set/Mapping/List/Tuple structures.\n\n  Args:\n    tree: The potentially deeply-nested object to deep-freeze.\n    is_mapping: Callable that decides whether a sub-object is a mapping.\n      Defaults to an `isinstance()` check for `collections.abc.Mapping`.\n    is_set: Callable that decides whether a sub-object is a set.\n      Defaults to an `isinstance()` check for `collections.abc.Set`.\n    is_sequence: Callable that decides whether a sub-object is a sequence.\n      Defaults to a check for being a `tuple` or `list` instance.\n    leaf_fn: Function to use for translating non-mapping/set/sequence\n      instances.\n\n  Returns:\n    Translated-to-deeply-immutable form of `tree`.\n  \"\"\"\n  idict = immutabledict.immutabledict\n  def freeze_node_handler(path, x):\n    if is_set(x):\n      return frozenset, ((None, y) for y in x)\n    if is_mapping(x):\n      # Mappings already have hashable, so\n      # (should-be-)deeply-immutable keys.\n      # Hence, we only need to deep-freeze the values.\n      #\n      # Note that non-`dict` mappings might not guarantee\n      # to respect iteration-order, so we have to be careful here:\n      items = list(x.items())\n      keys = [kv[0] for kv in items]\n      values = [kv[1] for kv in items]\n      return ((lambda ys: idict(zip(keys, ys))),\n              iter(items))\n    if is_sequence(x):\n      return tuple, enumerate(iter(x))\n    # Otherwise, this should not be traversed.\n    return None\n  def leaf_transform(revtuple_path, value):\n    del revtuple_path  # Unused.\n    return leaf_fn(value)\n  return pytree_map(tree, leaf_transform, freeze_node_handler)\n",
        "compression/python/pytree/pytree_transforms_test.py": "\"\"\"Basic tests for 'algebraic data type based pytree' transformations.\"\"\"\n\n\nimport collections.abc\nimport sys\nimport unittest\n\nimport pytree_transforms\n\n\ndef _get_deep_pytree(packaging_fn, bottom, depth):\n  current = bottom\n  for n in reversed(range(depth)):\n    current = packaging_fn(n, current)\n  return current\n\n\ndef _dict_node_handler(p, d):\n  del p  # Unused.\n  if isinstance(d, dict):\n    keys = d.keys()\n    newdict = lambda vals: dict(zip(keys, vals))\n    return (newdict, iter(d.items()))\n  else:\n    return None\n\n\nclass PyTreeTest(unittest.TestCase):\n  \"\"\"Basic correctness validation tests for PyTree transformations.\"\"\"\n\n  def test_linearize_revtuple_path(self):\n    \"\"\"Tests guarantees given by `linearize_revtuple_path`.\"\"\"\n    linearize_revtuple_path = pytree_transforms.linearize_revtuple_path\n    with self.subTest(guarantee='empty'):\n      self.assertEqual(linearize_revtuple_path(()), ())\n    with self.subTest(guarantee='typical'):\n      self.assertEqual(linearize_revtuple_path((30, (20, (10, ())))),\n                       (10, 20, 30))\n    with self.subTest(guarantee='present_as'):\n      self.assertEqual(\n          linearize_revtuple_path(\n              (30, (20, (10, ()))), present_as=list),\n          [10, 20, 30])\n\n  def test_everything_is_a_leaf_node_handler(self):\n    \"\"\"Tests guarantees given by `everything_is_a_leaf_node_handler`.\"\"\"\n    everything_is_a_leaf_node_handler = (\n        pytree_transforms.everything_is_a_leaf_node_handler)\n    self.assertEqual(everything_is_a_leaf_node_handler((), 'abc'),\n                     None)\n    self.assertEqual(everything_is_a_leaf_node_handler(('b', ()),\n                                                       dict(a=3)),\n                     None)\n\n  def test_leaf_summary(self):\n    \"\"\"Tests guarantees given by `leaf_summary`.\"\"\"\n    # Since the docstring only guarantees \"a human-readable presentation\",\n    # we can and should only do loose checks.\n    thing = (5678, 9531)\n    summary = pytree_transforms.leaf_summary(('key', ()), thing)\n    self.assertIsInstance(summary, str)\n    self.assertIn(str(thing[0]), summary)\n    self.assertIn(str(thing[1]), summary)\n\n  def test_pytree_leaf_iter(self):\n    \"\"\"Tests guarantees given by `pytree_leaf_iter`.\"\"\"\n    pytree_leaf_iter = pytree_transforms.pytree_leaf_iter\n    def leaf_transform(path, leaf):\n      return repr(leaf) if path and path[0].startswith('R') else leaf\n    with self.subTest(guarantee='returns_iterator'):\n      result = pytree_leaf_iter(7, leaf_transform, _dict_node_handler)\n      self.assertIsInstance(result, collections.abc.Iterator)\n    with self.subTest(guarantee='totally_empty'):\n      result = list(pytree_leaf_iter({}, leaf_transform, _dict_node_handler))\n      self.assertEqual(result, [])\n    with self.subTest(guarantee='no_leaves'):\n      result = list(pytree_leaf_iter(dict(a={}),\n                                     leaf_transform, _dict_node_handler))\n      self.assertEqual(result, [])\n    with self.subTest(guarantee='is_leaf'):\n      result = list(pytree_leaf_iter(777, leaf_transform, _dict_node_handler))\n      self.assertEqual(result, [777])\n    with self.subTest(guarantee='generic'):\n      result = list(pytree_leaf_iter(\n          dict(n0=dict(n01=dict(n012=1002,\n                                n013=1003,\n                                Rn014=1004,\n                                ),\n                       n02=1005),\n               n5=1006),\n          leaf_transform, _dict_node_handler))\n      self.assertEqual(result, [1002, 1003, '1004', 1005, 1006])\n    with self.subTest(guarantee='with_keys'):\n      result = list(pytree_leaf_iter(\n          dict(n0=dict(n01=dict(n012=1002,\n                                n013=1003)),\n               n1=1004),\n          lambda p, s: (pytree_transforms.linearize_revtuple_path(p), s),\n          _dict_node_handler))\n      self.assertEqual(result,\n                       [(('n0', 'n01', 'n012'), 1002),\n                        (('n0', 'n01', 'n013'), 1003),\n                        (('n1',), 1004)])\n\n  def test_pytree_map(self):\n    \"\"\"Tests guarantees given by `pytree_map`.\"\"\"\n    pytree_map = pytree_transforms.pytree_map\n    leaf_transform = lambda p, s: repr(s)\n    tree1 = dict(t0=dict(t10=1001,\n                     t11=dict(t110=1002,\n                              t111=1003),\n                     t12=dict(t120=1004,\n                              t121=1005,\n                              t122=1006)),\n             t1=1007)\n    with self.subTest(guarantee='no_leaves'):\n      result = pytree_map(dict(a={}),\n                          leaf_transform,\n                          _dict_node_handler)\n      self.assertEqual(result, dict(a={}))\n    with self.subTest(guarantee='is_leaf'):\n      result = pytree_map(777, leaf_transform, _dict_node_handler)\n      self.assertEqual(result, '777')\n    with self.subTest(guarantee='generic'):\n      result = pytree_map(tree1, leaf_transform, _dict_node_handler)\n      self.assertEqual(result['t0']['t10'], '1001')\n\n  def test_deeply_nested(self):\n    \"\"\"Tests correct behavior on deeply-nested data structures.\"\"\"\n    pytree_leaf_iter = pytree_transforms.pytree_leaf_iter\n    pytree_map = pytree_transforms.pytree_map\n    #\n    depth = max(10**5, sys.getrecursionlimit() + 100)\n    deep_tree = _get_deep_pytree(lambda n, t: {n: t},\n                                 'leaf', depth)\n    with self.subTest(function='pytree_leaf_iter'):\n      leaves = list(pytree_leaf_iter(deep_tree,\n                                     lambda p, s: s.upper(),\n                                     _dict_node_handler))\n      self.assertEqual(leaves, ['LEAF'])\n    with self.subTest(function='pytree_map'):\n      mapped_deep_tree = pytree_map(deep_tree,\n                                    lambda p, s: s,\n                                    _dict_node_handler)\n      self.assertIsInstance(mapped_deep_tree, dict)\n    with self.subTest(function='combined'):\n      leaves = list(\n          pytree_leaf_iter(\n              pytree_map(deep_tree,\n                         lambda p, s: s.capitalize(),\n                         _dict_node_handler),\n              lambda p, s: s + s,\n              _dict_node_handler))\n      self.assertEqual(leaves, ['LeafLeaf'])\n\n  def test_deep_freeze(self):\n    \"\"\"Tests guarantees given by `deep_freeze`.\"\"\"\n    frozen = pytree_transforms.deep_freeze(\n        dict(a=[1001, 1002, dict(b=(1003, [1004, {1005, 1006}]))]))\n    self.assertIsInstance(frozen, collections.abc.Mapping)\n    self.assertNotIsInstance(frozen, collections.abc.MutableMapping)\n    self.assertIsInstance(frozen['a'], tuple)\n    # `frozen` is hashable, and hashes to an integer.\n    self.assertIsInstance(hash(frozen), int)\n\n\nif __name__ == '__main__':\n  unittest.main()\n",
        "python/convert_from_safetensors.py": "# Copyright 2025 Google LLC\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Convert a PaliGemma[1/2] model from SafeTensors to gemma.cpp format.\"\"\"\n# Tested with:\n# - PG1: huggingface.co/google/paligemma-3b-pt-224\n# - PG1: huggingface.co/merve/paligemma_vqav2\n# - PG2: huggingface.co/google/paligemma2-3b-pt-448\n# - PG2: huggingface.co/merve/paligemma2-3b-vqav2\n# The last one above is a Lora model, so the merged weights were saved using:\n# model_name = \"google/paligemma2-3b-pt-448\"\n# lora_weights_path = \"merve/paligemma2-3b-vqav2\"\n# model = PaliGemmaForConditionalGeneration.from_pretrained(model_name)\n# model = PeftModel.from_pretrained(model, lora_weights_path)\n# model = model.merge_and_unload()\n# model.save_pretrained(\"/tmp/lora-model\")\n\nfrom collections.abc import Sequence\nimport csv\nimport json\nimport os\nimport sys\nfrom typing import Any, Dict\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport safetensors\nimport torch\n\nfrom compression.python import compression\nfrom python import configs\n\n\ndef flatten_f32(x: np.ndarray) -> np.ndarray:\n  \"\"\"Flattens an array.\n\n  Args:\n    x: input array\n\n  Returns:\n    Flattened array.\n  \"\"\"\n  return x.ravel().astype(np.float32, copy=False)\n\n\ndef compute_scale(x: np.ndarray) -> float:\n  \"\"\"Rescales weight tensor to fit max magnitude within 1.875.\n\n  Args:\n    x: input array\n\n  Returns:\n    Scale value (1.0 means no rescaling).\n  \"\"\"\n  magnitude = np.max(np.abs(x))\n  return max(1.0, magnitude / 1.875)\n\n\ndef _is_float_param(param_name: str) -> bool:\n  \"\"\"Returns whether the tensor should be stored as float32.\"\"\"\n  for prefix in [\n      \"img_pos_emb\",\n      \"attn_out_b\",\n      \"linear_0_b\",\n      \"linear_1_b\",\n      \"qkv_ein_b\",\n      \"img_emb_bias\",\n      \"img_head_bias\",\n  ]:\n    if param_name.startswith(prefix):\n      return True\n  return False\n\n\ndef _is_bf16_param(param_name: str) -> bool:\n  \"\"\"Returns whether the tensor should be stored as bf16.\"\"\"\n  for prefix in [\"pre_\", \"post_\", \"c_\", \"img_head_kernel\"]:\n    if param_name.startswith(prefix):\n      return True\n  return False\n\n\n# Layernorm names are slightly confusing in HF transformers between versions.\n# Gemma layernorms:\n# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py\n# input_layernorm attn residual post_attention_layernorm mlp residual\n# Gemma2 layernorms:\n# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma2/modeling_gemma2.py\n# input_layernorm attn post_attention_layernorm residual\n#   pre_feedforward_layernorm mlp post_feedforward_layernorm residual\n# Note that post_attention_layernorm denotes something different.\n# For comparison, the Big Vision Gemma2 keeps the same name for the same norm:\n# pre_attention_norm attn [post_attention_norm] residual\n#   pre_ffw_norm mlp [post_ffw_norm] residual\n\n\n# Tuples correspond to (transformers-name, shape, sbs-name).\n# The qkv-einsum weights are part of llm-layers but are handled separately and\n# thus not included in the list.\ndef _get_layer_config(dims: Dict[str, Any]):\n  \"\"\"Returns a dictionary of layer configurations.\n\n  Args:\n    dims: A dictionary of (mostly) dimension values.\n\n  Returns:\n    A dictionary of layer configurations.\n  \"\"\"\n  model_dim = dims[\"model_dim\"]\n  hidden_dim = dims[\"hidden_dim\"]\n  vit_seq_len = dims[\"vit_seq_len\"]\n  config = {\n      \"llm-non-layers\": [\n          (\n              \"language_model.model.embed_tokens.weight\",\n              (257152, model_dim),\n              \"c_embedding\",\n          ),\n          (\"language_model.model.norm.weight\", (model_dim,), \"c_final_norm\"),\n      ],\n      \"llm-layers\": [\n          (\n              \"language_model.model.layers.%d.mlp.down_proj.weight\",\n              (model_dim, hidden_dim),\n              \"linear_w\",\n          ),\n      ],\n      \"img-non-layers\": [\n          (\n              \"vision_tower.vision_model.post_layernorm.bias\",\n              (1152,),\n              \"enc_norm_bias\",\n          ),\n          (\n              \"vision_tower.vision_model.post_layernorm.weight\",\n              (1152,),\n              \"enc_norm_scale\",\n          ),\n          (\n              \"vision_tower.vision_model.embeddings.patch_embedding.bias\",\n              (1152,),\n              \"img_emb_bias\",\n          ),\n          (\n              \"vision_tower.vision_model.embeddings.patch_embedding.weight\",\n              (1152, 14, 14, 3),\n              \"img_emb_kernel\",\n          ),\n          (\"multi_modal_projector.linear.bias\", (model_dim,), \"img_head_bias\"),\n          (\n              \"multi_modal_projector.linear.weight\",\n              (model_dim, 1152),\n              \"img_head_kernel\",\n          ),\n          (\n              \"vision_tower.vision_model.embeddings.position_embedding.weight\",\n              (vit_seq_len, 1152),\n              \"img_pos_emb\",\n          ),\n      ],\n      \"img-layers\": [\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.layer_norm1.bias\",\n              (1152,),\n              \"ln_0_bias\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.layer_norm1.weight\",\n              (1152,),\n              \"ln_0_scale\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.layer_norm2.bias\",\n              (1152,),\n              \"ln_1_bias\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.layer_norm2.weight\",\n              (1152,),\n              \"ln_1_scale\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.mlp.fc1.bias\",\n              (4304,),\n              \"linear_0_b\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.mlp.fc1.weight\",\n              (4304, 1152),\n              \"linear_0_w\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.mlp.fc2.bias\",\n              (1152,),\n              \"linear_1_b\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.mlp.fc2.weight\",\n              (1152, 4304),\n              \"linear_1_w\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.self_attn.out_proj.bias\",\n              (1152,),\n              \"attn_out_b\",\n          ),\n          (\n              \"vision_tower.vision_model.encoder.layers.%d.self_attn.out_proj.weight\",\n              (1152, 16 * 72),\n              \"attn_out_w\",\n          ),\n      ],\n  }\n  if dims[\"has_post_norm\"]:  # See longer comment above.\n    config[\"llm-layers\"] += [\n        (\n            \"language_model.model.layers.%d.input_layernorm.weight\",\n            (model_dim,),\n            \"pre_att_ns\",\n        ),\n        (\n            \"language_model.model.layers.%d.pre_feedforward_layernorm.weight\",\n            (model_dim,),\n            \"pre_ff_ns\",\n        ),\n        (\n            \"language_model.model.layers.%d.post_attention_layernorm.weight\",\n            (model_dim,),\n            \"post_att_ns\",\n        ),\n        (\n            \"language_model.model.layers.%d.post_feedforward_layernorm.weight\",\n            (model_dim,),\n            \"post_ff_ns\",\n        ),\n    ]\n  else:\n    config[\"llm-layers\"] += [\n        (\n            \"language_model.model.layers.%d.input_layernorm.weight\",\n            (model_dim,),\n            \"pre_att_ns\",\n        ),\n        (\n            \"language_model.model.layers.%d.post_attention_layernorm.weight\",\n            (model_dim,),\n            \"pre_ff_ns\",\n        ),\n    ]\n  return config\n\n\ndef _get_dimensions(params):\n  \"\"\"Returns a dictionary of dimension values.\n\n  Args:\n    params: A dictionary with parameters.\n\n  Returns:\n    A dictionary of dimension values.\n  \"\"\"\n  dims = {}\n  # For PG1 and PG2-{3B,10B} head_dim is 256, would need update for PG2-28B.\n  # Unfortunately not easily available in any of the input sizes.\n  dims[\"head_dim\"] = 256\n  dims[\"model_dim\"] = params[\"multi_modal_projector.linear.bias\"].shape[0]\n  dims[\"hidden_dim\"] = params[\n      \"language_model.model.layers.0.mlp.gate_proj.weight\"\n  ].shape[0]\n  dims[\"num_heads\"] = (\n      params[\"language_model.model.layers.0.self_attn.q_proj.weight\"].shape[0]\n      // dims[\"head_dim\"]\n  )\n  dims[\"vit_seq_len\"] = params[\n      \"vision_tower.vision_model.embeddings.position_embedding.weight\"\n  ].shape[0]\n  dims[\"num_llm_layers\"] = len(\n      set([k for k in params.keys() if \"input_layernorm.weight\" in k])\n  )\n  dims[\"has_post_norm\"] = (\n      \"language_model.model.layers.0.post_feedforward_layernorm.weight\"\n      in params\n  )\n  return dims\n\n\ndef export_paligemma_sbs(\n    model_specifier: str,\n    load_path: str,\n    tokenizer_file: str,\n    csv_file: str,\n    sbs_file: str,\n) -> None:\n  \"\"\"Exports sbs file from paligemma safetensors file(s).\"\"\"\n\n  # If this is a multi-part checkpoint, get the list of files from the json.\n  if load_path.endswith(\".json\"):\n    with open(load_path, \"r\") as f:\n      j_obj = json.load(f)\n    files = list(set(j_obj[\"weight_map\"].values()))\n    files = [os.path.join(os.path.dirname(load_path), f) for f in files]\n  else:\n    files = [load_path]\n\n  # Read the parameters from the files.\n  params = {}\n  for file in files:\n    with safetensors.safe_open(file, framework=\"pt\") as f:\n      for k in f.keys():\n        params[k] = f.get_tensor(k)\n        print(k, params[k].shape, params[k].view(-1)[0].item())\n\n  # See https://tinyurl.com/paligemmavocab - HF transformers extends the\n  # embedding matrix by 64. Undo that here.\n  params[\"language_model.model.embed_tokens.weight\"] = params[\n      \"language_model.model.embed_tokens.weight\"\n  ][:-64]\n\n  writer = compression.SbsWriter(sbs_file)\n  metadata = []\n  scales = {}\n  dims = _get_dimensions(params)\n  layer_config = _get_layer_config(dims)\n\n  # Adds a parameter with expected shape to the writer.\n  def add_data(param_name, data, expected_shape, sbs_name, layer_index=None):\n    # Check shape.\n    if not isinstance(expected_shape, tuple):\n      expected_shape = (expected_shape,)\n    print(f\"Writing {param_name} with shape {data.shape} e:{expected_shape}\")\n    assert data.shape == expected_shape, param_name\n\n    # Here we assume that the read data is a torch tensor and then convert it to\n    # a numpy array.\n    assert isinstance(data, torch.Tensor)\n    data = data.to(torch.float32).numpy()\n    data = np.array(data)\n\n    # Add the layer index to the param name and sbs name if needed.\n    if layer_index is not None:\n      param_name = param_name % layer_index\n      sbs_name = sbs_name + f\"_{layer_index}\"\n\n    # Flatten the data and get scale.\n    value = flatten_f32(data)\n    scale = compute_scale(value)\n    both_names = param_name + \"::\" + sbs_name\n    print(f\"Param {both_names} has scale {scale}\")\n    metadata.append((both_names, data.dtype, data.shape, scale))\n\n    # Determine the type as which to insert.\n    if _is_float_param(sbs_name):\n      packed = configs.Type.kF32\n      print(f\"Inserting {both_names} as float (f32) (no scaling)\")\n    elif _is_bf16_param(sbs_name) or param_name.startswith(\"vision_tower\"):\n      packed = configs.Type.kBF16\n      print(f\"Inserting {both_names} as BF16 (no scaling)\")\n    else:\n      packed = configs.Type.kSFP\n      # Assumes that all scales are 1.0 for SFP. Consider adding scales.\n      # They would still need to be written, but would be collected here.\n      assert scale == 1.0, f\"Scale for {both_names} is not 1.0\"\n      if scale != 1.0:\n        value = value / scale\n      scales[sbs_name] = scale  # Unused at the moment.\n      print(f\"Inserting {both_names} as SFP with scale {scale}\")\n    sys.stdout.flush()\n\n    # Add the data to the writer.\n    info = configs.TensorInfo()\n    info.name = sbs_name\n    info.shape = data.shape\n    writer.insert(sbs_name, value, packed, info)\n\n  def add_qkv_einsum(i):  # Handle qkv for layer i.\n    name = \"language_model.model.layers.%d.self_attn.q_proj.weight\"  # (N*H, D)\n    q_i = params.pop(name % i)\n    (nh, d) = q_i.shape\n    h = dims[\"head_dim\"]\n    n = dims[\"num_heads\"]\n    assert nh == n * h\n    assert dims[\"model_dim\"] == d\n    q_i = q_i.reshape(n, h, d)\n    name = \"language_model.model.layers.%d.self_attn.k_proj.weight\"  # (K*H, D)\n    k_i = params.pop(name % i)\n    kh = k_i.shape[0]\n    k = kh // h\n    assert k_i.shape[1] == d\n    k_i = k_i.reshape(k, h, d)\n    name = \"language_model.model.layers.%d.self_attn.v_proj.weight\"  # (K*H, D)\n    v_i = params.pop(name % i)\n    assert v_i.shape[0] == kh\n    assert v_i.shape[1] == d\n    v_i = v_i.reshape(k, h, d)\n    # Stack and reshape KV to interleave (k,v), (k,v), ...\n    stacked = torch.stack((k_i, v_i), dim=0)  # (2, K, H, D)\n    transposed = stacked.transpose(0, 1)  # (K, 2, H, D)\n    reshaped = transposed.reshape(2 * k, h, d)  # (2K, H, D)\n    # Concatenate Q and KV to get the full qkv.\n    qkv_i = torch.cat([q_i, reshaped], dim=0)\n    name = \"language_model.model.layers.%d.self_attn.qkv_proj.weight\"\n    expected_shape = (n + 2 * k, h, d)  # (N+2K, H, D)\n    sbs_name = \"qkv_ein\"\n    add_data(name, qkv_i, expected_shape, sbs_name, i)\n\n  def add_att_einsum(i):  # Handle att_ein for layer i.\n    name = \"language_model.model.layers.%d.self_attn.o_proj.weight\"  # (D, N*H)\n    o_i = params.pop(name % i)\n    (d, nh) = o_i.shape\n    h = dims[\"head_dim\"]\n    n = dims[\"num_heads\"]\n    assert nh == n * h\n    o_i = o_i.reshape(d, n, h).permute(1, 0, 2)  # (D, N, H) -> (N, D, H)\n    expected_shape = (n, d, h)\n    sbs_name = \"att_ein\"\n    add_data(name, o_i, expected_shape, sbs_name, i)\n\n  # Join gate and up projection weights to gating_einsum for layer i.\n  def add_gating_einsum(i):\n    name = \"language_model.model.layers.%d.mlp.gate_proj.weight\"\n    gate_i = params.pop(name % i)\n    f, d = gate_i.shape\n    assert dims[\"hidden_dim\"] == f\n    assert dims[\"model_dim\"] == d\n    name = \"language_model.model.layers.%d.mlp.up_proj.weight\"\n    up_i = params.pop(name % i)\n    assert up_i.shape == gate_i.shape\n    gating_einsum_i = torch.stack([gate_i, up_i], dim=0)\n    name = \"language_model.model.layers.%d.mlp.gating_einsum.weight\"\n    expected_shape = (2, f, d)\n    sbs_name = \"gating_ein\"\n    add_data(name, gating_einsum_i, expected_shape, sbs_name, i)\n\n  # Handle the q and kv einsum parts for layer i in the ViT - merge into qkv.\n  def add_vit_qkv_einsum(i):\n    # Weights first.\n    prefix = \"vision_tower.vision_model.encoder.layers.%d.self_attn\"\n    name = prefix + \".q_proj.weight\"  # (16 * 72, 1152)\n    q_i = params.pop(name % i)\n    q_i = q_i.reshape(16, 72, 1152)\n    name = prefix + \".k_proj.weight\"  # (16 * 72, 1152)\n    k_i = params.pop(name % i)\n    k_i = k_i.reshape(16, 72, 1152)\n    name = prefix + \".v_proj.weight\"  # (16 * 72, 1152)\n    v_i = params.pop(name % i)\n    v_i = v_i.reshape(16, 72, 1152)\n    qkv_i, shape = torch.stack([q_i, k_i, v_i], dim=1), (16, 3, 72, 1152)\n    name = prefix + \".qkv_proj.weight\"\n    sbs_name = \"qkv_ein_w\"\n    add_data(name, qkv_i, shape, sbs_name, i)\n    # Now the biases.\n    name = prefix + \".q_proj.bias\"  # (16 * 72)\n    q_i = params.pop(name % i)\n    q_i = q_i.reshape(16, 72)\n    name = prefix + \".k_proj.bias\"  # (16 * 72)\n    k_i = params.pop(name % i)\n    k_i = k_i.reshape(16, 72)\n    name = prefix + \".v_proj.bias\"  # (16 * 72)\n    v_i = params.pop(name % i)\n    v_i = v_i.reshape(16, 72)\n    qkv_i, shape = torch.stack([q_i, k_i, v_i], dim=1), (16, 3, 72)\n    name = prefix + \".qkv_proj.bias\"\n    sbs_name = \"qkv_ein_b\"\n    add_data(name, qkv_i, shape, sbs_name, i)\n\n  # Handle the image embedding kernel transpose.\n  name = \"vision_tower.vision_model.embeddings.patch_embedding.weight\"\n  assert params[name].shape == (\n      1152,\n      3,\n      14,\n      14,\n  )\n  params[name] = params[name].permute(0, 2, 3, 1)\n\n  # Add the non-layer params.\n  for name, shape, sbs_name in (\n      layer_config[\"llm-non-layers\"] + layer_config[\"img-non-layers\"]\n  ):\n    add_data(name, params.pop(name), shape, sbs_name)\n\n  # Go through the LLM layers and add the weights.\n  for i in range(dims[\"num_llm_layers\"]):\n    add_att_einsum(i)\n    add_gating_einsum(i)\n    for name, shape, sbs_name in layer_config[\"llm-layers\"]:\n      add_data(name, params.pop(name % i), shape, sbs_name, i)\n    add_qkv_einsum(i)\n\n  # Go through the Vit layers and add the weights.\n  for i in range(27):\n    for name, shape, sbs_name in layer_config[\"img-layers\"]:\n      add_data(name, params.pop(name % i), shape, sbs_name, i)\n    add_vit_qkv_einsum(i)\n\n  assert not params, \"Some params were not used: %s\" % params.keys()\n\n  # Write everything to the sbs file.\n  assert model_specifier.startswith(\"paligemma\")\n  sbs_config = configs.ModelConfig(model_specifier)\n  writer.write(sbs_config, tokenizer_file)\n\n  # Write the metadata for manual inspection.\n  with open(csv_file, \"w\") as csv_handle:\n    csv.writer(csv_handle).writerows(metadata)\n\n\n_MODEL_SPECIFIER = flags.DEFINE_string(\n    \"model_specifier\",\n    None,\n    \"String specifying model, size, weight, wrapping (ModelConfig.Specifier)\",\n    required=True,\n)\n\n_LOAD_PATH = flags.DEFINE_string(\n    \"load_path\",\n    None,\n    \"Path to the safetensors index.json file to read\",\n    required=True,\n)\n_TOKENIZER_FILE = flags.DEFINE_string(\n    \"tokenizer_file\",\n    \"/tmp/tokenizer.spm\",\n    \"Path to the tokenizer file to read and embed\",\n)\n_METADATA_FILE = flags.DEFINE_string(\n    \"metadata_file\",\n    \"/tmp/gemmacpp.csv\",\n    \"Path to the metadata file to write\",\n)\n_SBS_FILE = flags.DEFINE_string(\n    \"sbs_file\",\n    \"/tmp/gemmacpp.sbs\",\n    \"Path to the sbs file to write\",\n)\n\n\ndef main(argv: Sequence[str]) -> None:\n  if len(argv) > 1:\n    raise app.UsageError(\"Too many command-line arguments.\")\n  logging.use_python_logging()\n  logging.set_verbosity(logging.INFO)\n  model_specifier = _MODEL_SPECIFIER.value\n  load_path = _LOAD_PATH.value\n  tokenizer_file = _TOKENIZER_FILE.value\n  metadata_file = _METADATA_FILE.value\n  sbs_file = _SBS_FILE.value\n\n  logging.info(\n      \"\\n====\\nReading %s from %s and %s, writing to %s\\n====\",\n      model_specifier,\n      load_path,\n      tokenizer_file,\n      sbs_file,\n  )\n  export_paligemma_sbs(\n      model_specifier, load_path, tokenizer_file, metadata_file, sbs_file\n  )\n\n\nif __name__ == \"__main__\":\n  app.run(main)\n",
        "python/run_example.py": "# Copyright 2024 Google LLC\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple example of using the gemma.cpp Python wrapper.\"\"\"\n\nfrom collections.abc import Sequence\nimport os\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\n\nfrom python import gemma\n\n\n_MODEL_DIR = flags.DEFINE_string(\n    \"model_dir\",\n    \"\",\n    \"Path to the Gemma model directory.\",\n)\n\n_PROMPT = flags.DEFINE_string(\n    \"prompt\",\n    \"Write an email to the moon.\",\n    \"Prompt to generate text with.\",\n)\n\n\ndef main(argv: Sequence[str]) -> None:\n  if len(argv) > 1:\n    raise app.UsageError(\"Too many command-line arguments.\")\n\n  tokenizer_path = os.path.join(_MODEL_DIR.value, \"tokenizer.spm\")\n  weights_path = os.path.join(_MODEL_DIR.value, \"gemma2-2b-it-sfp.sbs\")\n  print(f\"Loading model from {tokenizer_path} and {weights_path}\")\n  model = gemma.GemmaModel(\n      tokenizer_path=tokenizer_path,\n      weights_path=weights_path,\n      max_threads=24,\n  )\n\n  prompt = _PROMPT.value\n  print(f\"Running example with prompt='{prompt}'\")\n  output = model.generate(prompt)\n  print(f\"Generated output:\\n{output}\")\n\n  def callback(tok, _):\n    s = model.detokenize([tok])\n    print(s, end=\"\", flush=True)\n    return True\n\n  print(f\"\\n\\nRunning example with streaming callback, prompt='{prompt}'\")\n  print(\"Generating output:\\n\")\n  model.generate_ex(prompt, callback, skip_prompt=True)\n\n  prompts = [\n      prompt,\n      \"Tell me a joke.\",\n      \"Please recite the first paragraph of the Declaration of Independence.\",\n      prompt,\n  ]\n  print(\"\\n\\n\\nRunning example with batch generation\")\n  outputs = model.generate_batch(\n      prompts, max_generated_tokens=16, temperature=2.0, top_k=30, seed=123456,\n  )\n  print(\"Generated outputs:\")\n  for prompt, output in zip(prompts, outputs):\n    print(f\"Prompt: '{prompt}' --->\\nOutput: {output}\\n\")\n\n  # PaliGemma example.\n  tokenizer_path = os.path.join(_MODEL_DIR.value, \"paligemma_tokenizer.model\")\n  weights_path = os.path.join(_MODEL_DIR.value, \"paligemma-3b-mix-224-sfp.sbs\")\n  print(f\"Loading model from {tokenizer_path} and {weights_path}\")\n  model = gemma.GemmaModel(\n      tokenizer_path=tokenizer_path,\n      weights_path=weights_path,\n      max_threads=24,\n  )\n  image = np.array(\n      [\n          [[255, 0, 0], [0, 255, 0]],  # Red, Green\n          [[0, 0, 255], [255, 255, 255]],  # Blue, White\n      ],\n      dtype=np.float32,\n  )\n  model.set_image(image)\n  prompt = \"Describe this image.\"\n  print(f\"Running example with a tiny image and prompt='{prompt}'.\")\n  output, tokens = model.generate_with_image(prompt)\n  print(f\"Generated {len(tokens)} tokens, output:\\n{output}\")\n\n\nif __name__ == \"__main__\":\n  app.run(main)\n"
    }
}