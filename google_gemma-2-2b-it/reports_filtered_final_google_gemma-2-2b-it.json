{
  "1-1 (Weights)": "The project repeatedly emphasizes that the Gemma 2-series weights are openly released. The technical report states, ‚ÄúIn this work, we introduce Gemma 2 ‚Ä¶ We release all our models to the community,‚Äù and a second passage echoes the same pledge: ‚ÄúOverall, Gemma 2 ‚Ä¶ We release all our models to the community.‚Äù  Availability spans the whole size range (‚Äúlightweight ‚Ä¶ 2 billion to 27 billion parameters‚Äù).  Practical distribution channels are named: ‚ÄúPrevious Gemma models ‚Ä¶ are also available from Kaggle and Hugging Face,‚Äù and an example loading command is given‚Äî‚Äúmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", ‚Ä¶)‚Äù‚Äîshowing that a user can pull the 2 B model directly from Hugging Face.  The weights are explicitly tagged as ‚Äúopen‚Äù and can be used commercially: ‚ÄúGemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications.‚Äù  Training variants are covered as well: ‚ÄúWe also train the 2B ‚Ä¶ models with knowledge distillation ‚Ä¶ We release all our models to the community,‚Äù confirming that even distilled checkpoints are distributed.  Altogether, the quotes confirm: (1) public, no-barrier download of every Gemma 2 model (including the 2 B checkpoint); (2) hosting on both Kaggle and Hugging Face; (3) an explicit green-light for commercial fine-tuning and deployment via the ‚Äúopen weights‚Äù wording.",
  "1-2 (Code)": "None of the supplied quotes mention releasing the model‚Äôs training pipeline, data-preparation scripts, configuration files, or any other form of training code. Only inference usage is illustrated (the Hugging Face AutoModel loading snippet). Therefore, based on the provided material, there is no public release information for Gemma 2 training code.",
  "1-3 (License)": "Licensing language appears in three separate excerpts.  First, the project notes a broad permission scope tied to the weights: ‚ÄúGemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications.‚Äù  Second, it differentiates between site content and code samples: ‚ÄúExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License.‚Äù  The first clause applies CC-BY-4.0 to documentation, while the second places any accompanying code snippets under Apache-2.0, a permissive license covering use, modification, redistribution, and commercial exploitation.  Finally, the footer line‚Äî‚Äú¬© 2024 Google DeepMind. All rights reserved‚Äù‚Äîindicates that, unless expressly covered by the earlier CC-BY-4.0 or Apache-2.0 statements, remaining materials are retained under full copyright.  No additional restrictions such as ‚Äúnon-commercial,‚Äù ‚Äúno derivatives,‚Äù or ‚Äúevaluation only‚Äù are mentioned in the quoted text.",
  "1-4 (Paper)": "A formal technical report is repeatedly cited under the title ‚ÄúGemma 2: Improving Open Language Models at a Practical Size.‚Äù  The authorship is attributed to ‚ÄúGemma Team, Google DeepMind.‚Äù  One quote summarizes its scope: ‚ÄúIn this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2.‚Äù  The introductory sentence situates the work: ‚ÄúIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters.‚Äù  The Gemma 2 family also serves as the language component of the multi-modal PaliGemma 2 system: ‚ÄúPaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models,‚Äù and the paper mentions the use of ‚Äúthe whole range of Gemma 2 models, from the 2B one all the way up to the 27B model.‚Äù  Collectively, the quotes confirm that an official technical report exists, covers detailed methodology, and that derivative research (e.g., PaliGemma 2) builds directly on the publicly released Gemma 2 checkpoints.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. We release all our models to the community."
    },
    {
      "source": "[pdf_text]",
      "quote": "Overall, Gemma 2 significantly advances state-of-the-art performance relative to comparable-scale open models and are even competitive with some models more than twice their size ‚Ä¶ We release all our models to the community."
    },
    {
      "source": "[sections/Gemma 3 model overview webpage]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/Gemma 3 model overview webpage]",
      "quote": "Previous Gemma models You can work with previous generations of Gemma models, which are also available from Kaggle and Hugging Face . For more technical details about previous Gemma models, see the following model card pages: Gemma 2 Model Card"
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/base]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/base]",
      "quote": "You can download Gemma 3 models from Kaggle and Hugging Face."
    },
    {
      "source": "[sections/https://huggingface.co/docs/transformers/main/en/llm_optims?static-kv=basic+usage%3A+generation_config]",
      "quote": "model = AutoModelForCausalLM.from_pretrained( \"google/gemma-2b\" , dtype= \"auto\" , device_map= \"auto\" )"
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3√ó bigger. We release all our models to the community."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Gemma 3 model overview webpage]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/Gemma 3 model overview webpage]",
      "quote": "Ready to start building? Get started with Gemma models! Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License ."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/base]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Please send correspondence to gemma-2-report@google.com. ¬© 2024 Google DeepMind. All rights reserved"
    },
    {
      "source": "[web:https://ai.meta.com/llama/license/]",
      "quote": "https://ai.meta.com/llama/license/"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Gemma 2: Improving Open Language Models at a Practical Size\nGemma Team, Google DeepMind"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 2: Improving Open Language Models at a Practical Size"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Gemma 2: Improving Open Language Models at a Practical Size"
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model."
    }
  ],
  "1-5 (Architecture)": "Gemma 2 retains the same fundamental design principles as the earlier Gemma line: it is a decoder-only Transformer (Vaswani et al., 2017). The public description emphasizes several concrete hyper-parameter choices that mirror the first Gemma release. Each 2 B parameter variant (and the larger siblings in the same family) is built to handle a context window of 8 192 tokens. Rotary Position Embeddings (RoPE) are used for positional encoding, and the feed-forward blocks rely on the approximated GeGLU non-linearity. The paper also signals that the main layer counts, hidden sizes, and other core figures are summarized in an accompanying ‚ÄúTable 1.‚Äù Beyond the text-only backbone, the authors report combining the SigLIP-So400m vision encoder‚Äîpreviously used in PaliGemma‚Äîwith every Gemma 2 size, from the 2 B up to a 27 B variant. Multi-stage training is carried out at three image resolutions (224 px, 448 px, and 896 px) to furnish the models with transferable multimodal capabilities. In short, the 2 B configuration inherits Gemma‚Äôs decoder-only Transformer core, 8 192-token context, RoPE, and GeGLU, and it can be paired with a SigLIP-So400m vision front-end for multimodal use cases.",
  "1-6 (Tokenizer)": "For Gemma 2-2B-IT, no new tokenizer is introduced; instead, the model re-uses the identical SentencePiece tokenizer deployed in Gemma 1 and the Gemini series. That tokenizer keeps split digits, preserves white-space tokens, and operates with byte-level encodings. The published vocabulary size is 256 000 tokens, so downstream users inherit the same 256 k sub-word inventory that ships with prior Gemma checkpoints.",
  "2-1 (Hardware)": "Training was conducted exclusively on Google TPUs. The authors cite experiments on TPU v4, TPU v5e, and TPU v5p. For the Gemma 2 2 B model specifically, they provide an explicit topology: a 2 √ó 16 √ó 16 mesh of TPU v5e devices, which totals 512 separate chips. The run distributed the workload with 512-way data parallelism (one replica per chip) and performed no intra-model partitioning beyond that: ‚Äò1-way model sharding.‚Äô",
  "2-2 (Software)": "The Gemma 2 training stack follows the same recipe as Gemma 1. Computation is partitioned with GSPMD, and the low-level compilation is handled by the MegaScale XLA compiler. No other frameworks or optimizer libraries are enumerated in the excerpt, indicating the in-house JAX/XLA workflow remained unchanged for the 2 B training runs.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017)."
    },
    {
      "source": "[pdf_text]",
      "quote": "A few architectural elements are similar to the first version of Gemma models; namely, a context length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "A few architectural elements are similar to the first version of Gemma models; namely, a context\nlength of 8192 tokens, the use of Rotary Posi-\ntion Embeddings (RoPE) (Su et al., 2021), and\nthe approximated GeGLU non-linearity (Shazeer,\n2020)."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train our models with TPUv4, TPUv5e, and TPUv5p as outlined in Table 3. For the 2B model, we train on a 2√ó16√ó16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The provided material describes a multi-stage pre-training programme for the Gemma 2 family and includes explicit details for the 2 B variant. The 2 B model is exposed to 2 trillion primarily-English tokens, while larger companions (9 B and 27 B) consume 8 T and 13 T tokens respectively, establishing a common corpus but with size-specific scales. Training does not rely solely on maximum-likelihood over raw text: instead, a large teacher model supplies token-level probability targets, and the student (including the 2 B instance) minimises the negative log-likelihood between teacher and student distributions. This distillation-centric recipe means the 2 B model is intentionally trained on a token count that exceeds the compute-optimal frontier by more than 50√ó, trading additional data for quality while remaining tractable on smaller hardware.\n\nHardware and parallelism are also spelled out. For Gemma 2 2 B, the team deploys a 2 √ó 16 √ó 16 TPU-v5e mesh (512 chips in total) with 512-way data replication and a single slice of model sharding, indicating that the model weights fit comfortably on one shard while data is split across the entire pod. Safety considerations start at the data-collection stage: the authors emphasise ‚Äúconsiderable safety filtering‚Äù of the pre-training corpus so that even the raw checkpoints are less likely to emit disallowed or harmful content. \n\nFinally, vision-language capability is boot-strapped before any downstream adaptation by coupling the SigLIP-So400 M vision encoder (also used in PaliGemma) with every Gemma 2 size, including the 2 B checkpoint. The composite models are trained at three input resolutions‚Äî224 px, 448 px and 896 px‚Äîover multiple stages so that the resulting representations are broadly transferable when later fine-tuned. All of these elements‚Äîtoken counts, distillation, safety filtering, TPU topology, and early multi-modal pairing‚Äîdefine the pre-training pipeline for Gemma 2 2 B.",
  "3-2 (Fine-tuning)": "Fine-tuning for Gemma 2 2 B follows a structured, multi-objective strategy. First, the model inherits the same control-token inventory used in Gemma 1, but the surrounding formatting schema is updated to match the new generation of checkpoints. Second, safety alignment is reinforced during fine-tuning: in addition to supervised fine-tuning (SFT) on curated instruction data, RLHF is applied so that the model learns to avoid undesirable outputs in accordance with Google‚Äôs internal safety policies that are shared with the Gemini project.\n\nBeyond safety, the ecosystem offers a lightweight adaptation toolkit. Parameter-efficient techniques (as described by Mozes et al., 2023) allow users to build custom classifiers on top of Gemma with only a small number of task-specific datapoints, which makes the 2 B checkpoint practical for domain-specific deployments.\n\nFine-tuning is not restricted to pure text. The SigLIP-So400 M vision encoder is fused with every Gemma 2 size, and the composite models undergo additional training at three image resolutions (224, 448, 896 px). After that stage, the team performs broad transfer fine-tuning on an expanded suite of vision-language tasks, extending well past the original PaliGemma coverage to include table-structure OCR, chemical structure parsing, music-score transcription, long and fine-grained captioning, and radiography report generation. These efforts enable the resulting ‚ÄúPaliGemma 2‚Äù instantiation (which includes the 2 B backbone) to achieve state-of-the-art performance across the enlarged task set.",
  "3-3 (Reinforcement Learning)": "Reinforcement Learning from Human Feedback (RLHF) is explicitly integrated into the training stack for Gemma 2 2 B. The algorithmic framework mirrors that used in Gemma 1.1, ensuring continuity and reuse of proven components, but the reward model is upgraded: it is now an order of magnitude larger than the policy network itself, giving it greater capacity to evaluate and rank candidate outputs. RLHF is applied after supervised fine-tuning and works in tandem with SFT to steer the model towards helpful behaviour and away from harmful or policy-violating content, thereby reinforcing the safety objectives introduced during earlier training phases.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens."
    },
    {
      "source": "[url:https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code. We show that distillation is an effective method for training these models, and the benefits distillation confers over raw text training."
    },
    {
      "source": "[url:https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "A key pillar of Gemma‚Äôs approach to safety is to align fine-tuned models with Google‚Äôs safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Given a large model used as a teacher, we learn smaller models by distilling from the probability given by the teacher of each token ùë•given its context ùë•ùëê, i.e., ùëÉùëá(ùë•| ùë•ùëê). More precisely, we minimize the negative log-likelihood between the probabilities from the teacher and the student: ... Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50√ó the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Formatting. Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema."
    },
    {
      "source": "[url:https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "A key pillar of Gemma‚Äôs approach to safety is to align fine-tuned models with Google‚Äôs safety policies, in line with Gemini models (Gemini Team, 2023). For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior."
    },
    {
      "source": "[url:https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Additionally, the toolkit includes a methodology to build customized classifiers with Gemma using a limited number of datapoints thanks to parameter efficient tuning techniques (Mozes et al., 2023)"
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Formatting. Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Reinforcement Learning from Human Feed- back (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a differ- ent reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[url:https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "A key pillar of Gemma‚Äôs approach to safety is to align fine-tuned models with Google‚Äôs safety policies, in line with Gemini models (Gemini Team, 2023). For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    }
  ],
  "4-1 (Pre-training Data)": "Gemma-series models in the 2.x generation are trained on extremely large, mixed-source corpora. The quotes state that the 27 B parameter checkpoint is exposed to 13 trillion tokens, the 9 B checkpoint to 8 trillion tokens, and the 2 B checkpoint to 2 trillion tokens. The material is described as ‚Äúprimarily-English‚Äù and is drawn from ‚Äúa variety of data sources, including web documents, code, and science articles.‚Äù  In addition to scale and source composition, the pre-training pipeline includes an explicit safety component: the authors note that they carried out ‚Äúconsiderable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content.‚Äù",
  "4-2 (Fine-tuning Data)": "For supervised post-training/fine-tuning, the Gemma team enlarged the dataset used in Gemma 1.1 by blending new proprietary (internal) data with external public corpora. One concrete public component is the well-known LMSYS-chat-1M collection: the pipeline keeps the user prompts but deliberately discards the answers. The statement is repeated twice in the source text, underscoring that only the prompt side of that dataset is used. No additional quantitative counts are given, but the description emphasises the mixed provenance (internal + public) and the selective retention of conversation parts.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement training follows an RLHF set-up ‚Äúsimilar to Gemma 1.1‚Äù but substitutes a new reward model that is ‚Äúan order of magnitude larger than the policy.‚Äù Beyond that relative size statement, no further details (e.g., token counts, annotator hours, or accessibility) are provided. The quote confirms continuity with earlier Gemma pipelines while highlighting the single major change: a much larger reward network is paired with the policy during RL optimisation.",
  "4-4 (Data Filtering)": "The quotes describe a multi-stage, safety-oriented filtering regime that applies at different points of the data pipeline. For pre-training, the team ‚Äúuse[s] the same data filtering techniques as Gemma 1,‚Äù explicitly aiming to: (1) remove ‚Äúunwanted or unsafe utterances,‚Äù (2) excise ‚Äúcertain personal information or other sensitive data,‚Äù (3) ‚Äúdecontaminate evaluation sets‚Äù if they overlap with the model‚Äôs own training mixture, and (4) ‚Äúreduce the risk of recitation‚Äù by curbing the repetition of sensitive passages. A parallel policy-alignment step is referenced: ‚Äúconsiderable safety filtering‚Äù is carried out to keep the data consistent with Google safety policies and to limit harmful generations.  At the post-training/synthetic-data stage, additional filters are run on generated conversation examples. These passes target and delete items that reveal personal information, contain ‚Äúunsafe or toxic model outputs,‚Äù show ‚Äúmistaken self-identification,‚Äù or are merely duplicates. Altogether, the documentation depicts a layered defence-in-depth strategy‚Äîpre-training filters, evaluation de-contamination, and synthetic-data vetting‚Äîalthough the excerpts do not list concrete classifier names, thresholds, or percentage removal figures.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key pillar of Gemma‚Äôs approach to safety is to align fine-tuned models with Google‚Äôs safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key pillar of Gemma‚Äôs approach to safety is to align fine-tuned models with Google‚Äôs safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. Data filtering. When using synthetic data, we run several stages of filtering to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}