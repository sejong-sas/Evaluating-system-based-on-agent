{
  "model": "google/gemma-2-2b-it",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Gemma license allows use, modification, redistribution and commercial use with only very light restrictions (similar to Qwen), so it is counted as Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated Gemma 2 technical report exists and covers this exact model."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type (TPUv5e) and quantity (512 chips, 2×16×16 topology)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list JAX, ML Pathways, GSPMD and MegaScale XLA, but not full stack versions/configs."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tgi, https://cloud.google.com/vertex-ai/docs/predictions/serve-gemma-with-saxml-tpu"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method (knowledge-distillation objective, token count, teacher-student setup) is partly described but not in fully reproducible detail."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Gives control-token scheme and alignment goal, yet omits complete hyper-parameters/pipeline."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "States RLHF is used and reward model size, but does not give full reproducible algorithmic details."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web, code, science) and token counts stated, but not enough to rebuild corpus."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Identifies LMSYS-chat-1M prompts plus internal/public data, yet no sizes or full list."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions English-only preference data reused from SFT prompts; no dataset details."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Describes multi-stage filtering aims and use of Google Sensitive Data Protection, but pipeline specifics and thresholds are absent."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Gemma license allows use, modification, redistribution and commercial use with only very light restrictions (similar to Qwen), so it is counted as Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated Gemma 2 technical report exists and covers this exact model."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type (TPUv5e) and quantity (512 chips, 2×16×16 topology)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list JAX, ML Pathways, GSPMD and MegaScale XLA, but not full stack versions/configs."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tgi, https://cloud.google.com/vertex-ai/docs/predictions/serve-gemma-with-saxml-tpu"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method (knowledge-distillation objective, token count, teacher-student setup) is partly described but not in fully reproducible detail."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Gives control-token scheme and alignment goal, yet omits complete hyper-parameters/pipeline."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "States RLHF is used and reward model size, but does not give full reproducible algorithmic details."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web, code, science) and token counts stated, but not enough to rebuild corpus."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Identifies LMSYS-chat-1M prompts plus internal/public data, yet no sizes or full list."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions English-only preference data reused from SFT prompts; no dataset details."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Describes multi-stage filtering aims and use of Google Sensitive Data Protection, but pipeline specifics and thresholds are absent."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}