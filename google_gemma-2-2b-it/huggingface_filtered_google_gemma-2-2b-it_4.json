{
  "4-1 (Pre-training Data)": "The provided quote indicates that the gemma-2-2b model (“2B model”) was pre-trained on 2 trillion tokens drawn from several curated source classes. Those classes explicitly listed are: (1) Web Documents – primarily English web text selected to give the model broad topical and stylistic coverage; (2) Code – snippets and repositories that teach programming-language syntax and patterns for code understanding and generation; and (3) Mathematics – mathematical text that supplies symbolic expressions and logical-reasoning examples. The same quote also places the 2 B parameter model in the context of a family of larger variants (9 B and 27 B) that were trained on proportionately larger corpora (8 T and 13 T tokens, respectively), but the statement confirms that the 2 B variant specifically saw 2 T tokens during pre-training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\nHere are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "Two filtering practices are explicitly described for the gemma-2-2b pre-training pipeline. First, “Sensitive Data Filtering” employed automated techniques to strip out personal information and other sensitive content before the model saw the data, with the goal of keeping the model safe and reliable. Second, the team applied further filtering “based on content quality and safety” aligned with the project’s stated safety policies, indicating an additional quality-screening stage beyond the sensitive-data removal.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safety in line with\n  [our policies][safety-policies]."
    }
  ]
}