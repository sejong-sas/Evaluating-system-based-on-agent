{
  "pretrain_method": "Transformer models have driven this recent progress by pretraining on massive text corpora, including all of Wikipedia, thousands of books, and numerous websites. Before ﬁne-tuning on MATH, models pretrain on AMPS. We pretrain for one epoch, using AdamW (Loshchilov and Hutter, 2019), using a batch size of 128, and using a weight decay of 0.05.",
  "pretrain_data": "Transformer models have driven this recent progress by pretraining on massive text corpora, including all of Wikipedia, thousands of books, and numerous websites. AMPS contains over 100,000 problems pulled from Khan Academy and approximately 5 million problems generated from manually designed Mathematica scripts.",
  "__evidence": [
    {
      "source": "arxiv:2009.03300",
      "quote": "Transformer models have driven this recent progress by pretraining on massive text corpora, including all of Wikipedia, thousands of books, and numerous websites."
    },
    {
      "source": "arxiv:2103.03874",
      "quote": "Before ﬁne-tuning on MATH, models pretrain on AMPS."
    },
    {
      "source": "arxiv:2103.03874",
      "quote": "We pretrain for one epoch, using AdamW (Loshchilov and Hutter, 2019), using a batch size of 128, and using a weight decay of 0.05."
    },
    {
      "source": "arxiv:2009.03300",
      "quote": "Transformer models have driven this recent progress by pretraining on massive text corpora, including all of Wikipedia, thousands of books, and numerous websites."
    },
    {
      "source": "arxiv:2103.03874",
      "quote": "AMPS contains over 100,000 problems pulled from Khan Academy and approximately 5 million problems generated from manually designed Mathematica scripts."
    }
  ]
}