{
  "4-1 (Pre-training Data)": "Gemma-series models in the 2.x generation are trained on extremely large, mixed-source corpora. The quotes state that the 27 B parameter checkpoint is exposed to 13 trillion tokens, the 9 B checkpoint to 8 trillion tokens, and the 2 B checkpoint to 2 trillion tokens. The material is described as “primarily-English” and is drawn from “a variety of data sources, including web documents, code, and science articles.”  In addition to scale and source composition, the pre-training pipeline includes an explicit safety component: the authors note that they carried out “considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content.”",
  "4-2 (Fine-tuning Data)": "For supervised post-training/fine-tuning, the Gemma team enlarged the dataset used in Gemma 1.1 by blending new proprietary (internal) data with external public corpora. One concrete public component is the well-known LMSYS-chat-1M collection: the pipeline keeps the user prompts but deliberately discards the answers. The statement is repeated twice in the source text, underscoring that only the prompt side of that dataset is used. No additional quantitative counts are given, but the description emphasises the mixed provenance (internal + public) and the selective retention of conversation parts.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement training follows an RLHF set-up “similar to Gemma 1.1” but substitutes a new reward model that is “an order of magnitude larger than the policy.” Beyond that relative size statement, no further details (e.g., token counts, annotator hours, or accessibility) are provided. The quote confirms continuity with earlier Gemma pipelines while highlighting the single major change: a much larger reward network is paired with the policy during RL optimisation.",
  "4-4 (Data Filtering)": "The quotes describe a multi-stage, safety-oriented filtering regime that applies at different points of the data pipeline. For pre-training, the team “use[s] the same data filtering techniques as Gemma 1,” explicitly aiming to: (1) remove “unwanted or unsafe utterances,” (2) excise “certain personal information or other sensitive data,” (3) “decontaminate evaluation sets” if they overlap with the model’s own training mixture, and (4) “reduce the risk of recitation” by curbing the repetition of sensitive passages. A parallel policy-alignment step is referenced: “considerable safety filtering” is carried out to keep the data consistent with Google safety policies and to limit harmful generations.  At the post-training/synthetic-data stage, additional filters are run on generated conversation examples. These passes target and delete items that reveal personal information, contain “unsafe or toxic model outputs,” show “mistaken self-identification,” or are merely duplicates. Altogether, the documentation depicts a layered defence-in-depth strategy—pre-training filters, evaluation de-contamination, and synthetic-data vetting—although the excerpts do not list concrete classifier names, thresholds, or percentage removal figures.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. Data filtering. When using synthetic data, we run several stages of filtering to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples."
    }
  ]
}