{
  "1-1 (Weights)": "The quotes state that \"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.\" In the same sentence, the material clarifies that these models are \"text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants.\" Therefore, the weights for every released Gemma variant are openly provided. A second quote adds a practical access detail, explaining that \"To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license.\" From these sentences we learn (a) openness of the weights, (b) availability of both the raw pre-trained checkpoints and the instruction-tuned checkpoints, and (c) the distribution channel (Hugging Face) plus the fact that users must first accept Google’s usage license before downloading.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants."
    },
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    }
  ],
  "1-2 (Code)": "No quotation supplied mentions the release status of training, pre-training, fine-tuning, or RL code for Gemma 2-2b. Consequently, the provided material gives no information about any public availability (or absence) of such code.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Licensing is addressed in two separate sentences. First, \"To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license.\" Second, \"Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy][prohibited-use].\" Taken together, these quotes make clear that use of the Gemma 2-2b weights is gated by a Google-specific usage license and an accompanying Prohibited Use Policy that enumerates forbidden applications. The exact quoted phrases highlight both the requirement to affirm the license and the existence of explicit restrictions under the Prohibited Use Policy.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    },
    {
      "source": "[readme]",
      "quote": "Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy][prohibited-use]."
    }
  ],
  "1-4 (Paper)": "The single relevant quote notes that \"All evaluations are described in detail in [Evaluating Frontier Models for Dangerous Capabilities][eval-danger] and in brief in the [Gemma 2 technical report][tech-report].\" From this we learn that an official \"Gemma 2 technical report\" exists and that a separate document titled \"Evaluating Frontier Models for Dangerous Capabilities\" contains the full evaluation details for Gemma. No other papers, blogs, or documentation are mentioned in the supplied material.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "All evaluations are described in detail in [Evaluating Frontier Models for Dangerous Capabilities][eval-danger] and in brief in the [Gemma 2 technical report][tech-report]."
    }
  ],
  "1-5 (Architecture)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Gemma was trained on Google’s most recent Tensor Processing Unit infrastructure, specifically the TPUv5p generation of accelerators, indicating that all training compute was carried out on this state-of-the-art TPU hardware.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma was trained using the latest generation of [Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p)."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "Gemma can be obtained through the Hugging Face platform, but before any interaction developers must explicitly review and accept Google’s usage license that governs model use. Once this license step is completed, the model can be downloaded and incorporated into projects. For users who prefer a streamlined, script-friendly workflow, the open-source “local-gemma” repository provides a lightweight wrapper built on top of the Transformers library. This wrapper lets people launch and interact with Gemma 2 directly from a command-line interface, illustrating a simple, practical path to running the model locally without writing extensive custom code.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    },
    {
      "source": "[readme]",
      "quote": "The [local-gemma](https://github.com/huggingface/local-gemma) repository contains a lightweight wrapper around Transformers for running Gemma 2 through a command line interface, or CLI."
    }
  ],
  "3-1 (Pre-training)": "Gemma’s pre-training effort spans multiple model sizes. Concretely, the 27 B parameter variant processed 13 trillion tokens, the 9 B variant saw 8 trillion tokens, and the compact 2 B model consumed 2 trillion tokens during training. All sizes were trained on the latest-generation TPUv5p hardware, leveraging Google’s TPU accelerators for large-scale compute. The software stack consisted of JAX for numerical computation together with ML Pathways, allowing the team to coordinate and scale the training jobs efficiently across that TPU infrastructure.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "Gemma was trained using the latest generation of [Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p).\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways]."
    }
  ],
  "3-2 (Fine-tuning)": "Gemma is described as a family of lightweight, state-of-the-art, text-to-text, decoder-only large language models derived from the same research base that produced the Gemini models. Google releases both the raw pre-trained checkpoints and separate instruction-tuned checkpoints, so practitioners can either start from the generic language-modeling weights or adopt the instruction-tuned versions that were further optimized for following user prompts. All variants focus on English language output and carry fully open weights, making them straightforward to employ or further adapt.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The only explicit information disclosed about the google/gemma-2-2b-it pre-training corpus concerns its overall scale. A single sentence states that the 2 B parameter variant was trained on 2 trillion tokens, while simultaneously noting that larger 9 B and 27 B versions consumed 8 trillion and 13 trillion tokens, respectively. No other details—such as concrete data sources, domain composition, licensing status, geographical or linguistic coverage, or the balance of modalities—are provided in the quoted material. Consequently, all that can be concluded is that the 2 B model’s pre-training set contained approximately 2 trillion text tokens, positioning it as the smallest member of the family in terms of both parameter count and raw training-data volume.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens."
    }
  ],
  "4-2 (Fine-tuning Data)": "The supplied quotations contain no sentences that mention \"2b,\" \"gemma,\" \"gemma22bit,\" or \"gemmabit\" in connection with fine-tuning. Therefore, no public information is available regarding the origin, composition, size, licensing, or availability of any fine-tuning datasets used for google/gemma-2-2b-it.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "No qualifying quotations reference reinforcement-learning (RL) datasets for google/gemma-2-2b-it. As a result, there is no disclosed information about how, or even whether, RL data were collected, generated, filtered, or applied during training.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "One qualifying sentence reveals that automated sensitive-data filtering was applied: “As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.” From this, it can be inferred that the developers implemented programmatic sanitation procedures specifically aimed at excising personal or otherwise sensitive content from the pre-training corpus before or during training. No further operational details (e.g., rule-based vs. model-based detection, false-positive/negative rates, or downstream impacts on model utility) are provided.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}