{
  "1-1 (Weights)": "The available quotes explicitly state that the weights for the gemma-2 models, including the 2-b parameter variant, are openly released. One sentence says, “In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. We release all our models to the community.”  A second, nearly identical sentence reinforces the point: “We hope that releasing these models to the community will unlock access to capabilities previously only seen in large-scale LLMs ….”  The phrase “Given the open nature of Gemma models, responsibility for upholding principles of model safety also relies on downstream developers” further signals that the model files are meant to be broadly obtainable, because downstream users are expected to handle them responsibly.  Finally, the terse repository line “gemma-2-2b-it” confirms the specific 2-b-parameter instruction-tuned checkpoint as part of what is being made available.  No URL, hosting platform, authentication mechanism, or gated-access policy is mentioned in these excerpts, but, taken together, the quotations unequivocally indicate that the gemma-2-2b-it weights are published and intended for public download.",
  "1-2 (Code)": "None of the provided quotations mention any release, repository, or description of training code, data-processing pipelines, fine-tuning scripts, or evaluation harnesses for gemma-2-2b-it.  Consequently, based solely on the supplied material, there is no evidence that the full or partial training code (pre-training, fine-tuning, RL, or otherwise) has been made public.  The excerpts are therefore silent on whether only inference code or no code at all accompanies the weight release.",
  "1-3 (License)": "The single licensing-related quotation reads: “© 2024 Google DeepMind. All rights reserved.”  Because “All rights reserved” appears without any additional modifiers (e.g., Apache-2.0, CC-BY-NC, or similar), the text implies that Google DeepMind retains full ownership and does not, in that line, grant explicit permissions for use, modification, redistribution, or commercial exploitation.  No other sentence references a different license name, version, or special grant clauses (such as “research-only” or “non-commercial”).  Therefore, the only licensing information presently documented for gemma-2-2b-it is this blanket “All rights reserved” statement.",
  "1-4 (Paper)": "Two separate excerpts acknowledge an official written work: the title “Gemma 2: Improving Open Language Models at a Practical Size” and the statement, “In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code.”  From them we can conclude that a formal paper or technical report exists under the cited title and that it describes the gemma-2 series, including the 2-b-parameter variant.  No DOI, arXiv link, or conference venue is provided in the quotes, but the presence of both the title and self-referential wording (“In this work”) confirms the existence of an authored document that functions as the canonical reference for gemma-2-2b-it.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. We release all our models to the community."
    },
    {
      "source": "[sections/Discussion and Conclusion]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code. We hope that releasing these models to the community will unlock access to capabilities previously only seen in large-scale LLMs and fuel future waves of research and development."
    },
    {
      "source": "[sections/Responsible open models]",
      "quote": "Given the open nature of Gemma models, responsibility for upholding principles of model safety also relies on downstream developers."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters."
    },
    {
      "source": "[sections/Discussion and Conclusion]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code."
    },
    {
      "source": "[pdf_text/Table 14]",
      "quote": "gemma-2-2b-it 1126 +10 / -10 +"
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com.\n© 2024 Google DeepMind. All rights reserved"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[title]",
      "quote": "Gemma 2: Improving Open Language Models at a Practical Size"
    },
    {
      "source": "[sections/Discussion and Conclusion]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code."
    }
  ],
  "1-5 (Architecture)": "Google’s gemma-2-2b-it follows the architectural pattern laid out for the Gemma 2 family. According to the cited text, all Gemma 2 models, including the 2 B parameter variant, are “based on a decoder-only transformer architecture.” The design retains several core elements from the original Gemma release: a fixed context window of 8 192 tokens, Rotary Position Embeddings (RoPE) for positional encoding, and the approximated GeGLU non-linearity. Beyond these carry-overs, Gemma 2 introduces additional transformer refinements—most notably the interleaving of local-global attention blocks and the use of group-query attention. These modifications are intended to improve efficiency and long-context reasoning while keeping the model lightweight enough to fit in the 2 B parameter budget. As a result, gemma-2-2b-it sits in a family that spans 2 B–27 B parameters but keeps a common decoder-only backbone augmented with the techniques listed above.",
  "1-6 (Tokenizer)": "The gemma-2-2b-it model employs the same tokenizer that shipped with Gemma 1 and Google’s Gemini models: a SentencePiece tokenizer configured with three specific preprocessing rules—split digits, preserve whitespace, and encode at the byte level. This tokenizer yields a vocabulary of 256 k subword pieces. The Gemma team highlights that this shared tokenizer is a cornerstone for multilingual support; they cite its role in downstream projects such as the Navarasa 2.0 model covering 15 Indian languages. Because Gemma 2 kept the identical tokenizer spec, any user already relying on the Gemma 1 or Gemini vocabulary can plug into gemma-2-2b-it without conversion or re-training of tokenization assets.",
  "2-1 (Hardware)": "Training of the 2 B model was performed on TPUv5e hardware in a \"2 × 16 × 16\" topology. This layout amounts to 512 TPU chips. The compute strategy applies 512-way data replication with a single shard for the model weights (\"1-way model sharding\"), indicating that the entire 2 B parameter network fits inside the memory of each device replica while data parallelism supplies scale-out throughput.",
  "2-2 (Software)": "gemma-2-2b-it was trained with Google’s GSPMD partitioner orchestrating each training step and compiled through the MegaScale XLA compiler. In line with Gemma 1, GSPMD handles logical partitioning of tensors and operations across the 512 TPUv5e chips, while MegaScale XLA produces the optimized executable kernels. For downstream alignment, the Gemma team states that their fine-tuning pipeline blends Supervised Fine-Tuning (SFT) with Reinforcement Learning from Human Feedback (RLHF) so that the resulting instruction-tuned checkpoint adheres to Google safety policies.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/Model Architecture]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). A few architectural elements are similar to the first version of Gemma models; namely, a context length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017)."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/Pre-training/Training Data]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018)."
    },
    {
      "source": "[sections/8.1 Impact assessment]",
      "quote": "Since the launch of Gemma 1, we have seen our Gemma models drive a number of socially beneficial applications, relying on Gemma’s unique technologies like its tokenizer to facilitate the creation of multilingual models, such as for Navarasa 2.0, a Gemma tuned model for 15 Indian languages."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Compute Infrastructure]",
      "quote": "For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/Compute Infrastructure]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/8.2 Safety policies and train-time mitigations]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "For google/gemma-2-2b-it, the authors state that the 2 B parameter variant of Gemma 2 is trained on 2 trillion primarily-English tokens. The training regime deliberately over-scales data, using “more than 50 × the compute-optimal quantity” predicted by scaling theory. Instead of pure next-token prediction, the 2 B model is optimized by knowledge-distillation from a larger teacher language model; the paper emphasizes that learning “over output probabilities” yields stronger results than raw-text likelihood. Hardware details are given: the 2 B checkpoint is trained on a 2 × 16 × 16 TPU-v5e topology (512 chips total) with 512-way data parallelism and single-way model sharding. A terse hyper-parameter row (“2B 10 2560 20480 40 128 521k 1k”) is supplied, indicating model depth, hidden size, sequence length, batch, or training-step values, although the paper does not decode the column names. Gemma 2 continues to use the SentencePiece tokenizer from Gemma 1/Gemini (split digits, preserved whitespace, byte-level encoding). Prior to training, the corpus undergoes “considerable safety filtering” so that both pre-trained and fine-tuned checkpoints are less likely to emit harmful content.",
  "3-2 (Fine-tuning)": "The authors explain that Gemma 2 models, including the 2 B variant, are fine-tuned with the same control-token inventory introduced for Gemma 1, but under a newly defined formatting schema. The open-source toolkit accompanying Gemma 2 allows users to create bespoke classifiers on top of the 2 B base with only small, task-specific datasets by leveraging parameter-efficient tuning techniques. A core objective of fine-tuning is policy alignment: output behavior is explicitly matched to Google safety policies, continuing the alignment strategy used for Gemini models.",
  "3-3 (Reinforcement Learning)": "gemma-2-2b-it employs Reinforcement Learning from Human Feedback. The paper states that the algorithmic framework mirrors the approach used in Gemma 1.1 but introduces a reward model that is roughly an order of magnitude larger than the policy network itself. In the safety-alignment pipeline, the authors combine supervised fine-tuning (SFT) and RLHF to push the model away from undesirable content.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction."
    },
    {
      "source": "[sections/Pre-training Compute Infrastructure]",
      "quote": "For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/Discussion and Conclusion]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code. We show that distillation is an effective method for training these models, and the benefits distillation confers over raw text training. Specifically, we show how training over output probabilities can produce superior results over purely next token prediction."
    },
    {
      "source": "[sections/Appendix]",
      "quote": "2B 10 2560 20480 40 128 521k 1k"
    },
    {
      "source": "[abstract]",
      "quote": "We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction."
    },
    {
      "source": "[sections/3.1 Training Data]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens."
    },
    {
      "source": "[sections/3.1 Training Data]",
      "quote": "We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018)."
    },
    {
      "source": "[sections/Discussion and Conclusion]",
      "quote": "In this work, we have presented Gemma 2, the newest additions to the Gemma family of open language models for text and code. We show that distillation is an effective method for training these models, and the benefits distillation confers over raw text training."
    },
    {
      "source": "[sections/Responsibility, Safety, Security]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Post-Training]",
      "quote": "Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema."
    },
    {
      "source": "[sections/8.5 Responsible Open Models]",
      "quote": "Additionally, the toolkit includes a methodology to build customized classifiers with Gemma using a limited number of datapoints thanks to parameter efficient tuning techniques (Mozes et al., 2023)"
    },
    {
      "source": "[sections/4. Post-Training]",
      "quote": "Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema."
    },
    {
      "source": "[sections/Responsibility, Safety, Security]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023)."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Post-Training]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[sections/4. Post-Training]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy."
    },
    {
      "source": "[sections/Responsibility, Safety, Security]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior."
    }
  ],
  "4-1 (Pre-training Data)": "The google/gemma-2-2b-it checkpoint is trained on 2 trillion tokens of primarily English text. The same statement notes that other Gemma 2 variants (9 B and 27 B) receive 8 T and 13 T tokens respectively, but the 2 B figure is explicitly 2 T. The material is drawn from “a variety of data sources, including web documents, code, and science articles,” giving a blend of general web prose, programming code and scientific literature. No additional languages, modalities, or proprietary sources are mentioned in the cited text, so the publicly described corpus can be summarized as a large-scale, mostly-English, multi-domain text collection of 2 trillion tokens.",
  "4-2 (Fine-tuning Data)": "For supervised fine-tuning, the team “extended the post-training data from Gemma 1.1 with a mixture of internal and external public data.” One explicitly named public component is the LMSYS-chat-1M dataset, from which they keep “the prompts, but not the answers.” The SFT stage is performed “on a mix of text-only, English-only synthetic and human-generated prompt-response pairs.” Thus, after inheriting the Gemma 1.1 post-training base, the developers add additional conversational prompts, restrict everything to English text, and combine machine-generated with human-written dialogue pairs to refine instruction following. The excerpt does not specify the exact number of examples, but it clarifies the provenance (internal + public), modality (text), language (English), and mixture of synthetic and human contributions.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement Learning from Human Feedback (RLHF) is performed with “a similar RLHF algorithm as Gemma 1.1” while employing “a different reward model, which is an order of magnitude larger than the policy” and “oriented more towards conversational capabilities, specifically multi-turn.” After the SFT stage, the same prompts are reused: “We then apply RLHF on top of these models with the reward model trained on labelled English-only preference data and the policy based on the same prompts as the SFT phase.” Consequently, the preference dataset consists of English annotations in which human raters compare candidate answers to the identical instruction prompts used in supervised fine-tuning. The scale of the reward model (≈10× the policy) and its conversational focus are the only quantitative or architectural details revealed in the provided text.",
  "4-4 (Data Filtering)": "Gemma-2 employs “the same data filtering techniques as Gemma 1.” The stated objectives are to (1) “filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances,” (2) “filter out certain personal information or other sensitive data,” (3) “decontaminate evaluation sets from our pre-training data mixture,” and (4) “reduce the risk of recitation by minimizing the proliferation of sensitive outputs.” The authors emphasize that “considerable safety filtering” was carried out so that “our pre-trained and fine-tuned checkpoints” are less likely to produce harmful content, aligning the approach with Google safety policies used for Gemini models. Regarding personal data specifically, they state: “We use Google Cloud Sensitive Data Protection Tool to find potential instances of personal data,” applying “the same prevention methods at training time and the same evaluations as” earlier Gemma work. Although no numeric thresholds or classifier scores are disclosed in the excerpt, it is clear that multi-stage filtering—spanning pre-training corpora, evaluation splits, and fine-tuned outputs—forms a central pillar of the model’s safety and privacy strategy.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Post-Training]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. First, we apply supervised fine-tuning (SFT) on a mix of text-only, English-only synthetic and human-generated prompt-response pairs."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Post-Training]",
      "quote": "Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude larger than the policy. The new reward model is also oriented more towards conversational capabilities, specifically multi-turn."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. We then apply RLHF on top of these models with the reward model trained on labelled English-only preference data and the policy based on the same prompts as the SFT phase."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs."
    },
    {
      "source": "[sections/8.2 Safety policies and train-time mitigations]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    },
    {
      "source": "[sections/7. Memorization and Privacy]",
      "quote": "Personal Data We use the same prevention methods at training time and the same evaluations as Gemma Team (2024). In particular, we use Google Cloud Sensitive Data Protection Tool to find potential instances of personal data."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}