{
  "model_id": "nvidia/nvidia-nemotron-nano-12b-v2",
  "full_texts": [
    {
      "arxiv_id": "2504.03624",
      "full_text": "2025-9-9\nNemotron-H: A Family of Accurate and Efficient\nHybrid Mamba-Transformer Models\nNVIDIA\nAbstract. As inference-time scaling becomes critical for enhanced reasoning capabilities, it is\nincreasingly becoming important to build models that are efficient to infer. We introduce Nemotron-\nH, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost\nfor a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the\ncommon Transformer model architecture with Mamba layers that perform constant computation and\nrequire constant memory per generated token. We show that Nemotron-H models offer either better\nor on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer\nmodels (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3× faster at inference.\nTo further increase inference speed and reduce the memory required at inference time, we created\nNemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation\ntechnique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model,\nbut is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it\ncan achieve on par results with BF16-based training. This recipe is used to train the 56B model.\nWe are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.\n1. Introduction\nAttention (Vaswani et al., 2023) traditionally has been the powerhouse of large language models\n(LLMs). Yet, to generate one token during auto-regressive inference, the self-attention layer must\nperform computation that scales linearly with the number of tokens seen so far since it models\ninteractions between all pairs of tokens in a sequence (potentially in a causal way); as a result, self-\nattention layers also have to store state in a KV cache during inference that is linearly proportional to\nthe number of tokens in the sequence (Kwon et al., 2023). Recent reasoning LLMs, however, exhibit\ninference-time scaling, where generating more tokens at inference time can improve the quality of\nmodel responses (OpenAI, 2025; DeepSeek-AI, 2025b). Thus, attention layers can fundamentally\nlimit overall model intelligence.\nTo address this issue, much recent work has proposed alternative architectures (Katharopoulos et al.,\n2020; Beltagy et al., 2020; Gu & Dao, 2024; Dao & Gu, 2024). One such example is a series of\nhybrid models that replace the majority of the self-attention layers in the standard Transformer\narchitecture with more efficient layers (e.g., Mamba, Mamba-2, or sliding-window attention layers)\nthat have sub-linear or even constant compute and memory requirements. Recent work has shown\nhybrid models to be competitive with more traditional Transformer architectures (Waleffe et al.,\n2024; Lieber et al., 2024; DeepMind, 2025).\nMotivated by improving inference efficiency, we introduce the Nemotron-H family of hybrid Mamba-\nTransformer models. The Nemotron-H models consist of a mixture of Mamba-2, self-attention and\nMLP layers, and achieve state-of-the-art accuracy and improved inference speed when compared\nto open-sourced Transformer models of similar size (Figure 1). The Nemotron-H family has a\nseries of 8-billion-parameter models (Nemotron-H-8B-Base, Nemotron-H-8B-Instruct, Nemotron-\nH-8B-VLM), and a series of 47/56-billion-parameter models (Nemotron-H-47B-Base, Nemotron-\nH-56B-Base, Nemotron-H-56B-VLM) that offer either better or on-par accuracy compared to\n© 2025 NVIDIA. All rights reserved.\narXiv:2504.03624v4  [cs.CL]  5 Sep 2025\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n0\n5\n10\n15\n20\n25\n30\n35\n40\nThroughput (Output tokens/s/GPU)\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nMMLU-Pro accuracy (%)\n2.4x\n1.2x\nNemotron-H-56B\nNemotron-H-47B\nLlama-3.1-70B\nLlama-3.1-405B\nQwen-2.5-72B\nInput sequence length = 65536  / Output sequence length = 1024\nFigure 1 | MMLU-Pro accuracy (Wang et al., 2024a) versus per-GPU inference throughput for\nNemotron-H-56B/47B-Base compared to existing similarly-sized Transformer models. Nemotron-H\nmodels provide state-of-the-art accuracy and inference time speedups.\nQwen-2.5-7B/Llama-3.1-8B and Qwen-2.5-72B/Llama-3.1-70B, respectively. For example, Nemotron-\nH-56B-Base outperforms Llama-3.1-70B on 16 out of 17 tasks that we evaluated (§2.5). With larger\nsequences (65536 input sequence length, 1024 output tokens), we measure up to 3× higher inference\nthroughput on NVIDIA H100 GPUs.\nTo achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of\nhigh-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024). Nemotron-\nH-56B-Base is the first Nemotron model to be fully pre-trained using a FP8-based recipe. Our recipe,\ncalled per-tensor current scaling, is coarse grained and involves quantizing entire tensors using a\nsingle scaling factor. This factor is chosen to preserve the largest value in the given tensor; any value\ntoo small to fit in the desired FP8 format is flushed to zero. In addition, we find it important for\ntraining stability and convergence to leave the first and last 4 GEMMs of the model in BF16. In\nsmaller ablations with 8B-parameter models on token horizons up to 15 trillion tokens, we show that\nFP8 per-tensor scaling can reach equal or better downstream task accuracy compared to BF16.\nTo efficiently tailor Nemotron-H models for different deployment scenarios, we introduce a new\ncompression via pruning and distillation paradigm, called MiniPuzzle, that combines the simplicity\nof Minitron (Sreenivas et al., 2024) and the versatility of Puzzle (Bercovich et al., 2024). MiniPuzzle\ncan be used to turn a larger model into a smaller model satisfying specific memory, parameter count,\nor latency constraints. We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base,\nusing only 63 billion training tokens and FP8 training. Nemotron-H-47B-Base can be deployed in\nFP4 precision on a single NVIDIA RTX 5090 GPU with 32GiB of memory (Nemotron-H-56B-Base’s\nmodel weights would require roughly 29.5GiB alone, limiting maximum context size).\nThe Nemotron-H base models can also be effectively post-trained to produce models with high\naccuracies on vision-language, instruction following, and long-context (e.g., RULER) benchmarks.\nVision-Language Models (VLMs) based on both Nemotron-H-8B and Nemotron-H-56B have already\nbeen used to develop very strong reasoning models for physical AI as part of the Cosmos-Reason1\nproject (NVIDIA et al., 2025).\nOverall, the Nemotron-H family of models demonstrates that hybrid models can be state-of-the-art in\nterms of capabilities while offering improved inference speed. Moreover, we believe FP8 pre-training\nand compression techniques like MiniPuzzle make it cheaper and more efficient to create such models.\n2\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nNemotron-H-56B\nNemotron-H-8B\nx3\nx3\nx4\nx1\nx10\nx1\nFigure 2 | Nemotron-H-8B/56B model architectures. Roughly 8% of the total layers in the model\nare self-attention layers; these layers are evenly dispersed throughout the model. The rest of the\nmodel is made up of alternating Mamba-2 and FFN layers.\nTo enable further research, we have released the following base model checkpoints in Hugging Face\nand NeMo formats on the Hugging Face and NGC model repositories:\n• Nemotron-H-56B-Base. Hugging Face and NGC.\n• Nemotron-H-47B-Base. Hugging Face and NGC.\n• Nemotron-H-8B-Base. Hugging Face and NGC.\nThe rest of this technical report is organized as follows:\n• In §2, we discuss the Nemotron-H model architecture and the pre-training process (including\ndetails on the pre-training dataset and FP8 recipe used).\n• In §3, we describe the pruning and distillation methods used for model compression.\n• In §4, we introduce the vision-language models based on the Nemotron-H models.\n• In §5, we show that Nemotron-H models can be extended to create competitive instruct and\nlong-context versions.\n2. Base Models and Pre-Training\nIn this section, we discuss the Nemotron-H-8B-Base and Nemotron-H-56B-Base model architectures,\nas well as key details in the pre-training process used to produce these models. We also compare\nNemotron-H-8B/56B-Base with existing open-source state-of-the-art Transformer models on both\naccuracy (on common benchmark tasks used for base models) and inference speed.\n2.1. Model Architecture\nNemotron-H models consist of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN\nlayers as summarized in Figure 2 and Table 1. As suggested by prior work (Waleffe et al., 2024),\n3\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nModel\nNumber of\nlayers\nModel\ndimension\nFFN\ndimension\nQ\nheads\nKV\nheads\nState\ndimension\nMamba\ngroups\nNemotron-H-8B\n52\n4096\n21504\n32\n8\n128\n8\nNemotron-H-56B\n118\n8192\n32768\n64\n8\n256\n8\nTable 1 | Summary of the Nemotron-H hybrid Mamba-Transformer architectures.\nwe set the number of attention layers to be roughly 8% of the total number of layers and evenly\ndisperse them throughout the model. This amounts to 4 self-attention layers (out of 52 layers) for\nNemotron-H-8B and 10 for Nemotron-H-56B (out of 118 layers). The rest of the layers consist of an\neven split between FFN and Mamba-2 layers. We also ensure that a) the first layer in the model is a\nMamba-2 layer, b) the last layer in the model is a FFN layer, and c) self-attention layers always\nprecede FFN layers (as they do in a standard Transformer block like in Vaswani et al. (2023)).\nWe use a hidden dimension of 4096 for Nemotron-H-8B and 8192 for Nemotron-H-56B. For our\nsmaller model, we use FFN hidden dimension of 21504, 32 attention query heads, and Mamba-2 state\ndimension of 128; for the larger model, we use FFN hidden dimension of 32768, 64 attention query\nheads, and Mamba-2 state dimension of 256. Both models use Grouped-Query Attention (Ainslie\net al., 2023) with 8 key-value heads, 8 Mamba-2 groups, and squared ReLU (So et al., 2022) activation\nfor FFN layers. We do not use any position embeddings. For Mamba-2 layers, we retain the default\nvalues for head dimension (64), expansion factor (2), and window size for convolution (4). We also\nuse RMSNorm (Zhang & Sennrich, 2019) for normalization, separate embedding and output layer\nweights, and no dropout. We do not use bias weights for linear layers. We include a residual skip\nconnection around each Mamba-2, self-attention, and FFN layer in the architecture.\nNemotron-T-8B Transformer baseline.\nTo compare Nemotron-H-8B-Base to a Transformer\nmodel in an apples-to-apples fashion, we also trained Nemotron-T-8B-Base on exactly the same\ndata. The Nemotron-T-8B architecture follows the style of GPT-3 (Brown et al., 2020). We use 32\nTransformer layers (each has a self-attention layer followed by a FFN layer). As for Nemotron-H-8B,\nwe use a hidden dimension of 4096, 32 query heads, GQA with 8 kev-value heads, a FFN hidden\ndimension of 21504, squared ReLU activation, RMSNorm, no bias weights for linear layers, no\ndropout, and separate parameters for model embeddings and output layer weights. We use RoPE\nfor position embeddings (Su et al., 2023).\n2.2. Pre-Training Data\nNemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality\ncurated and synthetically-generated data.\n2.2.1. Curated Data\nWe have separate data curation pipelines for four broad data categories: general web crawl data,\nmath data, code data, and “academic” data. We discuss each in turn next.\nWeb crawl data.\nFor Nemotron-H, we made several key innovations in our processing of English\nCommon Crawl data compared to Nemotron-4 (Parmar et al., 2024; NVIDIA, 2024); these innovations\nsubstantially improved data quality. For full details on how this dataset was prepared and various\nablations, please refer to the dedicated paper (Su et al., 2024). We provide a brief summary here.\nWe focus primarily on extracting as many high quality tokens as possible, so that we can train\n4\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nQuality label\nTokens (billions)\nTokens (%)\nHigh\n554\n8.8\nMedium-High\n505\n8.1\nMedium\n2026\n32.4\nMedium-Low\n896\n14.3\nLow\n403\n6.4\nSynthetic-High\n1536\n24.6\nSynthetic-Low\n336\n5.4\nTable 2 | Quality distribution of the 6.3 trillion English Common Crawl tokens.\n59.0\n53.4\n53.0\n42.9\n42.4\nMMLU Accuracy\n0\n20\n40\n60\nNemotron-CC-HQ\nDCLM\nNemotron-CC\nFineWeb-Edu\nFineWeb-Edu-2\n+5.6 MMLU\n4x more data\nFigure 3 | MMLU scores for 8B-parameter models trained for 1 trillion tokens. Compared to DCLM,\nour methods enable us to either create a 4× larger dataset of similar quality or increase the MMLU\nusing a high quality subset of the tokens. Having a larger dataset (in terms of unique tokens), is\ncrucial when training over long horizons (e.g., 15 or 20 trillion tokens).\nNemotron-H for long token horizons (e.g., 15 trillion or more tokens). As is common, we begin\nwith HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring\nde-duplication. At this stage, however, we deviate from the norm and employ an ensemble of three\nmodel-based classifiers to bucket each document into five quality categories. By using an ensemble,\nwe obtain a larger and more diverse set of high-quality tokens compared to approaches that use a\nsingle model-based classifier. To retain as many high-quality tokens as possible, we apply heuristic\nand perplexity filters to the low, medium-low, and medium quality buckets. We also rephrase the\nlow-quality tokens to boost their quality; §2.2.2 describes the synthetic data generation pipeline.\nThe resulting dataset consists of 6.3 trillion tokens, including 4.4 trillion globally de-duplicated\n“real” tokens and 1.9 trillion tokens of rephrased synthetic data. The quality distribution is shown\nin Table 2. As shown in Figure 3, our dataset results in higher-quality models compared to other\nleading English Common Crawl preparations like DCLM (Li et al., 2024) and FineWeb-Edu (Penedo\net al., 2024): using a high-quality subset increases MMLU by 5.6 points over DCLM, whereas using\nthe full dataset without any weighting of quality buckets achieves comparable accuracy to DCLM\nwhile having 4× more unique tokens. Having more unique tokens enables pre-training over longer\ntoken horizons without having to do more than 4 to 8 epochs over the full dataset. Higher epoch\n5\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\ncounts lead to diminishing returns (Muennighoff et al., 2024; Feng et al., 2024).\nFor the benefit of the community, we have publicly released the dataset (as Nemotron-CC1) under\nthe Common Crawl terms of use.\nMath data.\nTo ensure that technical pages in Common Crawl retain their mathematical content,\nwe leverage the recipe and code base from Paster et al. (2023). We also find it essential to apply this\nrecipe to other high-quality data sources such as Wikipedia. We found that a smaller, higher-quality\ndataset led to a larger improvement on relevant benchmarks, similar to FineMath (Allal et al., 2025).\nCode data.\nWe pre-train Nemotron-H models with a considerable number of code tokens. We\nstarted from our previous work on Nemotron-4 (Parmar et al., 2024; NVIDIA, 2024), and chose to\nreduce the number of tokens in markup and configuration languages such as HTML, Markdown,\nYAML, CSS, JSON, and Makefile. Tokens that would have previously come from these languages\nare instead taken from popular languages like Python, C++, C, and Java.\nAcademic data.\nThe Nemotron-H pre-training dataset contains additional tokens gathered from\n“high information” English texts, including permissively-licensed books and articles. We utilize\nexamples across a wide variety of domains (e.g., science, math, humanities, law, and engineering) and\na large range of document types including peer-reviewed journal publications, textbooks, published\nnovels, and patent documents.\nAs the original data formats of these documents range widely—including EPuB, HTML, XML, plain\ntext, PDF, LaTeX, and markdown—we write custom functions to parse text from the input format\ninto a standardized output format. We maintain appropriate formatting in markdown or LaTeX for\ncomplex segments like tables, lists, and equations. We utilize Éclair (Karmanov et al., 2025) for\nPDF-to-text extraction. We also develop specialized heuristic filters to remove extraneous information\ncontained within headers or footers of pages. We then apply the Nemotron-4 heuristic and perplexity\nfilters to remove low-quality documents from the set of documents used for pre-training.\nIn order to better weight examples from these sources in our final data blend, we developed classifiers\nto rate documents on their educational content and difficulty. We also try to determine each\ndocument’s domain (one of biology, business, chemistry, computer science, economics, engineering,\nhistory, law, mathematics, medicine, philosophy, physics, psychology, or other). For educational\ncontent, we rated all documents on a numerical scale from 0 (no educational information) to 3\n(highly relevant educational information). For educational difficulty, we categorized documents with\nhigh educational content into the following levels: elementary school, middle school, high school,\nundergraduate, and graduate. We were able to use these buckets (e.g., “biology at the undergraduate\nlevel with high educational content”) to determine document weights (§2.2.3). As expected, we\nfound increasing the weight of highly educational content in important domains at the high school /\nundergraduate levels to be most helpful.\n2.2.2. Synthetically-Generated Data\nWe also synthetically generate data to de-noise low-quality data and to increase the diversity of\ntokens given to the model. We use different processing pipelines to synthetically re-write web crawl,\nmath, and code data.\nWeb crawl data.\nWe found rephrasing text by LLMs to be an effective way to reduce noise and\nerrors in low-quality crawl data, and produce additional variants of high-quality data with new\nunique tokens. This leads to better results on downstream tasks.\n1https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html.\n6\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nSource\nRaw tokens\nPrompt\nSynthetic tokens\nLow\n403.0\nWikipedia\n336.3\nHigh\n451.3\nWikipedia\n372.9\nDiverse QA Pairs\n499.5\nDistill\n157.6\nExtract Knowledge\n303.6\nKnowledge List\n203.2\nTable 3 | Synthetic data token counts (billions) for web crawl, generated with the instruct version of\nMistral NeMo 12B.\nWe rephrased our low-quality data using the “medium Wikipedia” prompt from Maini et al. (2024).\nFor high-quality documents, we generated synthetic data using four additional prompts:\n1. Diverse question-answer (QA) pairs. Ask questions in various forms (e.g., yes/no question,\nopen-ended question, multi-choice question) about factual information in the text.\n2. Distill. Rewrite the text into a concise and clear passage.\n3. Extract knowledge. Rewrite knowledge from the text and disregard uninformative content.\n4. Knowledge list. Extract key information from the text as an organized list.\nThese prompts required the model to provide clear and concise responses while preserving factual\ninformation and concrete details such as numbers. In total, we generated over 1.8 trillion synthetic\ntokens using the instruct version of Mistral NeMo 12B. The breakdown of source and generated\ntokens by quality and prompt is shown in Table 3. The full details of this synthetic data generation,\nincluding the prompts used, have been published separately in Su et al. (2024).\nMath data.\nWe use synthetic data to enhance mathematical reasoning benchmarks like GSM8K\nand MATH (Cobbe et al., 2021; Hendrycks et al., 2021b), as detailed in Akter et al. (2024).\nWe expand OpenWebMath (Paster et al., 2023) from 14 billion to over 100 billion tokens using\nNemotron-4-340B (NVIDIA, 2024), yielding an 18-point improvement on GSM8K in controlled\nexperiments.\nTo achieve this, we start with technical pre-training documents from Common Crawl and leverage\nNemotron-4-340B to generate dialogues, where a knowledgeable persona guides a less-experienced\none (e.g., an interaction between student and teacher). This approach aligns with insights from\nthe Physics of Language Models series (Allen-Zhu & Li, 2024), and incorporates strategies such as\npresenting incorrect answers alongside corrections. By structuring content as learning interactions,\nour method distills broad knowledge from public language models without overfitting to benchmarks.\nCode data.\nWith the goal of adding diverse code data centered around problem solving, we\nchose to synthetically generate mixed natural language and code datasets across 11 programming\nlanguages, e.g., Python, C++, C, C#, and Java. To do this, we prompted Mixtral 8x22B to generate\na programming problem inspired by sampled pieces of source code from our curated code dataset.\nThe samples range from 1 to 15 lines of code (LOC), and typically are just a small function. We\nalso prompted Mixtral 8x22B to solve the generated problems and removed clearly invalid solutions\non a minimal-effort basis. For example, we extracted the Python code generated by the model\nand attempted to parse it into an abstract syntax tree (AST), discarding samples which fail to\nparse. Finally, to form a sample for training, we combined the generated problem and answer into a\nsingle sample which is a mix of natural language instruction, generated code (typically enclosed in a\nmarkdown code block), and usually an explanation of the approach.\n7\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nmultilingual\n5.0%\ncrawl++\n4.0%\nacademic\n8.8%\ncode\n20.0%\nwiki\n0.3%\nmath\n2.9%\nweb crawl\n59.0%\n(a) Data mixture of Phase 1.\nmultilingual\n3.7%\ncrawl++\n7.0%\nacademic\n12.0%\ncode\n20.0%\nwiki\n1.7%\nweb crawl\n40.7%\nmath\n14.0%\n(b) Data mixture of Phase 2.\nSFT-style\n12.6%\nmultilingual\n3.7%\ncrawl++\n7.0%\nacademic\n12.0%\ncode\n20.0%\nweb crawl\n32.7%\nmath\n10.8%\nwiki\n1.1%\n(c) Data mixture of Phase 3.\nSFT-style\n31.8%\nmultilingual\n3.7%\ncrawl++\n7.0%\nacademic\n12.0%\nweb crawl\n13.5%\nmath\n10.8%\nwiki\n1.1%\ncode\n20.0%\n(d) Data mixture of Phase 4.\nFigure 4 | Data mixtures for each phase of Nemotron-H pre-training.\nSFT-style data.\nWe further add synthetic SFT-style data to the pre-training corpus; this\nimproves the ability of base models to follow instructions. We use the Qwen2.5 series, Mixtral\n8x22B, Nemotron-4-340B, and DeepSeek-R1 (only for 56B) models to produce these datasets. We\nimprove math abilities following the pipeline presented in OpenMathInstruct-2 (Toshniwal et al.,\n2024) using carefully curated seed data such as AoPS2. We also synthesize code data using the\napproach proposed in Genetic Instruct (Majumdar et al., 2024), with tigerbot-leetcode3 and\nthe-stack-v24 as seed data. In order to equip the base model with high-quality general knowledge,\nwe generate question-solution pairs with selected topics and domain-specific documents. In total, we\nadd 230 billion synthetic SFT-style tokens (174 billion math tokens, 35 billion code tokens, and 21\nbillion tokens with general knowledge) to the training corpus.\n2.2.3. Data Mixture and Ordering\nOur data mixture consists of seven high-level data categories: web crawl, math, wikipedia, code,\nacademic, crawl++, multilingual, and synthetic SFT-style data. Crawl++ consists of web-crawl\nderivatives like OpenWebText, BigScience and Reddit. Our multilingual data has nine languages:\nGerman, Spanish, French, Italian, Portuguese, Chinese, Japanese, Korean, and Russian. We design\nthe data mixtures in such a way that all data sources of a given quality are weighed similarly, and\ndata sources of higher quality are weighed higher than data sources of lower quality. We provide\nmore details on estimating the quality of datasets in Feng et al. (2024).\n2https://artofproblemsolving.com.\n3https://huggingface.co/datasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k.\n4https://huggingface.co/datasets/bigcode/the-stack-v2.\n8\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTokens\n1e12\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nRelative difference (%)\nTraining LM loss\n(BF16 - FP8) / BF16\nFigure 5 | The relative training loss gap between a pair of 8B-parameter Transformer models trained\nfor 3 trillion tokens in BF16 and FP8. The gap is initially large, then shrinks, and then grows again\nin the last stage of training.\nWe use a phased data-blending approach (Feng et al., 2024) to pre-train both Nemotron-H base\nmodels. In the first phase, we use a data mixture that promotes diversity in data; in the second\nand third phases, we primarily use high-quality datasets (e.g., Wikipedia). We switch to the second\nphase at the 60% point of training, and to the third phase at the 80% point of training. The fourth\nphase is performed for the last 380 billion training tokens. The data mixtures used in each phase\nare shown in Figure 4. Our initial experiments on a 8-billion-parameter model trained on 1 trillion\ntokens show that a phased approach for pre-training outperforms random data ordering by 3.4%.\n2.3. FP8 Recipe\nNemotron-H-56B-Base was pre-trained using layer-wise mixed precision: all linear layers in the\nmodel (including both linear layers in FFN blocks and the QKV / output projection in the attention\nblock) were computed in FP8 precision, except the first 4 and last 4 layers, which were kept in BF16.\nWe quantized both the forward and backward passes of the linear layers. As in Micikevicius et al.\n(2022), we use a hybrid FP8 approach which uses E4M3 for weight and activation tensors, and E5M2\nfor the gradient tensors.\nWe used FP8 per-tensor dynamic quantization, which has several steps: we first compute a quantiza-\ntion scale (with a check for division by zero); then, we multiply the input tensor with the quantization\nscale and cast it to the desired FP8 format. The quantization scale is computed as a ratio of the\nmaximum representable value for that FP8 data format divided by the maximum absolute value of\nthe input tensor.\nWe observed that the log-likelihood loss curves for FP8 pre-training experiments were almost always\nhigher than for their BF16 counterparts, but by a surprisingly small amount, with typical relative\ngaps less than 0.1% on both training and validation. This loss gap tended to decrease as training\nprogressed but then widened in the last quarter of training. We conjecture that this is because\nextremely small gradient updates, which dominate the latter parts of training, get flushed to zero\ndespite operating in E5M2. Figure 5 shows a typical relative loss gap between a BF16 and FP8\nexperiment during pre-training.\nDespite the marginally worse loss curves, we observed that downstream evaluations for models trained\nwith FP8 were as good or better than BF16 experiments. Log likelihood, which has traditionally\nserved as a proxy for downstream task, was not a reliable predictor in our experiments; in fact, we\noften observed models with better loss curves to perform worse on downstream evaluation tasks.\n9\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nTokens\n1e13\n45\n50\n55\n60\n65\n70\nAccuracy (%)\nMMLU\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nTokens\n1e13\n54\n56\n58\n60\n62\n64\nAccuracy (%)\nAverage commonsense understanding\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nTokens\n1e13\n25\n30\n35\n40\n45\n50\n55\nAccuracy (%)\nAverage code\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nTokens\n1e13\n20\n40\n60\nAccuracy (%)\nGSM8K\nBF16\nFP8\nFigure 6 | Downstream task accuracies for a pair of 8B-parameter Transformer models trained on\n15 trillion tokens with BF16 and mixed precision FP8. FP8 accuracies are consistently equal to or\nbetter than BF16 ones. The jump in accuracies at 9 trillion tokens is a result of switching data\nmixtures from Phase 1 to Phase 2.\nFor coding and math tasks, experiments using per-tensor current scaling often had results that\nwere substantially better than BF16 experiments. Figure 6 shows the accuracy gap between two\n8B-parameter Transformers trained for 15 trillion tokens, with one trained in FP8 and the other in\nBF16. Because of the equal or better downstream results on the same token horizon, we never had\nto overtrain FP8 models relative to BF16 ones. We ran evaluations in both BF16 and FP8 precision\nfor models trained in FP8; they were typically the same, with BF16 evaluations usually slightly\nbetter. However, for the 56B model, the FP8 evaluations eventually outperformed the BF16 ones.\nWe found it very important to do verification on a minimum of 8B parameters when constructing our\nFP8 recipe, as results with smaller models did not generalize. All experiments used token horizons\nof at least 1 trillion tokens.\n2.4. Pre-Training\nWe trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base\non a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of\n768 (6291456 tokens per batch). We do not use any batch size ramp-up. For Nemotron-H-8B-Base,\nwe used a peak learning rate of 8·10−4 and warmup over 8.3 billion tokens; for Nemotron-H-56B-Base,\nwe used a peak learning rate of 4 · 10−4 and warmup over 25 billion tokens. In both cases, we used\ncosine learning rate decay with a minimum value equal to 1% of the peak value, weight decay of 0.1,\n10\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nand 0.9 and 0.95 for Adam 𝛽1 and 𝛽2, respectively\nWe pre-train Nemotron-H using Megatron-LM5; we rely on Transformer Engine6 for FP8 support.\nWe use 8-way tensor model parallelism (Shoeybi et al., 2020) with sequence parallelism (Korthikanti\net al., 2022) for additional memory savings, and 768-way data parallelism with optimizer state\ndistributed over the data-parallel replicas (Rajbhandari et al., 2020).\n2.4.1. Resiliency\nWe also focus heavily on minimizing downtime when training models at scale: over long training\njobs on a large number of GPUs, job failures and slowdowns are inevitable, leading to lost training\nproductivity. Failures can occur due to GPU-related errors (e.g., uncorrectable ECC errors or\nGPUs falling off the bus), errors in shared infrastructure like the network fabric or Lustre, or\napplication-induced failures (e.g., numerical instability in certain operations or “bad” data). We\ncan also experience job slowdowns due to a variety of reasons (e.g., single-GPU throttling or slower\ncommunication collectives).\nTo minimize the impact of the above issues, we focus on a) accurately attributing failures to the\ncorrect infrastructure and application components to prevent repeated disruptions, and b) reducing\nthe recovery overhead of each failure. To do so, we leverage the DGX Cloud Resilience service7 to\nensure failures are correctly attributed and the appropriate hardware component (e.g., particular\nHGX node with failing GPU) is not included in the next instance of the training job, preventing\nrepeated failures. With DGX Cloud Resilience enabled, we obtained a 3.3× improvement in mean\ntime between failures (MTBF) in our cluster.\nWe also proactively save checkpoints frequently in order to minimize the amount of work lost when a\nfailure occurs (weight updates from the last checkpoint save to the failure are lost). We use NVRx8\nto facilitate checkpointing with low overhead by asynchronously saving checkpoints without blocking\ntraining. We also optimized job restart times (i.e., the set of operations needed by Megatron-LM\nbefore training iterations can start like distributed runtime initialization and checkpoint loading).\nAltogether, the above components enable us to train Nemotron-H-56B-Base with high efficiency on\n6144 NVIDIA H100 GPUs.\n2.5. Base Model Evaluations\nWe run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on\ntop of lm-evaluation-harness9 for fair comparisons, with the following changes:\n1. For mathematical reasoning, we evaluate on the GSM8K and MATH (Cobbe et al., 2021;\nHendrycks et al., 2021b) benchmarks. We also highlight the competition-level slice of the\nMATH benchmark as “MATH Level 5”. After carefully looking through disagreeing cases,\nwe use a combination of Math-Verify10 and the math grading utilities of NeMo-Skills11 to\ngrade solutions.\n2. We also include a MATH-R1 version of the MATH task, where we provide few-shot examples\n5https://github.com/nvidia/megatron-lm.\n6https://github.com/nvidia/transformerEngine.\n7https://developer.nvidia.com/blog/ensuring-reliable-model-training-on-nvidia-dgx-cloud/.\n8https://github.com/NVIDIA/nvidia-resiliency-ext.\n9https://github.com/EleutherAI/lm-evaluation-harness.\n10https://github.com/huggingface/math-verify.\n11https://github.com/NVIDIA/NeMo-Skills/blob/main/nemo_skills/code_execution/math_grader.py.\n11\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nTask\nN-H\n56B-Base\nN-H\n47B-Base\nQwen-2.5\n72B-Base\nLlama-3.1\n70B-Base\nDS-V3\n671B-Base\nLlama-3.1\n405B-Base\nGeneral\nMMLU-Pro (5-shot COT)\n60.5\n61.8\n58.8\n51.3\n64.4\n52.8\nMMLU (5-shot)\n84.2\n83.6\n86.1\n78.8\n87.1\n84.4\nMath\nGSM8k (8-shot COT)\n93.7\n93.3\n90.9\n83.9\n89.3\n83.5\nMATH (4-shot COT)\n59.4\n57.4\n64.6\n42.9\n61.6\n49.0\nMATH Level 5 (4-shot COT)\n35.2\n34.1\n41.2\n21.1\nMMLU STEM (5-shot)\n80.6\n79.8\n84.9\n70.5\nCode\nHumanEval (0-shot greedy pass@1)\n60.4\n61.0\n56.7\n57.3\n65.2\n54.9\nHumanEval+ (0-shot greedy pass@1)\n54.3\n56.1\n50.0\n52.4\nMBPP sanitized (3-shot greedy pass@1)\n77.8\n75.9\n78.2\n70.0\n75.4\n68.4\nMBPP+ (0-shot greedy pass@1)\n67.2\n65.6\n71.7\n66.9\nCommonsense understanding\nArc-Challenge (25-shot)\n95.0\n94.6\n95.8\n93.0\n95.3\n95.3\nHellaswag (10-shot)\n89.0\n87.9\n87.6\n88.1\n88.9\n89.2\nWinogrande (5-shot)\n84.5\n83.9\n84.4\n85.6\n84.9\n85.2\nPIQA (0-shot)\n85.0\n83.9\n83.6\n84.1\n84.7\n85.9\nOpenbookQA (0-shot)\n48.6\n48.8\n46.2\n47.6\nLong-thought reasoning\nMATH (R1-style 4-shot COT)\n87.8\n73.9\n38.7\nMATH Level 5 (R1-style 4-shot COT)\n74.8\n53.2\n16.7\nTable 4 | Accuracy of Nemotron-H-56B-Base (and its distilled version Nemotron-H-47B-Base,\ndiscussed in §3) versus existing SoTA models.\nN-H is short for Nemotron-H, DS is short for\nDeepSeek. Numbers in italics are from the DeepSeek-V3 report (DeepSeek-AI, 2025c), all other\nnumbers are run by us. We bold the highest accuracy in each row, excluding italicized numbers.\ngenerated by DeepSeek-R1 (DeepSeek-AI, 2025b). Both Nemotron-H and the Qwen 2.5 models\nshow significant improvement on the benchmark by simply changing the prompt.\n3. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and their EvalPlus\nvariants), we always sanitize the generations using EvalPlus (Liu et al., 2023b).\n4. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged\nexcept for ARC-Challenge (Clark et al., 2018), where we present all options at the same time,\nsimilar to MMLU (Hendrycks et al., 2021a).\nAccuracy results for Nemotron-H-56B-Base and Nemotron-H-8B-Base on common benchmark tasks\nare shown in Table 4 and Table 5. Nemotron-H-56B-Base and Nemotron-H-47B-Base are both\nevaluated in FP8, and all other models are evaluated in BF16. Across both model sizes, Nemotron-\nH base models reach comparable or better accuracy relative to similarly-sized state-of-the-art\nTransformer models.\nWe compare Nemotron-H-56B-Base directly with Qwen-2.5-72B-Base (Qwen, 2025) and Llama-3.1-\n70B-Base (Meta, 2024). Out of the 17 task we evaluate, Nemotron-H-56B-Base achieves the highest\naccuracy of the three models on 9 tasks, Qwen-2.5-72B-Base achieves the highest accuracy on 7 tasks,\nand Llama-3.1-70B-Base achieves the highest accuracy on 1 task; Nemotron-H-56B-Base outperforms\nLlama-3.1-70B-Base on all but that single task (Winogrande). We further compare Nemotron-H-56B-\nBase with two much larger state-of-the-art models, DeepSeek-V3-671B-Base (DeepSeek-AI, 2025c)\n12\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nTask\nNemotron-H\n8B-Base\nNemotron-T\n8B-Base\nQwen-2.5\n7B-Base\nLlama-3.1\n8B-Base\nGemma-3\n12B-Base\nGeneral\nMMLU-Pro (5-shot COT)\n44.0\n41.4\n48.3\n35.9\n45.3\nMMLU (5-shot)\n72.8\n73.2\n74.2\n65.3\n74.5\nMath\nGSM8k (8-shot COT)\n87.1\n89.0\n83.3\n55.5\n74.1\nMATH (4-shot COT)\n46.5\n46.7\n49.8\n19.5\n42.1\nMATH Level 5 (4-shot COT)\n22.9\n25.8\n24.6\n5.6\n17.5\nMMLU STEM (5-shot)\n65.4\n65.6\n71.2\n56.3\nCode\nHumanEval (0-shot greedy pass@1)\n58.5\n59.8\n56.7\n37.8\n46.3\nHumanEval+ (0-shot greedy pass@1)\n55.5\n53.0\n48.8\n31.7\n34.1\nMBPP sanitized (3-shot greedy pass@1)\n65.4\n63.4\n69.3\n57.2\n64.6\nMBPP+ (0-shot greedy pass@1)\n59.5\n61.4\n65.3\n51.6\n59.0\nCommonsense understanding\nArc-Challenge (25-shot)\n88.7\n88.3\n89.2\n80.4\nHellaswag (10-shot)\n83.2\n82.5\n80.3\n82.3\n84.2\nWinogrande (5-shot)\n80.5\n78.8\n76.1\n78.1\n74.3\nPIQA (0-shot)\n82.2\n82.0\n80.1\n81.0\n81.8\nOpenbookQA (0-shot)\n47.2\n44.8\n47.0\n45.4\nTable 5 | Accuracy of Nemotron-H-8B-Base versus existing SoTA models and a Transformer\n(Nemotron-T-8B-Base) trained on exactly the same data.\nLikelihood-based evaluations of the\nGemma 3 model are based on the Gemma 3 report (DeepMind, 2025), all other numbers are run by\nus. We bold the highest accuracy in each row, excluding italicized numbers.\nand Llama-3.1-405B-Base (Meta, 2024); we use publicly-reported numbers for these two models on the\nsubset of common tasks where numbers are reported. Surprisingly, Nemotron-H-56B-Base remains\ncompetitive with these models, outperforming DeepSeek-V3-671B-Base and Llama-3.1-405B-Base on\n4 and 5 out of the 10 overlapping tasks respectively.\nWe observe similar accuracy results for Nemotron-H-8B-Base compared to Qwen-2.5-7B-Base and\nLlama-3.1-8B-Base (Table 5). Out of the 15 tasks used for evaluation, Nemotron-H-8B-Base, Qwen-\n2.5-7B-Base, and Llama-3.1-8B-Base achieve the highest accuracy of the three models on 7, 8, and\n0 tasks respectively. Nemotron-H-8B-Base is particularly strong on commonsense understanding\ntasks, achieving the highest accuracy on 4 out of 5 tasks in this category. As above, we also include\ncomparisons to a larger model, Gemma-3-12B-Base (DeepMind, 2025); Nemotron-H-8B-Base is\ncompetitive with the larger model.\nTable 5 additionally includes the largest apples-to-apples comparison of a hybrid Mamba-Transformer\nand pure Transformer model to date (Nemotron-H-8B-Base versus Nemotron-T-8B-Base). Nemotron-\nH-8B-Base reaches higher accuracy than the Transformer trained on exactly the same data on 7 out\nof 15 tasks, and is within one point on an additional 4 tasks. Overall, this experiment shows that\nhybrid models can reach equal or better accuracy compared to Transformer models when trained at\nstate-of-the-art scales.\n2.6. Inference Speed\nDue to the reduction in self-attention layers, Nemotron-H-8B/56B-Base provide inference-time\nspeedups compared to the alternative Transformer models in Tables 4 and 5 above. To quantify these\nspeedups, we plot the MMLU-Pro accuracy versus inference throughput for Nemotron-H-56B-Base\n13\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\nThroughput (Output tokens/s/GPU)\n20\n25\n30\n35\n40\n45\n50\n55\nMMLU-Pro accuracy (%)\n1.8x\nNemotron-H-8B\nNemotron-T-8B\nLlama-3.1-8B\nLlama-3.2-3B\nQwen-2.5-7B\nGemma-3-12B\nGemma-3-27B\nInput sequence length = 65536  / Output sequence length = 1024\nFigure 7 | MMLU-Pro accuracy versus inference throughput (normalized by number of GPUs used)\nfor Nemotron-H-8B-Base compared to existing similarly-sized Transformer models.\nand similarly-sized Transformer models in Figure 1 and for Nemotron-H-8B-Base and relevant\nbaselines in Figure 7. We use an input sequence length of 65536 and ask the models to generate 1024\noutput tokens. We use an initial Megatron-LM implementation for Nemotron-H inference and vLLM\nv0.7.312 for baselines. In these experiments, we try to maximize per-GPU inference throughput by\nusing as large a batch size as possible, and we run all experiments on NVIDIA H100 GPUs. We\nreport results normalized by the number of GPUs used (i.e., output tokens per second per GPU).\nIn the setting described above, Nemotron-H-56B-Base can generate 2.4× more output tokens per\nsecond per GPU compared to Qwen-2.5-72B and Llama-3.1-70B (and 2.9× more after distillation\nto Nemotron-H-47B-Base, see §3). Compared to Llama-3.1-405B, Nemotron-H-56B-Base achieves\n19.6× higher throughput. Similar to the larger models, Nemotron-H-8B-Base is also faster to infer\nthan corresponding Transformer models. As shown in Figure 7, on longer contexts, we measure\na 1.8× and 3× speedup compared to Qwen-2.5-7B and Llama-3.1-8B. Inference speedups are a\nbyproduct of two factors: a) constant computation in the Mamba-2 layers as opposed to linear\ncomputation in self-attention layers, b) lower memory footprint from Mamba-2 layers facilitates\nusing higher batch sizes, increasing efficiency. We expect further optimizing Nemotron-H inference\nto lead to additional speedups compared to Transformers, which have been heavily optimzed over\nthe last couple of years.\n3. Compression and Distillation\nEfficient deployment of LLMs often requires tailoring the model architecture to specific constraints\nimposed by hardware. In this section, we describe the pruning and distillation process used to\ncompress the 56B model to 47B parameters with the goal of running longer context inference on the\nNVIDIA GeForce RTX 5090 GPU (storing the weights of a 56B parameter model in FP4 precision\non a RTX 5090 GPU with 32GiB of memory will require 29.5GiB, leaving only 2.5GiB for KV\ncache and activation buffers). We introduce a novel model compression framework called MiniPuzzle\nthat combines the simplicity of Minitron (Sreenivas et al., 2024; Muralidharan et al., 2024) with\nthe versatility of Puzzle (Bercovich et al., 2024). Our approach uses roughly 300× fewer tokens to\nobtain a 47B model compared to training from scratch. Our compressed 47B model achieves a 1.2×\ninference speedup (Figure 1) while achieving accuracy on par with the original 56B model.\n12https://github.com/vllm-project/vllm/tree/v0.7.3.\n14\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n3.1. MiniPuzzle Overview\nFigure 8 | MiniPuzzle’s optimization process: a) estimate layer and FFN importance scores, b) search\nfor the best model candidates using the conditional Neural Architecture Search (NAS) framework,\nconsidering the 32GiB GPU memory constraint, c) select the best candidate with lightweight\ndistillation and recover the accuracy lost due to pruning with longer distillation.\nMiniPuzzle combines lightweight pruning with neural architecture search (NAS) to produce efficient\ncompressed models that meet specific target deployment constraints. Figure 8 provides a high-level\noverview of the framework. MiniPuzzle’s optimization process consists of three stages: a) importance\nestimation (§3.2), b) conditional NAS (§3.3), and c) knowledge distillation (§3.4).\n3.2. Importance Estimation\nMiniPuzzle first collects importance or sensitivity scores for each model component (e.g., layers,\nFFN neurons) to help decide which components to remove; this is the importance estimation phase.\nThe scores computed in this phase are used to decide which model components can be pruned.\nWe note that sensitivity analysis based on gradient information is typically impractical at modern\nLLM scale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only on\nforward passes. In this work, we use a simplified approach that works well in our ablation studies:\na) prune layers, and b) prune FFN hidden dimensions (effectively neurons). We also experimented\nwith pruning Mamba heads and Mamba head dimension; unfortunately, both axes caused severe\naccuracy degradation (particularly head dimension pruning).\nWe now describe how we compute the importance of each layer and FFN neuron.\nLayer importance.\nWe use the scoring method from Puzzle to estimate the importance of each\nlayer in the model. Given an input tensor, layer importance is computed as the Mean Squared\nError (MSE) between the intermediate activation tensor just before the LM head of the full model\nand the same activation tensor for a model with the particular layer removed. We average these\nrankings over a small random subset of the training data (128 samples in our case) to obtain a\nreliable estimate of importance that takes into account sample variability.\nFigure 9 plots average importance scores of each layer in Nemotron-H-56B-Base. The green, blue\nand red dotted lines correspond to self-attention, FFN and Mamba layers. We notice from the figure\nthat the 56B model follows a typical pattern observed in Minitron: the most important layers are\nconcentrated at the beginning and end of the model. Additionally, MSE importance reveals that\neven though the 56B model only has 10 self-attention layers, some self-attention layers are ranked\namong the least important, particularly the 84th (7th self-attention layer). However, layers 40, 51,\n62 and 73 seem to be more important compared to the other layers in their immediate vicinity.\n15\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n0\n20\n40\n60\n80\n100\n118\nLayer index\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nMSE\nPer-layer importance\nAverage\nMamba layer\nMLP layer\nSelf-attention layer\nFigure 9 | Layer importance measured as the MSE between the full model and a model with that\nlayer removed, averaged over a small training subset. Vertical dotted lines indicate layer types:\nself-attention (green), FFN (blue), and Mamba-2 (red).\nFFN importance.\nFFN layers internally are composed of two linear operators with a non-linear\nactivation in between:\nFFN(X) = 𝛿\n(︂\nX · 𝑊𝑇\n1\n)︂\n· 𝑊2.\nHere, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN\nlayer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN\nhidden dimension respectively (Table 1). 𝛿(·) refers to the non-linear activation function (squared\nReLU in this work).\nFollowing the same procedure as Minitron, we compute the importance of each neuron in the first\nlinear operator of each FFN layer by examining the set of outputs it produces. We use a small\ncalibration dataset of 1024 samples for this purpose. Formally, we compute each neuron’s importance\nscore by aggregating its outputs given an input batch 𝑋:\n𝐹(𝑖)\nneuron =\n∑︁\nB,S\n𝛿\n(︂\nX\n(︀𝑊𝑖\n1\n)︀𝑇\n)︂\n.\nHere, 𝑊𝑖\n1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀\nB,S refers to aggregation along the batch\nand sequence dimensions. We use the mean and l2-norm aggregation functions along the batch and\nsequence dimensions, following the observations in the Minitron paper (Muralidharan et al., 2024).\nFor a sequence of scores S, mean aggregation is defined as 1\n𝑛\n∑︀𝑛\n𝑖=1 |S𝑖|, and l2-norm is\n√︁∑︀𝑛\n𝑖=1 S2\n𝑖.\n3.3. Conditional NAS\nMiniPuzzle’s conditional NAS utilizes the importance scores of every layer and FFN neuron (computed\nas described in §3.2) to identify architectures that meet memory constraints while preserving the\nmost critical components. This process allows us to efficiently explore the vast search space of\npossible layer configurations, and consists of three steps:\n1. Pruned candidate enumeration. We iterate over a grid of possible layer counts for each layer\ntype (i.e., number of self-attention, FFN, and Mamba-2 layers), and FFN hidden dimension\n(24576, 25600, . . . , 32768). For each target layer count and FFN hidden dimension size, we\nselect the top layers and neurons based on their respective importance scores (see §3.2 for\ndetails on importance estimation) and realize the corresponding architecture; i.e., we drop\n16\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\n30.3\n30.8\n31.2\n31.7\n32.2\n32.7\n33.2\n33.7\n34.2\nEstimated memory load (GiB)\n0.45\n0.50\n0.55\n0.60\n0.65\nAverage benchmark score\nFigure 10 | Average benchmark score for pruned candidates with different memory budgets. Each\ndot represents a different candidate, i.e., model with different set of layers pruned with respect to\nhow much memory it consumes. The green dotted line marks the upper memory limit we consider\nand the dots marked with red circles are the candidates selected for further distillation.\n0\n20\n40\n60\n80\n100\n118\nLayer index\nCandidate 1\nCandidate 2\nCandidate 3\nFigure 11 | Pruned layers for the top three candidates. Each row represents a candidate, with dots\nindicating dropped layers. Vertical dotted lines denote layer types: green for self-attention, blue for\nFFN, and red for Mamba.\nthe irrelevant layers and prune all FFN layers to the same target width. Only architectures\nthat meet the target memory constraint of less than 31.7 GiB (computed as the total memory\nrequired for FP4 inference on a context of size 1 million tokens) are retained, resulting in\naround 400 candidate architectures.\n2. Architecture ranking. We score the candidate architectures using a lightweight metric that\ncompares them to the parent model (Nemotron-H-56B-Base) on a small validation corpus\nwith 1 million tokens. Specifically, we use two different scores to estimate the quality of each\ncandidate model: a) next-token accuracy which computes how often the child model correctly\npredicts the next ground-truth token, and b) next-token parent agreement which computes\nhow often the child model agrees with the parent model about the greedy prediction of the\nnext token. We rank all candidate architectures using both scoring metrics, and retain the top\n130 performers.\n3. Candidate selection. We now refine the set of ranked architectures from 130 to a manageable\n3 by benchmarking them on a subset of evaluation tasks. The first column of Table 6 lists\nthe tasks we use to evaluate the final 3 candidates. We use the average scores across all these\ntasks for evaluation. Figure 10 plots the average benchmark scores for all 130 candidates, and\nFigure 11 illustrates the specific layers dropped by all three candidates. Candidates 1 and 2\nachieve the highest benchmark scores and drop similar layers. Candidate 3, which drops a\nslightly different set of layers, performs significantly worse after pruning. However, we include\nit here to study the extent to which distillation can compensate for the loss.\n17\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nBenchmark\nNemotron-H\n56B-Base\nCandidate 1\n47B-pruned\nCandidate 2\n47B-pruned\nCandidate 3\n47B-pruned\nMMLU\n84.2\n81.5 →83.7\n80.8 →83.2\n81.3 →83.6\nCommonsense understanding (average)\n67.6\n64.2 →66.6\n63.8 →67.2\n64.5 →66.8\nGSM8k\n91.4\n67.9 →89.2\n60.7 →91.1\n19.5 →90.5\nCode (average)\n67.0\n52.0 →64.9\n52.7 →64.6\n23.0 →64.9\nAVERAGE\n70.6\n63.3 →69.3\n62.1 →69.6\n53.7 →69.5\nTable 6 | Comparison of benchmark scores for pruned candidates before and after lightweight\ndistillation. Scores might be slightly different to those presented in Table 4 due to minor differences\nin evaluation settings and averages over different sets of tasks.\n3.4. Retraining with Distillation\nTo recover the accuracy lost due to pruning, the model undergoes continued training. Recent work\nhas demonstrated that distilling knowledge from the original model to the pruned model outperforms\nconventional fine-tuning (as shown in Minitron and Puzzle); we thus adopt logit-based distillation for\ncontinued training, employing forward KL divergence loss exclusively during the accuracy recovery\nphase (see §3 of the Minitron paper for more details on the distillation loss formulation).\nFollowing the initial candidate selection process described in §3.3, we perform short distillation of\nthe top 3 candidates using ∼7B tokens, with the 56B model as the teacher. We observe from our\nexperiments that this step is crucial for picking the best possible final candidate. Table 6 details\nthe benchmark scores for the 3 candidates before and after short distillation. The most accurate\ncandidate (averaged across all benchmarks) of the three is chosen for an extended distillation run\nusing 63 billion tokens to produce the final 47B model; in our case, Candidate 2 performs best, as\nshown in Table 6. We observe that the choice of the final candidate varies based on the ultimate\nobjective; for instance, prioritizing accuracy in coding tasks versus commonsense reasoning tasks.\nAll distillation runs use FP8 precision, a softmax temperature of 1.0, which controls the softness of\nthe probability distribution during knowledge distillation (Hinton et al., 2015), sequence length of\n8192, and a batch size of 768 samples.\n3.5. Results\nUsing the MiniPuzzle approach, we efficiently compress the 56B model to 47B parameters by\npruning full layers (depth) and FFN hidden dimension size, improving inference speed and enabling\nlonger-context inference on a 32GiB NVIDIA GeForce RTX 5090 GPU. The 47B model retains\nhalf the self-attention layers of 56B (5 instead of 10), along with 44 Mamba-2 layers (down from\n54) and 49 FFN layers (down from 54). Additionally, the FFN intermediate size was pruned from\n32768 to 30720. As shown in Figure 1 and Table 4, the resulting 47B model achieves a 1.2× faster\ninference on long contexts and near-lossless accuracy on benchmarks while requiring roughly 300×\nfewer tokens compared to training from scratch.\n4. Vision-Language Models\nIn this section, we introduce Nemotron-H-8B-VLM and Nemotron-H-56B-VLM, which are built on\nNemotron-H-8B-Instruct and Nemotron-H-56B-Base. Vision-Language Models (VLMs) are generally\nrecommended to be built on aligned models to enhance instruction-following capabilities (Dai et al.,\n2024). In this work, we chose to build Nemotron-H-8B-VLM and Nemotron-H-56B-VLM on Nemotron-\n18\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nH-8B-Instruct and Nemotron-H-56B-Base (since Nemotron-H-56B-Instruct was unavailable). Both\nNemotron-H-8B-VLM and Nemotron-H-56B-VLM have been used as the base models for building\nCosmos-Reason1 (NVIDIA et al., 2025), a family of frontier reasoning models for physical AI.\n4.1. Model Architecture\nIn previous studies, VLMs were constructed by integrating a text-only LLM with a vision encoder.\nTwo prevalent architectures for this integration are the decoder-only architecture (Liu et al., 2023a)\nand the cross-attention-based architecture (Alayrac et al., 2022). We use the decoder-only NVLM-D\narchitecture (Dai et al., 2024) due to its simplicity, parameter efficiency, ability to process high-\nresolution image inputs, and unified handling of multiple modalities by mapping non-text tokens\n(e.g., images, audio, videos) into the same embedding space as text tokens. Nemotron-H-VLM\ncomprises a vision encoder, a projector (a two-layer FFN), and the Nemotron-H LLM backbone.\nVision encoder.\nFor both the 8B and 56B VLMs, we use InternViT-300M-V2.5 (Chen et al.,\n2024) as the vision encoder. It processes static 448×448 pixel images as input with patch size 14×14,\nand generates 1024 visual tokens in total. Each visual token is represented by a 1024-dimensional\nvector. Following the NVLM design (Dai et al., 2024), we dynamically resize the input image to the\nclosest predefined aspect ratio {𝑎: 𝑏} (𝑎and 𝑏are integers) based on its resolution and segment it\ninto 𝑎× 𝑏≤12 tiles13, each corresponding to a 448×448-pixel image tile. To preserve global context,\nwe also generate a thumbnail 448×448 tile, which is a scaled-down version of the whole image. For\nexample, given a 1920×1080 image, the closest predefined aspect ratio is 3 : 2. The image is resized\nto 1344×896 and divided into 3 × 2 + 1 = 7 tiles, including one thumbnail tile.\nTo reduce processing overhead for the LLM, we downsample 1024 tokens to 256 by grouping four\nneighboring image tokens and concatenating them along the channel dimension. The image tokens\nfrom multiple tiles are concatenated with an interleaved tile ID tag in raw text format, which gives\nthe downstream LLM information about the dynamic tiling structure; we find this is crucial to\nimproving accuracies on various vision-language benchmarks. For more details, see the NVLM\ntechnical report (Dai et al., 2024). The concatenated visual tokens are processed by a two-layer FFN\nblock, which maps each visual token into the text token embedding space. These embeddings are\nthen fed into the LLM backbone.\n4.2. Training Method and Data\nFollowing NVLM (Dai et al., 2024), Nemotron-H-VLM is trained in two stages:\n1. VL pre-training. We train only the two-layer FFN for modality alignment while keeping\nboth the Nemotron-H backbone and vision encoder frozen.\n2. VL SFT. We fine-tune the vision encoder, FFN projector, and Nemotron-H backbone end-to-\nend on various task-oriented SFT data.\nFor VL pre-training, we utilize a large and diverse image-text pre-training dataset from NVLM (Dai\net al., 2024), including captioning (Lin et al., 2015; Sharma et al., 2018; Ordonez et al., 2011; Li\net al., 2022), visual question answering (VQA) on natural image (Goyal et al., 2017; Krishna et al.,\n2017), visual chart (Kafle et al., 2018) and document understanding (Marafioti & Laurencon, 2024),\n13The predefined ratios are {𝑎: 𝑏} = {1 : 1, 1 : 2, 1 : 3, 1 : 4, 1 : 5, 1 : 6, 1 : 7, 1 : 8, 1 : 9, 1 : 10, 1 : 11, 1 : 2, 2 :\n1, 2 : 2, 2 : 3, 2 : 4, 2 : 5, 2 : 6, 3 : 1, 3 : 2, 3 : 3, 3 : 4, 4 : 1, 4 : 2, 4 : 3, 5 : 1, 5 : 2, 6 : 1, 6 : 2, 7 : 1, 8 : 1, 9 :\n1, 10 : 1, 11 : 1, 12 : 1}.\n19\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nTask\nNemotron-H\n8B-VLM\nVLM w/ Llama3.1\n8B-Instruct\nVLM w/ Qwen2.5\n7B-Instruct\nMMMU (val)\n51.3\n44.8\n51.1\nMathVista\n62.5\n61.7\n63.8\nChartQA\n84.8\n85.4\n84.9\nAI2D\n89.6\n90.9\n91.0\nOCRBench\n840\n847\n836\nTextVQA\n76.2\n76.4\n76.1\nRealWorldQA\n62.2\n60.9\n60.6\nDocVQA\n90.6\n91.3\n91.2\nTable 7 | Evaluation of Nemotron-H-8B-VLM on vision-language benchmarks. We compare to the\nVLMs built with Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct using the same training recipe.\nTask\nNemotron-H\n56B-VLM\nVLM w/ Qwen2.5\n72B-Instruct\nNVLM-D-1.0\n72B (2024-09-17)\nMMMU (val)\n63.6\n65.1\n62.6†\nMathVista\n70.7\n70.5\n66.7†\nChartQA\n89.4\n88.9\n86.0\nAI2D\n94.7\n94.9\n94.2\nOCRBench\n862\n869\n853\nTextVQA\n81.1\n83.5\n82.1\nRealWorldQA\n68.4\n71.4\n69.7\nDocVQA\n93.2\n92.0\n92.6\nTable 8 | We evaluate Nemotron-H-56B-VLM on a range of vision-language benchmarks, and compare\nit to a VLM built using Qwen2.5-72B-Instruct with the same training recipe. We also compare\nwith NVLM-1.0-D 72B, a previous state-of-the-art VLM developed by NVIDIA. †We evaluate\nNVLM-1.0-D 72B on MMMU and MathVista using VLMEvalKit (Duan et al., 2024); this is the\nsame evaluation setup used for other models and yields better results than the official numbers.\noptical character recognition (OCR) (Mishra et al., 2019) and scene-text recognition (Veit et al.,\n2016), and visual math reasoning (Lindström & Abraham, 2022) data. Overall, our VL pre-training\ndataset consists of 130 million samples. We apply careful sanitization to remove any potentially\nharmful content.\nFor VL SFT, we leverage diverse and high-quality datasets from NVLM (Dai et al., 2024) and\nEagle2 (Li et al., 2025). In addition to the previously mentioned categories, the dataset also includes\nknowledge-based VQA (Marino et al., 2019), visual reasoning (Hudson & Manning, 2019), science-\nrelated VQA (Lu et al., 2022), and visual instruction-following data. Overall, our VL SFT dataset\nconsists of 6 million image-text samples. Dai et al. (2024) and Li et al. (2025) have more details.\n4.3. Vision-Language Benchmark Results\nWe evaluate Nemotron-H-8B-VLM and Nemotron-H-56B-VLM on a comprehensive set of vision-\nlanguage benchmarks, including MMMU (Yue et al., 2024), MathVista (Lu et al., 2024), ChartQA\n(Masry et al., 2022), AI2D (Kembhavi et al., 2016), OCRBench (Liu et al., 2024), TextVQA (Singh\net al., 2019), RealWorldQA (xAI, 2024), and DocVQA (Mathew et al., 2021). These benchmarks\nassess a broad range of capabilities such as multimodal reasoning, mathematical reasoning in visual\ncontexts, natural image understanding, scene-text recognition, chart & document understanding,\nreal-world perception, and OCR.\n20\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nWe compare Nemotron-H-8B-VLM with VLMs built on LLaMA-3.1-8B-Instruct and Qwen2.5-7B-\nInstruct, using the same training methodology and dataset. As shown in Table 7, Nemotron-H-8B\nproves to be a strong LLM backbone for developing best-in-class VLMs. We further compare\nNemotron-H-56B-VLM with NVLM-1.0-D (Dai et al., 2024) and the VLM built with Qwen2.5-72B-\nInstruct using the same training recipe in Table 8. Nemotron-H-56B-VLM achieves state-of-the-art\nresults, demonstrating superior quality compared to previous models.\n5. Nemotron-H Reasoning Models\n5.1. Nemotron-H Vs. Transformer Model Alignment\nWe first study the extent to which our hybrid Mamba-Transformer models can be effectively post-\ntrained into instruction-tuned and long-context variants. We conducted experiments on Transformer\n(Nemotron-T-8B-Exp-Base) and hybrid (Nemotron-H-8B-Exp-Base) models pretrained on identical\ndata resulting in two models with very similar pretraining performance as seen in Table 9.\nTask\nNemotron-T\n8B-Exp-Base\nNemotron-H\n8B-Exp-Base\nMMLU (5-shot)\n70.3\n69.9\nGSM8k\n64.8\n64.1\nMBPP+ (0-shot greedy pass@1)\n53.7\n54.5\nPIQA (0-shot)\n81.1\n82.8\nOpenBookQA (0-shot)\n45.4\n46.2\nTable 9 | Comparison of Transformer and hybrid base models on selected tasks.\nWe then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct\nand Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage1, we perform\nsupervised fine-tuning (SFT) on a blend of 6 million samples including code, math, and general\ninstruction-following tasks. To improve long-context performance, we include training on extended\nconversations up to 512k tokens by concatenating shorter samples. We further enhance context\nby inserting references to earlier turns and introducing semantically related segments across the\nconversation. This improves RULER (Hsieh et al., 2024) scores even at shorter context lengths (e.g.,\n128k).\nIn stage2, we switch to preference tuning using offline RPO (Sun et al., 2025) on general-domain\nprompts. We then perform an additional RPO round focused on narrow instruction following,\nfollowing IFEval (Zhou et al., 2023) style prompts. Samples are randomly extended to 32k tokens\nusing conversations from stage1, preserving long-context capabilities.\nIn stage3, we apply on-policy RPO, generating both preferred and dispreferred completions using\nthe stage2 checkpoint. We incorporate safety data from AEGIS2.0 (Ghosh et al., 2025) and increase\nreward scaling, which improves performance on downstream benchmarks.\nWe experimented with multiple rounds of iterative offline RPO and DPO. Both yielded similar\noutcomes; we report results for the best model from either method. The best Transformer model\nused three RPO rounds; the best hybrid model used two DPO rounds.\nTable 10 shows the results. These findings support that hybrid Mamba-Transformer models can be\npost-trained to match or exceed Transformer performance.\n21\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nBenchmark\nNemotron-T\n8B-Exp-Instruct\nNemotron-H\n8B-Exp-Instruct\nMT-Bench\n7.9\n8.0\nMMLU (0-shot generative)\n68.0\n67.4\nMBPP+\n65.6\n66.9\nMATH\n68.8\n68.5\nIFEval Prompt\n74.3\n74.7\nIFEval Instruct\n81.8\n82.1\nBFCL (V2)\n64.3\n65.4\nTable 10 | Comparison of instruction-tuned Transformer and hybrid models on alignment benchmarks.\n5.2. Training Nemotron-H Reasoning Models\nFollowing the pilot study in which we found that mamba2-hybrid models and pure transformer\nmodels have similar post-training properties we proceed to train our Nemotron-H 8B Reasoning and\nNemotron-H 47B Reasoning models.\n5.2.1. Supervised Finetuning Stages\nThe training pipeline began with supervised fine-tuning (SFT), using curated examples that include\nexplicit reasoning traces—enclosed in <think>...</think> tags—to guide the model through step-\nby-step problem-solving before reaching a final answer. These traces often represent multiple possible\nsolution paths and encourage the model to explore alternatives and iterate, improving accuracy.\nHowever, the added verbosity also increases inference cost, especially for longer traces. To balance\nthis, we introduced paired examples with the reasoning stripped out, allowing the model to learn\nwhen and how to respond directly. This dual-format training helps the model adapt fluidly to\ndifferent reasoning requirements.\nStage 1: Mastering Math, Code and Science Reasoning\nThe first phase of fine-tuning\nfocused on math, science, and coding—domains where explicit reasoning is especially valuable. The\ntraining data here used a 5:1 ratio of reasoning to non-reasoning samples, and some of these examples\nare available publicly in the Llama-Nemotron-Post-Training-Dataset.\nStage 2: Expanding Instructional Coverage, Dialogue and Safety\nThe second phase shifted\ntoward instruction following, safety alignment, and multi-turn dialogue—while continuing to sample\nfrom Stage 1 to retain strong STEM performance. This dataset was more compact—about 10×\nsmaller—and offered a balanced mix of reasoning and non-reasoning samples. This helped the model\ngeneralize across a broader range of tasks while improving control over reasoning mode switching.\n5.2.2. Long-Context Training\nIn order to improve long-context capabilities, we do additional training on an augmented version\nof our Stage 2 blend, consisting of samples up to 512k tokens each. The samples in this extended\nblend are created by concatenating the original Stage 2 SFT conversations, as well as adding several\nadditional types of turns, with the goal of improving the long-range memory of the model. The\nadded turns are constructed as follows:\n1. Long-range references to existing conversations. A random short conversation that appears\n22\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nFigure 12 | After selecting a short conversation\nfrom the original blend, we generate multiple\nturns referencing turns from that conversation.\nThese turns are inserted at some point later in\nthe extended sample.\nFigure 13 | Multiple segments are generated with\na unified theme and scattered throughout the\ncontext. Afterwards, we generate turns that that\nrefer to one or more of these segments which are\nplaced later in the sample.\nearlier in the concatenated sample is selected; we then generate follow-up user queries that\nexplicitly references this turn sequence by topic (e.g., “In the part of our conversation where we\ndiscussed modern authors...”). Both the referencing questions and answers are generated with\nDeepSeek-R1 (DeepSeek-AI, 2025a) These reference pairs are inserted throughout conversations\nin both the reasoning and non-reasoning subsets of the data blend (see Figure 12). This method\ndoes not require relying on the generator model’s long context understanding, as we only need\nthe referenced conversation as context.\n2. Multi-turn references In addition to turns referencing conversations from the short blend, we\nalso generate multiple turns with a unified topic that may reference each other. Topics for these\nsequences are chosen as a combination of 3 random keywords. We then generate multiple follow-up\nturns that each reference one or more of these segments, requiring the model to determine which\nof these related segments is being referred to. All of these turns are then inserted randomly\nthoughout the extended conversations (see Figure 13). Similarly to the previous type, this does\nnot require a long-context generator.\n3. Document aggregation QA. A user turn embeds a long document (up to 32k tokens) drawn\nfrom the pre-training data. The turn is followed by several questions whose answers require\nintegrating information scattered across the long document.\n4. Common-word retrieval with distractors. To target the RULER “common-words” subtask,\nwe insert user turns that list 30-80k tokens comprising a list of keywords interleaved with random\ndistractors. The assistant is asked to return the 𝑛most frequent words. This specialised task is\nused only in the non-CoT subset. It is created without the use of a language model.\nWe find that hybrid models can learn to operate on higher context length within just a few hundred\nsteps of training on these 512k-token extended blends. We evaluated this capability using the\nRULER (Hsieh et al., 2024) benchmark in non-reasoning mode, with the model achieving an 83%\nRULER score on 128k sequence length after this stage.\n5.2.3. Reinforcement Learning with GRPO\nAfter SFT, we applied Group Relative Policy Optimization (GRPO) in multiple phases (Shao\net al., 2024). Each phase targeted specific skills—like instruction following or tool use—by creating\ntask-specific datasets with automatic verifiers, followed by broader fine-tuning with a general reward\nmodel.\n23\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nInstruction-Following Tuning\nTo strengthen instruction adherence, we sampled 16,000 prompts\nfrom the LMSYS Chat dataset and paired them with IFEval-style instructions.\nA rule-based\nverifier scored outputs based on how well they satisfied each instruction, creating a reward signal\nthat prioritized following directions with precision. We have used TRT-LLM backend for policy\nrollouts. IFEval RL experiments provided significant boost to IFEval capabilities while the rest of\nthe benchmarks fluctuated slightly requiring careful checkpoint selection. We have used a learning\nrate of 1e−6 which showed consistent improvement in instruction following capabilities without\nregressions in other model capabilities.\nGeneral Helpfulness via Reward Model\nIn the final RL phase, we introduced a Qwen-32B-\nbased reward model (scoring 92.8 on RewardBench) to improve overall response helpfulness. We\ntrained using GRPO and prompts from HelpSteer2 Wang et al. (2024b). This final stage led to\nnoticeable gains in output quality.\n5.2.4. Controlled Reasoning Inference\nInference-time behavior can be customized using simple control tags in the system prompt:\n• {‘reasoning’: True} triggers reasoning mode\n• {‘reasoning’: False} triggers direct-answer mode\n• Omitting the tag lets the model choose\nOur Jinja chat template detects these control strings and modifies the assistant’s response accord-\ningly. When {‘reasoning’: True} is present, the response is prefixed with Assistant:<think>\\n,\nindicating the start of a reasoning trace. When {‘reasoning’: False} is found, the response is\nprefixed with Assistant:<think></think>, signaling a non-reasoning response. This mechanism\nresulted in near 100% control of reasoning or non-reasoning modes.\n5.3. Results\n5.3.1. Nemotron-H-47B-Reasoning-128K\nAcross benchmarks in math, coding, science, tool use, and dialogue, Nemotron-H-47B-Reasoning-\n128K achieves accuracy on par with or better than Llama-Nemotron Super 49B V1.0, and outperforms\nQwen3 32B on all non-coding benchmarks. The model supports post-training quantization of all\nlinear layers, enabling efficient deployment with minimal accuracy loss. We also provide the quantized\ncheckpoint and its corresponding results to demonstrate effectiveness in practical settings.\n5.3.2. Nemotron-H-8B-Reasoning-128K\nCheckpoint merging\nWe observed that the smaller 8B model struggled on internal safety\nbenchmarks. To address this, we opted for checkpoint interpolation Wortsman et al. (2022), blending\nin an earlier RL checkpoint aligned from a safer long-context extended checkpoint. Checkpoint\ninterpolation is performed by linearly interpolating model weights: (1−𝛼)*𝑤𝑚𝑜𝑑𝑒𝑙1 +𝛼*𝑤𝑚𝑜𝑑𝑒𝑙2. We\nexperimented with a parameter sweep over 𝛼values from 0.1 to 0.9 in increments of 0.1, and found\nthat values around 0.5 offered a good trade-off between reasoning capabilities and safe behavior.\nAcross benchmarks in AIME25, science, tool use, and instruction-following, Nemotron-H-8B-\nReasoning-128K achieves accuracy on par with or better than Llama-Nemotron Nano 8B V1.0.\nThe model supports post-training quantization of all linear layers, enabling efficient deployment with\n24\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nBenchmark\nLlama-Nemotron\nSuper V1\nNemotron-H 47B\nReasoning\nNemotron-H 47B\nReasoning (FP8)\nQwen3\n32B\nAIME25\n53.0\n54.2\n54.2\n55.8\nMATH500\n96.6\n96.2\n96.8\n91.4\nGPQA-D\n65.1\n65.7\n64.6\n65.2\nMBPP\n90.2\n91.8\n91.9\n95.5\nMBPP_PLUS\n75.1\n79.9\n79.9\n81.5\nLCB\n41.2\n50.2\n53.4\n64.2\nBFCL\n72.7\n72.9\n73.2\n71.3\nIFEVAL-Prompt\n81.8\n84.5\n84.5\n83.9\nIFEVAL-Instruction\n87.4\n89.7\n89.6\n88.6\nArena Hard\n85.0\n85.0\n80.5\n93.8\nTable 11 | Comparison of benchmark performance across multiple model variants. Best scores per\nrow are shown in bold.\nBenchmark\nLlama-Nemotron\nNano 8B v1.0\nNemotron-H 8B\nReasoning\nNemotron-H 8B\nReasoning (FP8)\nAIME25\n51.4\n51.7\n46.7\nMATH500\n94.8\n94.0\n94.0\nGPQA-D\n53.5\n55.1\n54.5\nMBPP\n84.1\n86.0\n86.2\nMBPP_PLUS\n71.4\n74.1\n73.3\nLCB\n52.6\n49.5\n44.8\nBFCL\n63.6\n68.8\n70.6\nIFEVAL-Prompt\n71.8\n71.4\n71.5\nIFEVAL-Instruction\n79.3\n79.6\n79.9\nTable 12 | Comparison of benchmark performance across multiple 8B model variants. Best scores\nper row are bolded.\nminimal accuracy loss. We also provide the FP8 quantized checkpoint and its corresponding results\nto demonstrate effectiveness in practical settings.\n6. Conclusions\nThe Nemotron-H family of models demonstrates that hybrid model architectures can offer the best\nof both worlds: comparable to state-of-the-art Transformer models in terms of capabilities while\noffering improved inference speed. We find the Nemotron-H base models offer either better or on-par\naccuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3× faster at inference. We also find\nthese base models can be adapted to yield effective VLMs as well as instruction-tuned models.\nAdditionally, both our FP8-based training recipe and MiniPuzzle can be used to reduce the cost of\ncreating these models.\nWe have released the Nemotron-H base checkpoints described in this paper with support in Hugging\nFace and NeMo to facilitate further research:\n• Nemotron-H-56B-Base. Hugging Face and NGC.\n• Nemotron-H-47B-Base. Hugging Face and NGC.\n• Nemotron-H-8B-Base. Hugging Face and NGC.\n25\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nNemotron-H Reasoning checkpoints described are also available in Hugging Face:\n• Nemotron-H-47B-Reasoning. Hugging Face.\n• Nemotron-H-8B-Reasoning. Hugging Face.\nContributors\nWe thank the following people for their invaluable contributions to Nemotron-H.\nData.\nAbhinav Khattar, Aleksander Ficek, Amala Sanjay Deshmukh, Andrew Tao, Ayush\nDattagupta, Brandon Norick*, Chengyu Dong*, Dan Su*, Daria Gitman, Evelina Bakhturina,\nIgor Gitman, Ilia Karmanov, Ivan Moshkov, Jane Polak Scowcroft, Jarno Seppanen, Jiaxuan You,\nJocelyn Huang, John Kamalu*, Joseph Jennings*, Jupinder Parmar*, Karan Sapra, Kateryna\nChumachenko, Kezhi Kong*, Lukas Voegtle, Lindsey Pavao, Markus Kliegl*, Matvei Novikov,\nMehrzad Samadi, Miguel Martinez, Mostofa Patwary*, Osvald Nitski, Philipp Fischer, Pritam\nGundecha, Rabeeh Karimi Mahabadi, Sean Narenthiran, Sanjeev Satheesh*, Seungju Han, Shrimai\nPrabhumoye*, Shubham Pachori, Shubham Toshniwal, Siddhartha Jain, Somshubra Majumdar,\nSyeda Nahida Akter*, Timo Roman, Ushnish De, Vahid Noroozi, Vitaly Kurin, Wasi Uddin Ahmad,\nWei Du, Yao Xu, Yejin Choi, Ying Lin*.\nFP8 recipe. Carlo del Mundo, Dusan Stosic, Eric Chung, Jinze Xue, John Kamalu, Kirthi Sivamani,\nMike Chrzanowski*, Mohammad Shoeybi, Mostofa Patwary, Oleg Rybakov*, Paulius Micikevicius,\nPeter Dykas*, Przemek Tredak, Zhongbo Zhu.\nModel architecture. Brandon Norick*, Duncan Riach*, Roger Waleffe*, Wonmin Byeon*.\nPre-training. Deepak Narayanan*, Hongbin Liu, Kunlun Li, Maciej Bala, Michael Andersch,\nMikolaj Blaz, Oleg Rybakov, Peter Dykas, Roger Waleffe*, Sangkug Lym, Selvaraj Anandaraj,\nSeonmyeong Bak, Slawek Kierat, Szymon Migacz, Xiaowei Ren.\nInfrastructure. Aaron Blakeman, Aarti Basant, Ashwin Poojary, Brian Butterfield, Christine\nHarvey, Ding Ma, Dong Ahn, Gargi Prasad, Hui Li, Jason Sewall, Jing Zhang, Jining Huang, Kumar\nAnik, Maer Rodrigues de Melo, Mohamed Fawzy, Ning Xu, Pasha Shamis, Pierre-Yves Aquilanti,\nRahul Kandu, Ruoxi Zhang, Sabrina Kavanaugh, Sergey Kashirsky, Shelby Thomas, Sirshak Das,\nSriharsha Niverty, Stefania Alborghetti, Tal Shiri.\nDistillation. Akhiad Bercovich*, Ali Taghibakhshi*, Daniel Korzekwa, Elad Segal*, Izik Golan*,\nMarcin Chochowski*, Mostofa Patwary, Pavlo Molchanov*, Ran El-Yaniv, Raviraj Joshi, Roger\nWaleffe, Saurav Muralidharan*, Sharath Turuvekere Sreenivas*, Tomer Ronen*.\nVLM. Andrew Tao, Boxin Wang*, Danny Yin, Fuxiao Liu, Guilin Liu, Guo Chen, Jason Lu, Jon\nBarker, Lukas Voegtle, Matthieu Le, Mike Ranzinger, Nayeon Lee*, Philipp Fischer, Song Han,\nTuomas Rintamaki, Tyler Poon, Wei Ping*, Wenliang Dai*, Zhiding Yu, Zhiqi Li, Zhuolin Yang*.\nAlignment and long context. Adithya Renduchintala*, Ali Taghibakhshi, Ameya Sunil Maha-\nbaleshwarkar*, Bilal Kartal*, David Mosallanezhad, Dima Rekesh*, Ellie Evans, Fei Jia*, Felipe\nSores*, Gerald Shen*, Haifeng Qian*, Hoo Chang Shin, Jiaqi Zeng, Julien Veron Vialard*, Luis\nVega*, Makesh Narsimhan Sreedhar, Michael Evans, Olivier Delalleau, Prasoon Varshney, Samuel\nKriman*, Shantanu Acharya*, Soumye Singhal, Tugrul Konuk, Yian Zhang*, Yoshi Suhara, Zijia\nChen.\nInference. Helen Ngo*, Keshav Santhanam*, Vijay Korthikanti*.\nSoftware support.\nAdithya Renduchintala, Ali Taghibakhshi, Anna Shors, Ashwath Aithal,\n26\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nBalaram Buddharaju, Bobby Chen, Cyril Meurillon, David Mosallanezhad, Deepak Narayanan,\nDmytro Pykhtar, Duncan Riach, Ellie Evans, Fei Jia, Felipe Sores, Gerald Shen, Helen Ngo, Jared\nCasper, Jimmy Zhang, Keshav Santhanam, Lawrence McAfee, Luis Vega, Michael Evans, Nima\nTajbakhsh, Olivier Delalleau, Parth Chadha, Piotr Bialecki, Roger Waleffe, Sahil Jain, Terry Kong,\nTyler Poon, Vijay Korthikanti, Yian Zhang, Yoshi Suhara, Zhiyu Li.\nEvaluations and safety. Ameya Sunil Mahabaleshwarkar*, Christopher Parisien, David Mosal-\nlanezhad*, Denys Fridman, Eileen Long, Erick Galinkin, Ewa Dobrowolska, Katherine Luna, Leon\nDerczynski, Marta Stepniewska-Dziubinska, Michael Evans, Roger Waleffe*, Sanjeev Satheesh*,\nShaona Ghosh, Shrimai Prabhumoye, Suseella Panguluri, Syeda Nahida Akter, Varun Singh.\nProgram management. Joey Conway, Krzysztof Pawelec, Shyamala Prayaga, Swetha Bhendigeri,\nTrisha Saar.\nLeadership. Alexis Bjorlin, Andrew Tao*, Boris Ginsburg*, Bryan Catanzaro*, Eric Chung, Jan\nKautz, Jonathan Cohen*, Kari Briski, Misha Smelyanskiy, Mohammad Shoeybi*, Mostofa Patwary*,\nOleksii Kuchaiev*, Sharon Clay, Song Han, Timo Roman, Wei Ping*.\n* Core contributor.\n27\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-\npoints, 2023. URL https://arxiv.org/abs/2305.13245.\nSyeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro. MIND: Math Informed syNthetic Dialogues\nfor Pretraining LLMs, 2024. URL https://arxiv.org/abs/2410.12881.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: A Visual Language\nModel for Few-shot Learning. Advances in Neural Information Processing Systems, 35:23716–23736,\n2022.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis\nTunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua\nLochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher,\nHaojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf.\nSmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model, 2025. URL\nhttps://arxiv.org/abs/2502.02737.\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.2, Knowledge Manipulation,\n2024. URL https://arxiv.org/abs/2309.14402.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with\nLarge Language Models, 2021. URL https://arxiv.org/abs/2108.07732.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Transformer,\n2020. URL https://arxiv.org/abs/2004.05150.\nAkhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah,\nIdo Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi\nKoren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny,\nRan Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and\nRan El-Yaniv.\nPuzzle: Distillation-Based NAS for Inference-Optimized LLMs, 2024.\nURL\nhttps://arxiv.org/abs/2411.19146.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about\nPhysical Commonsense in Natural Language, 2019. URL https://arxiv.org/abs/1911.11641.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language Models are Few-Shot Learners, 2020. URL https:\n//arxiv.org/abs/2005.14165.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, et al. Evaluating Large Language Models Trained on Code, 2021. URL https://arxiv.\norg/abs/2107.03374.\n28\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nZhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong\nYe, Hao Tian, Zhaoyang Liu, et al. Expanding Performance Boundaries of Open-Source Multimodal\nModels with Model, Data, and Test-Time Scaling. arXiv preprint arXiv:2412.05271, 2024.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge. ArXiv, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training Verifiers to Solve Math Word Problems, 2021. URL https://arxiv.org/\nabs/2110.14168.\nWenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki,\nMohammad Shoeybi, Bryan Catanzaro, and Wei Ping. NVLM: Open Frontier-class Multimodal\nLLMs. arXiv preprint arXiv:2409.11402, 2024.\nTri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality, 2024. URL https://arxiv.org/abs/2405.21060.\nGemma Team @ Google DeepMind. Gemma 3 Technical Report, 2025. URL https://arxiv.org/\nabs/2503.19786.\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\narXiv preprint arXiv:2501.12948, 2025a.\nDeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\n2025b. URL https://arxiv.org/abs/2501.12948.\nDeepSeek-AI. DeepSeek-V3 Technical Report, 2025c. URL https://arxiv.org/abs/2412.19437.\nHaodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong,\nYuhang Zang, Pan Zhang, Jiaqi Wang, et al. VLMEvalKit: An Open-Source Toolkit for Evaluating\nLarge Multi-Modality Models. In Proceedings of the 32nd ACM International Conference on\nMultimedia, pp. 11198–11201, 2024.\nSteven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, and\nBryan Catanzaro. Maximize Your Data’s Potential: Enhancing LLM Accuracy with Two-Phase\nPretraining, 2024. URL https://arxiv.org/abs/2412.15285.\nShaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian\nRebedea, Jibin Rajan Varghese, and Christopher Parisien. AEGIS2. 0: A Diverse AI Safety\nDataset and Risks Taxonomy for Alignment of LLM Guardrails. arXiv preprint arXiv:2501.09004,\n2025.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V\nin VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6904–6913,\n2017.\nAlbert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2024.\nURL https://arxiv.org/abs/2312.00752.\n29\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring Massive Multitask Language Understanding, 2021a. URL https:\n//arxiv.org/abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, 2021b.\nURL https://arxiv.org/abs/2103.03874.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network, 2015.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. RULER: What’s the Real Context Size of Your Long-Context\nLanguage Models?, 2024. URL https://arxiv.org/abs/2404.06654.\nDrew A Hudson and Christopher D Manning. GQA: A New Dataset for Real-World Visual Reasoning\nand Compositional Question Answering. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 6700–6709, 2019.\nKushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. DVQA: Understanding Data\nVisualizations via Question Answering. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 5648–5656, 2018.\nIlia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko,\nTimo Roman, Jarno Seppänen, Jupinder Parmar, Joseph Jennings, Andrew Tao, and Karan Sapra.\nÉclair – Extracting Content and Layout with Integrated Reading Order for Documents, 2025.\nURL https://arxiv.org/abs/2502.04223.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are\nRNNs: Fast Autoregressive Transformers with Linear Attention, 2020. URL https://arxiv.org/\nabs/2006.16236.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\nA Diagram Is Worth A Dozen Images. In ECCV, 2016.\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing Activation Recomputation in Large Transformer Models,\n2022. URL https://arxiv.org/abs/2205.05198.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual Genome: Connecting Language\nand Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer\nVision, 123:32–73, 2017.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model\nServing with PagedAttention, 2023. URL https://arxiv.org/abs/2309.06180.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik\nBansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. DataComp-LM: In Search of the Next\nGeneration of Training Sets for Language Models. Advances in Neural Information Processing\nSystems, 37:14200–14282, 2024.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping Language-Image Pre-\ntraining for Unified Vision-Language Understanding and Generation. In International Conference\non Machine Learning, pp. 12888–12900. PMLR, 2022.\n30\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nZhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang,\nYilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building Post-Training Data Strategies\nfrom Scratch for Frontier Vision-Language Models. arXiv preprint arXiv:2501.14818, 2025.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A Hybrid Transformer-Mamba\nLanguage Model, 2024. URL https://arxiv.org/abs/2403.19887.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common\nObjects in Context, 2015. URL https://arxiv.org/abs/1405.0312.\nAdam Dahlgren Lindström and Savitha Sam Abraham. CLEVR-Math: A Dataset for Compositional\nLanguage, Visual and Mathematical Reasoning. arXiv preprint arXiv:2208.05358, 2022.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. Advances\nin Neural Information Processing Systems, 36:34892–34916, 2023a.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by\nChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.\narXiv preprint arXiv:2305.01210, 2023b. doi: https://doi.org/10.48550/arXiv.2305.01210. URL\nhttps://arxiv.org/abs/2305.01210.\nYuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng\nlin Liu, Lianwen Jin, and Xiang Bai. OCRBench: On the Hidden Mystery of OCR in Large\nMultimodal Models, 2024. URL https://arxiv.org/abs/2305.07895.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for\nScience Question Answering. Advances in Neural Information Processing Systems, 35:2507–2521,\n2022.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,\nKai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating Mathematical Reasoning\nof Foundation Models in Visual Contexts. In International Conference on Learning Representations\n(ICLR), 2024.\nPratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing\nthe Web: A Recipe for Compute and Data-Efficient Language Modeling. In ICLR 2024 Workshop\non Navigating and Addressing Data Problems for Foundation Models, 2024.\nSomshubra Majumdar, Vahid Noroozi, Sean Narenthiran, Aleksander Ficek, Jagadeesh Balam, and\nBoris Ginsburg. Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for\nLarge Language Models. 2024. URL https://arxiv.org/abs/2407.21077.\nAndres Marafioti and Hugo Laurencon. Docmatix - A Huge Dataset for Document Visual Question\nAnswering, 2024.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A Visual\nQuestion Answering Benchmark Requiring External Knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition, pp. 3195–3204, 2019.\n31\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.\nChartQA: A\nBenchmark for Question Answering about Charts with Visual and Logical Reasoning. arXiv\npreprint arXiv:2203.10244, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocVQA: A Dataset for VQA on\nDocument Images. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 2200–2209, 2021.\nLlama Team @ Meta. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.\n21783.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenth-\nwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart\nOberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. FP8 Formats for Deep Learning, 2022.\nURL https://arxiv.org/abs/2209.05433.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering, 2018. URL https://arxiv.\norg/abs/1809.02789.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA:\nVisual Question Answering by Reading Text in Images. In 2019 International Conference on\nDocument Analysis and Recognition (ICDAR), pp. 947–952. IEEE, 2019.\nNiklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra\nPiktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling Data-Constrained Language\nModels. Advances in Neural Information Processing Systems, 36, 2024.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa\nPatwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact\nLanguage Models via Pruning and Knowledge Distillation, 2024. URL https://arxiv.org/abs/\n2407.14679.\nNVIDIA. Nemotron-4 340B Technical Report, 2024. URL https://arxiv.org/abs/2406.11704.\nNVIDIA, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin\nCui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth\nGururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan\nKhan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-\nChen Lin, Ming-Yu Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero,\nMisha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang\nWang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, and Zhe\nZhang. Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning, 2025. URL\nhttps://arxiv.org/abs/2503.15558.\nOpenAI.\nLearning\nto\nReason\nwith\nLLMs,\n2025.\nURL\nhttps://openai.com/index/\nlearning-to-reason-with-llms/.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing Images using 1 Million\nCaptioned Photographs. Advances in Neural Information Processing Systems, 24, 2011.\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian,\nDan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa,\nJiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel\n32\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nMartinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath\nAithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-\n4 15B Technical Report. arXiv preprint arXiv:2402.16819, 2024. URL https://arxiv.org/abs/\n2402.16819.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An Open\nDataset of High-Quality Mathematical Web Text, 2023.\nGuilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro\nVon Werra, Thomas Wolf, et al. The FineWeb datasets: Decanting the Web for the Finest Text\nData at Scale. Advances in Neural Information Processing Systems, 37:30811–30849, 2024.\nQwen. Qwen2.5 Technical Report, 2025. URL https://arxiv.org/abs/2412.15115.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations\nToward Training Trillion Parameter Models, 2020. URL https://arxiv.org/abs/1910.02054.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale, 2019. URL https://arxiv.org/abs/1907.\n10641.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A Cleaned,\nHypernymed, Image Alt-text Dataset for Automatic Image Captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556–2565, 2018.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism, 2020. URL https://arxiv.org/abs/1909.08053.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. Towards VQA Models That Can Read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 8317–8326, 2019.\nDavid R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer:\nSearching for Efficient Transformers for Language Modeling, 2022. URL https://arxiv.org/\nabs/2109.08668.\nSharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil\nMahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan\nYu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel\nKorzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, and Bryan\nCatanzaro. LLM Pruning and Distillation in Practice: The Minitron Approach, 2024. URL\nhttps://arxiv.org/abs/2408.11796.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a\nRefined Long-Horizon Pretraining Dataset, 2024. URL https://arxiv.org/abs/2412.02595.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced\nTransformer with Rotary Position Embedding, 2023. URL https://arxiv.org/abs/2104.09864.\n33\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nShengyang Sun, Yian Zhang, Alexander Bukharin, David Mosallanezhad, Jiaqi Zeng, Soumye\nSinghal, Gerald Shen, Adithya Renduchintala, Tugrul Konuk, Yi Dong, Zhilin Wang, Dmitry\nChichkov, Olivier Delalleau, and Oleksii Kuchaiev. Reward-aware Preference Optimization: A\nUnified Mathematical Framework for Model Alignment, 2025. URL https://arxiv.org/abs/\n2502.00203.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction\nData. arXiv preprint arXiv:2410.01560, 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need, 2023. URL https://arxiv.org/abs/\n1706.03762.\nAndreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie.\nCOCO-Text:\nDataset and Benchmark for Text Detection and Recognition in Natural Images. 2016. URL\nhttps://arxiv.org/abs/1601.07140.\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert\nGu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh,\nJared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An Empirical Study of\nMamba-based Language Models, 2024. URL https://arxiv.org/abs/2406.07887.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A More Robust and Challenging Multi-Task\nLanguage Understanding Benchmark, 2024a. URL https://arxiv.org/abs/2406.01574.\nZhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang,\nMakesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training\ntop-performing reward models. arXiv preprint arXiv:2406.08673, 2024b. doi: 10.48550/arXiv.\n2406.08673.\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig\nSchmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without\nincreasing inference time, 2022. URL https://arxiv.org/abs/2203.05482.\nxAI. Grok-1.5 Vision Preview: Connecting the Digital and Physical Worlds with our First Multimodal\nModel, 2024.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\nBoyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for\nExpert AGI. In Proceedings of CVPR, 2024.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine\nReally Finish Your Sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\nBiao Zhang and Rico Sennrich.\nRoot Mean Square Layer Normalization, 2019.\nURL https:\n//arxiv.org/abs/1910.07467.\n34\n\nNemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following Evaluation for Large Language Models. arXiv preprint\narXiv:2311.07911, 2023.\n35\n"
    },
    {
      "arxiv_id": "2508.14444",
      "full_text": "2025-9-3\nNVIDIA Nemotron Nano 2: An Accurate and\nEfficient Hybrid Mamba-Transformer Reasoning\nModel\nNVIDIA\nAbstract. We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving state-of-the-art accuracy\ncompared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture,\nin which the majority of the self-attention layers in the common Transformer architecture are replaced\nwith Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces\nneeded for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter\nmodel (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After\naligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill\nthe model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G\nGPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g.,\nQwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning\nbenchmarks while achieving up to 6× higher inference throughput in reasoning settings like 8k\ninput and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-\n12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and\npost-training datasets on Hugging Face.\n1. Introduction\nWe introduce NVIDIA Nemotron Nano 2, a hybrid Mamba-Transformer reasoning model (Waleffe\net al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better\nbenchmark accuracies at 3×–6× higher throughput than Qwen3-8B (Yang et al., 2025) for generation-\nheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron\nNano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and\nrecipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints,\nas well as the majority of the pre- and post-training datasets.\nThe initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over\n20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5). It then\nunderwent a continuous pre-training long-context extension phase to become 128k-capable without\ndegrading other benchmarks (§2.6). Overall, new and improved datasets led to significant accuracy\nimprovements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (§2.2).\nNemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT),\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang\net al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains,\nfollowed by targeted SFT on key areas such as tool use, long-context performance, and truncated\n(budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability,\nwhile additional DPO stages further strengthened tool use. Overall, post-training was performed\non roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning\n© 2025 NVIDIA. All rights reserved.\narXiv:2508.14444v4  [cs.CL]  2 Sep 2025\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nIFBench\n(Instr. Following)\nAIME24\n(Math)\nAIME25\n(Math)\nGPQA-D\n(Science)\nLiveCodeBench\n(Coding)\nBFCLv3\n(Tool Use)\nRULER 128k\n(Long Context)\nISL/OSL\n1k/8k\nISL/OSL\n8k/16k\n30\n40\n50\n60\n70\n80\n90\nAccuracy (%)\n34.6\n81.9\n72.0\n64.0\n71.1\n66.9\n78.9\n33.0\n75.8\n69.3\n59.6\n59.5\n66.3\n74.1\nMeasured Accuracy\nMeasured Throughput\nNVIDIA-Nemotron-Nano-9B-v2\nQwen3-8B\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative Throughput (Output tokens/s/GPU)\n3.3\n6.3\n1.0\n1.0\nFigure 1 | Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput.\nNemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while\nachieving up to 6.3× higher throughput for such workloads. We abbreviate input sequence length\nto ISL and output sequence length to OSL and measure throughput on a single A10G GPU in\nbfloat16.\ntraces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained\nthinking budget control at inference time (§3.4).\nFinally, both the base model and aligned model were compressed so as to enable inference over\ncontext lengths of 128k tokens on a single NVIDIA A10G GPU (22 GiB of memory, bfloat16\nprecision). This was done by extending a compression strategy based on Minitron (Muralidharan\net al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject\nto constraints.\nWe are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning.\nAdditionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-\nTraining-Dataset-v1 collection of more than 6 trillion tokens:\n• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional\nCommon Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic\nQ&A data translated into 15 languages.\n• Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM\npipeline (Mahabadi et al., 2025). Preserves equations, standardizes to LaTeX, outperforms\nprevious math datasets on benchmarks.\n• Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering,\ndeduplication, and quality filters. Includes code Q&A data in 11 programming languages.\n• Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual,\nacademic, and reasoning domains.\nFinally, we are releasing an updated post-training dataset:\n2\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nNemotron-Nano-12B-v2-Base\nx3\nx6\nx1\nFigure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the\ntotal layers in the model are self-attention layers which are evenly dispersed throughout the model.\nModel\nNumber of\nlayers\nModel\ndimension\nFFN\ndimension\nQ\nheads\nKV\nheads\nState\ndimension\nMamba\ngroups\nNemotron-Nano-12B-v2-Base\n62\n5120\n20480\n40\n8\n128\n8\nTable 1 | Summary of Nemotron-Nano-12B-v2-Base architecture.\n• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases\nwith an extension of SFT and RL data into five target languages: Spanish, French, German,\nItalian and Japanese. The data supports improvements of math, code, general reasoning, and\ninstruction following capabilities.\nThe rest of this technical report is organized as follows: In §2, we discuss the Nemotron Nano 2 model\narchitecture, pre-training process, and base model evaluation results. In §3, we discuss the alignment\nprocess. In §4, we describe the pruning and distillation methods used for model compression.\n2. Pretraining\nIn this section, we discuss the architecture and pretraining of the Nemotron-Nano-12B-v2-Base\nmodel. We also compare this model against other state-of-the-art models in terms of accuracy on\npopular benchmarks.\n2.1. Model Architecture\nAs in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-\n2 (Dao & Gu, 2024), self-attention, and FFN layers. The layer pattern and key architecture details\nare summarized in Figure 2 and Table 1. Concretely, we use 62 layers, with 6 of them being\nself-attention layers, 28 being FFN, and 28 being Mamba-2 layers. We use a hidden dimension of\n5120, FFN hidden dimension of 20480, and Grouped-Query Attention (Ainslie et al., 2023) with 40\nquery heads and 8 key-value heads. For Mamba-2 layers, we use 8 groups, a state dimension of 128,\na head dimension of 64, an expansion factor of 2, and a window size for convolution of 4. For FFN\nlayers, we use squared ReLU (So et al., 2022) activation. Again as in Nemotron-H, we do not use\nany position embeddings and use RMSNorm (Zhang & Sennrich, 2019), separate embedding and\noutput layer weights, no dropout, and we do not use bias weights for linear layers.\n3\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.2. Pre-Training Data\nNemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-\ngenerated data.\n2.2.1. Curated Data\nWe have separate data curation pipelines for the following broad data categories: general web crawl\ndata (English and multilingual), math data, and code data. We discuss each in turn next.\nEnglish web crawl data.\nWe used the Nemotron-CC dataset (Su et al., 2025), but updated to\ninclude eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13)\nusing the same pipeline. For synthetic rephrasing, we mostly switched to Qwen3-30B-A3B (from\nMistral Nemo 12B). Additionally, we used data from CC-NEWS through April 23, 2025, to help\nimprove the knowledge cutoff of the model. The CC-NEWS data was filtered for English and globally\nfuzzily de-duplicated; no other filtering was used.\nMultilingual data.\nWe extracted data for fifteen languages from the following three Common\nCrawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, and CC-MAIN-2025-18. The fifteen\nlanguages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean,\nPolish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual\nmodel-based quality classifiers available, we just applied heuristic filtering instead. This was done in\na similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except\nthat we had to selectively disable some heuristic filters that had very high false positive rates for\nsome languages. De-duplication was done in the same way as for Nemotron-CC. Additionally, we\nused data from Wikipedia and FineWeb-2 (Penedo et al., 2025) for these fifteen languages.\nMath data.\nMathematical content on the web is expressed in a wide range of formats, including\ninline and block LATEX, MathML, Unicode symbols, and custom renderers such as MathJax or\nKaTeX. We conducted a detailed analysis of prior math-specific extraction pipelines—including\nOpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025), jusText (Endrédy & Novák,\n2013), Trafilatura (Barbaresi, 2021), and Resiliparse (Bevendorff et al., 2018)—and found that none\ncould reliably preserve mathematical expressions or code structure. These tools frequently discard or\ndistort equations and flatten code formatting, severely limiting the utility of the extracted content\nfor pretraining.\nTo address this, we built a new pipeline specifically designed for high-fidelity mathematical ex-\ntraction from Common Crawl. We first aggregated a comprehensive list of math-related URLs\nfrom prior datasets (e.g., InfiMM-WebMath (Han et al., 2024), OpenWebMath (Paster et al., 2023),\nFineMath (Allal et al., 2025), and MegaMath (Zhou et al., 2025)), then re-fetched their raw HTML\ndocuments from 98 Common Crawl snapshots (2014–2024). Each page was rendered using the lynx\ntext-based browser to preserve layout and math structure. We then applied Phi-4 (Abdin et al.,\n2024)(14B-parameters) to remove boilerplate, standardize notation into LATEX, and correct inconsis-\ntencies. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed\nby fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk\n& Motwani, 1998) via the NeMo-Curator framework.1 We finally decontaminated the dataset using\nLLM Decontaminator (Yang et al., 2023).\nThis process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token\nsubset, Nemotron-CC-Math-4+, containing only the top-scoring samples. When used for pretraining,\nthis dataset yields substantial improvements across math (MATH-500), code (HumanEval+, MBPP+,\n1https://github.com/NVIDIA-NeMo/Curator\n4\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMBPP), and general-domain evaluations (MMLU, MMLU-STEM, MMLU-Pro), surpassing all\nexisting open math datasets. For full details, see Mahabadi et al. (2025).\nCode data.\nIn line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar\net al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. All source\ncode used to train this model originated from GitHub and went through a multi-stage processing\npipeline to arrive at the final source code training data. We performed license-based removal with a\nlicense detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but\nwith fewer accepted licenses (see Appendix A for additional details). De-duplication is especially\nimportant for source code, where many files can be found exactly duplicated across numerous\nrepositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using\nMinHash LSH). In order to build a better understanding of each file in our dataset, we annotated all\nfiles with a variety of measures and then performed filtering using these annotations. We found the\nheuristic filters from OpenCoder (Huang et al., 2025) to be effective and leveraged them to filter\nfiles that are less valuable or even detrimental for LLM pretraining.\n2.2.2. Synthetically-Generated Data\nSTEM data.\nWe generated synthetic data for STEM subjects, including Astronomy, Biology,\nChemistry, Math, and Physics using 88.6k questions collected from multiple sources as the seed\ndata. In addition to the widely used GSM8K, MATH, and AOPS training sets, we collected more\ndiverse questions from Stemez2 and textbooks with permissive licenses from OpenStax3 and Open\nTextbook Library.4\nWe used Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to extract questions\nfrom the exercise sections in the textbooks with additional instructions such as dropping question\nnumbering, ignoring questions that require image interpretation, and formatting equations using\nLaTeX. We manually curated the extracted questions to fix occasional OCR errors and removed\nnon-self-contained questions (e.g., a question that refers to an example in the same chapter).\nTo expand both the quantity and diversity of questions, we conducted three iterations of question\ngeneration using four models (i.e., Qwen3-30B-A3B and Qwen3-235B-A22B (Yang et al., 2025), both\nwith thinking mode enabled, Deepseek-R1 (DeepSeek-AI, 2025a), and Deepseek V3 (DeepSeek-AI,\n2025b)) and three prompts:\n1. Similar question: Create a new question that explores similar concepts but offers a fresh\nchallenge.\n2. Harder question: Create a new question that requires more logical steps or involves more\nadvanced concepts.\n3. Varied question: Create a new question that differs in type from the original question. We\ninstructed the model to avoid superficial or trivial modifications and think through the solution\nwhen creating a new question.\nWe filtered out duplicates and highly-similar questions using fuzzy de-duplication and generated\nsolutions to the remaining questions with the models used in the question generation step. We\nconverted a subset of examples to multiple-choice questions in MMLU or MMLU-Pro style. We\nconstructed a few thousand few-shot examples by concatenating random synthetic samples.\nMath data.\nWe also revisited and regenerated the Nemotron-MIND dataset (Akter et al., 2024),\na math-informed synthetic pretraining corpus originally built on OpenWebMath. In our updated\n2https://www.stemez.com/\n3https://openstax.org\n4https://open.umn.edu/opentextbooks/\n5\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nversion, we regenerated the MIND dataset using Nemotron-CC-Math-4+, our highest-quality math\nsubset comprising 52B tokens—as the source corpus. Following the original methodology, we applied\nseven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured\nmathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B\ntokens of lower-fidelity data, our version leverages significantly higher-quality input and processes\nit with a chunk size of 5K tokens. This regeneration produced a 73B-token synthetic dataset and\nled to consistent improvements across math reasoning and general knowledge (MMLU, MMLU-Pro.\nMMLU-Stem) benchmarks compared to the original MIND version, highlighting the critical role of\ninput data quality. Full details and results are available in Mahabadi et al. (2025).\nMultilingual data.\nWe generated multilingual diverse question and answer data (Diverse QA) (Su\net al., 2025) from two sources:\n1. We translated the English Diverse QA data to fifteen languages (see Multilingual data) using\nQwen3-30B-A3B (Yang et al., 2025).\n2. We generated synthetic data from Wikipedia articles in these languages using the Diverse QA\nprompt and instructed the model to write all questions and answers in the target language.\nIn addition, we translated a subset of our GSM8K augmentation data (see STEM data) into\nthese languages using Qwen3-30B-A3B. We post-processed each translated solution by appending\na concluding sentence meaning “The answer is ...” (e.g., “La respuesta es ...” in Spanish, “Die\nAntwort lautet ...” in German), where the final numerical answer is extracted from the original\nEnglish solution.\nCode data.\nWe generated question-answer (QA) data at scale for 11 different programming\nlanguages by prompting an LLM to generate questions based on short snippets from our curated\nsource code, asking the model to solve the generated question, and then performing post hoc filtering\nof the generated QA pairs based on heuristics as appropriate (e.g., Python AST parsing). This\ntechnique results in diverse synthetic data targeted at problem solving containing both natural\nlanguage and source code. Further details are covered in the Nemotron-H technical report (NVIDIA,\n2025), where we first leveraged this type of synthetic code data in pretraining.\nAcademic data.\nIn the pretraining set for the Nemotron-H (NVIDIA, 2025) series of models, we\nassigned attribute labels for educational quality, educational difficulty, and educational subject to\nall documents coming from academic data, which encompasses textbooks and academic papers. As\ncontent of higher educational difficulty in technical domains still proves challenging for models, we\nprioritized increasing model comprehension of such information in our current pretraining set via\nthe generation of question-answer (QA) pairs as such data has been shown to enhance knowledge\nstorage and extraction within language models (Allen-Zhu & Li, 2024).\nTo do so, we first gathered all documents with educational difficulty at the undergraduate and\ngraduate levels in the following technical subject areas: math, chemistry, biology, physics, and\nmedicine. Using this subset of documents, we aim to find the most relevant pieces of texts that could\nbe utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets\nof 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them\nwithin a Milvus vector database that enables approximate nearest neighbor search. We then curate\ndocuments from a set of complex subject areas (e.g. Mathematics: Real Analysis, Biology: Genetics,\nStatistics: Information Theory), and query the Milvus database for the 250 nearest neighbor text\nsnippets to each query document. The returned snippets function as our seed contexts that we then\npass into a Qwen-2.5 72B instruct model (Qwen, 2025) to generate multiple choice and free response\nstyle QA pairs based on the information contained in the snippet. With each QA pair, a justification\nfor the answer is additionally generated.\n6\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSFT-style data.\nUsing SFT-style data in the later stages of pretraining has shown to be helpful\nto foster more comprehensive model learning (Hu et al., 2024).\nTherefore, we synthesized and included different SFT-style data covering several domains: 1) code\nSFT data which is mainly focused on solving code problems; 2) math SFT data that is mostly focused\non reasoning; 3) MMLU-style SFT data which contains different question and answer examples\ncovering different knowledge topics; and 4) general instruction following SFT data.\nWe ensure that the SFT-style data covers diverse topics with different difficulty levels for each of\nthe above mentioned domains. Detailed synthesis methods and pipelines for the above mentioned\nSFT data can be found in prior work (Toshniwal et al., 2024; Moshkov et al., 2025; Bercovich et al.,\n2025a,b; Ahmad et al., 2025b,a; Majumdar et al., 2024).\nFundamental reasoning SFT-style data.\nWhile the above mentioned SFT-style data help\nenhance an LLM’s ability to answer questions in code, math and general language understanding\nbenchmarks, they do not help improve the model’s ability in deeper reasoning tasks to discern\nthe correct answer among a larger pool of potential distractors. We propose to mitigate that\nby synthesizing SFT-style data focused on analytical reasoning, logical reasoning, and reading\ncomprehension.\nSpecifically, we collected existing datasets including 1) the Law School Admission Test (LSAT)\ndataset from Wang et al. (2022); Zhong et al. (2022) which encompasses three tasks: logical\nreasoning, reading comprehension, and analytical reasoning, 2) the repurposed LogiQA dataset\nby Liu et al. (2020) which contains various types of logical reasoning questions collected from the\nNational Civil Servants Examination of China, and 3) the AQuA-RAT dataset which emphasizes\nalgebraic word problems by Ling et al. (2017). We then prompted DeepSeek-V3 (DeepSeek-AI,\n2025b) and Qwen3-30B-A3B (Yang et al., 2025) respectively to synthesize more similar questions\nwith corresponding options. For each question we generated, we prompted DeepSeek-V3 again to\ngenerate the chain-of-thought (CoT) process with the final solution. At the post-processing stage,\nwe apply majority voting to keep only the samples that have the most voted solutions. Overall, we\ngenerated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models.\n2.3. Data Mixture and Ordering\nOur data mixture consists of thirteen data categories. The largest is web crawl data, which we\nsubdivided into four categories based on the Nemotron-CC quality classification (Su et al., 2025):\ncrawl-medium, crawl-medium-high, crawl-high, syn-crawl-high denoting medium, medium-high, high\nand synthetic quality crawl data, respectively. Apart from these, our data mixture has additional\ncategories such as math, wikipedia, code, academic data, crawl++, multilingual, and synthetic\nSFT-style data which is further categorized as general-sft, stem-sft and code-sft. Crawl++ consists\nof web-crawl derivatives like OpenWebText, BigScience and Reddit. Our multilingual data has fifteen\nlanguages: Arabic, Danish, German, Spanish, French, Italian, Portuguese, Dutch, Polish, Swedish,\nThai, Chinese, Japanese, Korean, and Russian. We design the data mixtures to give similar weight\nto data sources that have similar quality. Data sources of higher quality are weighed higher than\ndata sources of lower quality. We provide detailed explanation on quality estimation of datasets and\nthe blend creation process in Feng et al. (2024) and NVIDIA (2025).\nWe used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-\n12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the\nsecond and third phases, we primarily used high-quality datasets (e.g., Wikipedia). We switched to\nthe second phase at the 60% point of training, and to the third phase at the 90% point of training.\nThe data mixtures used in each phase are shown in Figure 3.\n7\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nstem-sft\n3.1%\nmultilingual\n5.0%\nacademic\n4.4%\ncode\n20.0%\nmath\n3.2%\nsyn-crawl-high\n16.2%\ncrawl-medium\n18.3%\ncrawl-medium-high\n14.8%\ncrawl-high\n11.1%\n(a) Data mixture of Phase 1.\ncode-sft\n4.4%\nstem-sft\n14.5%\nmultilingual\n5.0%\ncrawl++\n4.4%\nacademic\n3.8%\nwiki\n0.9%\ncode\n20.0%\ncrawl-high\n16.0%\nsyn-crawl-high\n21.0%\nmath\n9.5%\n(b) Data mixture of Phase 2.\ncode-sft\n10.9%\nstem-sft\n32.0%\nmultilingual\n4.4%\ncrawl-high\n10.0%\nsyn-crawl-high\n12.7%\nmath\n11.0%\ncode\n16.0%\n(c) Data mixture of Phase 3.\nFigure 3 | Data mixtures for each phase of pre-training.\n8\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMultilingual Data\nAvg\nSp\nGe\nFr\nMa\nIt\nJa\nPo\nKo\nCommon Crawl\n37.0\n37.8\n36.5\n39.8\n34.3\n36.3\n35.3\n37.5\n38.8\nFineWeb-2\n35.1\n38.8\n35.0\n34.3\n31.5\n37.0\n33.0\n36.0\n35.3\nDiverseQA-wiki\n42.1\n44.8\n41.3\n41.8\n41.5\n44.0\n41.0\n42.3\n40.3\nDiverseQA-crawl\n47.0\n49.8\n50.8\n48.3\n46.0\n45.8\n44.5\n49.0\n42.0\nTable 2 | Comparison of multilingual datasets on the Global-MMLU Benchmark.\n2.3.1. Multilingual Data Ablation Study\nIn Section 2.2, we mentioned several large categories of multilingual data, both curated and synthetic:\n1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline.\n2. FineWeb-2 (Penedo et al., 2025).\n3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse\nQA prompt.\n4. DiverseQA-crawl: Translated from English Diverse QA data.\nIn order to decide the proper data mixture among these different multilingual data sources, we\nfirst conducted ablation experiments to compare the four multilingual data’s downstream tasks’\nperformance.\nSpecifically, we took a 1B model checkpoint that had been trained for 350B tokens, and continuous\npretrained it for another 100B tokens. We assigned 50% of the continuous pretraining data to\nmultilingual data, and the remaining 50% use our default pretraining data mixture. We evaluated\neach model’s performance using the Global-MMLU benchmark (Singh et al., 2024a); the results are\nshown in Table 2. Our curated Common Crawl-based multilingual data performed slightly better\nthan the Fineweb2-based multilingual data, while the synthesized multilingual QA pairs performed\nmuch better than the curated multilingual web crawl data. The diverse pairs translated from English\nCommon Crawl achieved the highest average score over the 8 languages we evaluated on. Therefore,\nwe assigned a much higher weight to the DiverseQA-crawl data than the other categories when\ndeciding our multilingual data mixture.\n2.3.2. Fundamental Reasoning SFT-Style Data Ablation Study\nTo show the effectiveness of the fundamental reasoning (FR) focused SFT-style data we introduced\nin Section 2.2, we took the Nemotron-H-8B (NVIDIA, 2025) intermediate checkpoint trained over\n14.5T tokens, and continuous pretrained it with another 100B tokens. We assigned 5% of the 100B\ntokens to the newly synthesized FR-SFT data (as a replacement for Common Crawl data), and kept\nall other data categories the same as in the Nemotron-H-8B’s phase 3 blend. We compared this\nmodel with Nemotron-H-8B, which had also been trained with 14.6T tokens. The detailed evaluation\nbenchmarks are introduced in Section 2.7. The comparison results are shown in Table 3. The\nSFT-style data helped improve the Nemotron-H 8B model’s performance on MMLU-Pro from 44.24\nto 56.36, and also helped increase the average MATH score by around 2 points. While MMLU-Pro\nis a more challenging benchmark that evaluates a model’s language understanding capability, it\nalso requires the model to have excellent reasoning capability to select the correct answer out of\nten choices. Our SFT data helps equip the model to select the correct answers from the other nine\ndistractors through fundamental reasoning. We noticed no decrease in the average commonsense\nreasoning and average code benchmarks.\n9\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nModel\nAvg Math\nAvg Code\nAvg Reasoning\nMMLU\nMMLU-Pro\nNemotron-H 8B\n37.92\n59.49\n71.79\n72.67\n44.24\nNemotron-H 8B\n(w/ FR-SFT data)\n39.70\n59.61\n71.43\n72.98\n56.36\nTable 3 | Ablation study of the Fundamental Reasoning (FR) focused SFT-style data.\n2.4. FP8 Recipe\nWe used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b).\nSpecifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles\nfor the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we\ncould do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas)\nin FP8; master weights are still kept in FP32. One exception to DeepSeek’s formula was that we left\nthe first and last four linear layers in BF16, as done with Nemotron-H. Also unlike the DeepSeek-V3\nrun, we left all optimizer state in FP32. We observed no training instabilities from this choice of\nnumerics.\n2.5. Hyperparameters\nWe trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence\nlength of 8192 and global batch size of 768 (6,029,312 tokens per batch). We did not use any batch\nsize ramp-up. We used a WSD (Warmup-Stable-Decay) (Hu et al., 2024) learning rate schedule\nwith a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was\ndecayed over the final 3.6 trillion tokens. Weight decay was set to 0.1, and Adam 𝛽1 and 𝛽2 were set\nto 0.9 and 0.95 respectively\n2.6. Long-Context Extension\nTo ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context\nphase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT)\nwith a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. Although\nthe target context length of Nemotron Nano 2 is 128k, in preliminary studies on the Nemotron-H\n8B model, we found it better to do CPT with 512k sequence length, instead of 256k or 128k. Our\nintuition is that longer training sequence can effectively lower the chance of long coherent documents\nbeing cut and separated by the Concat & Chunk algorithm for pretraining data loading. We used\n8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence\nlengths of 512k tokens still fits in GPU memory. We used a global batch size of 12 to ensure the\ntotal number of tokens per global batch during long-context CPT is the same as during pretraining:\naround 6M tokens. Phase LC consisted of 18.9 billion tokens.\nAdditionally, we did long-context synthetic data generation to create more high-quality data for Phase\nLC. Since the academic pretraining dataset is a good source of coherent long-context documents,\nwe used such documents that are longer than 32k tokens as seed data. We followed the methods\nmentioned in the Llama-3 (Meta, 2024) and Qwen-2.5 (Qwen, 2025) tech reports to generate long-\ncontext document QA data. We split each document into chunks of 1,024 tokens and then randomly\nselected 10% of the chunks to be fed into Qwen-2.5-72B-Instruct for data synthesis. We asked the\ngenerator to generate a QA pair based on the information in the text chunk. We concatenated the\nQA pairs and appended them to the end of the original document as a sample of the long-context\ndocument QA data. Such long-document QA provided good material for the model to learn long-\n10\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\ncontext dependencies. See Table 4 for ablation results on Nemotron-H 8B regarding train sequence\nlengths and the effects of synthetic data.\nThe data blend used in Phase LC was built based on that of Phase 3. We proportionally downscaled\nthe weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the\nnewly added long-context document-QA data. We found such a blend could effectively extend the\ncontext length of Nemotron-Nano-12B-v2-Base without degrading regular benchmark scores.\nTrain length\n128k\n256k\n256k\n512k\nSynthetic data\nyes\nno\nyes\nyes\nRULER-128k\n73.68\n70.19\n79.04\n81.04\nTable 4 | Comparisons of different train sequence lengths and synthetic data usages. Ablations were\nconducted on Nemotron-H 8B.\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGeneral\nMMLU\n78.24\n74.53\n76.44\n73.61\nMMLU-Pro 5-shot\n63.98\n59.43\n56.27\n45.12\nAGIEval English CoT\n68.03\n65.28\n59.54\n51.69\nMath\nGSM8K CoT\n91.66\n91.36\n84.00\n74.45\nMATH\n83.54\n80.50\n55.40\n42.40\nMATH Level 5\n67.61\n63.64\n29.91\n17.71\nAIME 2024 pass@32\n56.67\n30.00\n20.00\n16.67\nCode\nHumanEval+ avg@32\n61.03\n58.50\n57.55\n36.68\nMBPP+ avg@32\n61.55\n58.95\n58.56\n51.73\nCommonsense Understanding\nARC Challenge\n93.26\n90.70\n93.09\n90.44\nHellaSwag\n84.00\n79.90\n79.75\n84.15\nOpenBookQA\n46.00\n44.80\n42.00\n46.00\nPIQA\n82.54\n81.83\n79.43\n82.10\nWinoGrande\n79.24\n75.30\n75.93\n79.95\nLong Context\nRULER-128K\n84.74\n82.22\n-\n80.70\nTable 5 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models. N-Nano-V2 is\nshort for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base\nand Gemma3-12B-Base, and the best score is highlighted in each row.\n11\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.7. Base Model Evaluations\nWe run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on\ntop of lm-evaluation-harness5 for fair comparisons, with the following changes:\n1. For mathematical reasoning, we evaluate GSM8K and MATH (Cobbe et al., 2021; Hendrycks\net al., 2021b) benchmarks using greedy-decoding. We also highlight the competition-level\nslice of the MATH benchmark as “MATH Level 5”. Additionally, we report the pass@32\nperformance on AIME-2024. We use Math-Verify6 to grade all generations.\n2. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)) we evaluate\nthe EvalPlus variants along with the sanitization of generations (Liu et al., 2023), in a 0-shot\nsetup. We estimate avg@32, pass@1 from 32 generations per prompt.\n3. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged\nexcept for ARC-Challenge (Clark et al., 2018), where we present all options at the same time,\nsimilar to MMLU (Hendrycks et al., 2021a).\n4. For multilingual capability, we evaluate MGSM Shi et al. (2022) (8-shot, native CoT) and\nGlobal MMLU-Lite Singh et al. (2024b).\n5. We use RULER (Hsieh et al., 2024) as the long context benchmark. We report the average\nscores over all the 13 tasks included in RULER.\nAccuracy results for Nemotron-Nano-12B-v2-Base with comparsions to Qwen3-8B Base and Gemma3-\n12B Base are shown in Tables 5 and 6. We also include the accuracy of our 9B pruned variant of\nNemotron-Nano-12B-v2-Base which is discussed in Section 4.\n3. Alignment\nIn this section we will present the alignment process we followed to convert the base checkpoint into\nan aligned 12B checkpoint. Our process is outlined in Figure 4.\nBase\nSFT 1\nSFT 2\nSFT 3\nMerged\nGRPO\nRLHF\nDPO\nFigure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2\n12B checkpoint.\n5https://github.com/EleutherAI/lm-evaluation-harness.\n6https://github.com/huggingface/math-verify.\n12\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGlobal-MMLU-Lite\nGerman\n74.50\n68.25\n75.50\n69.75\nSpanish\n76.50\n72.75\n75.00\n74.00\nFrench\n78.25\n69.75\n74.25\n72.50\nItalian\n76.50\n73.25\n72.75\n74.00\nJapanese\n71.00\n67.00\n70.00\n71.50\nKorean\n72.50\n67.25\n67.25\n70.25\nPortuguese\n76.25\n71.25\n72.50\n75.75\nChinese\n75.50\n69.25\n75.25\n67.25\nAverage\n75.13\n69.94\n72.81\n71.88\nMultilingual Math (MGSM)\nSpanish\n93.20\n93.60\n87.60\n73.60\nGerman\n88.40\n88.40\n78.80\n66.00\nFrench\n82.40\n84.40\n82.00\n68.00\nChinese\n83.60\n82.00\n80.80\n62.00\nJapanese\n76.80\n68.80\n71.20\n56.00\nRussian\n91.20\n90.80\n85.20\n72.40\nAverage\n85.94\n84.67\n80.93\n66.33\nTable 6 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models on multilingual\nbenchmarks. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is\ncompared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each\nrow.\n3.1. Post-Training Data\nOur alignment begins with a large-scale SFT stage which trains the base model on approximately 80\nbillion tokens of prompt-response pairs. The distribution of domains is shown in Table 7.\nMath, science and coding.\nFor Math (Toshniwal et al., 2024; Moshkov et al., 2025), Science\nand Coding (Ahmad et al., 2025b,a; Majumdar et al., 2024) data, we generate responses using\nthe open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for\ntraining Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been\nreleased as part of Nemotron-Post-Training-Dataset-v17.\nTool calling.\nThe tool-calling dataset consists of single-turn, multi-turn, and multi-step conversa-\ntions.\nFor single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive-\nfunction-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using\nQwen3-235B-A22B10. Inspired by ToolACE (Liu et al., 2024) and APIGen-MT (Prabhakar et al.,\n2025), we extend this to multi-turn and multi-step settings by simulating conversations where\n7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\n8https://huggingface.co/datasets/xlam-function-calling-60k\n9https://huggingface.co/datasets/glaive-function-calling-v2\n10https://huggingface.co/Qwen/Qwen3-235B-A22B\n13\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDomain\nNumber of Samples\nMath\n1.5M\nCoding\n1.1M\nScience\n2.0M\nTool-calling\n400K\nConversational\n1.5M\nSafety\n2K\nMultilingual (all domains)\n5.0M\nTable 7 | Post-training data distribution across domains used for our SFT stages.\nQwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The\nUser-Agent reviews available tools, poses challenging queries, interacts when addressed by the\nAssistant, and judges task success at the end. Each instance is paired with a random persona from\nNemotron-Personas11 to enrich diversity of queries.\nThe Assistant-Agent receives the initial query and available tools, executes tasks by invoking tools,\ninterpreting their responses, and interacting with the User-Agent across single-turn, multi-turn,\nor multi-step scenarios. Meanwhile, the API-Server-Agent acts as a mock API server, checking\nparameters and returning either valid outputs or error messages depending on correctness. A\nlightweight rule-based tool-call verification layer further strengthens reliability by ensuring outputs\nare consistent and verifiable, and only successful trajectories are retained.\nMultilingual data.\nOur multilingual synthetic post-training data are constructed by translating\nexisting English post-training data. To address the challenges of Large Language Model (LLM)\nhallucinations and quality degradation on long inputs when generating synthetic translation data, we\nimplement a robust quality assurance pipeline. Our method involves translating inputs line-by-line\nto manage complexity and skip non-translatable content like code. We also enforce a strict bracket\nformat for reliable extraction and use language identification to filter out off-target translations,\nthereby ensuring high-quality final outputs.\nConversational data.\nFor conversational data, we use prompts from the LMSYS dataset (Zheng\net al., 2023) and generate responses using the Qwen3-235B-A22B reasoning model (Yang et al., 2025).\nWe also incorporate prompts from HelpSteer2 and HelpSteer3, paired with responses generated by\nthe same model. In addition, we draw on a subset of approximately 550k prompts from WildChat-\n1M (Li et al., 2024b), again generating reasoning responses with Qwen3-235B-A22B. We also include\nmulti-turn conversations with Deepseek R1 responses using the multi-turn conversational prompts\nused in NVIDIA (2025).\nSafety.\nWe leveraged a mix of harmful and benign prompts drawn from the Nemotron Content\nSafety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo\net al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To\nensure safety, we applied a two-step approach: initial prompting followed by filtering with guard\nmodels to verify that outputs remained safe.\n11https://huggingface.co/datasets/NVIDIA/Nemotron-Personas\n12https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n13https://huggingface.co/deepseek-ai/DeepSeek-R1\n14\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3.2. Post Training\nStage 1 SFT.\nAs Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning.\nStage 1 uses the full dataset described in Section 3.1, augmented with a subsample of roughly 10% of\nprompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces,\nenabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve\nlong-context ability from pretraining, we concatenate samples into sequences of approximately 128k\ntokens, reducing padding overhead and encouraging long-range learning.\nStage 2 SFT.\nStage 2 targets tool-calling. Although Stage 1 improved performance on most\nbenchmarks, tool-calling accuracy degraded. We attribute this to sample concatenation at 128k, which\nlikely disrupted learning of tool-calling patterns. Thus, Stage 2 was trained without concatenation,\nusing the full tool-calling dataset and a representative subsample of other domains.\nStage 3 SFT.\nStage 3 reinforces long-context capability. It incorporates long-context data following\nthe recipe used in Nemotron-H preparation (NVIDIA, 2025), along with augmented examples across\ndomains where reasoning traces were abruptly truncated to 1–2k tokens while preserving the final\nanswer. This truncation strategy improved robustness under varying inference-time thinking budgets.\nIFeval RL.\nTo improve instruction adherence, we sampled 16,000 prompts from the LMSYS Chat\ndataset and augmented them with IFEval-style instructions. A rule-based verifier scored outputs\nbased on how well they satisfied each instruction, creating a reward signal that prioritized following\ndirections with precision. IFEval RL experiments provided significant boost to IFEval capabilities\nwhile the rest of the benchmarks fluctuated slightly requiring careful checkpoint selection.\nDPO.\nIn another branch of training, we apply the DPO algorithm to improve tool-calling. We\nevaluate performance using the BFCL v3 benchmark, which extends BFCL v2 with greater emphasis\non multi-step (multiple tool calls to achieve a goal) and multi-turn (multiple user–agent interactions).\nTo strengthen these capabilities in the Nano V2 aligned model, we use the WorkBench environment,\na multi-step verifiable tool-calling setup adapted from Styles (Styles et al., 2024). In each WorkBench\ntask, the model must issue a sequence of tool calls across multiple steps, with correctness verified\nthrough database state comparisons.\nNano V2 undergoes reinforcement learning in this environment through iterative stages of Direct\nPreference Optimization. For each candidate checkpoint from the long-context stage, we generate\non-policy data consisting of positive examples (successful tool calls) and negative examples (failed\ngenerations) for every WorkBench prompt.\nThis process ensures that iterative DPO remains\non-policy.\nRLHF.\nWe evaluate the model’s overall helpfulness and chat capabilities using the Arena-Hard\nbenchmark. To improve performance on this benchmark, we use GRPO to train candidate checkpoints\nfrom the SFT stage using English-only contexts from HelpSteer3 (Wang et al., 2025). During training,\nwe generate responses both with and without thinking traces and use a Qwen-based reward model\nto judge the rollouts.\nModel Merging.\nDuring training, we observed a trade-off between reasoning capabilities and\nchat capabilities. To address this, we opted for checkpoint interpolation Wortsman et al. (2022),\n15\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nEvaluation\nNemotron-Nano-v2-12B\nQwen3-8B\nQwen3-14B\nAIME-2024\n85.42\n75.83\n81.53\nAIME-2025\n76.25\n69.31\n66.6\nMATH-500\n97.75\n96.3\n96.85\nGPQA-Diamond\n64.48\n59.61\n64.53\nLiveCodeBench (07/24–12/24)\n70.79\n59.5\n63.08\nSciCode Sub-Task\n18.75\n24.65\n26.04\nHumanity’s Last Exam\n6.30\n4.40\n5.38\nIFEval (Inst. Strict)\n89.81\n89.39\n91.32\nBFCL v3\n66.98\n66.34\n68.01\nRULER @ 128k\n83.36\n74.13\n73.55\nArenaHard\n74\n78.4\n87.7\nTable 8 | Evaluation results with reasoning \"ON\" (for Nemotron-Nano-v2-12B, Qwen3-8B, and\nQwen3-14B across reasoning and general capability benchmarks.\nblending in an RL checkpoint with strong reasoning capabilities with an RL checkpoint with strong\nchat capabilities. Checkpoint interpolation is performed by linearly interpolating model weights:\n(1 −𝛼) · 𝑤𝑚𝑜𝑑𝑒𝑙1 + 𝛼· 𝑤𝑚𝑜𝑑𝑒𝑙2. We experimented with a parameter sweep over 𝛼values from 0.1 to\n0.9 in increments of 0.1, and found that values around 0.5 offered a good trade-off.\n3.3. Evaluation\nOur 12B model’s performance is summarized in Table 8. To test reasoning capabilities across domains,\nwe evaluate the models on MATH-500 (Lightman et al., 2023), AIME-2024, AIME-2025,\nGPQA-Diamond (Rein et al., 2023), LiveCodeBench (07/24 - 12/24) (Jain et al.,\n2024), SciCode (Tian et al., 2024), and Humanity’s Last Exam (Phan et al., 2025). For\nbroader evaluation on diverse capabilities, we use IFEval (Zhou et al., 2023) for instruction\nfollowing capabilities, BFCL v3 (Yan et al., 2024) for tool-calling, RULER for long-context,\nand ArenaHard (Li et al., 2024a) for chat capability.\nWe conduct evaluations using NeMo-Skills14. We report Pass@1 average of 16 runs for AIME-\n2024, AIME-2025; average of 4 runs for MATH-500, GPQA-Diamond, LiveCodeBench,\nIFEval; and score of 1 run for BFCL v3, SciCode, Humanity’s Last Exam, RULER,\nand ArenaHard.\n3.4. Budget Control Evaluation\nNemotron Nano V2 allows users to specify how many thinking tokens the model may generate before\nproducing the final answer. The final answer is the portion of text typically shown to end users.\nThis feature is implemented by counting tokens after the model begins generating the <think>\ntoken. Once the budget is reached, the inference setup attempts to insert a closing </think> tag.\nRather than inserting it immediately, we let the model finish its current sentence and place the\ntag at the next newline. In extreme cases where no newline appears, the system enforces closure\nwithin 500 tokens past the budget: if no newline occurs by the (budget + 500)th token, the </think>\ntag is forcibly inserted. Figure 5b shows our models budget control behavior. Apart from just\npresenting the accuracy of the model at various budgets, we also inspect if the model generations\nare well-formatted at various budgets. We inspect for two kinds of failure modes:\n14https://github.com/NVIDIA/NeMo-Skills\n16\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(a)\n(b)\nFigure 5 | Comparison of budget control before truncation training (a) and after truncation training\nwas included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens.\n• In one failure mode, the model uses more tokens in the final answer to “compensate” for\nrestrictions in the thinking traces. Without truncated training examples in the SFT stage,\nthis compensation effect is prevalent (Figure 5a, center). With truncated training, however,\nthe effect is absent (Figure 5b, center).\n• Another issue is that the model can remain in “thinking mode” even after the closing tag\n</think> is inserted. This is evident when the model generates the closing tag again after the\nforced insertion, suggesting it does not fully “register” the artificial closure. We evaluate this\nusing “Well-Formedness,” where a well-formed response should contain only a single closing\ntag (either forced by the budget or produced naturally). Figure 5a (right) shows that for short\nbudgets, the percentage of well-formed responses drops sharply. With truncation training,\nhowever, the model consistently produces well-formed responses (Figure 5b, right).\n4. Pruning and Distillation\nIn this section, we describe the pruning and distillation process to compress the aligned 12B model\nto the Nano 2 model with the goal of running longer context (128k sequence length) inference on\nthe NVIDIA A10G GPU. Note that storing just the weights of a 12B parameter model in bfloat16\nprecision requires 22.9 GiB, which is more than the 22 GiB memory capacity of an A10G GPU; this\nclearly indicates the need for compression.\nOur compression strategy builds on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024;\nTaghibakhshi et al., 2025), which is a lightweight model pruning framework for LLMs. While\nMinitron was originally designed for compressing pretrained base models targeting user-defined\nparameter budgets, in this work, we extend it to compress reasoning models while also incorporating\n17\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nthe memory constraints and throughput-based objectives stated above.\n4.1. Importance Estimation\nWe collect importance or sensitivity scores for each model component (e.g., layers, FFN neurons)\nto help decide which components to remove; this is the importance estimation phase. The scores\ncomputed in this phase are used to decide which model components can be pruned. We note\nthat sensitivity analysis based on gradient information is typically impractical at modern LLM\nscale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only forward\npasses. In this work, we use a simplified approach that works well in our ablation studies: a) prune\nlayers, and b) prune FFN hidden dimensions (effectively neurons) and embedding channels. We\nalso experimented with pruning Mamba heads; unfortunately, this axis caused severe accuracy\ndegradation. We now describe how we compute the importance of each layer, embedding channel,\nFFN neuron and Mamba head.\nLayer importance.\nWe compute layer importance in an iterative fashion: for each candidate layer,\nwe temporarily remove it from the model and compute the mean squared error (MSE) between the\noriginal model’s logits and those produced by the pruned model. This MSE reflects the contribution\nof that layer to the model’s predictions: lower values indicate smaller impact. At each pruning step,\nwe remove the layer with the lowest MSE, as it has the least influence on the final output. We repeat\nthis process until the desired depth is reached. This strategy ensures that pruning preferentially\nremoves layers whose absence minimally affects the model’s behavior. For more details on iterative\nMSE-based layer importance, please refer to NVIDIA (2025).\nFFN and embedding channel importance.\nFFN layers internally are composed of two linear\noperators with a non-linear activation in between:\nFFN(X) = 𝛿\n(︂\nX · 𝑊𝑇\n1\n)︂\n· 𝑊2.\nHere, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN\nlayer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN\nhidden dimension respectively. 𝛿(·) refers to the non-linear activation function (squared ReLU in\nthis work).\nFollowing the same procedure as Minitron (Muralidharan et al., 2024), we compute the importance\nof each neuron in the first linear operator of each FFN layer by examining the set of outputs it\nproduces. We use a small calibration dataset of 1024 samples for this purpose. Formally, we compute\neach neuron’s importance score by aggregating its outputs given an input batch 𝑋:\n𝐹(𝑖)\nneuron =\n∑︁\nB,S\n𝛿\n(︂\nX\n(︀𝑊𝑖\n1\n)︀𝑇\n)︂\n.\nHere, 𝑊𝑖\n1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀\nB,S refers to aggregation along the\nbatch and sequence dimensions. We use the mean and l2-norm aggregation functions along the\nbatch and sequence dimensions, following the observations in the Minitron paper. For a sequence of\nscores S, mean aggregation is defined as 1\n𝑛\n∑︀𝑛\n𝑖=1 |S𝑖|, and l2-norm is\n√︁∑︀𝑛\n𝑖=1 S2\n𝑖. Embedding channel\nimportance is computed similarly, by examining the outputs of LayerNorm layers instead; we refer\nthe reader to Muralidharan et al. (2024) for more details.\n18\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba importance.\nMamba layers process inputs through multiple projection matrices (𝑊𝑥,\n𝑊𝑧, 𝑊𝐵, 𝑊𝐶, 𝑊𝑑𝑡) that produce intermediate representations before causal convolution and selective\nstate space model (SSM) updates, followed by gated normalization and an output projection (𝑊𝑂).\nWe follow the methodology described in Taghibakhshi et al. (2025) for importance estimation:\nspecifically, we adopt a nested activation-based scoring strategy over a small calibration dataset of\n1024 samples, similar to FFN importance but adapted to Mamba’s group-aware structure. First,\nwe obtain activation scores from the 𝑊𝑥projection, denoted 𝑠∈R𝑚ℎ×𝑚𝑑, where 𝑚ℎis the number\nof Mamba heads and 𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is\ncomputed as\n𝑠𝑑=\n⃦⃦⃦⃦⃦⃦\n∑︁\nB,S\n𝑠:,𝑑\n⃦⃦⃦⃦⃦⃦\n2\n,\nwhere the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and\nl2-norm metrics. Next, head scores are computed by using the l2-norm over the Mamba head\nchannel set:\n𝑓ℎ= ‖𝑠ℎ,𝑚𝑑‖2 ,\n∀ℎ∈{1, . . . , 𝑚ℎ},\nand heads are ranked within each Mamba group 𝒢𝑔to preserve group-aware computation semantics:\nℛ𝑔= argsortℎ∈𝒢𝑔(𝑓ℎ).\nwhich ensures that pruning decisions respect the model’s structural constraints and SSM’s sequence\nmodeling. The lowest-scoring heads are pruned by trimming the corresponding rows from all affected\nprojection, convolution, and SSM parameter matrices. This strategy preserves the integrity of the\nSSM block while removing less important Mamba heads. As shown in Taghibakhshi et al. (2025),\npruning Mamba heads yields a better accuracy–throughput trade-off than pruning head channels;\nwe consequently focus on head pruning in this work.\n4.2. Lightweight Neural Architecture Search\nWe first define the constraints and objectives for the Nano 2 model, and then describe our lightweight\nNeural Architecture Search (NAS) framework that finds the most promising architectural candidates\nthat meet our objectives and constraints.\nMemory constraints.\nMemory requirements during inference consist of two distinct components\nwith different scaling behaviors.\nThe parameter memory, while substantial, remains constant\nregardless of the input size. In contrast, the key-value cache memory scales linearly with both batch\nsize and sequence length, often becoming the dominant factor in long-sequence scenarios. For the\nNano 2 model, our goal was to be able to perform inference at a sequence length of 128k and a batch\nsize of at least 1 within a memory budget of 19.66 GiB. We obtained the budget as follows: from the\n22.06 GiB available memory on an NVIDIA A10G GPU, we subtract a 5% buffer for frameworks\nsuch as vLLM and TensorRT-LLM and another 1.3 GiB to allow sufficient space for a vision encoder.\nMeasuring throughput.\nFor the experiments below, unless otherwise specified, we measure\nthroughput on an input and output sequence length of 8k and 16k tokens respectively, which we\nbelieve represents a typical reasoning scenario. For this combination of input and output sequence\nlength, we report vLLM output token generation throughput at the maximum batch size that fits on\nthe A10G GPU.\n19\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n4.2.1. Candidate enumeration.\nOur compression strategy explores multiple axes within the 19.66 GiB memory budget through\ncombinatorial pruning. Our search space includes depth reduction (removing 6-10 layers from the\noriginal 62-layer architecture) combined with width pruning of embedding channels (4480-5120),\nFFN dimension (13440-20480), and Mamba heads (112-128). This multi-axis search space results in\nhundreds of candidate architectures meeting the memory constraint.\n4.2.2. Finding the Best Architecture\nSince performing knowledge distillation and throughput benchmarking on the full set of candidates\nwould be prohibitively expensive, we break down the problem into two parts: (1) find the optimal\ndepth for the compressed model, and (2) find the optimal width-pruned architecture given the depth.\nEffect of depth.\nWe compare the accuracy of three depth-pruned candidates obtained from the\n12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all\nthree variants so as to achieve a good balance between KV cache size and long-context performance;\nprior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA,\n2025). We leave the width dimensions untouched for this experiment. Table 9 lists average reasoning\naccuracy at different depths after 6B tokens of distillation; in line with our previous observations on\nthe strong correlation between depth and task performance (Muralidharan et al., 2024; Sreenivas et al.,\n2024), we notice that reducing depth beyond 56 layers results in significant accuracy degradation; as\na result, we fix the depth at 56 for further width pruning.\nAccuracy (Avg)\n52 Layers\n44.92\n54 Layers\n47.35\n56 Layers\n51.48\nTable 9 | Effect of depth on reasoning accuracy. Results are after distilling with 6B tokens.\nCombining depth and width pruning.\nAs described above, we fix the depth of our target\nmodel to 56 layers with 4 attention layers. We perform 60B tokens of distillation on this checkpoint\n(see Section 4.3 for additional details) and perform further width pruning along the embedding, FFN,\nand Mamba axes. We enumerate all candidate pruned architectures that meet our memory budget,\nand sort them in decreasing order of estimated memory consumption at 128k context length and\nbatch size 1. The top 3 candidates from this list are picked for further evaluation: in particular, we\nperform short Knowledge Distillation (KD) on these candidates for 19B tokens after depth+width\npruning; we also benchmark throughput to pick the final architectural candidate. Table 10 lists the\narchitectural details of the top 3 candidates, along with the achieved task performance (post KD)\nand throughput. As shown in the Table, Candidate 2 achieves the best accuracy while still having\nreasonable runtime performance; consequently, we use this architecture for Nano 2.\nFFN vs.\nMamba pruning.\nWe ablate the number of Mamba heads following the recipe\nin Taghibakhshi et al. (2025), considering configurations with 87.5% and 93.75% of the original\nheads. However, due to the relatively smaller compression ratios explored in this work (less than 15%\nafter depth pruning) compared to those in Taghibakhshi et al. (2025) (around 50%), we find that\napplying Mamba head pruning yields limited benefit, and in these cases, pruning only the FFN and\n20\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n#Layers Hidden\nFFN\nMamba #Heads Params. (B) Accuracy Throughput\nCandidate 1\n56\n4480\n17920\n112\n8.92\n59.07\n161.02\nCandidate 2\n56\n4480\n15680\n128\n8.89\n63.02\n156.42\nCandidate 3\n56\n4800\n14400\n120\n8.97\n62.94\n155.86\nTable 10 | Top 3 candidates for architecture selection. Accuracy is the average across reasoning\nbenchmarks after distillation with 19B tokens. The last column shows vLLM output generation\nthroughput (ISL/OSL=8k/16k and batch size=8).\nembedding dimensions—after depth pruning—proves sufficient to achieve the desired compression\nwhile preserving accuracy. Candidates 1 and 2 in Table 10 highlight this difference.\n4.3. Retraining with Distillation\nTo recover the accuracy lost due to pruning, the model undergoes continued training. Recent work\nhas demonstrated that distilling knowledge from the original model to the pruned model outperforms\nconventional fine-tuning (Muralidharan et al., 2024; Sreenivas et al., 2024; Bercovich et al., 2024);\nwe thus adopt logit-based distillation for continued training, employing forward KL divergence loss\nexclusively during the accuracy recovery phase (see §3 of the Minitron paper (Muralidharan et al.,\n2024) for more details on the distillation loss formulation). Building on the candidate selection\nprocess described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below,\nto yield the final Nano 2 reasoning and base models.\n% Reasoning-SFT data\n% Pretraining data\nAccuracy (Avg)\n50\n50\n57.5\n70\n30\n58.5\n90\n10\n57.2\nTable 11 | Effect of varying reasoning data proportion on math accuracy after ∼6B tokens of KD.\nReasoning model.\nThe reasoning model is distilled in stages with increasing sequence lengths to\nstrengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement\nlearning (RL), preference optimization and model merging to retain desired behaviors and ensure\nrobustness across diverse tasks. We now describe these various stages:\n1. Depth pruning to 56 layers; Knowledge Distillation (KD) with ∼60B tokens at 8,192 sequence\nlength.\n2. Width pruning and KD with:\n• ∼50B tokens at 8,192 sequence length.\n• ∼25B tokens at 49,152 sequence length.\n• ∼1B tokens at 262,144 sequence length.\n3. Direct Preference Optimization (DPO).\n4. Group Relative Policy Optimization (GRPO).\n5. KD with ∼0.4B tokens at 262,144 sequence length to recover post-RL drops.\n6. RLHF for alignment with human preferences.\n7. Model merging between steps 5 and 6 via 0.5 linear interpolation.\n21\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMore details on DPO, GRPO and RLHF can be found in Section 3. Figure 6 shows the effects of\nstaged training on model accuracy across different reasoning benchmarks. Here, the 𝑥-axis represents\nthe various stages (starting from Step 2 above), and the 𝑦-axis shows the scores obtained for the\nvarious benchmarks as training progresses. As shown in the Figure, DPO and GRPO are critical for\nenhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the\nlatter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next\nstep (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard)\nbut causes additional benchmark drops, which are then recovered through model merging.\nKD+LCExt\nDPO\nGRPO\nKD\nRLHF\nMerge\nPipeline Stage\n50\n55\n60\n65\n70\n75\n80\n85\n90\nScore (%)\nDistillation Pipeline\nAIME-25\nGPQA-D\nBFCLv3\nIFEval (Pr.)\nMMLU-Pro\nArenaHard\nLiveCodeBench\nFigure 6 | Task accuracy at different stages of the distillation pipeline for Nemotron Nano 2.\nDataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining\n(Section 2.2) data yields the highest accuracy (Table 11). For KD at sequence length 262,144, we\nuse 100% stage 3 post-training data (Section 3.2).\nBase model.\nDistillation proceeds in stages: depth-only pruning and KD on ∼120B tokens,\nfollowed by width pruning and KD on ∼360B tokens (both at sequence length 8,192), and finally\nKD on ∼2.5B tokens at sequence length 524,288 to instill long-context capabilities.\nDataset: Following Sreenivas et al. (2024), we use 100% pretraining data described in sections 2.2\nand 2.6 for distillation of the base model at sequence lengths 8,192 and 524,288, respectively.\n4.4. Results\nWe efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden\nsize, and embedding channels, improving inference throughput and enabling long-context inference on\nan NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally,\nthe number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was\npruned from 20480 to 15680. As shown in Figure 1 and Tables 5 and 6, Nemotron-Nano-9B-v2\nachieves 3×-6× higher throughput than Qwen3-8B for generation-heavy scenarios, while surpassing\n22\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nit in accuracy and remaining comparable to the 12B teacher on most benchmarks.\n5. Conclusion\nIn this report, we introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model\nthat achieves comparable or better accuracies at up to 6× higher throughput than existing state-\nof-the-art models such as Qwen3-8B. To create Nemotron-Nano-9B-v2, we started by pre-training\nNemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and\nsynthetically generated data. We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT,\nGRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy\nto produce the final model. As a result of this compression, Nemotron-Nano-9B-v2 can run inference\non context lengths of up to 128k tokens in bfloat16 precision on a single NVIDIA A10G GPU with\n22 GiB of memory. We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling\nNemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of\nits pre- and post-training data on HuggingFace (links at the bottom of Section 1).\nContributors\nWe thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2.\nData. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick,\nDan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak\nScowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov,\nMehrzad Samadi, Miguel Martinez, Mohammad Shoeybi, Mostofa Patwary, Pavlo Molchanov, Pritam\nGundecha, Rabeeh Karimi Mahabadi, Ranjit Rajan, Rima Shahbazyan, Sanjeev Satheesh, Sarah\nYurick, Sean Narenthiran, Seungju Han, Shizhe Diao, Shrimai Prabhumoye, Shubham Toshniwal,\nSiddhartha Jain, Somshubra Majumdar, Syeda Nahida Akter, Vahid Noroozi, Vineeth Kalluru,\nVitaly Kurin, Wasi Uddin Ahmad, Wei Du, Ximing Lu, Yejin Choi, Ying Lin.\nFP8. Hua Huang, Jinze Xue, Keith Wyss, Kunlun Li, Mike Chrzanowski, Oleg Rybakov, Przemek\nTredak, Tim Moon, Zhongbo Zhu.\nArchitecture. Bita Darvish Rouhani, Brandon Norick, Duncan Riach, Nidhi Bhatia, Roger Waleffe,\nWonmin Byeon, Ritika Borkar, Xin Dong, Yonggan Fu.\nPretraining. Aarti Basant, Abhijit Paithankar, Abhinav Khattar, Deepak Narayanan, Herman\nSahota, Hexin Wang, Jupinder Parmar, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja,\nRoger Waleffe, Russell J. Hewett, Ryan Prenger, Seonmyeong Bak.\nInfrastructure. Alex Kondratenko, Alex Shaposhnikov, Anubhav Mandarwal, Ashwin Poojary,\nDong Ahn, Gargi Prasad, Haim Elisha, Harsh Sharma, Kumar Anik, Maer Rodrigues de Melo, Ruoxi\nZhang, Shelby Thomas, Stefania Alborghetti, Tony Wang.\nLong Context. Deepak Narayanan, Dima Rekesh, Duncan Riach, John Kamalu, Kezhi Kong,\nMarkus Kliegl, Roger Waleffe, Samuel Kriman.\nInference. Daniel Afrimi, Helen Ngo, Keshav Santhanam, Kushan Ahmadian, Lawrence McAfee,\nLuis Vega, Nave Assaf, Peter Dykas, Shanmugam Ramasamy, Siddharth Singh, Tomer Asida, Vijay\nKorthikanti.\nAlignment. Adithya Renduchintala, Alexander Bukharin, Ameya Sunil Mahabaleshwarkar, Banghua\nZhu, Bilal Kartal, Brian Yu, Charles Wang, Christian Munley, David Mosallanezhad, Gerald Shen,\nHaifeng Qian, Hayley Ross, Hoo Chang Shin, Igor Gitman, Jian Zhang, Jiaqi Zeng, Julien Veron\n23\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nVialard, Junkeun Yi, Kezhi Kong, Luis Vega, Makesh Narsimhan Sreedhar, Oleksii Hrinchuk, Oleksii\nKuchaiev, Peter Jin, Prasoon Varshney, Ritu Gala, Shuoyang Ding, Soumye Singhal, Tugrul Konuk,\nVenkat Srinivasan, Vitaly Lavrukhin, Yian Zhang, Yoshi Suhara, Zhen Dong, Zijia Chen.\nCompression. Aditya Malte, Akhiad Bercovich, Akshay Hazare, Ali Taghibakhshi, Ameya Sunil\nMahabaleshwarkar, Ashwath Aithal, Banghua Zhu, Daniel Korzekwa, Deepak Narayanan, Gerald\nShen, Hayley Ross, Julien Veron Vialard, Luis Vega, Marcin Chochowski, Mostofa Patwary, Nima\nTajbakhsh, Oluwatobi Olabiyi, Pavlo Molchanov, Ran El-Yaniv, Roger Waleffe, Saurav Muralidharan,\nSepehr Sameni, Sharath Turuvekere Sreenivas, Tomer Asida, Yashaswi Karnati, Yian Zhang, Yoshi\nSuhara, Zijia Chen.\nSoftware Support. Abhijit Khairnar, Adithya Renduchintala, Ali Taghibakhshi, Anna Shors,\nAshwath Aithal, Balaram Buddharaju, Bobby Chen, Charlie Truong, Deepak Narayanan, Dmytro\nPykhtar, Duncan Riach, Gerald Shen, Helen Ngo, Jared Casper, Jimmy Zhang, Keshav Santhanam,\nKezhi Kong, Lawrence McAfee, Luis Vega, Nima Tajbakhsh, Parth Chadha, Piotr Bialecki, Prashant\nGaikwad, Rajen Patel, Roger Waleffe, Sahil Jain, Terry Kong, Tyler Poon, Vijay Korthikanti, Vikram\nFugro, Yoshi Suhara, Zhiyu Li.\nEvaluations and Safety.\nChristopher Parisien, Dan Su, Daniel Rohrer, Eileen Long, Erick\nGalinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta\nStepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien,\nMichal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona\nGhosh, Tomasz Hliwiak.\nLegal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham,\nKatherine Cheung, Laya Sleiman, Meredith Price, Michael Boone, Nikki Pope, Ria Cheruvu, Saori\nKaji.\nMarketing. Amelia Barton, Chris Alexiuk, Mark Cai, Nirmal Kumar Juluru, Shreya Gopal.\nProject Management. Alejandra Rico, Amy Shen, Ann Guan, Ashton Sharabiani, Elliott Ning,\nKrzysztof Pawelec, Negar Habibi, Twinkle Vashishth.\nProduct. Arun Venkatesan, Chintan Patel, Chris Alexiuk, Joey Conway, Padmavathy Subramanian,\nUdi Karpas.\nLeadership. Andrew Tao, Boris Ginsburg, Bryan Catanzaro, Eric Chung, Jan Kautz, Joey Conway,\nJonathan Cohen, Kari Briski, Mohammad Shoeybi, Mostofa Patwary, Oleksii Kuchaiev, Pavlo\nMolchanov.\nWe also thank Chen Zhang, Michael Goin, Thomas Parnell from the vLLM team for their assistance.\n24\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nReferences\nGretel synthetic safety alignment dataset, 12 2024. URL https://huggingface.co/datasets/\ngretelai/gretel-safety-alignment-en-v1.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical\nreport. arXiv preprint arXiv:2412.08905, 2024.\nWasi Uddin Ahmad, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Vahid Noroozi, Somshubra\nMajumdar, and Boris Ginsburg. Opencodeinstruct: A large-scale instruction tuning dataset for\ncode llms. arXiv preprint arXiv:2504.04030, 2025a.\nWasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha\nJain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data\ndistillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025b.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-\npoints, 2023. URL https://arxiv.org/abs/2305.13245.\nSyeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro. MIND: Math Informed syNthetic Dialogues\nfor Pretraining LLMs, 2024. URL https://arxiv.org/abs/2410.12881.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis\nTunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua\nLochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher,\nHaojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf.\nSmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model, 2025. URL\nhttps://arxiv.org/abs/2502.02737.\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and\nextraction, 2024. URL https://arxiv.org/abs/2309.14316.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with\nLarge Language Models, 2021. URL https://arxiv.org/abs/2108.07732.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL\nhttps://arxiv.org/abs/2502.13923.\nAdrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Dis-\ncovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing: System Demonstrations, pp. 122–131. Association for Computational\nLinguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.\nAkhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah,\nIdo Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi\nKoren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny,\n25\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nRan Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and\nRan El-Yaniv.\nPuzzle: Distillation-Based NAS for Inference-Optimized LLMs, 2024.\nURL\nhttps://arxiv.org/abs/2411.19146.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido\nGalil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas,\nRan Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk,\nGerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia\nChen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei\nJia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander\nFicek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du,\nShubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman,\nEvelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl,\nRabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon\nNorick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav\nKhattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry\nKong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky,\nRobert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen,\nManoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka\nDong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro\nLarroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla,\nMuthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury,\nOmri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon\nDerczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo\nRibalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala\nPrayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan\nCatanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron:\nEfficient reasoning models, 2025a. URL https://arxiv.org/abs/2505.00949.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil,\nZach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models.\narXiv preprint arXiv:2505.00949, 2025b.\nJanek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search\nEngine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi,\nand Benjamin Piwowarski (eds.), Advances in Information Retrieval. 40th European Conference\non IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York,\nMarch 2018. Springer.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about\nPhysical Commonsense in Natural Language, 2019. URL https://arxiv.org/abs/1911.11641.\nAndrei Z Broder. Identifying and filtering near-duplicate documents. In Annual symposium on\ncombinatorial pattern matching, pp. 1–10. Springer, 2000.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, et al. Evaluating Large Language Models Trained on Code, 2021. URL https://arxiv.\norg/abs/2107.03374.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Advances in Neural Information Processing\nSystems (NeurIPS), NIPS ’17, 2017.\n26\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge. ArXiv, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training Verifiers to Solve Math Word Problems, 2021. URL https://arxiv.org/\nabs/2110.14168.\nTri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality, 2024. URL https://arxiv.org/abs/2405.21060.\nGemma Team @ Google DeepMind. Gemma 3 Technical Report, 2025. URL https://arxiv.org/\nabs/2503.19786.\nDeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\n2025a. URL https://arxiv.org/abs/2501.12948.\nDeepSeek-AI. DeepSeek-V3 Technical Report, 2025b. URL https://arxiv.org/abs/2412.19437.\nIstván Endrédy and Attila Novák. More effective boilerplate removal-the goldminer algorithm.\nPolibits, 48:79–83, 12 2013. doi: 10.17562/PB-48-10.\nSteven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, and\nBryan Catanzaro. Maximize Your Data’s Potential: Enhancing LLM Accuracy with Two-Phase\nPretraining, 2024. URL https://arxiv.org/abs/2412.15285.\nShaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian\nRebedea, Jibin Rajan Varghese, and Christopher Parisien. AEGIS2.0: A diverse AI safety dataset\nand risks taxonomy for alignment of LLM guardrails. In Luis Chiruzzo, Alan Ritter, and Lu Wang\n(eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp.\n5992–6026, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics.\nISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.306. URL https://aclanthology.\norg/2025.naacl-long.306/.\nXiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo\nHuang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal\npre-training for enhanced mathematical reasoning, 2024. URL https://arxiv.org/abs/2409.\n12568.\nAdib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak resistance\nin aligned llms without fine-tuning. arXiv preprint arXiv:2401.10862, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring Massive Multitask Language Understanding, 2021a. URL https:\n//arxiv.org/abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, 2021b.\nURL https://arxiv.org/abs/2103.03874.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\nmodels? arXiv preprint arXiv:2404.06654, 2024.\n27\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng,\nYewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao,\nChenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai\nli, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models\nwith scalable training strategies. In First Conference on Language Modeling, 2024. URL https:\n//openreview.net/forum?id=3X2L2TFr0f.\nSiming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu,\nChenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang,\nZili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code\nlarge language models, 2025. URL https://arxiv.org/abs/2411.04905.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of\ndimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,\npp. 604–613, 1998.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion\nStoica. From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline, April 2024a.\nURL https://lmsys.org/blog/2024-04-19-arena-hard/.\nXuehai Li, Zi Ye, Xiaoxin Zhang, Xinshi Lu, Yingqiang Xia, Bairu Wu, Shihan Dong, Qipeng Jin,\nJialu Wang, Heng Ji, et al. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint\narXiv:2405.01470, 2024b.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A Hybrid Transformer-Mamba\nLanguage Model, 2024. URL https://arxiv.org/abs/2403.19887.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050, 2023.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146,\n2017.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A\nchallenge dataset for machine reading comprehension with logical reasoning. arXiv preprint\narXiv:2007.08124, 2020.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by\nChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.\narXiv preprint arXiv:2305.01210, 2023. doi: https://doi.org/10.48550/arXiv.2305.01210. URL\nhttps://arxiv.org/abs/2305.01210.\nZuxin Liu et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920,\n2024.\n28\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane\nTazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis\nKocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil\nPaul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii\nZheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli\nHe, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham\nOblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan\nHui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu,\nTorsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,\nMostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2\nand the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173.\nWeidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv: A benchmark for\nassessing the robustness of multimodal large language models against jailbreak attacks. arXiv\npreprint arXiv:2404.03027, 2024.\nRabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad\nShoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math\npretraining dataset, 2025. URL https://arxiv.org/abs/2508.15096.\nSomshubra Majumdar, Vahid Noroozi, Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek,\nWasi Uddin Ahmad, Jocelyn Huang, Jagadeesh Balam, and Boris Ginsburg. Genetic instruct:\nScaling up synthetic generation of coding instructions for large language models. arXiv preprint\narXiv:2407.21077, 2024.\nLlama Team @ Meta. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.\n21783.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering, 2018. URL https://arxiv.\norg/abs/1809.02789.\nIvan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schif-\nferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical\nreasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa\nPatwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact\nLanguage Models via Pruning and Knowledge Distillation, 2024. URL https://arxiv.org/abs/\n2407.14679.\nNVIDIA. Nemotron-4 340B Technical Report, 2024. URL https://arxiv.org/abs/2406.11704.\nNVIDIA. Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models, 2025.\nURL https://arxiv.org/abs/2504.03624.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\n29\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian,\nDan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa,\nJiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel\nMartinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath\nAithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-\n4 15B Technical Report. arXiv preprint arXiv:2402.16819, 2024. URL https://arxiv.org/abs/\n2402.16819.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An Open\nDataset of High-Quality Mathematical Web Text, 2023.\nGuilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Amir Hossein\nKargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. Fineweb2: One\npipeline to scale them all – adapting pre-training data processing to every language, 2025. URL\nhttps://arxiv.org/abs/2506.20920.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra,\nAdam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry\nDodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes,\nMobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav\nKazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou,\nZihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon,\nYongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala,\nNoah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney,\nAntrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng,\nJennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben\nMcCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui\nLi, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G.\nWillcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward\nVendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow,\nNatanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis\nEfremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod,\nGözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang,\nPaolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin\nImperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel\nLoiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad\nHogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov,\nPhilippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den\nHoute, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust,\nBikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong\nYang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu,\nAriel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner,\nJames Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene,\nJoseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan,\nSergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova,\nDaniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster,\nDaniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes,\nAlexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider,\nZakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias\nKreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger,\n30\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nKaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov,\nVáclav Rozhoň, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Poświata, Josef Tkadlec, Alan\nGoldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan\nStendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek\nShukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow,\nHao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K Wang, Kalyan\nRamakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi,\nEthan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw,\nJP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam,\nHieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael\nKrause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon\nLee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik\nKirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie,\nAnna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani,\nShreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob\nPlatnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson,\nMarco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu,\nHannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William\nAlley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob\nLoader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida\nBosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes,\nJeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane\nDurand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff,\nLynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh\nShah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee,\nRobin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt,\nJiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak\nPradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz\nFirsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones,\nShashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K.\nZhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan,\nAndrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed,\nJulian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska,\nClaudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy\nManik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David\nStap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo\nRodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna\nSamuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough\nMohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez,\nDaniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu,\nMike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira,\nSimon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank\nReidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung\nKim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon\nPark, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael\nKirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan,\nKrzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua\nRobinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin\nWhite, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M.\n31\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin\nYong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo\nAlbani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier,\nLawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel\nObikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara\nPopescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew\nBrooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua\nNewbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover,\nTing Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder\nde Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent\nCheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie\nHausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang,\nDavid Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik,\nAaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul,\nMohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna\nLiakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth\nAnderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh,\nWentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson,\nMohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José\nMoyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier,\nOmid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali\nM. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh\nDucey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu,\nJack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah,\nMarc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark,\nAssaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel\nPoesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam,\nJuan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel\nBugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti,\nAbdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal,\nMohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry\nMalishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume,\nWiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo,\nJakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa\nGonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo\nJiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader\nDendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca\nArnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony\nFruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan,\nInnocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach,\nChris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John\nLai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling\nDuclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith\nKrenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P\nV, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubić,\nSamuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella,\nAlex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang,\nJason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha,\nYinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng\n32\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał\nPerełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa\nNguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario\nAbbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del\nRio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny\nReddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd,\nThom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt,\nSatyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar\nShridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang,\nAdam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena,\nXing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming\nYin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez,\nCostin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin\nBriański, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-\nOrallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen,\nAlexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan\nTodoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther\nYap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi\nWang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira\nArrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam\nBouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas\nSubramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim,\nYushun Chen, Sara Vera Marjanović, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen,\nDawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica\nWeng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony\nGitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin,\nPhilipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan,\nJun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang,\nIsha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao\nZheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen,\nDeepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gonçalves,\nWei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline\nGeirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon\nChristof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac\nPark, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng,\nZhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang,\nBruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang,\nMarc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying\nLiu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe,\nHongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset,\nZishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia\nChernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park,\nHieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui\nPan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W.\nBartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish\nCheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay\nPaek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent\nCheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath,\nViolet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin\n33\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny\nSun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert\nYang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang,\nAndrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David\nSun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan\nZhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks.\nHumanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249.\nAkshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Thai Lan, Shirley Kokane,\nJuntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese,\nJuan Carlos Niebles, Shelby Heinecke, Huan Wang, and et al. Apigen-mt: Agentic pipeline for\nmulti-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601,\n2025.\nQwen. Qwen2.5 Technical Report, 2025. URL https://arxiv.org/abs/2412.15115.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023.\nHayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara. When2Call: When (not) to\ncall tools. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), pp. 3391–3409, Albuquerque, New Mexico, April 2025. Association for Computational\nLinguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.174/.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale, 2019. URL https://arxiv.org/abs/1907.\n10641.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. Deepseekmath: Pushing\nthe limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300,\n2024.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\nare multilingual chain-of-thought reasoners, 2022. URL https://arxiv.org/abs/2210.03057.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al.\nGlobal mmlu: Understanding and addressing cultural and linguistic biases in multilingual evalua-\ntion. arXiv preprint arXiv:2412.03304, 2024a.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond\nNg, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T.\nMartins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and\nSara Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in\nmultilingual evaluation, 2024b. URL https://arxiv.org/abs/2412.03304.\n34\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDavid R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer:\nSearching for Efficient Transformers for Language Modeling, 2022. URL https://arxiv.org/\nabs/2109.08668.\nSharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil\nMahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan\nYu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel\nKorzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, and Bryan\nCatanzaro. LLM Pruning and Distillation in Practice: The Minitron Approach, 2024. URL\nhttps://arxiv.org/abs/2408.11796.\nOlly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, and Bertie Vidgen.\nWorkbench: a benchmark dataset for agents in a realistic workplace setting. arXiv preprint\narXiv:2405.00823, 2024. doi: 10.48550/arXiv.2405.00823.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a\nrefined long-horizon pretraining dataset. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2459–2475, Vienna, Austria, July\n2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.\nacl-long.123. URL https://aclanthology.org/2025.acl-long.123/.\nAli Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi\nKarnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi\nOlabiyi, et al. Efficient hybrid language model compression through group-aware ssm pruning.\narXiv preprint arXiv:2504.11409, 2025.\nMinyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland\nHaas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong,\nKha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu,\nKilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu\nHuerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024. URL\nhttps://arxiv.org/abs/2407.13168.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert\nGu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh,\nJared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An Empirical Study of\nMamba-based Language Models, 2024. URL https://arxiv.org/abs/2406.07887.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024. URL\nhttps://arxiv.org/abs/2212.03533.\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan\nDuan. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 30:2201–2216, 2022.\n35\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nZhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin,\nEllie Evans, Yi Dong, and Oleksii Kuchaiev. Helpsteer3-preference: Open human-annotated\npreference data across diverse tasks and languages. arXiv preprint arXiv:2505.11475, 2025.\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig\nSchmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without\nincreasing inference time, 2022. URL https://arxiv.org/abs/2203.05482.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and\nJoseph E. Gonzalez. Berkeley Function Calling Leaderboard. 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\nbenchmark and contamination for language models with rephrased samples, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine\nReally Finish Your Sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\nBiao Zhang and Rico Sennrich.\nRoot Mean Square Layer Normalization, 2019.\nURL https:\n//arxiv.org/abs/1910.07467.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Yonghao Li, Zhuohan Chen, Zhewei Wong, Siyuan\nZhuang, Yakun Shao, Kai Xu, Zhenyu Zhang, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2309.11998, 2023.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian\nYin, Ming Zhou, and Nan Duan. Analytical reasoning of text. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pp. 2306–2319, 2022.\nFan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong\nLiu, and Eric P Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint\narXiv:2504.02807, 2025.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint\narXiv:2311.07911, 2023.\nA. Permissive Source Code Licenses\nWe remove source code with a license not in the following list:\n36\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3Com Microcode 3com-microcode, 3D Slicer License 1.0 [3dslicer-1.0], 4Suite 1.1 [4suite-1.1], AAL\n[attribution], Abstyles License [abstyles], ACE TAO License [ace-tao], AdaCore Doc License [adacore-\ndoc], ADI BSD [adi-bsd], Adobe Glyph License [adobe-glyph], Adobe Postscript AFM License\n[apafml], Adobe Source Code License 2006 [adobe-scl], AES-128 3.0 License [aes-128-3.0], AFL\n1.1 [afl-1.1], AFL 1.2 [afl-1.2], AFL 2.0 [afl-2.0], AFL 2.1 [afl-2.1], AFL 3.0 [afl-3.0], afmparse\nLicense [afmparse], Agere BSD [agere-bsd], Alexisisaac Freeware License [alexisisaac-freeware],\nAllegro 4 License [allegro-4], Altera License [xnet], Amazon Digital Services License [adsl], AMD\nHistorical License [amd-historical], AMD PLPA License [amdplpa], AMPAS BSD-Style License\n[ampas], AMSFonts license [ams-fonts], Andre Adrian DFS license [adrian], ANTLR-PD [antlr-\npd], ANTLR-PD with fallback [antlr-pd-fallback], ANU License [anu-license], Apache 1.0 [apache-\n1.0], Apache 1.1 [apache-1.1], Apache 2.0 [apache-2.0], Apache Patent Provision Exception Terms\n[apache-patent-exception], App::s2p License [app-s2p], Apple Attribution 1997 [apple-attribution-\n1997], Apple Attribution License [apple-attribution], Apple Example Code License [apple-excl],\nApple MIT License [aml], Apple Sample Source Code License [apple-sscl], Aravindan Premkumar\nLicenase [aravindan-premkumar], ArgoUML License [argouml], ARM LLVM Grant [arm-llvm-sga],\nArray Input Method Public License [array-input-method-pl], Artistic 1.0 [artistic-1.0], Artistic 1.0\nw/clause 8 [artistic-1.0-cl8], Artistic 2.0 [artistic-2.0], Artistic-Perl-1.0 [artistic-perl-1.0], ASMUS\nLicense [asmus], ASN.1 Object Dumping Code License [asn1], Atkinson Hyperlegible Font License\n[atkinson-hyperlegible-font], Baekmuk Fonts License [baekmuk-fonts], Bahyph License [bahyph],\nBaKoMa Fonts Licence 1995 [bakoma-fonts-1995], Barr TeX License [barr-tex], BEA 2.1 [bea-2.1],\nBeal Screamer License [beal-screamer], Beer-Ware License [beerware], BERI Hardware-Software\nLicense v1.0 [beri-hw-sw-1.0], BigDigits License [bigdigits], Bigelow & Holmes Lucida Fonts License\n[bigelow-holmes], Biopython License [biopython], Bitstream Vera Font License [bitstream], Bitzi-PD\n[bitzi-pd], BLAS License 2017 [blas-2017], Blue Oak Model License 1.0.0 [blueoak-1.0.0], BOHL-\n0.2 [bohl-0.2], Boost 1.0 [boost-1.0], Boost Original [boost-original], Borceux License [borceux],\nBoutell libgd declarations 2021 [boutell-libgd-2021], bpmn.io License [bpmn-io], Brent Corkum\nLicense [brent-corkum], Brian Clapper License [brian-clapper], Brian Gladman 3-Clause License\n[brian-gladman-3-clause], Brian Gladman Dual BSD-GPL [brian-gladman-dual], Brian Gladman\nLicense [brian-gladman], Broadcom CFE License [broadcom-cfe], Broadcom Warranty Disclaimer\n[broadcom-linux-timer], Brocade Firmware License [brocade-firmware], Bruno Podetti License [bruno-\npodetti], BSD 1988 [bsd-1988], BSD 3-Clause Devine [bsd-3-clause-devine], BSD 3-Clause FDA\n[bsd-3-clause-fda], BSD 3-Clause jtag [bsd-3-clause-jtag], BSD 3-Clause No Change [bsd-3-clause-no-\nchange], BSD 3-Clause No Nuclear Warranty [bsd-3-clause-no-nuclear-warranty], BSD 3-Clause no\ntrademark [bsd-3-clause-no-trademark], BSD 3-Clause Open MPI variant [bsd-3-clause-open-mpi],\nBSD 3-Clause Sun [bsd-3-clause-sun], BSD 3-Clause with GPL reference [bsd-top-gpl-addition],\nBSD Acknowledgment (Carrot2) License [bsd-ack-carrot2], BSD Acknowledgment License [bsd-\nack], BSD Advertising Acknowledgement License [bsd-advertising-acknowledgement], BSD Artwork\n[bsd-artwork], BSD Atmel License [bsd-atmel], BSD DPT [bsd-dpt], BSD plus modification notice\n[bsd-plus-mod-notice], BSD Simplified Darwin [bsd-simplified-darwin], BSD Source Code Attribution\n[bsd-source-code], BSD Unchanged [bsd-unchanged], BSD Unmodified [bsd-unmodified], BSD Zero\nClause License [bsd-zero], BSD-1-Clause [bsd-1-clause], BSD-1-Clause Build [bsd-1-clause-build],\nBSD-2-Clause [bsd-simplified], BSD-2-Clause no disclaimer [bsd-no-disclaimer], BSD-2-Clause no\ndisclaimer Unmod [bsd-no-disclaimer-unmodified], BSD-2-Clause Plus Patent [bsd-plus-patent], BSD-\n2-Clause-plus-advertizing [bsd-2-clause-plus-advertizing], BSD-2-Clause-Views [bsd-2-clause-views],\nBSD-3-Clause [bsd-new], BSD-3-Clause tcpdump variant [bsd-new-tcpdump], BSD-3-Clause without\nnotice modification [bsd-new-nomod], BSD-3-Clause X11 disclaimer [bsd-x11], BSD-4-Clause with\nVoices [bsd-original-voices], BSD-4-Clause-Shortened [bsd-4-clause-shortened], BSD-Axis without\nmodification [bsd-axis-nomod], BSD-Credit [bsd-credit], BSD-Derivative [bsd-new-derivative], BSD-\nExport [bsd-export], BSD-InnoSys [bsd-innosys], BSD-Mylex [bsd-mylex], BSD-Original [bsd-original],\n37\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nBSD-Original-Muscle [bsd-original-muscle], BSD-Original-UC [bsd-original-uc], BSD-Original-UC-\n1986 [bsd-original-uc-1986], BSD-Simplified Intel [bsd-simplified-intel], BSD-Simplified source [bsd-\nsimplified-source], BSD-Top [bsd-top], BSLA [bsla], BSLA no advertizing [bsla-no-advert], Business\nSource License 1.0 [bsl-1.0], BYTEmark License [bytemark], bzip2 License 2010 [bzip2-libbzip-2010],\nCaldera License [caldera], Careware [careware], Carnegie Mellon Contributors [carnegie-mellon-\ncontributors], Carnegie Mellon License [carnegie-mellon], Cavium malloc License [cavium-malloc],\nCC-BY-1.0 [cc-by-1.0], CC-BY-2.0 [cc-by-2.0], CC-BY-2.0-UK [cc-by-2.0-uk], CC-BY-2.5 [cc-by-\n2.5], CC-BY-3.0 [cc-by-3.0], CC-BY-3.0-AT [cc-by-3.0-at], CC-BY-3.0-US [cc-by-3.0-us], CC-BY-4.0\n[cc-by-4.0], CC-PD [cc-pd], CC-PD Mark 1.0 [cc-pdm-1.0], CC0-1.0 [cc0-1.0], CDLA Permissive\n1.0 [cdla-permissive-1.0], CDLA Permissive 2.0 [cdla-permissive-2.0], CeCILL-B License [cecill-b],\nCeCILL-B License English [cecill-b-en], CERN Attribution 1995 [cern-attribution-1995], CERN\nOpen Hardware Licence v1.2 [cern-ohl-1.2], CERN Open Hardware License v1.1 [cern-ohl-1.1],\nCERN-OHL-P-2.0 [cern-ohl-p-2.0], CFITSIO License [cfitsio], Checkmk License [checkmk], Chicken\nDance License v0.2 [chicken-dl-0.2], Chris Maunder License [chris-maunder], Chris Stoy Attribution\nLicense [chris-stoy], Clarified Artistic License [artistic-clarified], Classic VB License [classic-vb], Clear\nBSD 1-Clause License [clear-bsd-1-clause], Clear BSD License [clear-bsd], Click License [click-license],\nCLIPS License 2017 [clips-2017], CMU Computing Services License [cmu-computing-services], CMU\nLicense [cmu-template], CMU MIT-style [cmu-mit], CMU Simple License [cmu-simple], CMU Style\n[cmu-uc], CNRI Jython License [cnri-jython], CNRI Python 1.6 [cnri-python-1.6], CNRI Python\n1.6.1 [cnri-python-1.6.1], Code Credit License v1.0.1 [code-credit-license-1.0.1], Code Credit License\nv1.1.0 [code-credit-license-1.1.0], CodeGuru Permissions [codeguru-permissions], CodeSourcery 2004\n[codesourcery-2004], COIL-1.0 [coil-1.0], Common Lisp LOOP License [loop], CommonJ Timer\nLicense [commonj-timer], Compass License [compass], ComponentAce JCraft License [componentace-\njcraft], compuphase Linking Exception to Apache 2.0 [compuphase-linking-exception], Condor Public\nLicense 1.1 [condor-1.1], Copyheart [copyheart], Cornell Lossless JPEG License [cornell-lossless-jpeg],\nCougaar Open Source License [cosl], CP/M License 2022 [cpm-2022], CppCoreGuidelines License\n[cpp-core-guidelines], CRCalc license [crcalc], Creative Commons Attribution 2.5 Australia [cc-by-\n2.5-au], Creative Commons Attribution 3.0 Germany [cc-by-3.0-de], Creative Commons Attribution\n3.0 Netherlands [cc-by-3.0-nl], Crossword License [crossword], Crypto++ License [cryptopp], Crystal\nStacker License [crystal-stacker], CSL-1.0 [csl-1.0], CSPRNG [csprng], Cube License [cube], cURL\nLicense [curl], CVE ToU [cve-tou], CWE ToU [cwe-tou], CxImage License [cximage], D Zlib [d-zlib],\nDAMAIL [damail], Dante Treglia License [dante-treglia], DBAD License 1.1 [dbad-1.1], Debian\nreportbug License [reportbug], Delorie Historical License [delorie-historical], dhtmlab Public License\n[dhtmlab-public], diffmark License [diffmark], dl-de/by-1-0-de [dl-de-by-1-0-de], dl-de/by-1-0-en\n[dl-de-by-1-0-en], dl-de/by-2-0-de [dl-de-by-2-0-de], dl-de/by-2-0-en [dl-de-by-2-0-en], dmalloc License\n[dmalloc], DMTF License 2017 [dmtf-2017], Docbook License [docbook], Dom4j License [dom4j],\nDotseqn License [dotseqn], Douglas Young License [douglas-young], DRL-1.0 [drl-1.0], DRL-1.1\n[drl-1.1], Dropbear License [dropbear], Dropbear-2016 [dropbear-2016], DSDP License [dsdp], Dtree\nLicense [dtree], dvipdfm License [dvipdfm], DWTFNMFPL-3.0 [dwtfnmfpl-3.0], Dynamic Drive TOU\n[dynamic-drive-tou], ECL 1.0 [ecl-1.0], ECL 2.0 [ecl-2.0], EFL 1.0 [efl-1.0], EFL 2.0 [efl-2.0], EFL\nMIT-Style License [enlightenment], eGenix Public License 1.0.0 [egenix-1.0.0], eGenix Public License\n1.1.0 [egenix-1.1.0], EllisLab License [ellis-lab], EMX Library License [emx-library], EnergyPlus\nBSD-Style License [energyplus-bsd], Enhanced MIT License [emit], enna License [enna], Entessa 1.0\n[entessa-1.0], ePaperPress License [epaperpress], EPICS Open License [epics], Eric Glass License\n[eric-glass], Errbot exception [errbot-exception], Etalab Open License 2.0 [etalab-2.0], Etalab Open\nLicense 2.0 English [etalab-2.0-en], EU DataGrid Software License [eu-datagrid], Fabien Tassin\nLicense [fabien-tassin], Fair License [fair], FAL 1.3 [free-art-1.3], Far Manager exception to BSD-\n3-Clause [far-manager-exception], FASTBuild License 2012-2020 [fastbuild-2012-2020], FastCGI\nDevKit [fastcgi-devkit], FastCGI License for Spec Implementation [openmarket-fastcgi], FatFs\n38\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nLicense [fatfs], FFTPACK License 2004 [fftpack-2004], Filament Group MIT License [filament-\ngroup-mit], Flex 2.5 [flex-2.5], Flora License v1.1 [flora-1.1], font-alias License [font-alias], FPLOT\nLIcense [fplot], Fraunhofer ISO 14496-10 License [fraunhofer-iso-14496-10], FreeBSD Boot [freebsd-\nboot], FreeBSD Doc License [freebsd-doc], FreeBSD unmodified first lines License [freebsd-first],\nFreeMarker License [freemarker], FreeTTS License [freetts], FreeType Project License [freetype],\nFreeware Public License (FPL) [fpl], FSF All Permissive License [fsf-ap], FSF Free Software License\n[fsf-free], FSF Notice [fsf-notice], FSF Unlimited License No Warranty [fsf-unlimited-no-warranty],\nFSF-Unlimited [fsf-unlimited], Fujion Clinical Exception to Apache 2.0 [fujion-exception-to-apache-\n2.0], Gareth McCaughan License [gareth-mccaughan], Gary S. Brown License [gary-s-brown], GDCL\nLicense [gdcl], Generic patent disclaimer [patent-disclaimer], Geoff Kuenning License 1993 [geoff-\nkuenning-1993], Ghostpdl Permissive [ghostpdl-permissive], Glulxe License [glulxe], GLUT License\n[glut], GLWTPL [glwtpl], Good Boy License [good-boy], Graphics Gems License [graphics-gems],\nGreg Roelofs License [greg-roelofs], Gregory Pietsch Liberal License [gregory-pietsch], GStreamer\nException (2005) [gstreamer-exception-2005], GTPL-v1 [gtpl-v1], GTPL-v2 [gtpl-v2], GTPL-v3 [gtpl-\nv3], Haskell Report License [haskell-report], HDF4 License [hdf4], HDF5License [hdf5], HDPARM\nLicense [hdparm], Henry Spencer License 1999 [henry-spencer-1999], Henry Spencer Regexp License\n[hs-regexp], HIDAPI License [hidapi], Historical Notice - NTP [historical-ntp], Historical Permission\nNotice and Disclaimer [historical], Homebrewed License [homebrewed], HP 1986 License [hp-1986],\nHPND sell variant with MIT disclaimer [hpnd-sell-variant-mit-disclaimer], HTML 5 spec License\n[html5], httpget notice and disclaimer [httpget], Ian Kaplan License [ian-kaplan], Ian Piumarta\nLicense [ian-piumarta], IBM AS-IS License [ibm-as-is], IBM DHCP License [ibm-dhcp], IBM Non-\nWarranted Sample Code License [ibm-nwsc], IBM PowerPC Software [ibm-pibs], IBM Sample License\n[ibm-sample], IBPP License [ibpp], ICANN-Public [icann-public], ICOT Free Software [icot-free],\nICU Composite License [ibm-icu], ICU License 58 and later [unicode-icu-58], IDT License Notice\n[idt-notice], IETF License [ietf], IETF Trust License [ietf-trust], ilmid License [ilmid], ImageMagick\nLicense [imagemagick], Independent JPEG Group License - short [ijg-short], Indiana Extreme\nLicense 1.1.1 [indiana-extreme], Indiana Extreme License 1.2 [indiana-extreme-1.2], Infineon Free\nSoftware License [infineon-free], Info-Zip License 1997-10 [info-zip-1997-10], Info-Zip License 2001-01\n[info-zip-2001-01], Info-Zip License 2002-02 [info-zip-2002-02], Info-Zip License 2003-05 [info-zip-\n2003-05], Info-Zip License 2004-05 [info-zip-2004-05], Info-Zip License 2005-02 [info-zip-2005-02],\nInfo-Zip License 2007-03 [info-zip-2007-03], Info-Zip License 2009-01 [info-zip-2009-01], Info-Zip\nLicense [info-zip], Inno Setup License [inno-setup], Intel ACPI SLA [intel-acpi], Intel BSD - Export\nControl [intel-bsd-export-control], Intel BSD 2 Clause License [intel-bsd-2-clause], Intel BSD License\n[intel-bsd], Intel Limited Patent License [intel], Intel OSL 1989 [intel-osl-1989], Intel OSL 1993\n[intel-osl-1993], Intel Royalty Free License [intel-royalty-free], ISC License [isc], ISO 14496-10 [iso-\n14496-10], ISO 8879 [iso-8879], ITU License [itu], JA-SiG License [ja-sig], Jam License [jam], Jason\nMayes License [jason-mayes], Jasper 1.0 [jasper-1.0], JasPer 2.0 [jasper-2.0], Java App Stub License\n[java-app-stub], JDBM License v1.00 [jdbm-1.00], JDOM License [jdom], Jetty License [jetty], JGraph\nLicense [jgraph], JPEG License [ijg], JPNIC idnkit License [jpnic-idnkit], JPNIC mdnkit License\n[jpnic-mdnkit], JPython 1.1 [jpython-1.1], jQuery-Tools-PD [jquery-pd], Jscheme License [jscheme],\nJSFromHell License [jsfromhell], JSON License [json], JSON-js-PD [json-js-pd], JSON-PD [json-pd],\nJython License [jython], Kalle Kaukonen License [kalle-kaukonen], Kazlib [kazlib], Keith Rule\nLicense [keith-rule], Kerberos License [kerberos], Kevan Stannard License [kevan-stannard], Kevlin\nHenney License [kevlin-henney], Khronos License [khronos], Knuth CTAN License [knuth-ctan],\nKumar Robotics License [kumar-robotics], latex-ec-fonts [ecfonts-1.0], Latex2e License [latex2e],\nLatex2e with translated notice permission [latex2e-translated-notice], LBNL BSD Variant [lbnl-\nbsd], LCS-Telegraphics License [lcs-telegraphics], Leptonica License [leptonica], libgd License 2018\n[libgd-2018], libgeoTiff License [libgeotiff], LibMib License [libmib], libmng License 2007 [libmng-\n2007], Libpng License [libpng], LIbpng License v2 [libpng-v2], libselinux License [libselinux-pd],\n39\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nlibsrv License v1.0.2 [libsrv-1.0.2], Lil License v1 [lil-1], LILO License [lilo], Linux Device Drivers\n[linux-device-drivers], Linux-OpenIB [linux-openib], LinuxBIOS License [linuxbios], linuxhowtos\nLicense [linuxhowtos], LLNL [llnl], LLVM Exception to Apache 2.0 [llvm-exception], Logica OSL 1.0\n[logica-1.0], LPPL 1.3c [lppl-1.3c], Lucent Public License 1.0 [lucent-pl-1.0], Lucent Public License\n1.02 [lucent-pl-1.02], Lucre License [lucre], LZMA SDK License (versions 9.22 and beyond) [lzma-sdk-\n9.22], LZMA SDK Public Domain [lzma-sdk-pd], M+ Fonts license [m-plus], MakeHuman License\n[make-human-exception], Markus Kuhn License [markus-kuhn-license], Martin Bergmeier License\n[martin-birgmeier], Matrix Template Library License [mtll], Matt Gallagher Attribution License\n[matt-gallagher-attribution], Matt Kruse License [mattkruse], Matthew Kwan License [matthew-\nkwan], MediaInfo(Lib) License [mediainfo-lib], metamail License [metamail], MgOpen Font License\n[mgopen-font-license], Michael Barr License [michael-barr], Minpack Copyright Notice [minpack],\nMirOS License [mir-os], MIT (SEI) [vince], MIT 1995 [mit-1995], MIT Acknowledgment License\n[mit-ack], MIT Addition License [mit-addition], MIT License 1998 [mit-license-1998], MIT License\n[mit], MIT Modern Variant [mit-modern], MIT Nagy Variant [mit-nagy], MIT no advertising with\nExport Control [mit-no-advert-export-control], MIT No Commercial Use of Trademarks [mit-no-\ntrademarks], MIT no false attribution License [mit-no-false-attribs], MIT Old Style [mit-old-style],\nMIT Old Style no advertising [mit-old-style-no-advert], MIT Old Style Spare [mit-old-style-sparse],\nMIT README License [mit-readme], MIT Synopsys License [mit-synopsys], MIT Taylor Variant\n[mit-taylor-variant], MIT Veillard Variant [mit-veillard-variant], MIT with Export Control [mit-\nexport-control], MIT with Specification Disclaimer [mit-specification-disclaimer], MIT Xfig Variant\n[mit-xfig], MIT-0-Clause [mit-0], mod_dav License 1.0 [mod-dav-1.0], Modified MIT License for\nPublic Domain software [pd-mit], Motorola Microprocessor License [motorola], Mozilla GC License\n[mozilla-gc], MPEG SSG License [mpeg-ssg], MPEG-2 NBC MPEG-4 Audio ISO [mpeg-iso], MPICH\nLicense [mpich], MS Systems Journal Sample Code License [msj-sample-code], MS WS Routing\nSpecifications License [ms-ws-routing-spec], MS-LPL [ms-lpl], MS-PL [ms-pl], MS-SS-PL [ms-sspl],\nMulan PSL v1 [mulanpsl-1.0], Mulan PSL v1.0 (En) [mulanpsl-1.0-en], Mulan PSL v2 [mulanpsl-2.0],\nMulan PSL v2.0 (En) [mulanpsl-2.0-en], Mulle Kybernetik License [mulle-kybernetik], Multics\nLicense [multics], Mup License [mup], musl attribution exception [musl-exception], MX4J License\n1.0 [mx4j], Nara Institute License 2003 [naist-2003], NASA 1.3 [nasa-1.3], NAUMEN Public License\n[naumen], NBPL-1.0 [nbpl-1.0], NCBI Public Domain Notice [ncbi], NCSA Open Source License\n[uoi-ncsa], Net SNMP License [net-snmp], Netcat License [netcat], NetCDF License [netcdf], Netron\nProject License [netron], Newlib Historical License [newlib-historical], Newran License [newran],\nNewsletr License [newsletr], Nice License [nice], NICTA Public Software Licence 1.0 [nicta-psl],\nNiels Ferguson License [niels-ferguson], Nilsson Historical License [nilsson-historical], NIST Public\nDomain Notice [nist-pd], NIST Public Domain Notice with fallback [nist-pd-fallback], NIST Software\nLicense [nist-software], NIST SRD License [nist-srd], NLOD-1.0 [nlod-1.0], NLOD-2.0 [nlod-2.0],\nNLPL [nlpl], Node License [node-js], Non White Heterosexual Male [nwhm], Nonexclusive License\n[nonexclusive], Nortel DASA License [nortel-dasa], Notre Dame License [notre-dame], NRL License\n[nrl], NRL permission [nrl-permission], NTLM License [ntlm], NTP Origin License [ntpl-origin],\nNTP-0 [ntp-0], NVIDIA 2002 License [nvidia-2002], NVIDIA License [nvidia], NVIDIA License\nwith Government Qualifications [nvidia-gov], NYSL 0.9982 [nysl-0.9982], NYSL 0.9982 JP [nysl-\n0.9982-jp], O Young Jong License [o-young-jong], O’Reilly Code Sample Notice [oreilly-notice],\nO-UDA-1.0 [o-uda-1.0], Oasis WS Security Specification License [oasis-ws-security-spec], Object\nForm Exception to MIT [object-form-exception-to-mit], ODC-By-1.0 [odc-by-1.0], ODMG License\n[odmg], OFFIS License [offis], OFL 1.0 [ofl-1.0], OFL 1.0 no Reserved Font Name [ofl-1.0-no-\nrfn], OFL 1.0 Reserved Font Name [ofl-1.0-rfn], OFL 1.1 no Reserved Font Name [ofl-1.1-no-rfn],\nOGC 1.0 [ogc-1.0], OGC Software Notice [ogc], OGL 1.0a [ogl-1.0a], OGL Alberta 2.1 [can-ogl-\nalberta-2.1], OGL British Columbia 2.0 [can-ogl-british-columbia-2.0], OGL Canada 2.0 [can-ogl-2.0-\nen], OGL Canada 2.0 Francais [ogl-canada-2.0-fr], OGL Nova Scotia 1.0 [can-ogl-nova-scotia-1.0],\n40\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nOGL Ontario 1.0 [can-ogl-ontario-1.0], OGL Toronto 1.0 [can-ogl-toronto-1.0], OGL-UK-1.0 [ogl-\nuk-1.0], OGL-UK-2.0 [ogl-uk-2.0], OGL-UK-3.0 [ogl-uk-3.0], OGL-WPD-3.0 [ogl-wpd-3.0], Open\nDirectory License [odl], Open Group Test Suite License [opengroup], Open Publication License 1.0\n[openpub], OpenLDAP Public License 1.1 [openldap-1.1], OpenLDAP Public License 1.2 [openldap-\n1.2], OpenLDAP Public License 1.3 [openldap-1.3], OpenLDAP Public License 1.4 [openldap-\n1.4], OpenLDAP Public License 2.0 [openldap-2.0], OpenLDAP Public License 2.0.1 [openldap-\n2.0.1], OpenLDAP Public License 2.1 [openldap-2.1], OpenLDAP Public License 2.2 [openldap-2.2],\nOpenLDAP Public License 2.2.1 [openldap-2.2.1], OpenLDAP Public License 2.2.2 [openldap-\n2.2.2], OpenLDAP Public License 2.3 [openldap-2.3], OpenLDAP Public License 2.4 [openldap-\n2.4], OpenLDAP Public License 2.5 [openldap-2.5], OpenLDAP Public License 2.6 [openldap-\n2.6], OpenLDAP Public License 2.7 [openldap-2.7], OpenLDAP Public License 2.8 [openldap-2.8],\nOpenORB Community License 1.0 [openorb-1.0], OpenSAML License v1 [opensaml-1.0], OpenSSH\nLicense [openssh], OpenSSL License [openssl], OpenSSL/SSLeay License [openssl-ssleay], OPML 1.0\n[opml-1.0], OPNL-1.0 [opnl-1.0], OPNL-2.0 [opnl-2.0], Oracle BSD-Style with Nuclear Restrictions\n[oracle-bsd-no-nuclear], Original SSLeay License [ssleay], Original SSLeay License with Windows\nClause [ssleay-windows], Oswego Concurrent License [oswego-concurrent], Other Permissive Licenses\n[other-permissive], OWTChart License [owtchart], OZPLB 1.0 [ozplb-1.0], OZPLB 1.1 [ozplb-1.1],\nPaolo Messina 2000 [paolo-messina-2000], ParaView License 1.2 [paraview-1.2], Paul Mackerras\nBinary License [paul-mackerras-binary], Paul Mackerras License [paul-mackerras], Paul Mackerras\nNew License [paul-mackerras-new], Paul Mackerras Simplified License [paul-mackerras-simplified],\nPaulo Soares License [paulo-soares], PayPal SDK License 2013-2016 [paypal-sdk-2013-2016], PBM\nLibrary License [libpbm], PCRE License [pcre], PD’Programming License [pd-programming], PDDL\n1.0 [pddl-1.0], Perl 1.0 [perl-1.0], Peter Deutsch Document License [peter-deutsch-document], Phil\nBunce License [phil-bunce], Philippe De Muyter License [philippe-de-muyter], Phorum License 2.0\n[phorum-2.0], PHP License 2.0.2 [php-2.0.2], PHP License 3.0 [php-3.0], PHP License 3.01 [php-3.01],\nPine License [pine], PngSuite License [pngsuite], Politepix Public License 1.0 [politepix-pl-1.0],\nPostgreSQL License [postgresql], ppp License [ppp], Protobuf License [protobuf], PS Utilities License\n[psutils], PSF Python License 3.7.2 [psf-3.7.2], PSF-2.0 [psf-2.0], psfrag License [psfrag], Psytec\nFree Software License [psytec-freesoft], Public Domain [public-domain], Public Domain Disclaimer\n[public-domain-disclaimer], Purdue BSD-Style License [purdue-bsd], pybench License [pybench],\nPyCrypto License [pycrypto], PyGres License 2.2 [pygres-2.2], Python CWI License [python-cwi],\nPython License 2.0 [python], Python License 2.0.1 [python-2.0.1], Qhull License [qhull], QLogic\nMicrocode [qlogic-microcode], Qpopper License [qpopper], Qualcomm Turing License [qualcomm-\nturing], Quirksmode Copyright Notice [quirksmode], radvd License [radvd], Rdisc License [rdisc],\nRed Hat Attribution License [red-hat-attribution], Red Hat BSD-Simplified [red-hat-bsd-simplified],\nRegexp License [regexp], Repoze License [repoze], RiceBSD [ricebsd], Richard Black License [richard-\nblack], Robert Hubley License [robert-hubley], RSA 1990 [rsa-1990], RSA Cryptoki License [rsa-\ncryptoki], RSA Demo License [rsa-demo], RSA-MD4 License [rsa-md4], RSA-MD5 License [rsa-md5],\nRTools.Util License [rtools-util], Ruby License [ruby], Runtime Library Exception to Apache 2.0\n[apple-runtime-library-exception], Rute Users Tutorial and Exposition License 0.8.0 [rute], Ryszard\nSzopa License [ryszard-szopa], SaaS MIT License [saas-mit], Sash Notice [sash], SATA License [sata],\nSAX-PD [sax-pd], Saxpath License [saxpath], SBIA Part B [sbia-b], ScanCode acknowledgment\n[scancode-acknowledgment], scanlogd License [scanlogd-license], ScanSoft Public License 1.2 [scansoft-\n1.2], SCEA Shared Source License 1.0 [scea-1.0], Scheme Language Report License [schemereport],\nScheme Widget Library (SWL) Software License [swl], Scintilla License [scintilla], Scribbles Demos\nRecognizer Notice [scribbles], Script Asylum License [script-asylum], Secret Labs License 2011 [secret-\nlabs-2011], selinux-nsa-declaration-1.0 [selinux-nsa-declaration-1.0], Sendmail License [sendmail],\nService Availability Forum License [saf], Service Component Architecture License [service-comp-arch],\nSFL License Agreement [sfl-license], SGI CID Font Code Public License 1.0 [sgi-cid-1.0], SGI Free\n41\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSoftware License B 1.1 [sgi-freeb-1.1], SGI Free Software License B 2.0 [sgi-freeb-2.0], SGI GLX Public\nLicense 1.0 [sgi-glx-1.0], Sglib License [sglib], SGP4 Permission Notice [sgp4], Shital Shah License\n[shital-shah], SIL Open Font License 1.1 with Reserved Font Name [ofl-1.1-rfn], SimPL 1.1 [simpl-1.1],\nSNMP++ License [hp-snmp-pp], snprintf License [snprintf], SoftFloat [softfloat], SoftFloat Legal\nNotice 2.0 [softfloat-2.0], softSurfer License [softsurfer], SolderPad Hardware License v0.5 [shl-0.5],\nSolderpad Hardware License v2.0 [shl-2.0], Solderpad Hardware License v2.1 [shl-2.1], SolderPad\nHardware License, Version 0.51 [shl-0.51], Sparky License [sparky], SpeechWorks Public License\n1.1 [speechworks-1.1], SQLite Blessing [blessing], Standard ML of New Jersey [standard-ml-nj],\nStanford PVRG License [stanford-pvrg], STLport License 2000 [stlport-2000], STLport License 4.5\n[stlport-4.5], STREAM Benchmark License [stream-benchmark], Stu Nicholls License [stu-nicholls],\nSun RPC License [sun-rpc], Sun source code License [sun-source], SunPro Attribution License\n[sunpro], Sunsoft License [sunsoft], Supervisor License [supervisor], svndiff License [svndiff], SWIG\nLibrary License [swig], Symlinks License [symlinks], Symphonysoft [symphonysoft], Synopsys MIT\nLicense [synopsys-mit], Synthesis Toolkit License [synthesis-toolkit], SystemC Open Source License\nAgreement [accellera-systemc], Taiwan Open Government Data License, version 1.0 [ogdl-taiwan-1.0],\nTakao Abe License [takao-abe], Takuya OOURA License [takuya-ooura], Talis Community License\n[ttcl], Tatu Ylonen License [tatu-ylonen], TCG Spec License v1 [tcg-spec-license-v1], TCL/TK\nLicense [tcl], TCP Wrappers License [tcp-wrappers], TekHVC License [tekhvc], Term Readkey\nLicense [term-readkey], Tested Software License [tested-software], TeX Live License [tex-live], Text-\nTabs+Wrap License [ttwl], TFL [tfl], The Happy Bunny License [happy-bunny], Theodore Ts’o license\n[tso-license], Things I Made (TIM) Public License [things-i-made-public-license], Tidy License [tidy],\nTiger Cryptography License [tiger-crypto], Tigra Calendar 3.2 License [tigra-calendar-3.2], Tigra\nCalendar 4.0 License [tigra-calendar-4.0], Tim Janik License 2003 [tim-janik-2003], Time::ParseDate\nLicense [tpdl], Timestamp Picker License [timestamp-picker], TTYP0 License [ttyp0], TU Berlin\nLicense 1.0 [tu-berlin], TU Berlin License 2.0 [tu-berlin-2.0], Tumbolia Public License [tumbolia],\nTwistedSNMP License [twisted-snmp], UCAR License [ucar], UnboundID LDAP SDK Free Use\nLicense [ldap-sdk-free-use], Unicode DFS 2015 [unicode-dfs-2015], Unicode DFS 2016 [unicode-dfs-\n2016], Unicode Inc License Agreement [unicode], Unicode Mappings License [unicode-mappings],\nUniversity of British Columbia License [ubc], University of Michigan OSL [michigan-disclaimer],\nUNIX Network Programming Book License [unpbook], UnixCrypt License [unixcrypt], Unlicense\n[unlicense], Unlimited Binary Use Exception [unlimited-binary-use-exception], UPL 1.0 [upl-1.0], US\nGovernment Public Domain [us-govt-public-domain], US Government Unlimited Rights [us-govt-\nunlimited-rights], USRobotics Permissive License [usrobotics-permissive], Utopia Typeface License\n[utopia], VCalendar License [vcalendar], Vic Metcalfe Public Domain [vic-metcalfe-pd], VIM License\n[vim], Visual Idiot [visual-idiot], Visual Numerics License [visual-numerics], Vixie Cron License [vixie-\ncron], Vovida Software License 1.0 [vsl-1.0], W3C 3-Clause BSD License [w3c-03-bsd-license], W3C\nSoftware Notice and License [w3c], W3C-SOFTWARE-19980720 [w3c-software-19980720], W3C-\nSOFTWARE-DOC-20150513 [w3c-software-doc-20150513], w3m License [w3m], Westhawk License\n[westhawk], Whistle Communications License [whistle], Whitecat License [whitecat], WIDE License\n[wide-license], Wide Open License [wol], Widget Workshop License [widget-workshop], William\nAlexander License [william-alexander], wingo License [wingo], Wordnet License [wordnet], Wrox Press\nLicense [wrox], WS-Addressing Specification License [ws-addressing-spec], WS-Policy Specification\n[ws-policy-specification], WS-Trust Specification [ws-trust-specification], Wsuipa License [wsuipa],\nWTFNMFPL-1.0 [wtfnmfpl-1.0], WTFPL 1.0 [wtfpl-1.0], WTFPL 2.0 [wtfpl-2.0], WTHPL 1.0\n[wthpl-1.0], wxWidgets Licence [wxwidgets], wxWindows Unrestricted Licence 3.0 [wxwindows-u-3.0],\nX11 Documentation License [x11-doc], X11 License [x11], X11-R5 [x11-x11r5], X11-Style (Acer)\n[x11-acer], X11-Style (Adobe) [x11-adobe], X11-Style (Adobe-DEC) [x11-adobe-dec], X11-Style\n(Bitstream Charter) [x11-bitstream], X11-Style (David R. Hanson) [x11-hanson], X11-Style (DEC\n1) [x11-dec1], X11-Style (DEC 2) [x11-dec2], X11-Style (DSC Technologies) [x11-dsc], X11-Style\n42\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(FSF) [x11-fsf], X11-Style (Keith Packard) [x11-keith-packard], X11-Style (Lucent) [x11-lucent],\nX11-Style (Lucent-variant) [x11-lucent-variant], X11-Style (OAR) [x11-oar], X11-Style (Open Group)\n[x11-opengroup], X11-Style (OpenGL) [x11-opengl], X11-Style (Quarterdeck) [x11-quarterdeck],\nX11-Style (Realmode) [x11-realmode], X11-Style (Silicon Graphics) [x11-sg], X11-Style (Stanford\nUniversity) [x11-stanford], X11-Style (Tektronix) [x11-tektronix], X11-Style (Tiff) [x11-tiff], X11-Style\n(X Consortium Veillard) [x11-xconsortium-veillard], X11-Style (X Consortium) [x11-xconsortium],\nXdebug License v 1.03 [xdebug-1.03], XFree86 License 1.0 [xfree86-1.0], XFree86 License 1.1 [xfree86-\n1.1], xinetd License [xinetd], XML:DB Initiative Software License 1.0 [xmldb-1.0], XSkat License\n[xskat], xxd License [xxd], Yale CAS License [yale-cas], Yensdesign License [yensdesign], Zed License\n[zed], Zend Engine License 2.0 [zend-2.0], ZeusBench notice [zeusbench], ZLIB License [zlib], ZLIB\nLicense with Acknowledgment [zlib-acknowledgement], ZPL 1.0 [zpl-1.0], ZPL 1.1 [zpl-1.1], ZPL\n2.0 [zpl-2.0], ZPL 2.1 [zpl-2.1], zsh License [zsh], Zuora Software License [zuora-software], Zveno\nResearch License [zveno-research]\nThe list above gives the short name (or name, if no short name exists) along with the key, in square\nbrackets, from the ScanCode license dataset available at https://github.com/aboutcode-org/\nscancode-toolkit/tree/develop/src/licensedcode/data/licenses.\n43\n"
    },
    {
      "arxiv_id": "2412.02595",
      "full_text": "arXiv:2412.02595v2  [cs.CL]  30 May 2025\nNemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\nPretraining Dataset\nDan Su*, Kezhi Kong*, Ying Lin*, Joseph Jennings, Brandon Norick,\nMarkus Kliegl†, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro\nNVIDIA\n*Equal contribution. †Correspondence to mkliegl@nvidia.com.\nAbstract\nRecent English Common Crawl datasets like\nFineWeb-Edu and DCLM achieved significant\nbenchmark gains via aggressive model-based\nfiltering, but at the cost of removing 90% of\ndata. This limits their suitability for long token\nhorizon training, such as 15T tokens for Llama\n3.1. In this paper, we show how to achieve\nbetter trade-offs between accuracy and data\nquantity by a combination of classifier ensem-\nbling, synthetic data rephrasing, and reduced\nreliance on heuristic filters.\nWhen training\n8B parameter models for 1T tokens, using\na high-quality subset of our data improves\nMMLU by 5.6 over DCLM, demonstrating\nthe efficacy of our methods for boosting\naccuracies over a relatively short token horizon.\nFurthermore, our full 6.3T token dataset\nmatches DCLM on MMLU, but contains\nfour times more unique real tokens than\nDCLM. This unlocks state-of-the-art training\nover a long token horizon: an 8B parameter\nmodel trained for 15T tokens, of which 7.2T\ncame from our dataset, is better than the\nLlama 3.1 8B model: +5 on MMLU, +3.1 on\nARC-Challenge, and +0.5 on average across\nten diverse tasks. The dataset is available at\nhttps://data.commoncrawl.org/contrib/\nNemotron/Nemotron-CC/index.html.\n1\nIntroduction\nInternet crawl is the largest source of unique to-\nkens for training LLMs and can be seen as serving\ntwo main purposes: high-quality content and diver-\nsity. Recent English datasets derived from Com-\nmon Crawl1 such as FineWeb-Edu (Penedo et al.,\n2024) and DCLM (Li et al., 2024) have empha-\nsized high-quality content that boosts benchmark\naccuracies over data quantity. They have demon-\nstrated significant strides in achieving benchmark\nresults competitive with some of the best closed\nmodels at a small scale (e.g., DCLM’s 7B model\n1https://commoncrawl.org/\n59.0\n53.4\n53.0\n42.9\n42.4\nMMLU Accuracy\n0\n20\n40\n60\nNemotron-CC-HQ\nDCLM\nNemotron-CC\nFineWeb-Edu\nFineWeb-Edu-2\n+5.6 MMLU\n4x more data\nFigure 1: MMLU scores for 8B parameter models\ntrained for 1T tokens. Compared to DCLM, our meth-\nods enable us to either create a 4× larger dataset of\nsimilar quality or increase the MMLU using a high qual-\nity subset of the tokens. Having a larger dataset, in the\nsense of unique real tokens, is crucial when training\nover long horizons such as 15T tokens.\ntrained over 2.6T tokens), primarily thanks to the\nuse of model-based filters to extract high-quality\neducational and instructional content. However,\nthis comes at the cost of data quantity: they remove\naround 90% of the data. Such aggressive pruning\nmay not be the most effective strategy when train-\ning larger models over longer token horizons (e.g.,\nLlama 3.1 includes 8B–405B parameter models,\ntrained for 15T tokens (Dubey et al., 2024) and\nGemma 2 27B was trained for 13T tokens (Team\net al., 2024)). Both DCLM and FineWeb-Edu\ncontain around 80% near-duplicates (1T and 0.2T\nunique tokens, respectively) (Ben Allal, 2024; Li\net al., 2024) and to train on these datasets for many\ntrillions of tokens implies seeing essentially the\nsame samples many times during training. This\ncould lead to inferior models, as Muennighoff et al.\n\n(2024) find there are diminishing returns after four\nepochs compared to training on more unique to-\nkens.\nIn this paper, we show how to achieve a better\ntrade-off between benchmark accuracy and data\nquantity with a combination of classifier ensem-\nbling, synthetic data generation, and reduced re-\nliance on heuristic filters. Our main contributions\nare:\n1. We propose a method for transforming En-\nglish Common Crawl into a 6.3T token long-\nhorizon pretraining dataset, consisting of 4.4T\nglobally deduplicated original tokens and 1.9T\nsynthetically generated tokens. We release the\ndataset2 under the Common Crawl Terms of\nUse and a reference implementation as part\nof the Apache 2.0 open-source NeMo Curator\nlibrary.3 The quality classifier models have\nbeen released as well.4\n2. We prove the effectiveness of this method\nby comparing to the state-of-the-art open En-\nglish Common Crawl datasets DCLM and\nFineWeb-Edu (Figure 1).\n(a) A 1.1T-token high-quality subset of our\ndata achieves a 5.6 MMLU improvement\nover DCLM, showing the superiority of\nour method over a relatively short token\nhorizon.\n(b) Our full dataset performs on par with\nDCLM while having 4× as many unique\nreal tokens.\n(c) This larger size enables state-of-the-art\nresults over long token horizons: An\n8B parameter model trained for 15T to-\nkens using a weighted version of our\ndataset achieves higher overall accuracy\nthan Llama 3.1 8B, and in particular\nMMLU 70.3 vs. Llama’s 65.3. Note\nthat Llama 3.1 8B was also trained on\n15T tokens (Dubey et al., 2024).\n3. We conduct ablation studies and find:\n(a) Ensembling different model-based clas-\nsifiers can help select a larger and more\n2https://data.commoncrawl.org/contrib/Nemotron/\nNemotron-CC/index.html\n3https://github.com/NVIDIA/NeMo-Curator\n4https://huggingface.co/nvidia/nemocurator-\nfineweb-nemotron-4-edu-classifier\nand\nhttps:\n//huggingface.co/nvidia/nemocurator-fineweb-\nmixtral-edu-classifier\ndiverse set of high quality tokens.\n(b) Rephrasing can effectively reduce noise\nand errors in low-quality data and pro-\nduce diverse variants with fresh unique\ntokens from high-quality data, leading to\nbetter results in downstream tasks.\n(c) Disabling traditional non-learned heuris-\ntic filters for high-quality data can further\nboost high quality token yield without\nhurting accuracy.\nFinally, we remark that our overall guiding prin-\nciple is to shift from a static, non-learned, heuristic\npipeline towards a more learned flywheel whose\nperformance will naturally get better over time. As\nour data improves, so will the LLMs we train, and\nthese improved LLMs will in turn improve our data\nas we use them to generate better synthetic data\nand quality classifications.\n2\nMethods\nIn this section we explain our efforts to build the\nbest English Common Crawl pretraining dataset\nfor LLMs. Our efforts can be split into three folds.\nFirst, we talk about our efforts in boosting token\nyield by utilizing text extractor and heuristic filters\nmore properly in Section 2.1. Second, we introduce\nthe model-based quality labeling pipeline methods\nin Section 2.2. Third, we introduce our synthetic\ndata generation method to further improve the data\nquality in Section 2.3. For a schematic overview\nof our final pipeline, please see Figure 3 in Ap-\npendix A.\n2.1\nHTML-to-text Extractor & Filter\nExtracted texts from HTMLs are the foundation\nand major source of LLM pretraining dataset, so it\nis of great significance to analyze and understand\nthe extraction tools for optimal data quality and\ntoken yield. Moreover, heuristic filters are often\nutilized to remove low-quality tokens with human-\ndesigned heuristics (Li et al., 2024; Parmar et al.,\n2024; Penedo et al., 2024; Dubey et al., 2024),\nwhich may also put good tokens at the risk of being\nremoved. We carefully examine both aspects with\nthe assist of the FineWeb-Edu classifier (Penedo\net al., 2024), a model-based quality classifier that\nhad shown effectiveness in identifying high-quality\ntokens that are significant in boosting the strength\nof LLMs.\n\n#Tokens\n#HQ tokens\n#HQ +%\nTrafilatura-filtered\n994\n80\n-\nJustext-filtered\n1,380\n104\n28.6%\nJustext\n1,804\n127\n57.4%\nTable 1: Extraction and filteration token count statistics\n(billion). Tokens counted after deduplication.\nHTML-to-text Extraction\nWe test two HTML-\nto-text extractors, Justext (Pomikálek, 2011) and\nTrafilatura (Barbaresi, 2021). Qualitatively, we\nview both extractors at the same level of quality.\nQuantitatively, we calculate token yields of both\nextractors on 13 selected snapshots of Common\nCrawl (see Appendix F). The statistics are reported\nin Table 1. We see that Justext can yield more to-\nkens, notably more high-quality tokens (+28.6%)\nby the standard of Fineweb-Edu classifier (score 3,\n4, and 5). We highlight that boosting unique token\namount is of great importance when building long-\nhorizon pretraining dataset, e.g., 15T tokens for\nLlama3.1. Even though there is a slight decline in\nthe percentage of HQ tokens for Justext vs. Justext-\nfiltered (7.0% vs. 7.5%), what we aim to maximize\nhere is the absolute number of HQ tokens (127B\nvs. 104B). We will later sort the data into quality\nbuckets, which enables exact control of the propor-\ntion of HQ vs. non-HQ data seen during training\ninstead of reliance on the natural distribution for\na particular extraction tool. After extraction, we\napply filtering to keep only English text, as deter-\nmined by pycld25 and the FastText lid176 language\nclassifier6 with threshold 0.3 (Joulin et al., 2016,\n2017). We then apply global fuzzy deduplication as\nwell as exact substring deduplication over eighths\nof snapshots (Lee et al., 2022), using the NeMo\nCurator library7 and the deduplicate-text-datasets\nlibrary,8 respectively.\nFiltering\nConventionally, heuristic filters are\nleveraged to remove low-quality tokens from the\npretraining dataset as a post-processing step (Li\net al., 2024; Parmar et al., 2024; Penedo et al.,\n2024; Dubey et al., 2024). We revisit the filter-\ning pipeline as in (Parmar et al., 2024).\nSuch\npipeline sequentially consists of a set of heuris-\ntic filters proposed by Raffel et al. (2020); Rae et al.\n5https://pypi.org/project/pycld2/\n6https://fasttext.cc/docs/en/language-\nidentification.html\n7https://github.com/NVIDIA/NeMo-Curator\n8https://github.com/google-research/\ndeduplicate-text-datasets\n(2021) and a perplexity filter based on a KenLM\nmodel (Heafield, 2011) trained on Wikipedia and\nbooks data (Wenzek et al., 2020). To quantitatively\nbetter understand the effectiveness of the filtering\npipeline, we calculate the token yield and report the\nnumbers in Table 1. We find the filtering pipeline\nremoves a non-trivial portion of high-quality to-\nkens (-18.1%) classified by FineWeb-Edu classifier\nfrom the dataset.\nGiven the impact that the heuristic filters have on\nthe high-quality token yield, we propose to NOT\napply such filters to the high-quality tokens dis-\ntinguished by model-based quality classifers (de-\nscribed in the next section), but only use those on\nthe low-quality splits. In the experiment section we\nempirically verify the impact of both the extractor\nand filter on pretraining data quality through down-\nstream benchmarks. We refer readers to Section 3.3\nfor detailed results.\n2.2\nModel-based Quality Labeling\nRecent work (Li et al., 2024; Penedo et al., 2024)\nuse model-based classifiers to extract high-quality\npretraining documents from English Common\nCrawl. However, both of the two quality classi-\nfiers have a limited recall (around 10%) of high-\nquality tokens (see Table 9), and this will become\na bottleneck to train an LLM over a long horizon.\nAlso, the quality labels assigned by the quality\nclassifier are not necessarily aligned with LLM’s\ndownstream task performance. Therefore, we pro-\npose our ensemble-based quality labeling pipeline\nmethod. Specifically, we first build three quality\nclassifiers, each of which has different high-quality\npreferences. Then, we ensemble the three clas-\nsifiers to score all the documents, and split the\ncrawl corpus into different quality buckets based\non the quality score. Finally, we regroup the fine-\ngrained document buckets into 5 different quality\nlevels based on their corresponding performance\non downstream task.\nQuality Classifier Training\nPreparing pretrain-\ning documents with quality annotations is the first\nkey step in building a quality classifier (Dubey\net al., 2024; Abdin et al., 2024; Yang et al., 2024).\nSimilar to the work (Penedo et al., 2024)9, we\nconstructed two versions of quality annotation\ndata. We prompt Mistral 8x22B-instruct10 and\n9We use the same 460K document samples as in the\nFineWeb-Edu-Annotation dataset.\n10https://mistral.ai/news/mixtral-8x22b/\n\nNemotron-340B-instruct (Adler et al., 2024), to\nscore web documents from FineWeb based on\ntheir educational value on a scale from 0 to 5.\nWe then fine-tune a linear regression model on\ntop of the Snowflake-arctic-embed-m embedding\nmodel (Merrick et al., 2024) using the two different\nversion of training sets. The two models have been\ntrained for 20 epochs with a learning rate of 3e-\n4, with the embedding and encoder layers frozen,\nand we selected the checkpoint with the highest F1\nscore on the held-out validation set.\nWe also employ the DCLM classifier which is a\nfastText-based classifier released by Li et al. (2024).\nThe DCLM classifier is trained on a combination\nof instruction-formatted data (Teknium, 2023) and\nhigh-scoring posts data from ELI5 subreddit (Fan\net al., 2019), and has shown stronger performance\nin identifying high-quality pretraining tokens, com-\npared to the FineWeb-Edu classifier (Penedo et al.,\n2024). The DCLM classifier will offer a new per-\nspective in labeling high-quality pretraining doc-\numents, and will help increase the recall of high-\nquality tokens.\nQuality Scoring and Bucketing\nFirst, we use\neach of the three classifiers to predict the qual-\nity scores for all the documents. Then based on\nthe ranked quality score from each classifier, we\nrounded the model’s output score to integers from\n0 to 19. So that each score bucket will have around\n5% of the documents, and bucket 19 will have the\ntop 5% highest quality documents. We then assign\nthe final quality score for each document by en-\nsembling the three classifiers’ integer score by a\nmaximum operation. The number of documents\ndistribution in each buckets will be skewed by the\nensemble operation.\nQuality Labeling\nIn order to assign a quality la-\nbel that is more aligned with their real performance\non downstream tasks, we further group the fine-\ngrained quality score predicted by three classifiers\ninto 5 downstream quality categories. We used an-\nnealing to assess each data bucket’s downstream\ntask’s quality. Specifically, we measure the quality\nof each bucket by continuous pretraining with 50B\ntokens on a 70% trained 8B models. We assign\n66% of weight to the default data mix and 34%\nto the dataset that we are evaluating. By compar-\ning the average performance of each bucket over 9\ntasks, we group the 20 buckets into 5 big categories,\nwith the final distribution shown in Table 2. For\nmore details, please see Appendix C.\nQuality Label\nBuckets\n# Tokens (B)\nToken (%)\nHigh\n19\n553\n12.63\nMedium-High\n18\n504\n11.52\nMedium\n12-17\n2,023\n46.24\nMedium-Low\n7-11\n894\n20.43\nLow\n0-6\n402\n9.18\nTable 2: Common Crawl quality labels statistics.\n2.3\nSynthetic Data Generation\nUpon reviewing samples across the quality tiers,\nwe observe that documents with lower scores tend\nto contain more noise and errors, while those scor-\ning higher generally exhibit good writing and for-\nmatting. Therefore, we employ different strategies\nwhen generating data from low- and high-quality\ndocuments.\nFor low-quality data, our goal is to improve the\nquality by reducing noise and errors while preserv-\ning useful information, thereby decreasing train-\ning compute expenses. As shown by Maini et al.\n(2024), rephrasing web data using a medium-sized\nlanguage model yields an enhanced parallel cor-\npus of synthetic data, thereby reducing model per-\nplexity and boosting its accuracy on downstream\ntasks. Unlike existing methods that create new\ncontent such as textbooks and short stories (Wang\net al., 2023; Eldan and Li, 2023; Gunasekar et al.,\n2023), our rephrasing-based approach does not uti-\nlize the language model as a knowledge bank but\nfocuses on transforming provided texts into another\nstyle, allowing it to operate with a lighter-weight\nmodel.\nWe adopt the Wikipedia style prompt\nfrom (Maini et al., 2024) to rewrite low-quality\ndocuments (Prompt 5 in Appendix H), which ef-\nfectively reduces errors and redundancies and im-\nproves formatting.\nFor high-quality data, we aim to obtain more\nunique tokens and condense essential knowledge.\nAccording to (Muennighoff et al., 2024), adding\nrepeated tokens yields a diminishing return, espe-\ncially after 4 epochs. For high-quality documents,\nwe generate synthetic data using four additional\nprompts: (1) Diverse Question-Answer (QA) pairs:\nask questions in various forms (e.g., yes/no ques-\ntion, open-ended question, multi-choice question)\nabout factual information in the text and provide\nthe correct answers; (2) Distill: rewrite the text into\na concise and clear passage; (3) Extract knowledge:\nrewrite knowledge from the text and disregard un-\ninformative content; (4) Knowledge list: extract\nkey information from the text as an organized list.\n\nWe require the model to provide clear and concise\nresponses while preserving factual information and\nconcrete details such as numbers. The full prompts\nare shown in Appendix H.\nAs we increase the length of provided text, the\nmodel shows a tendency to produce over-simplified\noutputs with reduced detail. Therefore, we chunk\neach document into segments, each of which con-\ntains one or more complete lines and is shorter than\na specific token limit.11 Over-length lines exceed-\ning the token limit are discarded.\nQuestion: Which year did the United Nations\nimplement the 2030 agenda for SDGs?\nAnswer: January 1, 2016\nQuestion: What are the three key dimensions of\nsustainable development covered by the SDGs?\nAnswer: (a) economic growth, (b) social\ninclusion, and (c) environmental protection\nQuestion: Which of the following can flossing\nprevent? A) Cavities B) Gum disease C) Both A and\nB D) Neither A nor B\nAnswer: C) Both A and B\nQuestion: Is flossing important even if you\nbrush your teeth twice a day?\nAnswer: Yes, flossing is important as it reaches\nareas that brushing alone cannot.\nFigure 2: Examples of generated question-answer pairs.\nOur post-processing steps include removing in-\ncomplete results, eliminating specific Markdown\nformatting (e.g., double asterisks), stripping away\nprefixes of certain patterns (e.g., “Here is a para-\nphrased version:” and “Paraphrased Text:”), re-\nmoving quotation marks enclosing the entire re-\nsponse, and filtering out under-length outputs (i.e.,\nshorter than 50 tokens). For Wikipedia results, we\nconcatenate passages generated from segments be-\nlonging to the same original document. For Diverse\nQA Pairs results, we shuffle the generated question\nand answer pairs, retain up to a number based on\nthe length of the segment, and append the pairs to\nthe end of the segment.\nUsing the instruct version of Mistral NeMo\n12B12 with FP8 inference, a top-p value of 0.9, and\na sampling temperature of 0.5, we synthesize over\n1.8T tokens as Table 3 shows, including 336.3B\ntokens from low-quality documents and 1.5T to-\nkens from high-quality documents. We do not use\nmedium-quality documents for synthetic data gen-\n11The token limit is set to 512 for Wikipedia, 2,000 for\nDistill, 1,400 for Extract Knowledge and 1,000 for Diverse\nQA Pairs and Knowledge List, including tokens from the\nprompt and chat format.\n12https://mistral.ai/news/mistral-nemo\neration due to time and resource constraints. We\nemploy TensorRT-LLM13 and NeMo-Skills14 to\nenable large-scale data synthesis.\nSource\n#Raw\nPrompt\n#Synthetic\nLow\n403.0\nWikipedia\n336.3\nHigh\n451.3\nWikipedia\n372.9\nDiverse QA Pairs\n499.5\nDistill\n157.6\nExtract Knowledge\n303.6\nKnowledge List\n203.2\nTable 3: Synthetic data token count statistics (billion).\n2.4\nPutting It All Together\nDataset\nTotal\nUnique\nSynthetic\nFineWebEdu-2\n5.4\n1.1\n-\nFineWebEdu\n1.3\n0.2\n-\nDCLM\n3.8\n1.0\n-\nNemotron-CC\n6.3\n4.4\n1.9\nNemotron-CC-HQ\n1.1\n0.6\n0.5\nTable 4: Dataset sizes in trillions of tokens. \"Unique\"\nshows the estimated number of tokens after global fuzzy\ndeduplication of the real tokens.\nCombining the techniques above to the 99\nsnapshots CC-MAIN-2013-20 through CC-MAIN-\n2024-30 of Common Crawl, we create a 6.3T token\ndataset (Nemotron-CC), consisting of 4.4T globally\ndeduplicated tokens and 1.9T synthetically derived\ntokens. This dataset has roughly 4× more unique\ntokens than FineWebEdu-2 and DCLM, since both\nof those datasets only underwent a sharded form\nof approximate deduplication and contain roughly\n80% fuzzy duplicates (Ben Allal, 2024; Li et al.,\n2024). To enable a fairer comparison over relatively\nshort token horizons, we thus also consider a 1.1T\ntoken high quality subset of our data (Nemotron-\nCC-HQ), consisting of just the highest-scoring real\nand diverse QA pairs synthetic data. The size break-\ndown of the datasets is shown in Table 4.\n3\nExperiments\n3.1\nExperiment Setup\nTraining\nSetup\nWe\nuse\nthe\nopen\nsource\nMegatron-LM library15 (Shoeybi et al., 2019) to\ntrain standard 8B parameter transformer LLMs.\n13https://github.com/NVIDIA/TensorRT-LLM\n14https://github.com/NVIDIA/NeMo-Skills\n15https://github.com/NVIDIA/Megatron-LM\n\nDataset\nARC-E\nARC-C\nH\nW\nRACE\nPIQA\nSIQA\nCSQA\nOBQA\nMMLU\nAvg\nFineWebEdu-2\n71.9\n44.7\n75.4\n67.0\n36.8\n79.5\n45.2\n25.5\n43.8\n42.4\n53.2\nFineWebEdu\n73.6\n48.0\n70.7\n64.6\n38.0\n76.4\n43.5\n30.0\n44.4\n42.9\n53.2\nDCLM\n74.7\n47.0\n76.3\n69.1\n36.5\n79.7\n45.6\n44.1\n44.0\n53.4\n57.0\nNemotron-CC\n75.3\n50.7\n75.9\n67.8\n37.9\n80.5\n45.1\n47.7\n44.2\n53.0\n57.8\nNemotron-CC-HQ\n78.8\n52.9\n76.6\n69.4\n36.4\n80.1\n46.6\n55.8\n45.4\n59.0\n60.1\nTable 5: Results for 8B parameter models trained on 1T tokens (73% English Common Crawl from the tested\ndataset, 27% the same, fixed non-Crawl datasets). The models were evaluated on ARC-Easy, ARC-Challenge,\nHellaswag, Winogrande, RACE, PIQA, Social IQA, Commonsense QA, Openbook QA, and MMLU.\nModel\nARC-E\nARC-C\nH\nW\nRACE\nPIQA\nSIQA\nCSQA\nOBQA\nMMLU\nAvg\nLlama 3.1\n82.4\n55.0\n79.3\n74.7\n39.1\n81.2\n48.3\n70.6\n46.0\n65.3\n64.2\nOurs\n82.7\n58.1\n80.8\n73.8\n37.8\n81.1\n47.4\n69.9\n45.4\n70.3\n64.7\nTable 6: Comparison of our 8B parameter model vs Llama 3.1 8B. Both were trained for 15T tokens. The numbers\nfor Llama 3.1 are from our own lm-evaluation-harness setup described in Section 3.1 and may not match Meta’s\npublicly reported numbers, as Meta made various customizations to the benchmarks.\nThe hyperparameter details are shown in Ap-\npendix D.\nData Blend\nUnless otherwise noted, we train\nfor 1T tokens on a blend of 73% English Com-\nmon Crawl data and 27% a fixed mix of special-\nized code, papers, books, patents, and Wikipedia\ndatasets (Adler et al., 2024). When comparing\ndatasets, we vary only the 73% English Common\nCrawl portion. See Table 12 in Appendix D.\nEvaluation Setup\nWe use the open source LM\nEvaluation Harness library16 (Gao et al., 2023)\nto evaluate on the following ten common sense\nand reasoning tasks (reported metric in parenthe-\nses): ARC-Easy and ARC-Challenge (normalized\naccuracy) (Clark et al., 2018), Hellaswag (nor-\nmalized accuracy) (Zellers et al., 2019), Wino-\ngrande (accuracy) (Sakaguchi et al., 2021), RACE\n(accuracy) (Lai et al., 2017), PIQA (normalized\naccuracy) (Bisk et al., 2020), Social IQA (accu-\nracy) (Sap et al., 2019), Commonsense QA (accu-\nracy) (Talmor et al., 2019), Openbook QA (normal-\nized accuracy) (Mihaylov et al., 2018), and MMLU\n(accuracy) (Hendrycks et al., 2021).\n3.2\nMain Results\nShort Token Horizon (1T)\nTo validate the qual-\nity of our datasets, we first train standard 8B pa-\nrameter transformer LLMs over a relatively short\n1T token horizon. The results are shown in Ta-\nble 5. Our high quality dataset (Nemotron-CC-HQ)\n16https://github.com/EleutherAI/lm-evaluation-\nharness\nshows accuracy gains over DCLM and FineWeb-\nEdu on all tasks except RACE. In particular, there\nis a 5.6 MMLU and 3.1 average gain over DCLM.\nThis shows the effectiveness of our classifier en-\nsembling and synthetic data even in the non-data-\nconstrained setting.\nOur complete 6.3T token\ndataset (Nemotron-CC) gives MMLU and average\naccuracies roughly on par with DCLM. But since\nthis dataset contains 4× more unique real tokens,\nwe expect it to be superior in data-constrained set-\ntings like 15T token training runs.\nLong Token Horizon (15T)\nOur dataset con-\ntributed 7.2T of the tokens used to train an 8B\nmodel for 15T tokens. As shown in Table 6, our\nmodel achieves a higher average accuracy than\nLlama 3.1 8B, which was also trained for 15T\ntokens, including an MMLU score of 70.3 vs.\nLlama’s 65.3. This shows that our dataset is in-\ndeed suitable for state-of-the-art training over long\ntoken horizons. For more details on this experi-\nment, please see Appendix E.\n3.3\nAblation Study\nTo further investigate the contribution and effect\nof each module in our method, we conducted thor-\nough ablation studies.\nExtractor & Filter Comparison\nAs we have dis-\ncussed in Section 2.1, by deploying Justext instead\nof Trafilatura and removing filter from the post-\nprocessing step, we can attain significantly 57.4%\nmore high-quality tokens. We also conduct abla-\ntion studies to better understand the impact of the\nextractor selection and the removal of filter through\n\ndownstream benchmarks. We carry out four 8B-1T\nexperiments. We report the benchmark scores in\nTable 7. Beyond the token-yield benefit by lever-\naging Justext instead of Trafilatura and not using\nheuristic filters, we see that combining these two\ndoes not impact the downstream task accuracies\nwith only marginal differences (comparing Trafi-\nlatura filtered vs. Justext unfiltered). Moreover,\nwhen we ONLY remove filter from high-quality\ntokens, the results get further improved (comparing\nJustext unfiltered vs. Justext HQ unfiltered). In\nparticular, MMLU gets boosted by +2%. Note that,\nthe motivation behind removing filter is to boost\ntoken yield, especially on high-quality tokens due\nto the notable scarcity of such. Given the experi-\nmental results and considering the overall growth\nin token yield, we opt to only remove filter from\nhigh-quality tokens.\nExp name\nMMLU\nAvg (non-MMLU)\nTrafilatura filtered\n55.4\n60.6\nJustext filtered\n54.1\n60.9\nJustext unfiltered\n55.5\n60.3\nJustext HQ-unfiltered\n57.5\n60.6\nTable 7: Ablation studies on extractor and filter. HQ\nmeans high-quality data judged by FineWeb-Edu clas-\nsifier (score 3,4,5). HQ-unfiltered means filtering is\napplied only to LQ data. See Appendix G for more\ndetails.\nClassifiers Comparison\nAssembling different\nclassifiers to label the document quality is one of\nthe key steps in constructing our datasets, so we\ndid thorough analysis and comparison of the com-\nponent.\nWe did a detailed comparison of two types of\nclassifiers that we employ in our method: the\nFineWeb-Edu classifier which score document\nquality based on their educational-level, and the\nDCLM-based classifier which value the informa-\ntiveness of the document. We compare the high-\nquality documents predicted by the two classifiers\non a randomly selected Common Crawl Snapshot\n(CC-MAIN-2021-21). Table 8 shows the document\nstatistics comparison. We can see that only 10% of\nthe documents are predicted as high quality by both\nclassifiers, while 35.4% documents are predicted\nas high quality by FineWeb-Edu classifier only, and\n54.4% of documents are predicted as high-quality\nby DCLM classifier. Therefore, ensembling differ-\nent classifiers can increase the recall of high-quality\ndocuments from Common Crawl.17\nWe further compare each of the classifiers with\nthe ensembled method18 by their downstream\ntasks’ performances. We pretrain 8B parameters\nLLMs with 1T tokens, using the high-quality docu-\nments labeled by different classifiers on randomly\nselected 13 Common Crawl snapshots (see Ap-\npendix F). Table 9 shows the detailed comparison\non different evaluation tasks. We can see that the\nensembled method greatly boost the high-quality\ntokens percentage from 9% to 25%, while still\nachieving the highest general language understand-\ning performance on average on all the tasks. The\nensembled method also outperforms the FineWeb-\nEdu classifier and the DCLM classifier, in terms\nof the high-quality token percentage, and is on-par\nor slightly better on the 9 evaluation tasks. This\nis very important since more unique high-quality\ntokens is the key in pretraining larger LLMs on\nlonger tokens horizons.\nWhat\n#Docs\nTotal unique(%)\nTotal unique in union\n11,359,655\n100.0%\nIn intersection\n1,152,821\n10.1%\nIn FineWeb-Edu only\n4,022,294\n35.4%\nIn DCLM only\n6,184,540\n54.4%\nTable 8: High-quality documents overlap analysis.\nEvaluating Synthetic Data\nAs Table 10 shows,\nthis ablation study aim to answer two questions: (1)\nDoes rephrasing low-quality improve accuracies on\ndownstream tasks? (2) Can synthetic data help off-\nset the decreasing value of duplicated data reported\nin (Muennighoff et al., 2024)? To answer these\nquestions, we train four 8B models with the same\nhyperparameters on different blends of 1T tokens:\n(1) LQ-Base: original Common Crawl data in-\ncluding low-quality documents; (2) LQ-Synthetic:\nan augmented version of LQ-Base where the low-\nquality documents are rephrased; (3) HQ-Base: a\nblend containing eightfold high-quality documents\nand less low- and medium-quality documents; (4)\nHQ-Synthetic: a variant of HQ-Base where 4 repe-\ntitions of the high-quality documents are swapped\nout for synthetic datasets.\nBy comparing the results between LQ-Base and\nLQ-Synthetic, we can see that rephrasing low-\n17Detailed URL domain comparison can be found in Ap-\npendix B\n18Note that we did not employ FineWeb-Edu classifier in\nour ensemble for license issue, since it is trained with annota-\ntions from Llama3.\n\nClassifier\nHQ(%) ARC-E ARC-C\nH\nW\nRACE PIQA SIQA CSQA OBQA MMLU Avg\nFineWeb-Edu\n8%\n77.7\n50.1\n74.9 67.3\n39.5\n78.8\n45.8\n53.6\n43.0\n55.4\n59.0\nDCLM\n11%\n76.0\n49.2\n76.5 70.2\n38.2\n80.8\n33.9\n55.2\n45.8\n56.0\n58.4\nOurs-mistral\n9%\n75.8\n49.2\n75.9 66.9\n37.5\n80.1\n46.2\n46.9\n44.8\n53.2\n58.1\nOurs-nemotron-340B\n14%\n76.3\n50.3\n75.6 67.5\n37.8\n80.2\n34.3\n54.0\n46.2\n54.9\n58.0\nOurs-ensembled\n25%\n78.0\n49.7\n75.3 67.1\n37.2\n79.6\n45.7\n56.8\n44.8\n56.4\n59.4\nTable 9: Different classifiers comparison. Our ensemble method includes the three classifiers: Ours-mistral, Ours-\nnemotron-340B and DCLM.\nBlend\nARC-E\nARC-C\nH\nW\nRACE\nPIQA\nSIQA\nCSQA\nOBQA\nMMLU\nAvg\nLQ-Base\n67.7\n41.8\n75.2\n67.1\n37.4\n78.8\n45.3\n36.9\n41.0\n48.2\n52.5\nLQ-Synthetic\n71.3\n45.2\n75.0\n66.9\n37.4\n79.4\n46.2\n41.6\n42.8\n47.1\n54.0\nHQ-Base\n74.2\n47.7\n74.8\n66.9\n37.3\n78.2\n46.0\n47.3\n43.6\n53.4\n55.8\nHQ-Synthetic\n76.7\n49.2\n74.5\n67.3\n38.2\n78.8\n45.2\n47.9\n45.8\n53.6\n56.7\nTable 10: Impact of incorporating synthetic data.\nquality data leads to 1.50 absolute gains on average\nscore. We also observe noticeable boosts from\n1.80% to 4.75% on ARC-Easy, ARC-Challenge,\nOpenbookQA, CommonsenseQA; however, we\nalso encounter slight accuracy drops on some tasks,\nwhich may indicate potential misinformation intro-\nduced by data synthesis. Current practices typically\nutilize data curation approaches to detect and elim-\ninate noisy examples. Due to time and resource\nconstraints, we leave the detailed exploration of\nthis issue for future efforts.\nThe comparison between HQ-Base and HQ-\nSynthetic shows that swapping 4 out of 8 epochs of\nhigh-quality data with a mix of synthetic datasets\nimproves accuracy on most benchmarks. This im-\nprovement could potentially result from two fac-\ntors: the incorporation of fresh unique tokens and\nstyles that enable the model to learn specific abili-\nties (e.g., question answering) or absorb knowledge\nmore efficiently.\n4\nRelated Work\nThe Phi series of models pioneered training on\nsmall amounts of very high quality data, includ-\ning curated Web and synthetic data (Gunasekar\net al., 2023; Li et al., 2023; Abdin et al., 2024).\nHowever, their focus is on shorter token hori-\nzons and they share limited details.\nFineWeb-\nEdu and DCLM are the main points of compar-\nison for our paper (Li et al., 2024; Penedo et al.,\n2024). We build upon their core idea of model-\nbased filtering, but show how to improve the fil-\ntering and data quantity through a combination of\nother techniques. Other English Common Crawl\ndatasets such as C4, DOLMA, Gopher, Refined-\nWeb, TxT360 largely focus on extraction and non-\nlearned heuristics (Penedo et al., 2023; Soldaini\net al., 2024; Rae et al., 2021; Raffel et al., 2020;\nTang et al., 2024). Just as for FineWeb-Edu and\nDCLM, the core pipeline we started from incorpo-\nrates many of these ideas, but our paper describes\nhow to modify and go beyond these non-learned\ntechniques to achieve state-of-the-art accuracy and\ndiversity. Concurrent work Zyda-2 shows how to\nfilter, cross-deduplicate, and combine the FineWeb-\nEdu, DCLM, Zyda-1, and Dolma-CC datasets\ninto a higher-accuracy and larger whole (Tokpanov\net al., 2024). In contrast, we focus on techniques\nfor the creation of a new English Common Crawl\ndataset rather than combinations or modifications\nof existing datasets. Finally, many works have fo-\ncused on creating multilingual datasets (Xue et al.,\n2021; Brack et al., 2024; Abadji et al., 2022; Wen-\nzek et al., 2020; Kudugunta et al., 2023). We leave\nextension of our ideas beyond English to the future.\nSynthetic datasets have been widely used in\nlanguage model pre-training and post-training.\nIn (Cheng et al., 2024), instruction-response pairs\nare synthesized for pre-training. In (Eldan and\nLi, 2023), the authors show that smaller or sim-\npler models trained on a synthetic dataset of short\nstories are capable of generating fluent and con-\nsistent stories. Similarly, smaller models trained\nusing high-quality synthetic textbook and exercise\ndatasets can achieve impressive high accuracy on\ncoding benchmarks (Gunasekar et al., 2023; Li\net al., 2023). These approaches typically require\na powerful language model, such as GPT-3.5 and\nGPT-4 in (Eldan and Li, 2023), to synthesize new\ncontents. Instead, (Maini et al., 2024) shows that\n\ncompact models such as Qwen-1.8B and Mistral-\n7B are adequate to rephrase web data. This ap-\nproach generates diverse, high-quality synthetic\ndata that effectively lowers model perplexity and\nboosts performance across benchmarks. We adopt\nthis main idea, but explore more prompts and show\nhow to specialize them for low and high quality\ndata.\n5\nConclusion\nFor producing long-horizon pretraining tokens for\nLLMs from English Common Crawl data, we\nshowed how to improve upon the state of the art\nand achieve better trade-offs between benchmark\naccuracy and data quantity, as measured by number\nof unique real tokens. Specifically, we showed the\nefficacy of ensembling model-based quality filters,\nrephrasing low and high quality documents, and re-\nducing the reliance on non-learned heuristics. The\ndataset is public and split by quality level and type\n(actual data vs. different types of synthetic data),\nenabling the community to do further experiments\non quality vs. diversity and how to build effective\nshort and long horizon curricula.\n6\nLimitations\nSome of the key limitations of our work are as\nfollows. For the model-based filter ensembling\nand quality bucketing, we only had time and re-\nsources to try a single strategy. Though it is ef-\nfective, it is possible this could be improved upon\nin future work, especially to improve the sensitiv-\nity at the higher-quality end of the spectrum. For\nthe rephrased data, we did not verify the factual\naccuracy or fidelity to the original contents. More\nwork is required to understand the risks of halluci-\nnations or loss of content diversity in this setting\nand how to mitigate them. We also only looked at\nrephrasing low and high quality data. It could be\ninteresting to explore how to best rephrase medium\nquality data as well. We did not do ablations on\nall parts of the pipeline. There is probably room\nfor improvement with, for example, the language\nidentification. Overall, we tried our methods only\non English text. More work is needed to adapt our\nmethods to other languages.\nFinally, we did not decontaminate the dataset, as\nthere is not yet a strong consensus on how to best do\nthis and the impact is uncertain and debated, espe-\ncially for large models trained over large token hori-\nzons. We note that the datasets we compare against\n(FineWeb-Edu, DCLM) were released without de-\ncontamination, and the model we compare against\n(Meta Llama 3.1) was also trained on contaminated\ndata. DCLM reports some contamination analy-\nsis, but the findings suggest contamination is not\na key factor: e.g., MMLU actually increases af-\nter decontamination, and DCLM does better than\nFineWeb on MMLU, even though FineWeb has\nmore MMLU contamination (see Section 4.6 and\nAppendix N in Li et al. (2024)). Still, it would be\ninteresting to better understand the impact of con-\ntamination for different model sizes and different\ntoken horizons, and we hope the community can\nexplore such questions on this public dataset.\nAcknowledgments\nWe thank the Common Crawl Foundation for host-\ning the dataset.\nWe thank Pedro Ortiz Suarez\nfor valuable feedback that improved the paper\nand Greg Lindahl for help with improving the\ndata formatting and layout.\nFurthermore, we\nthank Ayush Dattagupta, Randy Gelhausen, Smita\nIthape, Vibhu Jawa, Nirmal Kumar Juluru, Vineeth\nKalluru, Mehran Maghoumi, Arham Mehta, Ran-\njit Rajan, Janaki Vamaraju, Ryan Wolf, and Sarah\nYurick for help with NeMo Curator and for open\nsourcing Nemotron-CC recipes and classifiers as\npart of NeMo Curator.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4344–4355, Marseille, France.\nEuropean Language Resources Association.\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\nrat Behl, et al. 2024. Phi-3 technical report: A highly\ncapable language model locally on your phone. arXiv\npreprint arXiv:2404.14219.\nBo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh,\nPallab Bhattacharya, Annika Brundyn, Jared Casper,\nBryan Catanzaro, Sharon Clay, Jonathan Cohen, et al.\n2024.\nNemotron-4 340b technical report.\narXiv\npreprint arXiv:2406.11704.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebron, and Sumit Sanghai.\n2023. Gqa: Training generalized multi-query trans-\nformer models from multi-head checkpoints. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\n\nods in Natural Language Processing, pages 4895–\n4901.\nAdrien Barbaresi. 2021.\nTrafilatura: A Web Scrap-\ning Library and Command-Line Tool for Text Dis-\ncovery and Extraction. In Proceedings of the Joint\nConference of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations, pages 122–131.\nAssociation for Computational Linguistics.\nLoubna Ben Allal. 2024.\nMost of the data is du-\nplicated?\nhttps://huggingface.co/datasets/\nHuggingFaceFW/fineweb-edu/discussions/7.\nAccessed: October 24, 2024.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nManuel Brack, Malte Ostendorff, Pedro Ortiz Suarez,\nJosé Javier Saiz, Iñaki Lacunza Castilla, Jorge\nPalomar-Giner,\nAlexander\nShvets,\nPatrick\nSchramowski,\nGeorg\nRehm,\nMarta\nVillegas,\nand Kristian Kersting. 2024. Community OSCAR:\nA community effort for multilingual web data. In\nProceedings of the Fourth Workshop on Multilingual\nRepresentation\nLearning\n(MRL\n2024),\npages\n232–235, Miami, Florida, USA. Association for\nComputational Linguistics.\nDaixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi,\nMinlie Huang, and Furu Wei. 2024. Instruction pre-\ntraining: Language models are supervised multitask\nlearners. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2529–2550, Miami, Florida, USA. Association\nfor Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nRonen Eldan and Yuanzhi Li. 2023. Tinystories: How\nsmall can language models be and still speak coherent\nenglish? arXiv preprint arXiv:2305.07759.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558–3567.\nSteven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan\nSu, Mostofa Patwary, Mohammad Shoeybi, and\nBryan Catanzaro. 2024. Maximize your data’s po-\ntential: Enhancing llm accuracy with two-phase pre-\ntraining. Preprint, arXiv:2412.15285.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,\nKyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, An-\nish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n2023. A framework for few-shot language model\nevaluation.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, et al. 2023. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644.\nKenneth Heafield. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187–197.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hérve Jégou, and Tomas Mikolov.\n2016. Fasttext.zip: Compressing text classification\nmodels. arXiv preprint arXiv:1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efficient\ntext classification. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431, Valencia, Spain. Association\nfor Computational Linguistics.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, and Orhan Firat. 2023. Madlad-400:\nA multilingual and document-level large audited\ndataset. Advances in Neural Information Process-\ning Systems, 36:67284–67296.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445.\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi,\nMatt Jordan, Samir Gadre, Hritik Bansal, Etash Guha,\nSedrick Keh, Kushal Arora, et al. 2024. Datacomp-\nLM: In search of the next generation of training sets\nfor language models. In The Thirty-eight Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track.\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie\nDel Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.\nTextbooks are all you need ii: phi-1.5 technical report.\narXiv preprint arXiv:2309.05463.\nPratyush Maini, Skyler Seto, He Bai, David Grangier,\nYizhe Zhang, and Navdeep Jaitly. 2024. Rephras-\ning the web: A recipe for compute and data-efficient\nlanguage modeling. In ICLR 2024 Workshop on Navi-\ngating and Addressing Data Problems for Foundation\nModels.\nLuke Merrick, Danmei Xu, Gaurav Nuti, and Daniel\nCampos. 2024. Arctic-embed: Scalable, efficient,\nand accurate text embedding models. arXiv preprint\narXiv:2405.05374.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391.\nNiklas Muennighoff, Alexander Rush, Boaz Barak,\nTeven Le Scao, Nouamane Tazi, Aleksandra Piktus,\nSampo Pyysalo, Thomas Wolf, and Colin A Raffel.\n2024.\nScaling data-constrained language models.\nAdvances in Neural Information Processing Systems,\n36.\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings,\nBo Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro.\n2024. Data, data everywhere: A guide for pretraining\ndataset construction. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 10671–10695.\nGuilherme Penedo, Hynek Kydlíˇcek, Loubna Ben al-\nlal, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\nLeandro Von Werra, and Thomas Wolf. 2024. The\nfineweb datasets: Decanting the web for the finest\ntext data at scale. In The Thirty-eight Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Hamza Alobeidli, Alessandro\nCappelli, Baptiste Pannier, Ebtesam Almazrouei, and\nJulien Launay. 2023. The refinedweb dataset for fal-\ncon llm: Outperforming curated corpora with web\ndata only. Advances in Neural Information Process-\ning Systems, 36:79155–79172.\nJan Pomikálek. 2011. Removing boilerplate and du-\nplicate content from web corpora. Disertacnı práce,\nMasarykova univerzita, Fakulta informatiky.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research,\n21(140):1–67.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social iqa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 4463–4473.\nNoam Shazeer. 2020. Glu variants improve transformer.\narXiv preprint arXiv:2002.05202.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin\nSchwenk, David Atkinson, Russell Authur, Ben Bo-\ngin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,\net al. 2024. Dolma: an open corpus of three trillion\ntokens for language model pretraining research. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 15725–15788.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge.\nIn Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158.\nLiping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi\nLiang, Zhen Wang, Li An, Bhaskar Rao, Ling-\nhao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun,\nCun Mu, Victor Miller, Xuezhe Ma, Yue Peng,\nZhengzhong Liu, and Eric P. Xing. 2024. Txt360:\nA top-quality llm pre-training dataset requires the\nperfect blend. https://huggingface.co/spaces/\nLLM360/TxT360. Accessed: October 24, 2024.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\n\nShahriari, Alexandre Ramé, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\narXiv preprint arXiv:2408.00118.\nTeknium. 2023.\nOpenhermes 2.5:\nAn open\ndataset of synthetic data for generalist llm as-\nsistants.\nhttps://huggingface.co/datasets/\nteknium/OpenHermes-2.5. Accessed: October 24,\n2024.\nYury Tokpanov, Paolo Glorioso, Quentin Anthony, and\nBeren Millidge. 2024. Zyda-2: a 5 trillion token\nhigh-quality dataset. Preprint, arXiv:2411.06068.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4791–4800.\nA\nPipeline Overview\nAn overview of the pipeline is shown in Figure 3.\nB\nComparison of FineWeb-Edu and\nDCLM Classifier\nDifferent classifiers have different standards for\nhigh-quality documents. Thus, ensemble multi-\nple classifiers will help increase the recall of high-\nquality documents. We did a detailed compari-\nson of two of the classifiers that we employ in our\nmethod: the FineWeb-Edu classifier which score\ndocument quality based on their educational-level,\nand the DCLM based classifier which value the\ninformativeness of the document.\nWe compare the high-quality documents pre-\ndicted by the two classifiers on one Common Crawl\nsnapshot (dated 2021-21). Table 8 show the doc-\nument statistics comparison. We further show the\ndetailed URL domains comparison between the\ntwo classifiers’ predictions in Table 11. We can\nsee that each classifier has their own high-quality\ndomain preferences. Among the top 1k domains,\nonly 368 domains are in the intersection. Therefore,\nensemble of different classifiers can help increase\nretrieving more high-quality documents from Com-\nmon Crawl.\nC\nBucket Comparison\nTo better understand the quality of data in each of\nour 20 data buckets, we carry out ablation studies to\ntest their benchmark accuracies. For each study, we\ntake a 900B-token checkpoint and continue the pre-\ntraining for 50B more tokens. For 34% of the 50B\ntokens we used the bucket data being tested, while\nwe fixed the other 66% as the same data distribution\nof the 900B pretraining process to make sure the\ndistribution did not shift too much. See Figure 4\nfor the results. The average accuracy is calculated\nacross 13 downstream tasks. Note that Bucket 19\ngreatly outperforms all other buckets and the differ-\nences within bucket 12-18 are marginal. We used\nthe results here as a reference when designing the\nquality labels in Table 2.\nD\nTraining Details: Ablations\nAs mentioned in Section 3.1, we use the open\nsource Megatron-LM library19 (Shoeybi et al.,\n2019) to train 8B parameter transformer LLMs\nfor 1T tokens.\nThe key hyperparameters are\n19https://github.com/NVIDIA/Megatron-LM\n\nCommon Crawl data\nText extraction\nLanguage\nidentification\nDeduplication\nEnsemble-\nof-classifier-\nbased filtering\nBucketing\nEnglish\nMedium quality data\nHeuristic filtering\nFinal dataset\nLow quality data\nHeuristic filtering\nSDG (Wikipedia-\nstyle rephrasing)\nHigh quality data\nSDG (Diverse\nQA, Distillation,\nKnowledge\nextraction,\nKnowledge listing,\nWikipedia-style\nrephrasing)\nFigure 3: Pipeline overview\n\nTop domains and domain overlap analysis =>368 domains are in top 1k domains of both\nTop 1k Domains\nFineWeb-Edu\nTop Domains\nCount\nDCLM\nTop Domains\nCount\nIntersection (368)\nIn FineWeb-Edu only\nIn DCLM only\nwordpress.com\n39228\nwordpress.com\n85378\n123helpme.com\n111papers.com\n4archive.org\nthefreedictionary.com\n20420\nstackexchange.com\n64831\n24houranswers.com\n3dprint.com\n4channel.org\nstackexchange.com\n17853\nlivejournal.com\n36521\nabc.net.au\naafp.org\n4hw.com.cn\nbritannica.com\n14761\nmedium.com\n27347\nabovetopsecret.com\naappublications.org\n5winebar.com\nipl.org\n13132\nfandom.com\n13986\nacademickids.com\nabs.gov.au\naawsat.com\nmedium.com\n11539\nipl.org\n12282\nadafruit.com\naccessgenealogy.com\nabc11.com\nnih.gov\n10624\nanswers.com\n10790\nadobe.com\nachrnews.com\nabc30.com\nigi-global.com\n9136\nnih.gov\n9091\nalchetron.com\nacm.org\nabc7chicago.com\nslideplayer.com\n8460\ntypepad.com\n8078\naljazeera.com\nadidasshoesoutletwholesale.com\nable2know.org\nanswers.com\n8103\ncommonsensemedia.org\n7772\nallegancountyedc.com\nadslspeedtest.net\naceshowbiz.com\nwikipedia.org\n6867\nwsj.com\n7652\nallinterview.com\naero-net.org\nactiverain.com\ndictionary.com\n6763\nimdb.com\n7263\namazon.com\nagwired.com\naddicted2success.com\nen-academic.com\n5292\ntheatlantic.com\n7008\namericanbar.org\nahdictionary.com\nadditudemag.com\nsciencemag.org\n5254\nyahoo.com\n5921\nangelfire.com\najol.info\nagingcare.com\nbrainscape.com\n5129\nfanfiction.net\n5499\nanswers.com\nakjournals.com\nagnostic.com\nencyclopedia.com\n4698\nhuffpost.com\n5471\nantiessays.com\naleteia.org\nairmilescalculator.com\nnasa.gov\n4615\nadobe.com\n5182\napple.com\nalison.com\nairportia.com\nslideserve.com\n4538\nscribd.com\n4948\narchive.org\nall-creatures.org\nalarabiya.net\nscribd.com\n4430\nthefreedictionary.com\n4847\narduino.cc\nallaboutheaven.org\nalex-in-wonderland.com\nkiddle.co\n4323\nmathworks.com\n4655\narstechnica.com\nallthatsinteresting.com\nalexa-gueguen.com\nTable 11: High Quality Documents Domains Comparison. 368 Top Domains are in the intersection.\n12\n13\n14\n15\n16\n17\n18\n19\nBucket index\n47.8\n48.0\n48.2\n48.4\n48.6\n48.8\nAverage Acc\nFigure 4: Ablation study on the buckets.\nCategory\nBlend %\nEnglish Common Crawl\n73\nBooks and patents\n9\nPapers\n9\nCode\n5\nConversational\n3\nWikipedia\n1\nTable 12: Data blend for the experiments with 8B pa-\nrameter transformer LLMs trained for 1T tokens. Ex-\nperiments in this paper varied only the 73% English\nCommon Crawl portion.\nas follows: We use 32 transformer layers with\nhidden dimension 4096, 32 attention heads, and\nSwiGLU activations (Shazeer, 2020). For the atten-\ntion, we use grouped query attention with 8 query\ngroups (Ainslie et al., 2023). We use the Adam\noptimizer with β1 = 0.9, β2 = 0.95, ϵ = 1e−8,\nweight decay 0.1, and the cosine learning rate\nschedule with peak learning rate at 3e-4 and min-\nimum learning rate at 3e-6. A single training run\ntakes about 40 hours using 1024 NVIDIA H100\nGPUs.\nThe data blend breakdown for these experiments\nis shown in Table 12. Experiments in this paper\nvaried only the 73% English Common Crawl por-\ntion.\nE\nLong-Horizon Curriculum Details\nFor the 15T token training run, a two-phase curricu-\nlum was employed that is described in more detail\nin Feng et al. (2024). The first phase of 9T tokens\nused 59% English Common Crawl data (5.31T) and\nthe second phase of 6T tokens used 31% (1.86T),\nfor a combined total of 47.8% (7.17T). In the first\nphase, we used medium, medium-high, and high\nquality data (real and synthetic), and in the sec-\nond phase we used only high quality data (real and\nsynthetic).\nF\nCommon Crawl Snapshots\nFor the main datasets, we used the 99 snapshots\nCC-MAIN-2013-20 through CC-MAIN-2024-30.\nThe thirteen Common Crawl snapshots we use\nin some of the analysis and 1T token experiments\n\nare CC-MAIN-2023-23, CC-MAIN-2023-14, CC-\nMAIN-2023-06, CC-MAIN-2022-49, CC-MAIN-\n2022-27, CC-MAIN-2022-05, CC-MAIN-2021-\n43, CC-MAIN-2021-21, CC-MAIN-2021-04, CC-\nMAIN-2020-45, CC-MAIN-2020-29, CC-MAIN-\n2020-05, CC-MAIN-2019-35.\nG\nExtractor & Filter Ablation\nThe Avg tasks include ARC-Easy, ARC-Challenge,\nHellaswag, Winogrande, RACE, PIQA, Common-\nsense QA, Openbook QA.\nNote that we only use FineWeb-Edu classifier\nfor the quality labels of this ablation study and\nanalysis. We do not use it in the final preparation\nof our dataset. See Section 2.2 for the details of\nour classifiers being used eventually to prepare the\ndata.\nH\nPrompt Templates\nPrompts 1-5 show the prompt templates we use for\nsynthetic data generation.\n\nTask: Read the text, ask questions and answer them.\nFollow these instructions:\n1. Ask diverse questions that require different cognitive skills or cover different aspects of the\ntext.\n2. Ask questions in various forms such as:\n- Yes/No questions that require determining whether a statement is true or false.\n- Open-ended questions that begin with words like what, how, when, where, why and who.\n- Multi-choice questions that offers two or more options to choose from. Include the options in the\nquestion.\n- Comparison questions that compare two quantities or objects and determine the relationship\nbetween them.\n- Reading comprehension questions that test the ability to understand and analyze the text.\n- Problem-solving questions that test the ability to solve mathematical, physical, or logical\nproblems.\n3. Focus on asking questions about factual information, important knowledge, or concrete details in\nthe text.\n4. Write questions and answers using clear and concise language.\n5. Use plain text. Do not use Markdown.\n6. Each question and answer pair should be on a separate line. Tag the question with \"Question:\" and\nthe answer with \"Answer:\".\nText:\n[DOCUMENT SEGMENT]\nTask:\nAfter reading the above text, ask up to 8 questions and provide the correct answers following the\ninstructions. Give your response in this format:\nHere are the questions and answers based on the provided text:\n- Question: [first question] Answer: [first answer]\n- Question: [second question] Answer: [second answer]\n....\nPrompt 1: Prompt template: Diverse QA pairs\nYour task is to read and paraphrase the provided text following these instructions:\n- Aim to create a condensed but accurate and informative version of the original text, not a\nsimplistic summary.\n- Capture and preserve the crucial information, key concepts, important values, and factual details\nin the original text, while making it more readable and accessible.\n- Retain technical terms, specialized vocabulary, and complex concepts.\n- Retain examples, explanations of reasoning processes, and supporting evidence to maintain the text'\ns depth and context.\n- Only include information that is present in the original text. Do not adding new or unsubstantiated\nclaims.\n- Write in plain text.\nHere is the text:\n[DOCUMENT SEGMENT]\nTask:\nAfter thoroughly reading the above text, paraphrase it in high-quality and clear English following\nthe instructions.\nPrompt 2: Prompt template: Distill.\n\nReview the text and extract the key information. Follow these instructions:\n- Carefully read the above text and provide a concise and organized list of factual information,\nconcrete details, key concepts, and important numbers and statistics extracted from the text.\n- Ensure each point is clear, specific, and supported by the original text.\n- Ensure the extract text is information-dense and easier to learn from.\n- Do not add titles or headings.\nText:\n[DOCUMENT SEGMENT]\nTask:\nExtract the factual information, concrete details, and key concepts from the above text following the\ninstructions.\nPrompt 3: Prompt template: Knowledge list.\nYour task is to rewrite knowledge from the provided text following these instructions:\n- Rewrite the text as a passage or passages using easy-to-understand and high-quality English like\nsentences in textbooks and Wikipedia.\n- Focus on content in disciplines such as humanities, social sciences, natural sciences, technology,\nengineering, math, law and legal, business, management, art, education, agricultural sciences,\npolitics, and history.\n- Disregard content that does not contain useful facts or knowledge.\n- Retain examples, explanations of reasoning processes, and supporting evidence to maintain the text'\ns depth and context.\n- Do not add or alter details. Only restate what is already in the text.\n- Write in plain text.\n- Do not add titles, subtitles, note, or comment.\nText:\n[DOCUMENT SEGMENT]\nTask:\nRewrite facts and knowledge from the above text as a passage or passages following the instructions.\nPrompt 4: Prompt template: Extract knowledge.\nFor the following paragraph give me a diverse paraphrase of the same in high quality English language\nas in sentences on Wikipedia. Begin your answer on a separate line with \"Here is a paraphrased\nversion:\".\nText: [DOCUMENT SEGMENT]\nPrompt 5: Prompt template: Wikipedia-style rephrasing (Maini et al., 2024).\n"
    }
  ]
}