{
  "4-1 (Pre-training Data)": "Nemotron-Nano-12B-v2-Base (12 B parameters) is the foundation model that all later Nano variants inherit.  It is first pre-trained for a total horizon of 20 trillion tokens under an FP8 recipe.  The corpus is explicitly described as a \"large corpus of high-quality curated and synthetically-generated data\" that is blended through a three-phase curriculum: Phase 1 maximises source diversity, Phases 2–3 lean on high-quality sets such as Wikipedia, after which a dedicated \"Phase LC\" reallocates 20 % of Phase 3 weight to newly introduced long-context document-QA material (Phase 3 data weights are down-scaled to 80 %).  \n\nPublic release.  NVIDIA is releasing most of the material as Nemotron-Pre-Training-Dataset-v1 (> 6 trillion tokens).  Named components that feed this collection are:\n• Nemotron-CC-v2 – an evolution of Nemotron-CC with eight additional 2024–2025 Common Crawl snapshots; contains synthetic rephrasing, aggressive de-duplication, and synthetic Q&A translated into 15 languages.\n• Nemotron-CC-Math-v1 – a 133 B-token math-specialised set mined from Common Crawl, equations preserved and normalised to LaTeX.\n• Nemotron-Pretraining-Code-v1 – curated GitHub source-code references that have passed multi-stage filtering, licence checking and deduplication.\n\nEarlier internal predecessors (1.1 T and 3.5 T token mixtures, 70 % English / 15 % non-English / 15 % code) are also mentioned as precursors to Nemotron-4.  All together, the pre-training pipeline therefore mixes Common Crawl, Wikipedia-style encyclopaedic text, multilingual synthetic Q&A, LaTeX-formatted mathematics, and heavily filtered open-source code.",
  "4-2 (Fine-tuning Data)": "After backbone pre-training, \"Nemotron Nano 2\" is post-trained with a layered scheme that combines Supervised Fine-Tuning (SFT), Group Relative Policy Optimisation (GRPO), Direct Preference Optimisation (DPO) and Reinforcement Learning from Human Feedback (RLHF).  Multiple SFT passes are run: broad-domain SFT is followed by targeted SFT focusing on tool use, long-context behaviour and truncated-budget runs.  \n\nThe main supervised/feedback corpora come from the Nemotron-Post-Training-Dataset series.  • v1 already included math, science and coding dialogues generated with the open-weights DeepSeek-R1-0528 model (same prompt set as Nemotron-H-8B/47B).  • v2 extends the material and specifically broadens SFT and RL examples into five additional languages (Spanish, French, German, Italian, Japanese).\n\nTask-specific prompt/response sources that contribute to the SFT stages include xlam-function-calling-60k8, glaive-function-calling-v29 and NVIDIA-When2Call; replies for these single-turn prompts are produced with Qwen3-235B-A22B10.  \n\nSafety and persona conditioning are integral: harmful as well as benign prompts are sampled from Nemotron Content Safety Dataset V2, HarmfulTasks, RedTeam2K and gretel-v1, and every instance is paired with a random character from Nemotron-Personas to diversify style.  An accompanying GitHub LLaVA pipeline example explains how to package the pre-training and SFT datasets in Megatron-LM–compatible WebDataset shards.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning for Nano V2 is intertwined with its post-training cycle.  The model first undergoes Group Relative Policy Optimisation, then Direct Preference Optimisation, and is finally refined with full Reinforcement Learning from Human Feedback (RLHF).  Nemotron-Post-Training-Dataset-v2 supplies the key RL material and now contains RL dialogues in five added languages (Spanish, French, German, Italian, Japanese) aimed at improving reasoning, code‐generation and instruction following.  \n\nDuring RL, candidate checkpoints coming out of the long-context phase are evaluated in an environment where on-policy data are generated per WorkBench prompt: positives are successful tool-call completions, negatives are failed generations.  These positive/negative pairs drive the iterative DPO optimisation before the final HF-based updates.",
  "4-4 (Data Filtering)": "Filtering is pervasive and multi-layered.  \n\nCode.  Raw GitHub data are passed through Nemotron-Pretraining-Code-v1, which applies \"multi-stage filtering, deduplication and quality filters\".  A dedicated licence-detection pipeline—modeled on BigCode but with an even stricter allow-list—is run, and \"source code with a licence not in the following list\" is removed.  De-duplication is executed both exactly (hashing) and fuzzily (MinHash LSH), recognising that code files frequently appear verbatim across repositories.  \n\nWeb/Common Crawl.  Nemotron-CC-v2 adds eight 2024–2025 snapshots to the original pipeline.  Processing includes synthetic rephrasing, full de-duplication and automatic translation of Q&A data into 15 languages.  For the news subset (CC-NEWS) an explicit English-language filter is applied and the data are again \"globally fuzzily de-duplicated\"; no further heuristic filters are applied.  Research into Nemotron-CC highlights methods such as \"classifier ensembling, synthetic data rephrasing and reduced reliance on heuristic filters\" to balance quality and quantity, contrasting with recent datasets that discard \"90 % of data\" through aggressive model-based filtering.  \n\nSafety filtering.  When harmful-prompt data are produced, a two-step safety gate is used: first the model is prompted, then its outputs are passed through guard models that must accept the response.  \n\nOverall, the pipeline relies on explicit licence allow-lists, language filters, exact and fuzzy deduplication (hashing + MinHash LSH), classifier ensembles and guard-model checks to enforce quality and safety before data enter pre-training, SFT or RL stages.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing the majority of our pre-training dataset in the Nemotron-Pre-Training-Dataset-v1 collection of more than 6 trillion tokens:"
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM pipeline (Mahabadi et al., 2025). Preserves equations, standardizes to LaTeX, outperforms previous math datasets on benchmarks."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters. Includes code Q&A data in 11 programming languages."
    },
    {
      "source": "[pdf_text]",
      "quote": "To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data."
    },
    {
      "source": "[title]",
      "quote": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset"
    },
    {
      "source": "[pdf_text: An Empirical Study of Mamba-based Language Models]",
      "quote": "We train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English, and 15% code."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Additionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-Training-Dataset-v1 collection of more than 6 trillion tokens:"
    },
    {
      "source": "[pdf_text]",
      "quote": "We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. We proportionally downscaled the weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the newly added long-context document-QA data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math pretraining dataset, 2025."
    },
    {
      "source": "[pdf_text]",
      "quote": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a refined long-horizon pretraining dataset."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.02595]",
      "quote": "Title: Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.02595]",
      "quote": "Title: Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2406.07887]",
      "quote": "Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English, and 15% code."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Additionally, we are releasing an updated post-training dataset: • Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Math (Toshniwal et al., 2024; Moshkov et al., 2025), Science and Coding data, we generate responses using the open-weights DeepSeek-R1-0528 model ... The training data has been released as part of Nemotron-Post-Training-Dataset-v1."
    },
    {
      "source": "[pdf_text]",
      "quote": "For single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive-function-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using Qwen3-235B-A22B10."
    },
    {
      "source": "[pdf_text]",
      "quote": "Safety. We leveraged a mix of harmful and benign prompts drawn from the Nemotron Content Safety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo et al., 2024), and gretel-v1 (gre, 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Each instance is paired with a random persona from Nemotron-Personas11 to enrich diversity of queries."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Math (Toshniwal et al., 2024; Moshkov et al., 2025), Science and Coding (Ahmad et al., 2025b,a; Majumdar et al., 2024) data, we generate responses using the open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for training Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been released as part of Nemotron-Post-Training-Dataset-v17."
    },
    {
      "source": "[sections/https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/]",
      "quote": "For a complete multimodality reference pipeline with LLaVA, visit NVIDIA/Megatron-LM on GitHub. The LLaVA pipeline example walks you through how to: Prepare pretraining and supervised fine-tuning (SFT) datasets for Megatron webdataset-based formats."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese. The data supports improvements of math, code, general reasoning, and instruction following capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization. For each candidate checkpoint from the long-context stage, we generate on-policy data consisting of positive examples (successful tool calls) and negative examples (failed generations) for every WorkBench prompt."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF)."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages."
    },
    {
      "source": "[pdf_text]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. We performed license-based removal with a license detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but with fewer accepted licenses (see Appendix A for additional details). Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used the Nemotron-CC dataset (Su et al., 2025), but updated to include eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13) using the same pipeline. The CC-NEWS data was filtered for English and globally fuzzily de-duplicated; no other filtering was used."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "In line with previous models in the Nemotron family, we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. We performed license-based removal with a license detection pipeline similar to that used by the BigCode project, but with fewer accepted licenses. De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories; consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters."
    },
    {
      "source": "[sections/https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf]",
      "quote": "Nemotron-CC-v2: Follow-up to Nemotron-CC with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages."
    },
    {
      "source": "[pdf_text]",
      "quote": "Safety. We leveraged a mix of harmful and benign prompts drawn from the Nemotron Content Safety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo et al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To ensure safety, we applied a two-step approach: initial prompting followed by filtering with guard models to verify that outputs remained safe."
    },
    {
      "source": "[pdf_text]",
      "quote": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model We remove source code with a license not in the following list:"
    },
    {
      "source": "[pdf_text]",
      "quote": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a refined long-horizon pretraining dataset."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.02595]",
      "quote": "Title: Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.02595]",
      "quote": "Title: Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset. Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data."
    }
  ]
}