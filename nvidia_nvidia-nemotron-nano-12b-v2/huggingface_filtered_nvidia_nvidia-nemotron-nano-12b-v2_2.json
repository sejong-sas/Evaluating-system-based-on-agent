{
  "1-5 (Architecture)": "NVIDIA/Nemotron-Nano-12B-v2 is described as a hybrid “Nemotron-Hybrid / Mamba2-Transformer” network.  The checkpoint tagged “NVIDIA-Nemotron-Nano-12B-v2-Base” was first fine-tuned and later compressed down to a 9-billion-parameter sibling, but the 12 B-parameter variant itself keeps a mixed-layer design: most blocks are made up of Mamba-2 and MLP components, and only six layers employ classical Attention.  In configuration files it declares \"model_type\": \"nemotron_h\", and the reference implementation is the \"PyTorch NemotronH\" model class.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "The model was fine-tuned from [NVIDIA-Nemotron-Nano-12B-v2-Base](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base) was further compressed into [NVIDIA-Nemotron-Nano-9B-v2](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2). The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just six Attention layers."
    },
    {
      "source": "[readme]",
      "quote": "## Model Architecture\n\n- Architecture Type: Mamba2-Transformer Hybrid\n- Network Architecture: Nemotron-Hybrid"
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"nemotron_h\","
    },
    {
      "source": "[py_files/modeling_nemotron_h.py]",
      "quote": "\"\"\"PyTorch NemotronH model.\"\"\""
    },
    {
      "source": "[py_files/modeling_nemotron_h.py]",
      "quote": "_CHECKPOINT_FOR_DOC = \"nvidia/Nemotron-H-56B-Base-8K\""
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer can be obtained directly with\n   tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\")\nso it is hosted and downloadable from the Hugging Face repository under the exact model name.  After loading, the same object supports higher-level helpers such as tokenizer.apply_chat_template(...), showing that a ready-made chat template is included.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\")"
    },
    {
      "source": "[readme]",
      "quote": "tokenized_chat = tokenizer.apply_chat_template("
    }
  ],
  "2-1 (Hardware)": "Documentation lists official GPU support for NVIDIA A10G, NVIDIA H100-80GB, and NVIDIA A100 devices.  Example test runs were carried out on an A10G (24 GB) and an H100 (80 GB).  No additional compute-scale metrics are published in the excerpt, but these statements confirm that Nemotron-Nano-12B-v2 targets modern NVIDIA data-center GPUs across both Ampere and Hopper generations.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- Supported Hardware Microarchitecture Compatibility: NVIDIA A10G, NVIDIA H100-80GB, NVIDIA A100"
    },
    {
      "source": "[readme]",
      "quote": "- ## Test Hardware NVIDIA A10G 24GB, H100 80GB"
    }
  ],
  "2-2 (Software)": "Training was carried out with the NVIDIA Megatron-LM framework in combination with NeMo-RL.  At inference or deployment time the documented runtime engine is “NeMo 25.07.nemotron-nano-v2”.  The model is fully compatible with the Hugging Face Transformers library—examples were verified on version 4.48.3, and a config key records \"transformers_version\": \"4.51.3\".  Code snippets and the underlying class identify it as a \"PyTorch NemotronH\" implementation.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "The model was trained using [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) and [NeMo-RL](https://github.com/NVIDIA-NeMo/RL)."
    },
    {
      "source": "[readme]",
      "quote": "Runtime Engine(s): NeMo 25.07.nemotron-nano-v2"
    },
    {
      "source": "[readme]",
      "quote": "The snippet below shows how to use this model with Huggingface Transformers (tested on version 4.48.3)."
    },
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.51.3\""
    },
    {
      "source": "[py_files/modeling_nemotron_h.py]",
      "quote": "\"\"\"PyTorch NemotronH model.\"\"\""
    }
  ]
}