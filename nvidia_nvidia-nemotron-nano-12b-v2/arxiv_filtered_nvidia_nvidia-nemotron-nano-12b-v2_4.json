{
  "4-1 (Pre-training Data)": "The quotes describe a multi–trillion-token pre-training effort that is explicitly tied to the Nemotron line of models and, in particular, to the Nemotron-Nano-12B-v2 family.  Nemotron-Nano-12B-v2-Base was trained in FP8 over a 20-trillion-token horizon using a Warmup-Stable-Decay learning-rate schedule.  Pre-training data were collected through a curriculum of three successive data-blending phases that first emphasized diversity and later focused on very high-quality corpora such as Wikipedia, followed by a special long-context (Phase LC) extension to teach the model to handle extended sequences.  \n\nThe total corpus combines real-world and synthetic material at massive scale.  A key component is the Nemotron-CC collection, produced from 99 Common Crawl snapshots (CC-MAIN-2013-20 through CC-MAIN-2024-30).  After global and fuzzy de-duplication this yields 6.3 trillion tokens: 4.4 trillion “real” tokens and 1.9 trillion synthetically derived or re-phrased tokens, giving roughly four times more unique text than FineWebEdu-2 or DCLM.  A separate follow-up, Nemotron-CC-v2, extends the crawl period into 2024–2025 and adds synthetic Q&A data translated into 15 languages.  \n\nDomain-focused subsets were also constructed.  For mathematics, Nemotron-CC-Math-3+ contains 133 billion tokens, and an even stricter 52 billion-token Nemotron-CC-Math-4+ keeps only the top-scoring items as judged by a FineMath classifier.  For code, Nemotron-Pretraining-Code-v1 gathers large-scale GitHub material with multi-stage filtering, while Nemotron-Pretraining-SFT-v1 contributes synthetic instruction-following data covering STEM, multilingual, academic and reasoning topics.  All together, the publicly released Nemotron-Pre-Training-Dataset-v1 collection exceeds 6 trillion tokens.  \n\nThroughout these efforts, NVIDIA emphasizes high-quality curation: language filtering, HTML-to-text extraction, MinHash-based fuzzy de-duplication, exact hashing de-duplication for code, and heuristic/perplexity screens.  By mixing curated web, code, math, academic and synthetic instruction data, the team assembled the 20-trillion-token stream that ultimately shaped Nemotron-Nano-12B-v2-Base.",
  "4-2 (Fine-tuning Data)": "Fine-tuning (or post-training) for the Nemotron-Nano family combines very large supervised and synthetic instruction sets with multiple optimization paradigms.\n\nSynthetic SFT-style data are produced with several strong teacher LLMs (Qwen2.5, Mixtral-8×22B, Nemotron-4-340B, and DeepSeek-R1) and injected directly into the training mix.  In total 230 billion additional tokens are generated: 174 billion covering math, 35 billion covering code, and 21 billion general-knowledge tokens.  These examples improve instruction-following capacity even before explicit SFT.  \n\nFormal SFT then occurs in multiple waves.  One cited stage uses 6 million prompt-response samples spanning code, math and generic instructions to build Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct; a similar multistage recipe is applied to the 12B-parameter Nano line.  Subsequent targeted SFT concentrates on tool use, long-context behavior and truncated-budget scenarios.  Altogether, Nemotron Nano 2 post-training consumes roughly 90 billion tokens, most in single-turn prompt–response format enriched with intermediate reasoning traces.  Empirically, a blend of 70 % post-training Stage-2 data and 30 % raw pre-training data yields the best validation accuracy for the final Nano 2 models.  \n\nTo aid the community, NVIDIA released Nemotron-Post-Training-Dataset-v2, which extends earlier SFT and RL corpora to five additional languages (Spanish, French, German, Italian and Japanese).",
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning for Nemotron-Nano 2 builds on its SFT foundation and leverages several preference-optimization techniques.  The model is first post-trained with Supervised Fine-Tuning and then undergoes Group Relative Policy Optimization and Direct Preference Optimization.  Reinforcement Learning from Human Feedback is applied in later stages, following the classical RLHF methodology of Christiano et al. and Ouyang et al.  \n\nAdditional instruction-tuned variants (e.g., Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct) employ a two-stage schedule: SFT in Stage 1 and preference tuning via offline RPO in Stage 2.  Nano V2, specifically, iterates through Direct Preference Optimization loops to refine reward-aligned behavior, after which targeted RL and model merging help retain desirable qualities while enhancing robustness on long-context and intricate reasoning tasks.  \n\nThe Nemotron-Post-Training-Dataset-v2 release includes the RL preference data used here and broadens coverage into Spanish, French, German, Italian and Japanese, ensuring multilingual reward alignment.",
  "4-4 (Data Filtering)": "Data cleaning for the Nemotron family—especially Nemotron-Nano-12B-v2—relies on a well-defined, multi-stage pipeline with explicit quantitative objectives.\n\n1. Initial Crawl Processing:  HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication are applied to the raw Common Crawl snapshots.  These steps remove large-scale noise and exact duplicates before any quality scoring.  \n\n2. Quality Bucketing and Scoring:  For Nemotron-H and downstream Nano models, an ensemble of model-based classifiers assigns every document to one of five quality categories.  Documents in low, medium-low and medium buckets then pass through heuristic and perplexity filters so that only the cleaner portions are kept.  \n\n3. Math-Focused Filtering:  The Nemotron-CC-Math pipeline starts from 133 billion tokens (Nemotron-CC-Math-3+) and applies the FineMath classifier together with MinHash-based Locality-Sensitive Hashing inside the NeMo-Curator framework.  This prunes the set to a 52 billion-token Nemotron-CC-Math-4+ subset that retains only top-scoring mathematics content.  \n\n4. Source-Code De-duplication:  Because duplicate files are rampant on GitHub, Nemotron-Nano-12B-v2-Base employs both exact hashing and fuzzy MinHash-LSH de-duplication when building its large-scale code corpus.  \n\n5. Aggregate Result and Impact:  Applying these techniques across 99 Common Crawl snapshots (CC-MAIN-2013-20 → CC-MAIN-2024-30) yields Nemotron-CC—a 6.3 trillion-token dataset comprising 4.4 trillion globally de-duplicated “real” tokens and 1.9 trillion synthetic tokens.  Because FineWebEdu-2 and DCLM rely only on sharded approximate de-duplication, Nemotron-CC ends up with roughly four times more unique tokens.  The same filtering framework is reused in Nemotron-CC-v2, which adds eight more crawls and multilingual synthetic Q&A data.  \n\nTogether, the HTML-to-text conversion, language checks, ensemble quality classifiers, MinHash-LSH fuzzy de-duplication, exact hashing, FineMath classifier, heuristic rules and perplexity thresholds constitute the concrete, tool-driven filtering strategy that underpins the high-quality training data for Nemotron-Nano-12B-v2.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Code data. We pre-train Nemotron-H models with a considerable number of code tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). The resulting dataset consists of 6.3 trillion tokens, including 4.4 trillion globally de-duplicated “real” tokens and 1.9 trillion tokens of rephrased synthetic data."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "We used the Nemotron-CC dataset (Su et al., 2025), but updated to include eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13) using the same pipeline."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "This process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token subset, Nemotron-CC-Math-4+, containing only the top-scoring samples."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a refined long-horizon pretraining dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math pretraining dataset, 2025."
    },
    {
      "source": "[sections/Methods]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data. We have separate data curation pipelines for four broad data categories: general web crawl data, math data, code data, and “academic” data."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "For the benefit of the community, we have publicly released the dataset (as Nemotron-CC1) under the Common Crawl terms of use."
    },
    {
      "source": "[sections/Training Method and Data]",
      "quote": "1. VL pre-training. We train only the two-layer FFN for modality alignment while keeping both the Nemotron-H backbone and vision encoder frozen. For VL pre-training, we utilize a large and diverse image-text pre-training dataset from NVLM (Dai et al., 2024), including captioning (Lin et al., 2015; Sharma et al., 2018; Ordonez et al., 2011; Li et al., 2022), visual question answering (VQA) on natural image (Goyal et al., 2017; Krishna et al., 2017), visual chart (Kafle et al., 2018) and document understanding (Marafioti & Laurencon, 2024), optical character recognition (OCR) (Mishra et al., 2019) and scene-text recognition (Veit et al., 2016), and visual math reasoning (Lindström & Abraham, 2022) data."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Additionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-Training-Dataset-v1 collection of more than 6 trillion tokens: • Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages. • Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM pipeline (Mahabadi et al., 2025). • Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters. • Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual, academic, and reasoning domains."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "The dataset is available at https://data.commoncrawl.org/contrib/ Nemotron/Nemotron-CC/index.html."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over DCLM and FineWeb-Edu on all tasks except RACE."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with DCLM."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further add synthetic SFT-style data to the pre-training corpus; this improves the ability of base models to follow instructions. We use the Qwen2.5 series, Mixtral 8x22B, Nemotron-4-340B, and DeepSeek-R1 (only for 56B) models to produce these datasets. In total, we add 230 billion synthetic SFT-style tokens (174 billion math tokens, 35 billion code tokens, and 21 billion tokens with general knowledge) to the training corpus."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training. Overall, post-training was performed on roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning traces."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "SFT-style data. We further add synthetic SFT-style data to the pre-training corpus; this improves the ability of base models to follow instructions. We use the Qwen2.5 series, Mixtral 8x22B, Nemotron-4-340B, and DeepSeek-R1 (only for 56B) models to produce these datasets."
    },
    {
      "source": "[sections/Training Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage1, we perform supervised fine-tuning (SFT) on a blend of 6 million samples including code, math, and general instruction-following tasks."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[sections/4.3 Retraining with Distillation]",
      "quote": "Building on the candidate selection process described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below, to yield the final Nano 2 reasoning and base models. Dataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining (Section 2.2) data yields the highest accuracy (Table 11)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/Training Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage2, we switch to preference tuning using offline RPO (Sun et al., 2025) on general-domain prompts."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[sections/4.3 Retraining with Distillation]",
      "quote": "The reasoning model is distilled in stages with increasing sequence lengths to strengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement learning (RL), preference optimization and model merging to retain desired behaviors and ensure robustness across diverse tasks."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "For Nemotron-H, we made several key innovations in our processing of English Common Crawl data compared to Nemotron-4 (Parmar et al., 2024; NVIDIA, 2024); these innovations substantially improved data quality. As is common, we begin with HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication."
    },
    {
      "source": "[pdf_text]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). To retain as many high-quality tokens as possible, we apply heuristic and perplexity filters to the low, medium-low, and medium quality buckets."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "This process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token subset, Nemotron-CC-Math-4+, containing only the top-scoring samples. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed by fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk & Motwani, 1998) via the NeMo-Curator framework."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[sections/Methods]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens. This dataset has roughly 4× more unique tokens than FineWebEdu-2 and DCLM, since both of those datasets only underwent a sharded form of approximate deduplication and contain roughly 80% fuzzy duplicates"
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). As is common, we begin with HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "At this stage, however, we deviate from the norm and employ an ensemble of three model-based classifiers to bucket each document into five quality categories."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To retain as many high-quality tokens as possible, we apply heuristic and perplexity filters to the low, medium-low, and medium quality buckets."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code… De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    }
  ]
}