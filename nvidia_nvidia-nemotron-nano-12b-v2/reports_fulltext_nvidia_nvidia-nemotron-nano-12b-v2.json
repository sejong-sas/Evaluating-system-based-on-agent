{
  "model_id": "nvidia/nvidia-nemotron-nano-12b-v2",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2504.03624",
      "full_text": " [2504.03624] Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2504.03624 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2504.03624 (cs) [Submitted on 4 Apr 2025 ( v1 ), last revised 5 Sep 2025 (this version, v4)] Title: Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models Authors: NVIDIA : Aaron Blakeman , Aarti Basant , Abhinav Khattar , Adithya Renduchintala , Akhiad Bercovich , Aleksander Ficek , Alexis Bjorlin , Ali Taghibakhshi , Amala Sanjay Deshmukh , Ameya Sunil Mahabaleshwarkar , Andrew Tao , Anna Shors , Ashwath Aithal , Ashwin Poojary , Ayush Dattagupta , Balaram Buddharaju , Bobby Chen , Boris Ginsburg , Boxin Wang , Brandon Norick , Brian Butterfield , Bryan Catanzaro , Carlo del Mundo , Chengyu Dong , Christine Harvey , Christopher Parisien , Dan Su , Daniel Korzekwa , Danny Yin , Daria Gitman , David Mosallanezhad , Deepak Narayanan , Denys Fridman , Dima Rekesh , Ding Ma , Dmytro Pykhtar , Dong Ahn , Duncan Riach , Dusan Stosic , Eileen Long , Elad Segal , Ellie Evans , Eric Chung , Erick Galinkin , Evelina Bakhturina , Ewa Dobrowolska , Fei Jia , Fuxiao Liu , Gargi Prasad , Gerald Shen , Guilin Liu , Guo Chen , Haifeng Qian , Helen Ngo , Hongbin Liu , Hui Li , Igor Gitman , Ilia Karmanov , Ivan Moshkov , Izik Golan , Jan Kautz , Jane Polak Scowcroft , Jared Casper , Jarno Seppanen , Jason Lu , Jason Sewall , Jiaqi Zeng , Jiaxuan You , Jimmy Zhang , Jing Zhang , Jining Huang , Jinze Xue , Jocelyn Huang , Joey Conway , John Kamalu , Jon Barker , Jonathan Cohen , Joseph Jennings , Jupinder Parmar , Karan Sapra , Kari Briski , Kateryna Chumachenko , Katherine Luna , Keshav Santhanam , Kezhi Kong , Kirthi Sivamani , Krzysztof Pawelec , Kumar Anik , Kunlun Li , Lawrence McAfee , Leon Derczynski , Lindsey Pavao , Luis Vega , Lukas Voegtle , Maciej Bala , Maer Rodrigues de Melo , Makesh Narsimhan Sreedhar , Marcin Chochowski , Markus Kliegl , Marta Stepniewska-Dziubinska , Matthieu Le , Matvei Novikov , Mehrzad Samadi , Michael Andersch , Michael Evans , Miguel Martinez , Mike Chrzanowski , Mike Ranzinger , Mikolaj Blaz , Misha Smelyanskiy , Mohamed Fawzy , Mohammad Shoeybi , Mostofa Patwary , Nayeon Lee , Nima Tajbakhsh , Ning Xu , Oleg Rybakov , Oleksii Kuchaiev , Olivier Delalleau , Osvald Nitski , Parth Chadha , Pasha Shamis , Paulius Micikevicius , Pavlo Molchanov , Peter Dykas , Philipp Fischer , Pierre-Yves Aquilanti , Piotr Bialecki , Prasoon Varshney , Pritam Gundecha , Przemek Tredak , Rabeeh Karimi , Rahul Kandu , Ran El-Yaniv , Raviraj Joshi , Roger Waleffe , Ruoxi Zhang , Sabrina Kavanaugh , Sahil Jain , Samuel Kriman , Sangkug Lym , Sanjeev Satheesh , Saurav Muralidharan , Sean Narenthiran , Selvaraj Anandaraj , Seonmyeong Bak , Sergey Kashirsky , Seungju Han , Shantanu Acharya , Shaona Ghosh , Sharath Turuvekere Sreenivas , Sharon Clay , Shelby Thomas , Shrimai Prabhumoye , Shubham Pachori , Shubham Toshniwal , Shyamala Prayaga , Siddhartha Jain , Sirshak Das , Slawek Kierat , Somshubra Majumdar , Song Han , Soumye Singhal , Sriharsha Niverty , Stefania Alborghetti , Suseella Panguluri , Swetha Bhendigeri , Syeda Nahida Akter , Szymon Migacz , Tal Shiri , Terry Kong , Timo Roman , Tomer Ronen , Trisha Saar , Tugrul Konuk , Tuomas Rintamaki , Tyler Poon , Ushnish De , Vahid Noroozi , Varun Singh , Vijay Korthikanti , Vitaly Kurin , Wasi Uddin Ahmad , Wei Du , Wei Ping , Wenliang Dai , Wonmin Byeon , Xiaowei Ren , Yao Xu , Yejin Choi , Yian Zhang , Ying Lin , Yoshi Suhara , Zhiding Yu , Zhiqi Li , Zhiyu Li , Zhongbo Zhu , Zhuolin Yang , Zijia Chen et al. (100 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models, by NVIDIA: Aaron Blakeman and 198 other authors View PDF HTML (experimental) Abstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2504.03624 [cs.CL] &nbsp; (or arXiv:2504.03624v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2504.03624 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Deepak Narayanan [ view email ] [v1] Fri, 4 Apr 2025 17:41:58 UTC (716 KB) [v2] Thu, 10 Apr 2025 05:31:53 UTC (721 KB) [v3] Tue, 15 Apr 2025 14:36:01 UTC (716 KB) [v4] Fri, 5 Sep 2025 17:58:38 UTC (749 KB) Full-text links: Access Paper: View a PDF of the paper titled Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models, by NVIDIA: Aaron Blakeman and 198 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-04 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2508.14444",
      "full_text": " [2508.14444] NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2508.14444 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2508.14444 (cs) [Submitted on 20 Aug 2025 ( v1 ), last revised 2 Sep 2025 (this version, v4)] Title: NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Authors: NVIDIA : Aarti Basant , Abhijit Khairnar , Abhijit Paithankar , Abhinav Khattar , Adithya Renduchintala , Aditya Malte , Akhiad Bercovich , Akshay Hazare , Alejandra Rico , Aleksander Ficek , Alex Kondratenko , Alex Shaposhnikov , Alexander Bukharin , Ali Taghibakhshi , Amelia Barton , Ameya Sunil Mahabaleshwarkar , Amy Shen , Andrew Tao , Ann Guan , Anna Shors , Anubhav Mandarwal , Arham Mehta , Arun Venkatesan , Ashton Sharabiani , Ashwath Aithal , Ashwin Poojary , Ayush Dattagupta , Balaram Buddharaju , Banghua Zhu , Barnaby Simkin , Bilal Kartal , Bita Darvish Rouhani , Bobby Chen , Boris Ginsburg , Brandon Norick , Brian Yu , Bryan Catanzaro , Charles Wang , Charlie Truong , Chetan Mungekar , Chintan Patel , Chris Alexiuk , Christian Munley , Christopher Parisien , Dan Su , Daniel Afrimi , Daniel Korzekwa , Daniel Rohrer , Daria Gitman , David Mosallanezhad , Deepak Narayanan , Dima Rekesh , Dina Yared , Dmytro Pykhtar , Dong Ahn , Duncan Riach , Eileen Long , Elliott Ning , Eric Chung , Erick Galinkin , Evelina Bakhturina , Gargi Prasad , Gerald Shen , Haifeng Qian , Haim Elisha , Harsh Sharma , Hayley Ross , Helen Ngo , Herman Sahota , Hexin Wang , Hoo Chang Shin , Hua Huang , Iain Cunningham , Igor Gitman , Ivan Moshkov , Jaehun Jung , Jan Kautz , Jane Polak Scowcroft , Jared Casper , Jian Zhang , Jiaqi Zeng , Jimmy Zhang , Jinze Xue , Jocelyn Huang , Joey Conway , John Kamalu , Jonathan Cohen , Joseph Jennings , Julien Veron Vialard , Junkeun Yi , Jupinder Parmar , Kari Briski , Katherine Cheung , Katherine Luna , Keith Wyss , Keshav Santhanam , Kezhi Kong , Krzysztof Pawelec , Kumar Anik , Kunlun Li , Kushan Ahmadian , Lawrence McAfee , Laya Sleiman , Leon Derczynski , Luis Vega , Maer Rodrigues de Melo , Makesh Narsimhan Sreedhar , Marcin Chochowski , Mark Cai , Markus Kliegl , Marta Stepniewska-Dziubinska , Matvei Novikov , Mehrzad Samadi , Meredith Price , Meriem Boubdir , Michael Boone , Michael Evans , Michal Bien , Michal Zawalski , Miguel Martinez , Mike Chrzanowski , Mohammad Shoeybi , Mostofa Patwary , Namit Dhameja , Nave Assaf , Negar Habibi , Nidhi Bhatia , Nikki Pope , Nima Tajbakhsh , Nirmal Kumar Juluru , Oleg Rybakov , Oleksii Hrinchuk , Oleksii Kuchaiev , Oluwatobi Olabiyi , Pablo Ribalta , Padmavathy Subramanian , Parth Chadha , Pavlo Molchanov , Peter Dykas , Peter Jin , Piotr Bialecki , Piotr Januszewski , Pradeep Thalasta , Prashant Gaikwad , Prasoon Varshney , Pritam Gundecha , Przemek Tredak , Rabeeh Karimi Mahabadi , Rajen Patel , Ran El-Yaniv , Ranjit Rajan , Ria Cheruvu , Rima Shahbazyan , Ritika Borkar , Ritu Gala , Roger Waleffe , Ruoxi Zhang , Russell J. Hewett , Ryan Prenger , Sahil Jain , Samuel Kriman , Sanjeev Satheesh , Saori Kaji , Sarah Yurick , Saurav Muralidharan , Sean Narenthiran , Seonmyeong Bak , Sepehr Sameni , Seungju Han , Shanmugam Ramasamy , Shaona Ghosh , Sharath Turuvekere Sreenivas , Shelby Thomas , Shizhe Diao , Shreya Gopal , Shrimai Prabhumoye , Shubham Toshniwal , Shuoyang Ding , Siddharth Singh , Siddhartha Jain , Somshubra Majumdar , Soumye Singhal , Stefania Alborghetti , Syeda Nahida Akter , Terry Kong , Tim Moon , Tomasz Hliwiak , Tomer Asida , Tony Wang , Tugrul Konuk , Twinkle Vashishth , Tyler Poon , Udi Karpas , Vahid Noroozi , Venkat Srinivasan , Vijay Korthikanti , Vikram Fugro , Vineeth Kalluru , Vitaly Kurin , Vitaly Lavrukhin , Wasi Uddin Ahmad , Wei Du , Wonmin Byeon , Ximing Lu , Xin Dong , Yashaswi Karnati , Yejin Choi , Yian Zhang , Ying Lin , Yonggan Fu , Yoshi Suhara , Zhen Dong , Zhiyu Li , Zhongbo Zhu , Zijia Chen et al. (116 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model, by NVIDIA: Aarti Basant and 214 other authors View PDF HTML (experimental) Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2508.14444 [cs.CL] &nbsp; (or arXiv:2508.14444v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2508.14444 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Markus Kliegl [ view email ] [v1] Wed, 20 Aug 2025 06:00:57 UTC (1,188 KB) [v2] Thu, 21 Aug 2025 04:18:04 UTC (1,188 KB) [v3] Mon, 25 Aug 2025 18:10:44 UTC (1,188 KB) [v4] Tue, 2 Sep 2025 16:12:36 UTC (1,188 KB) Full-text links: Access Paper: View a PDF of the paper titled NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model, by NVIDIA: Aarti Basant and 214 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-08 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://github.com/NVIDIA/TensorRT-LLM/blob/46c5a564446673cdd0f56bcda938d53025b6d04e/docs/source/installation/build-from-source-linux.md#option-2-build-tensorrt-llm-step-by-step",
      "full_text": " TensorRT-LLM/docs/source/installation/build-from-source-linux.md at 46c5a564446673cdd0f56bcda938d53025b6d04e · NVIDIA/TensorRT-LLM · GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} NVIDIA / TensorRT-LLM Public Notifications You must be signed in to change notification settings Fork 1.7k Star 11.6k Code Issues 744 Pull requests 372 Discussions Actions Projects 2 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://research.nvidia.com/labs/adlr/files/NVIDIA-Nemotron-Nano-2-Technical-Report.pdf",
      "full_text": "2025-9-2\nNVIDIA Nemotron Nano 2: An Accurate and\nEfficient Hybrid Mamba-Transformer Reasoning\nModel\nNVIDIA\nAbstract. We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving state-of-the-art accuracy\ncompared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture,\nin which the majority of the self-attention layers in the common Transformer architecture are replaced\nwith Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces\nneeded for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter\nmodel (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After\naligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill\nthe model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G\nGPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g.,\nQwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning\nbenchmarks while achieving up to 6× higher inference throughput in reasoning settings like 8k\ninput and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-\n12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and\npost-training datasets on Hugging Face.\n1. Introduction\nWe introduce NVIDIA Nemotron Nano 2, a hybrid Mamba-Transformer reasoning model (Waleffe\net al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better\nbenchmark accuracies at 3×–6× higher throughput than Qwen3-8B (Yang et al., 2025) for generation-\nheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron\nNano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and\nrecipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints,\nas well as the majority of the pre- and post-training datasets.\nThe initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over\n20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5). It then\nunderwent a continuous pre-training long-context extension phase to become 128k-capable without\ndegrading other benchmarks (§2.6). Overall, new and improved datasets led to significant accuracy\nimprovements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (§2.2).\nNemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT),\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang\net al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains,\nfollowed by targeted SFT on key areas such as tool use, long-context performance, and truncated\n(budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability,\nwhile additional DPO stages further strengthened tool use. Overall, post-training was performed\non roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning\n© 2025 NVIDIA. All rights reserved.\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nIFBench\n(Instr. Following)\nAIME24\n(Math)\nAIME25\n(Math)\nGPQA-D\n(Science)\nLiveCodeBench\n(Coding)\nBFCLv3\n(Tool Use)\nRULER 128k\n(Long Context)\nISL/OSL\n1k/8k\nISL/OSL\n8k/16k\n30\n40\n50\n60\n70\n80\n90\nAccuracy (%)\n34.6\n81.9\n72.0\n64.0\n71.1\n66.9\n78.9\n33.0\n75.8\n69.3\n59.6\n59.5\n66.3\n74.1\nMeasured Accuracy\nMeasured Throughput\nNVIDIA-Nemotron-Nano-9B-v2\nQwen3-8B\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative Throughput (Output tokens/s/GPU)\n3.3\n6.3\n1.0\n1.0\nFigure 1 | Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput.\nNemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while\nachieving up to 6.3× higher throughput for such workloads. We abbreviate input sequence length\nto ISL and output sequence length to OSL and measure throughput on a single A10G GPU in\nbfloat16.\ntraces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained\nthinking budget control at inference time (§3.4).\nFinally, both the base model and aligned model were compressed so as to enable inference over\ncontext lengths of 128k tokens on a single NVIDIA A10G GPU (22 GiB of memory, bfloat16\nprecision). This was done by extending a compression strategy based on Minitron (Muralidharan\net al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject\nto constraints.\nWe are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning.\nAdditionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-\nTraining-Dataset-v1 collection of more than 6 trillion tokens:\n• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional\nCommon Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic\nQ&A data translated into 15 languages.\n• Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM\npipeline (Mahabadi et al., 2025). Preserves equations, standardizes to LaTeX, outperforms\nprevious math datasets on benchmarks.\n• Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering,\ndeduplication, and quality filters. Includes code Q&A data in 11 programming languages.\n• Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual,\nacademic, and reasoning domains.\nFinally, we are releasing an updated post-training dataset:\n2\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nNemotron-Nano-12B-v2-Base\nx3\nx6\nx1\nFigure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the\ntotal layers in the model are self-attention layers which are evenly dispersed throughout the model.\nModel\nNumber of\nlayers\nModel\ndimension\nFFN\ndimension\nQ\nheads\nKV\nheads\nState\ndimension\nMamba\ngroups\nNemotron-Nano-12B-v2-Base\n62\n5120\n20480\n40\n8\n128\n8\nTable 1 | Summary of Nemotron-Nano-12B-v2-Base architecture.\n• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases\nwith an extension of SFT and RL data into five target languages: Spanish, French, German,\nItalian and Japanese. The data supports improvements of math, code, general reasoning, and\ninstruction following capabilities.\nThe rest of this technical report is organized as follows: In §2, we discuss the Nemotron Nano 2 model\narchitecture, pre-training process, and base model evaluation results. In §3, we discuss the alignment\nprocess. In §4, we describe the pruning and distillation methods used for model compression.\n2. Pretraining\nIn this section, we discuss the architecture and pretraining of the Nemotron-Nano-12B-v2-Base\nmodel. We also compare this model against other state-of-the-art models in terms of accuracy on\npopular benchmarks.\n2.1. Model Architecture\nAs in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-\n2 (Dao & Gu, 2024), self-attention, and FFN layers. The layer pattern and key architecture details\nare summarized in Figure 2 and Table 1. Concretely, we use 62 layers, with 6 of them being\nself-attention layers, 28 being FFN, and 28 being Mamba-2 layers. We use a hidden dimension of\n5120, FFN hidden dimension of 20480, and Grouped-Query Attention (Ainslie et al., 2023) with 40\nquery heads and 8 key-value heads. For Mamba-2 layers, we use 8 groups, a state dimension of 128,\na head dimension of 64, an expansion factor of 2, and a window size for convolution of 4. For FFN\nlayers, we use squared ReLU (So et al., 2022) activation. Again as in Nemotron-H, we do not use\nany position embeddings and use RMSNorm (Zhang & Sennrich, 2019), separate embedding and\noutput layer weights, no dropout, and we do not use bias weights for linear layers.\n3\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.2. Pre-Training Data\nNemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-\ngenerated data.\n2.2.1. Curated Data\nWe have separate data curation pipelines for the following broad data categories: general web crawl\ndata (English and multilingual), math data, and code data. We discuss each in turn next.\nEnglish web crawl data.\nWe used the Nemotron-CC dataset (Su et al., 2025), but updated to\ninclude eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13)\nusing the same pipeline. For synthetic rephrasing, we mostly switched to Qwen3-30B-A3B (from\nMistral Nemo 12B). Additionally, we used data from CC-NEWS through April 23, 2025, to help\nimprove the knowledge cutoff of the model. The CC-NEWS data was filtered for English and globally\nfuzzily de-duplicated; no other filtering was used.\nMultilingual data.\nWe extracted data for fifteen languages from the following three Common\nCrawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, and CC-MAIN-2025-18. The fifteen\nlanguages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean,\nPolish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual\nmodel-based quality classifiers available, we just applied heuristic filtering instead. This was done in\na similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except\nthat we had to selectively disable some heuristic filters that had very high false positive rates for\nsome languages. De-duplication was done in the same way as for Nemotron-CC. Additionally, we\nused data from Wikipedia and FineWeb-2 (Penedo et al., 2025) for these fifteen languages.\nMath data.\nMathematical content on the web is expressed in a wide range of formats, including\ninline and block LATEX, MathML, Unicode symbols, and custom renderers such as MathJax or\nKaTeX. We conducted a detailed analysis of prior math-specific extraction pipelines—including\nOpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025), jusText (Endrédy & Novák,\n2013), Trafilatura (Barbaresi, 2021), and Resiliparse (Bevendorff et al., 2018)—and found that none\ncould reliably preserve mathematical expressions or code structure. These tools frequently discard or\ndistort equations and flatten code formatting, severely limiting the utility of the extracted content\nfor pretraining.\nTo address this, we built a new pipeline specifically designed for high-fidelity mathematical ex-\ntraction from Common Crawl. We first aggregated a comprehensive list of math-related URLs\nfrom prior datasets (e.g., InfiMM-WebMath (Han et al., 2024), OpenWebMath (Paster et al., 2023),\nFineMath (Allal et al., 2025), and MegaMath (Zhou et al., 2025)), then re-fetched their raw HTML\ndocuments from 98 Common Crawl snapshots (2014–2024). Each page was rendered using the lynx\ntext-based browser to preserve layout and math structure. We then applied Phi-4 (Abdin et al.,\n2024)(14B-parameters) to remove boilerplate, standardize notation into LATEX, and correct inconsis-\ntencies. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed\nby fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk\n& Motwani, 1998) via the NeMo-Curator framework.1 We finally decontaminated the dataset using\nLLM Decontaminator (Yang et al., 2023).\nThis process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token\nsubset, Nemotron-CC-Math-4+, containing only the top-scoring samples. When used for pretraining,\nthis dataset yields substantial improvements across math (MATH-500), code (HumanEval+, MBPP+,\n1https://github.com/NVIDIA-NeMo/Curator\n4\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMBPP), and general-domain evaluations (MMLU, MMLU-STEM, MMLU-Pro), surpassing all\nexisting open math datasets. For full details, see Mahabadi et al. (2025).\nCode data.\nIn line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar\net al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. All source\ncode used to train this model originated from GitHub and went through a multi-stage processing\npipeline to arrive at the final source code training data. We performed license-based removal with a\nlicense detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but\nwith fewer accepted licenses (see Appendix A for additional details). De-duplication is especially\nimportant for source code, where many files can be found exactly duplicated across numerous\nrepositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using\nMinHash LSH). In order to build a better understanding of each file in our dataset, we annotated all\nfiles with a variety of measures and then performed filtering using these annotations. We found the\nheuristic filters from OpenCoder (Huang et al., 2025) to be effective and leveraged them to filter\nfiles that are less valuable or even detrimental for LLM pretraining.\n2.2.2. Synthetically-Generated Data\nSTEM data.\nWe generated synthetic data for STEM subjects, including Astronomy, Biology,\nChemistry, Math, and Physics using 88.6k questions collected from multiple sources as the seed\ndata. In addition to the widely used GSM8K, MATH, and AOPS training sets, we collected more\ndiverse questions from Stemez2 and textbooks with permissive licenses from OpenStax3 and Open\nTextbook Library.4\nWe used Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to extract questions\nfrom the exercise sections in the textbooks with additional instructions such as dropping question\nnumbering, ignoring questions that require image interpretation, and formatting equations using\nLaTeX. We manually curated the extracted questions to fix occasional OCR errors and removed\nnon-self-contained questions (e.g., a question that refers to an example in the same chapter).\nTo expand both the quantity and diversity of questions, we conducted three iterations of question\ngeneration using four models (i.e., Qwen3-30B-A3B and Qwen3-235B-A22B (Yang et al., 2025), both\nwith thinking mode enabled, Deepseek-R1 (DeepSeek-AI, 2025a), and Deepseek V3 (DeepSeek-AI,\n2025b)) and three prompts:\n1. Similar question: Create a new question that explores similar concepts but offers a fresh\nchallenge.\n2. Harder question: Create a new question that requires more logical steps or involves more\nadvanced concepts.\n3. Varied question: Create a new question that differs in type from the original question. We\ninstructed the model to avoid superficial or trivial modifications and think through the solution\nwhen creating a new question.\nWe filtered out duplicates and highly-similar questions using fuzzy de-duplication and generated\nsolutions to the remaining questions with the models used in the question generation step. We\nconverted a subset of examples to multiple-choice questions in MMLU or MMLU-Pro style. We\nconstructed a few thousand few-shot examples by concatenating random synthetic samples.\nMath data.\nWe also revisited and regenerated the Nemotron-MIND dataset (Akter et al., 2024),\na math-informed synthetic pretraining corpus originally built on OpenWebMath. In our updated\n2https://www.stemez.com/\n3https://openstax.org\n4https://open.umn.edu/opentextbooks/\n5\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nversion, we regenerated the MIND dataset using Nemotron-CC-Math-4+, our highest-quality math\nsubset comprising 52B tokens—as the source corpus. Following the original methodology, we applied\nseven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured\nmathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B\ntokens of lower-fidelity data, our version leverages significantly higher-quality input and processes\nit with a chunk size of 5K tokens. This regeneration produced a 73B-token synthetic dataset and\nled to consistent improvements across math reasoning and general knowledge (MMLU, MMLU-Pro.\nMMLU-Stem) benchmarks compared to the original MIND version, highlighting the critical role of\ninput data quality. Full details and results are available in Mahabadi et al. (2025).\nMultilingual data.\nWe generated multilingual diverse question and answer data (Diverse QA) (Su\net al., 2025) from two sources:\n1. We translated the English Diverse QA data to fifteen languages (see Multilingual data) using\nQwen3-30B-A3B (Yang et al., 2025).\n2. We generated synthetic data from Wikipedia articles in these languages using the Diverse QA\nprompt and instructed the model to write all questions and answers in the target language.\nIn addition, we translated a subset of our GSM8K augmentation data (see STEM data) into\nthese languages using Qwen3-30B-A3B. We post-processed each translated solution by appending\na concluding sentence meaning “The answer is ...” (e.g., “La respuesta es ...” in Spanish, “Die\nAntwort lautet ...” in German), where the final numerical answer is extracted from the original\nEnglish solution.\nCode data.\nWe generated question-answer (QA) data at scale for 11 different programming\nlanguages by prompting an LLM to generate questions based on short snippets from our curated\nsource code, asking the model to solve the generated question, and then performing post hoc filtering\nof the generated QA pairs based on heuristics as appropriate (e.g., Python AST parsing). This\ntechnique results in diverse synthetic data targeted at problem solving containing both natural\nlanguage and source code. Further details are covered in the Nemotron-H technical report (NVIDIA,\n2025), where we first leveraged this type of synthetic code data in pretraining.\nAcademic data.\nIn the pretraining set for the Nemotron-H (NVIDIA, 2025) series of models, we\nassigned attribute labels for educational quality, educational difficulty, and educational subject to\nall documents coming from academic data, which encompasses textbooks and academic papers. As\ncontent of higher educational difficulty in technical domains still proves challenging for models, we\nprioritized increasing model comprehension of such information in our current pretraining set via\nthe generation of question-answer (QA) pairs as such data has been shown to enhance knowledge\nstorage and extraction within language models (Allen-Zhu & Li, 2024).\nTo do so, we first gathered all documents with educational difficulty at the undergraduate and\ngraduate levels in the following technical subject areas: math, chemistry, biology, physics, and\nmedicine. Using this subset of documents, we aim to find the most relevant pieces of texts that could\nbe utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets\nof 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them\nwithin a Milvus vector database that enables approximate nearest neighbor search. We then curate\ndocuments from a set of complex subject areas (e.g. Mathematics: Real Analysis, Biology: Genetics,\nStatistics: Information Theory), and query the Milvus database for the 250 nearest neighbor text\nsnippets to each query document. The returned snippets function as our seed contexts that we then\npass into a Qwen-2.5 72B instruct model (Qwen, 2025) to generate multiple choice and free response\nstyle QA pairs based on the information contained in the snippet. With each QA pair, a justification\nfor the answer is additionally generated.\n6\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSFT-style data.\nUsing SFT-style data in the later stages of pretraining has shown to be helpful\nto foster more comprehensive model learning (Hu et al., 2024).\nTherefore, we synthesized and included different SFT-style data covering several domains: 1) code\nSFT data which is mainly focused on solving code problems; 2) math SFT data that is mostly focused\non reasoning; 3) MMLU-style SFT data which contains different question and answer examples\ncovering different knowledge topics; and 4) general instruction following SFT data.\nWe ensure that the SFT-style data covers diverse topics with different difficulty levels for each of\nthe above mentioned domains. Detailed synthesis methods and pipelines for the above mentioned\nSFT data can be found in prior work (Toshniwal et al., 2024; Moshkov et al., 2025; Bercovich et al.,\n2025a,b; Ahmad et al., 2025b,a; Majumdar et al., 2024).\nFundamental reasoning SFT-style data.\nWhile the above mentioned SFT-style data help\nenhance an LLM’s ability to answer questions in code, math and general language understanding\nbenchmarks, they do not help improve the model’s ability in deeper reasoning tasks to discern\nthe correct answer among a larger pool of potential distractors. We propose to mitigate that\nby synthesizing SFT-style data focused on analytical reasoning, logical reasoning, and reading\ncomprehension.\nSpecifically, we collected existing datasets including 1) the Law School Admission Test (LSAT)\ndataset from Wang et al. (2022); Zhong et al. (2022) which encompasses three tasks: logical\nreasoning, reading comprehension, and analytical reasoning, 2) the repurposed LogiQA dataset\nby Liu et al. (2020) which contains various types of logical reasoning questions collected from the\nNational Civil Servants Examination of China, and 3) the AQuA-RAT dataset which emphasizes\nalgebraic word problems by Ling et al. (2017). We then prompted DeepSeek-V3 (DeepSeek-AI,\n2025b) and Qwen3-30B-A3B (Yang et al., 2025) respectively to synthesize more similar questions\nwith corresponding options. For each question we generated, we prompted DeepSeek-V3 again to\ngenerate the chain-of-thought (CoT) process with the final solution. At the post-processing stage,\nwe apply majority voting to keep only the samples that have the most voted solutions. Overall, we\ngenerated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models.\n2.3. Data Mixture and Ordering\nOur data mixture consists of thirteen data categories. The largest is web crawl data, which we\nsubdivided into four categories based on the Nemotron-CC quality classification (Su et al., 2025):\ncrawl-medium, crawl-medium-high, crawl-high, syn-crawl-high denoting medium, medium-high, high\nand synthetic quality crawl data, respectively. Apart from these, our data mixture has additional\ncategories such as math, wikipedia, code, academic data, crawl++, multilingual, and synthetic\nSFT-style data which is further categorized as general-sft, stem-sft and code-sft. Crawl++ consists\nof web-crawl derivatives like OpenWebText, BigScience and Reddit. Our multilingual data has fifteen\nlanguages: Arabic, Danish, German, Spanish, French, Italian, Portuguese, Dutch, Polish, Swedish,\nThai, Chinese, Japanese, Korean, and Russian. We design the data mixtures to give similar weight\nto data sources that have similar quality. Data sources of higher quality are weighed higher than\ndata sources of lower quality. We provide detailed explanation on quality estimation of datasets and\nthe blend creation process in Feng et al. (2024) and NVIDIA (2025).\nWe used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-\n12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the\nsecond and third phases, we primarily used high-quality datasets (e.g., Wikipedia). We switched to\nthe second phase at the 60% point of training, and to the third phase at the 90% point of training.\nThe data mixtures used in each phase are shown in Figure 3.\n7\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nstem-sft\n3.1%\nmultilingual\n5.0%\nacademic\n4.4%\ncode\n20.0%\nmath\n3.2%\nsyn-crawl-high\n16.2%\ncrawl-medium\n18.3%\ncrawl-medium-high\n14.8%\ncrawl-high\n11.1%\n(a) Data mixture of Phase 1.\ncode-sft\n4.4%\nstem-sft\n14.5%\nmultilingual\n5.0%\ncrawl++\n4.4%\nacademic\n3.8%\nwiki\n0.9%\ncode\n20.0%\ncrawl-high\n16.0%\nsyn-crawl-high\n21.0%\nmath\n9.5%\n(b) Data mixture of Phase 2.\ncode-sft\n10.9%\nstem-sft\n32.0%\nmultilingual\n4.4%\ncrawl-high\n10.0%\nsyn-crawl-high\n12.7%\nmath\n11.0%\ncode\n16.0%\n(c) Data mixture of Phase 3.\nFigure 3 | Data mixtures for each phase of pre-training.\n8\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMultilingual Data\nAvg\nSp\nGe\nFr\nMa\nIt\nJa\nPo\nKo\nCommon Crawl\n37.0\n37.8\n36.5\n39.8\n34.3\n36.3\n35.3\n37.5\n38.8\nFineWeb-2\n35.1\n38.8\n35.0\n34.3\n31.5\n37.0\n33.0\n36.0\n35.3\nDiverseQA-wiki\n42.1\n44.8\n41.3\n41.8\n41.5\n44.0\n41.0\n42.3\n40.3\nDiverseQA-crawl\n47.0\n49.8\n50.8\n48.3\n46.0\n45.8\n44.5\n49.0\n42.0\nTable 2 | Comparison of multilingual datasets on the Global-MMLU Benchmark.\n2.3.1. Multilingual Data Ablation Study\nIn Section 2.2, we mentioned several large categories of multilingual data, both curated and synthetic:\n1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline.\n2. FineWeb-2 (Penedo et al., 2025).\n3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse\nQA prompt.\n4. DiverseQA-crawl: Translated from English Diverse QA data.\nIn order to decide the proper data mixture among these different multilingual data sources, we\nfirst conducted ablation experiments to compare the four multilingual data’s downstream tasks’\nperformance.\nSpecifically, we took a 1B model checkpoint that had been trained for 350B tokens, and continuous\npretrained it for another 100B tokens. We assigned 50% of the continuous pretraining data to\nmultilingual data, and the remaining 50% use our default pretraining data mixture. We evaluated\neach model’s performance using the Global-MMLU benchmark (Singh et al., 2024a); the results are\nshown in Table 2. Our curated Common Crawl-based multilingual data performed slightly better\nthan the Fineweb2-based multilingual data, while the synthesized multilingual QA pairs performed\nmuch better than the curated multilingual web crawl data. The diverse pairs translated from English\nCommon Crawl achieved the highest average score over the 8 languages we evaluated on. Therefore,\nwe assigned a much higher weight to the DiverseQA-crawl data than the other categories when\ndeciding our multilingual data mixture.\n2.3.2. Fundamental Reasoning SFT-Style Data Ablation Study\nTo show the effectiveness of the fundamental reasoning (FR) focused SFT-style data we introduced\nin Section 2.2, we took the Nemotron-H-8B (NVIDIA, 2025) intermediate checkpoint trained over\n14.5T tokens, and continuous pretrained it with another 100B tokens. We assigned 5% of the 100B\ntokens to the newly synthesized FR-SFT data (as a replacement for Common Crawl data), and kept\nall other data categories the same as in the Nemotron-H-8B’s phase 3 blend. We compared this\nmodel with Nemotron-H-8B, which had also been trained with 14.6T tokens. The detailed evaluation\nbenchmarks are introduced in Section 2.7. The comparison results are shown in Table 3. The\nSFT-style data helped improve the Nemotron-H 8B model’s performance on MMLU-Pro from 44.24\nto 56.36, and also helped increase the average MATH score by around 2 points. While MMLU-Pro\nis a more challenging benchmark that evaluates a model’s language understanding capability, it\nalso requires the model to have excellent reasoning capability to select the correct answer out of\nten choices. Our SFT data helps equip the model to select the correct answers from the other nine\ndistractors through fundamental reasoning. We noticed no decrease in the average commonsense\nreasoning and average code benchmarks.\n9\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nModel\nAvg Math\nAvg Code\nAvg Reasoning\nMMLU\nMMLU-Pro\nNemotron-H 8B\n37.92\n59.49\n71.79\n72.67\n44.24\nNemotron-H 8B\n(w/ FR-SFT data)\n39.70\n59.61\n71.43\n72.98\n56.36\nTable 3 | Ablation study of the Fundamental Reasoning (FR) focused SFT-style data.\n2.4. FP8 Recipe\nWe used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b).\nSpecifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles\nfor the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we\ncould do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas)\nin FP8; master weights are still kept in FP32. One exception to DeepSeek’s formula was that we left\nthe first and last four linear layers in BF16, as done with Nemotron-H. Also unlike the DeepSeek-V3\nrun, we left all optimizer state in FP32. We observed no training instabilities from this choice of\nnumerics.\n2.5. Hyperparameters\nWe trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence\nlength of 8192 and global batch size of 768 (6,029,312 tokens per batch). We did not use any batch\nsize ramp-up. We used a WSD (Warmup-Stable-Decay) (Hu et al., 2024) learning rate schedule\nwith a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was\ndecayed over the final 3.6 trillion tokens. Weight decay was set to 0.1, and Adam 𝛽1 and 𝛽2 were set\nto 0.9 and 0.95 respectively\n2.6. Long-Context Extension\nTo ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context\nphase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT)\nwith a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. Although\nthe target context length of Nemotron Nano 2 is 128k, in preliminary studies on the Nemotron-H\n8B model, we found it better to do CPT with 512k sequence length, instead of 256k or 128k. Our\nintuition is that longer training sequence can effectively lower the chance of long coherent documents\nbeing cut and separated by the Concat & Chunk algorithm for pretraining data loading. We used\n8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence\nlengths of 512k tokens still fits in GPU memory. We used a global batch size of 12 to ensure the\ntotal number of tokens per global batch during long-context CPT is the same as during pretraining:\naround 6M tokens. Phase LC consisted of 18.9 billion tokens.\nAdditionally, we did long-context synthetic data generation to create more high-quality data for Phase\nLC. Since the academic pretraining dataset is a good source of coherent long-context documents,\nwe used such documents that are longer than 32k tokens as seed data. We followed the methods\nmentioned in the Llama-3 (Meta, 2024) and Qwen-2.5 (Qwen, 2025) tech reports to generate long-\ncontext document QA data. We split each document into chunks of 1,024 tokens and then randomly\nselected 10% of the chunks to be fed into Qwen-2.5-72B-Instruct for data synthesis. We asked the\ngenerator to generate a QA pair based on the information in the text chunk. We concatenated the\nQA pairs and appended them to the end of the original document as a sample of the long-context\ndocument QA data. Such long-document QA provided good material for the model to learn long-\n10\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\ncontext dependencies. See Table 4 for ablation results on Nemotron-H 8B regarding train sequence\nlengths and the effects of synthetic data.\nThe data blend used in Phase LC was built based on that of Phase 3. We proportionally downscaled\nthe weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the\nnewly added long-context document-QA data. We found such a blend could effectively extend the\ncontext length of Nemotron-Nano-12B-v2-Base without degrading regular benchmark scores.\nTrain length\n128k\n256k\n256k\n512k\nSynthetic data\nyes\nno\nyes\nyes\nRULER-128k\n73.68\n70.19\n79.04\n81.04\nTable 4 | Comparisons of different train sequence lengths and synthetic data usages. Ablations were\nconducted on Nemotron-H 8B.\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGeneral\nMMLU\n78.24\n74.53\n76.44\n73.61\nMMLU-Pro 5-shot\n63.98\n59.43\n56.27\n45.12\nAGIEval English CoT\n68.03\n65.28\n59.54\n51.69\nMath\nGSM8K CoT\n91.66\n91.36\n84.00\n74.45\nMATH\n83.54\n80.50\n55.40\n42.40\nMATH Level 5\n67.61\n63.64\n29.91\n17.71\nAIME 2024 pass@32\n56.67\n30.00\n20.00\n16.67\nCode\nHumanEval+ avg@32\n61.03\n58.50\n57.55\n36.68\nMBPP+ avg@32\n61.55\n58.95\n58.56\n51.73\nCommonsense Understanding\nARC Challenge\n93.26\n90.70\n93.09\n90.44\nHellaSwag\n84.00\n79.90\n79.75\n84.15\nOpenBookQA\n46.00\n44.80\n42.00\n46.00\nPIQA\n82.54\n81.83\n79.43\n82.10\nWinoGrande\n79.24\n75.30\n75.93\n79.95\nLong Context\nRULER-128K\n84.74\n82.22\n-\n80.70\nTable 5 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models. N-Nano-V2 is\nshort for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base\nand Gemma3-12B-Base, and the best score is highlighted in each row.\n11\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.7. Base Model Evaluations\nWe run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on\ntop of lm-evaluation-harness5 for fair comparisons, with the following changes:\n1. For mathematical reasoning, we evaluate GSM8K and MATH (Cobbe et al., 2021; Hendrycks\net al., 2021b) benchmarks using greedy-decoding. We also highlight the competition-level\nslice of the MATH benchmark as “MATH Level 5”. Additionally, we report the pass@32\nperformance on AIME-2024. We use Math-Verify6 to grade all generations.\n2. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)) we evaluate\nthe EvalPlus variants along with the sanitization of generations (Liu et al., 2023), in a 0-shot\nsetup. We estimate avg@32, pass@1 from 32 generations per prompt.\n3. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged\nexcept for ARC-Challenge (Clark et al., 2018), where we present all options at the same time,\nsimilar to MMLU (Hendrycks et al., 2021a).\n4. For multilingual capability, we evaluate MGSM Shi et al. (2022) (8-shot, native CoT) and\nGlobal MMLU-Lite Singh et al. (2024b).\n5. We use RULER (Hsieh et al., 2024) as the long context benchmark. We report the average\nscores over all the 13 tasks included in RULER.\nAccuracy results for Nemotron-Nano-12B-v2-Base with comparsions to Qwen3-8B Base and Gemma3-\n12B Base are shown in Tables 5 and 6. We also include the accuracy of our 9B pruned variant of\nNemotron-Nano-12B-v2-Base which is discussed in Section 4.\n3. Alignment\nIn this section we will present the alignment process we followed to convert the base checkpoint into\nan aligned 12B checkpoint. Our process is outlined in Figure 4.\nBase\nSFT 1\nSFT 2\nSFT 3\nMerged\nGRPO\nRLHF\nDPO\nFigure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2\n12B checkpoint.\n5https://github.com/EleutherAI/lm-evaluation-harness.\n6https://github.com/huggingface/math-verify.\n12\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGlobal-MMLU-Lite\nGerman\n74.50\n68.25\n75.50\n69.75\nSpanish\n76.50\n72.75\n75.00\n74.00\nFrench\n78.25\n69.75\n74.25\n72.50\nItalian\n76.50\n73.25\n72.75\n74.00\nJapanese\n71.00\n67.00\n70.00\n71.50\nKorean\n72.50\n67.25\n67.25\n70.25\nPortuguese\n76.25\n71.25\n72.50\n75.75\nChinese\n75.50\n69.25\n75.25\n67.25\nAverage\n75.13\n69.94\n72.81\n71.88\nMultilingual Math (MGSM)\nSpanish\n93.20\n93.60\n87.60\n73.60\nGerman\n88.40\n88.40\n78.80\n66.00\nFrench\n82.40\n84.40\n82.00\n68.00\nChinese\n83.60\n82.00\n80.80\n62.00\nJapanese\n76.80\n68.80\n71.20\n56.00\nRussian\n91.20\n90.80\n85.20\n72.40\nAverage\n85.94\n84.67\n80.93\n66.33\nTable 6 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models on multilingual\nbenchmarks. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is\ncompared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each\nrow.\n3.1. Post-Training Data\nOur alignment begins with a large-scale SFT stage which trains the base model on approximately 80\nbillion tokens of prompt-response pairs. The distribution of domains is shown in Table 7.\nMath, science and coding.\nFor Math (Toshniwal et al., 2024; Moshkov et al., 2025), Science\nand Coding (Ahmad et al., 2025b,a; Majumdar et al., 2024) data, we generate responses using\nthe open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for\ntraining Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been\nreleased as part of Nemotron-Post-Training-Dataset-v17.\nTool calling.\nThe tool-calling dataset consists of single-turn, multi-turn, and multi-step conversa-\ntions.\nFor single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive-\nfunction-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using\nQwen3-235B-A22B10. Inspired by ToolACE (Liu et al., 2024) and APIGen-MT (Prabhakar et al.,\n2025), we extend this to multi-turn and multi-step settings by simulating conversations where\n7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\n8https://huggingface.co/datasets/xlam-function-calling-60k\n9https://huggingface.co/datasets/glaive-function-calling-v2\n10https://huggingface.co/Qwen/Qwen3-235B-A22B\n13\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDomain\nNumber of Samples\nMath\n1.5M\nCoding\n1.1M\nScience\n2.0M\nTool-calling\n400K\nConversational\n1.5M\nSafety\n2K\nMultilingual (all domains)\n5.0M\nTable 7 | Post-training data distribution across domains used for our SFT stages.\nQwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The\nUser-Agent reviews available tools, poses challenging queries, interacts when addressed by the\nAssistant, and judges task success at the end. Each instance is paired with a random persona from\nNemotron-Personas11 to enrich diversity of queries.\nThe Assistant-Agent receives the initial query and available tools, executes tasks by invoking tools,\ninterpreting their responses, and interacting with the User-Agent across single-turn, multi-turn,\nor multi-step scenarios. Meanwhile, the API-Server-Agent acts as a mock API server, checking\nparameters and returning either valid outputs or error messages depending on correctness. A\nlightweight rule-based tool-call verification layer further strengthens reliability by ensuring outputs\nare consistent and verifiable, and only successful trajectories are retained.\nMultilingual data.\nOur multilingual synthetic post-training data are constructed by translating\nexisting English post-training data. To address the challenges of Large Language Model (LLM)\nhallucinations and quality degradation on long inputs when generating synthetic translation data, we\nimplement a robust quality assurance pipeline. Our method involves translating inputs line-by-line\nto manage complexity and skip non-translatable content like code. We also enforce a strict bracket\nformat for reliable extraction and use language identification to filter out off-target translations,\nthereby ensuring high-quality final outputs.\nConversational data.\nFor conversational data, we use prompts from the LMSYS dataset (Zheng\net al., 2023) and generate responses using the Qwen3-235B-A22B reasoning model (Yang et al., 2025).\nWe also incorporate prompts from HelpSteer2 and HelpSteer3, paired with responses generated by\nthe same model. In addition, we draw on a subset of approximately 550k prompts from WildChat-\n1M (Li et al., 2024b), again generating reasoning responses with Qwen3-235B-A22B. We also include\nmulti-turn conversations with Deepseek R1 responses using the multi-turn conversational prompts\nused in NVIDIA (2025).\nSafety.\nWe leveraged a mix of harmful and benign prompts drawn from the Nemotron Content\nSafety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo\net al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To\nensure safety, we applied a two-step approach: initial prompting followed by filtering with guard\nmodels to verify that outputs remained safe.\n11https://huggingface.co/datasets/NVIDIA/Nemotron-Personas\n12https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n13https://huggingface.co/deepseek-ai/DeepSeek-R1\n14\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3.2. Post Training\nStage 1 SFT.\nAs Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning.\nStage 1 uses the full dataset described in Section 3.1, augmented with a subsample of roughly 10% of\nprompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces,\nenabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve\nlong-context ability from pretraining, we concatenate samples into sequences of approximately 128k\ntokens, reducing padding overhead and encouraging long-range learning.\nStage 2 SFT.\nStage 2 targets tool-calling. Although Stage 1 improved performance on most\nbenchmarks, tool-calling accuracy degraded. We attribute this to sample concatenation at 128k, which\nlikely disrupted learning of tool-calling patterns. Thus, Stage 2 was trained without concatenation,\nusing the full tool-calling dataset and a representative subsample of other domains.\nStage 3 SFT.\nStage 3 reinforces long-context capability. It incorporates long-context data following\nthe recipe used in Nemotron-H preparation (NVIDIA, 2025), along with augmented examples across\ndomains where reasoning traces were abruptly truncated to 1–2k tokens while preserving the final\nanswer. This truncation strategy improved robustness under varying inference-time thinking budgets.\nIFeval RL.\nTo improve instruction adherence, we sampled 16,000 prompts from the LMSYS Chat\ndataset and augmented them with IFEval-style instructions. A rule-based verifier scored outputs\nbased on how well they satisfied each instruction, creating a reward signal that prioritized following\ndirections with precision. IFEval RL experiments provided significant boost to IFEval capabilities\nwhile the rest of the benchmarks fluctuated slightly requiring careful checkpoint selection.\nDPO.\nIn another branch of training, we apply the DPO algorithm to improve tool-calling. We\nevaluate performance using the BFCL v3 benchmark, which extends BFCL v2 with greater emphasis\non multi-step (multiple tool calls to achieve a goal) and multi-turn (multiple user–agent interactions).\nTo strengthen these capabilities in the Nano V2 aligned model, we use the WorkBench environment,\na multi-step verifiable tool-calling setup adapted from Styles (Styles et al., 2024). In each WorkBench\ntask, the model must issue a sequence of tool calls across multiple steps, with correctness verified\nthrough database state comparisons.\nNano V2 undergoes reinforcement learning in this environment through iterative stages of Direct\nPreference Optimization. For each candidate checkpoint from the long-context stage, we generate\non-policy data consisting of positive examples (successful tool calls) and negative examples (failed\ngenerations) for every WorkBench prompt.\nThis process ensures that iterative DPO remains\non-policy.\nRLHF.\nWe evaluate the model’s overall helpfulness and chat capabilities using the Arena-Hard\nbenchmark. To improve performance on this benchmark, we use GRPO to train candidate checkpoints\nfrom the SFT stage using English-only contexts from HelpSteer3 (Wang et al., 2025). During training,\nwe generate responses both with and without thinking traces and use a Qwen-based reward model\nto judge the rollouts.\nModel Merging.\nDuring training, we observed a trade-off between reasoning capabilities and\nchat capabilities. To address this, we opted for checkpoint interpolation Wortsman et al. (2022),\n15\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nEvaluation\nNemotron-Nano-v2-12B\nQwen3-8B\nQwen3-14B\nAIME-2024\n85.42\n75.83\n81.53\nAIME-2025\n76.25\n69.31\n66.6\nMATH-500\n97.75\n96.3\n96.85\nGPQA-Diamond\n64.48\n59.61\n64.53\nLiveCodeBench (07/24–12/24)\n70.79\n59.5\n63.08\nSciCode Sub-Task\n18.75\n24.65\n26.04\nHumanity’s Last Exam\n6.30\n4.40\n5.38\nIFEval (Inst. Strict)\n89.81\n89.39\n91.32\nBFCL v3\n66.98\n66.34\n68.01\nRULER @ 128k\n83.36\n74.13\n73.55\nArenaHard\n74\n78.4\n87.7\nTable 8 | Evaluation results with reasoning \"ON\" (for Nemotron-Nano-v2-12B, Qwen3-8B, and\nQwen3-14B across reasoning and general capability benchmarks.\nblending in an RL checkpoint with strong reasoning capabilities with an RL checkpoint with strong\nchat capabilities. Checkpoint interpolation is performed by linearly interpolating model weights:\n(1 −𝛼) · 𝑤𝑚𝑜𝑑𝑒𝑙1 + 𝛼· 𝑤𝑚𝑜𝑑𝑒𝑙2. We experimented with a parameter sweep over 𝛼values from 0.1 to\n0.9 in increments of 0.1, and found that values around 0.5 offered a good trade-off.\n3.3. Evaluation\nOur 12B model’s performance is summarized in Table 8. To test reasoning capabilities across domains,\nwe evaluate the models on MATH-500 (Lightman et al., 2023), AIME-2024, AIME-2025,\nGPQA-Diamond (Rein et al., 2023), LiveCodeBench (07/24 - 12/24) (Jain et al.,\n2024), SciCode (Tian et al., 2024), and Humanity’s Last Exam (Phan et al., 2025). For\nbroader evaluation on diverse capabilities, we use IFEval (Zhou et al., 2023) for instruction\nfollowing capabilities, BFCL v3 (Yan et al., 2024) for tool-calling, RULER for long-context,\nand ArenaHard (Li et al., 2024a) for chat capability.\nWe conduct evaluations using NeMo-Skills14. We report Pass@1 average of 16 runs for AIME-\n2024, AIME-2025; average of 4 runs for MATH-500, GPQA-Diamond, LiveCodeBench,\nIFEval; and score of 1 run for BFCL v3, SciCode, Humanity’s Last Exam, RULER,\nand ArenaHard.\n3.4. Budget Control Evaluation\nNemotron Nano V2 allows users to specify how many thinking tokens the model may generate before\nproducing the final answer. The final answer is the portion of text typically shown to end users.\nThis feature is implemented by counting tokens after the model begins generating the <think>\ntoken. Once the budget is reached, the inference setup attempts to insert a closing </think> tag.\nRather than inserting it immediately, we let the model finish its current sentence and place the\ntag at the next newline. In extreme cases where no newline appears, the system enforces closure\nwithin 500 tokens past the budget: if no newline occurs by the (budget + 500)th token, the </think>\ntag is forcibly inserted. Figure 5b shows our models budget control behavior. Apart from just\npresenting the accuracy of the model at various budgets, we also inspect if the model generations\nare well-formatted at various budgets. We inspect for two kinds of failure modes:\n14https://github.com/NVIDIA/NeMo-Skills\n16\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(a)\n(b)\nFigure 5 | Comparison of budget control before truncation training (a) and after truncation training\nwas included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens.\n• In one failure mode, the model uses more tokens in the final answer to “compensate” for\nrestrictions in the thinking traces. Without truncated training examples in the SFT stage,\nthis compensation effect is prevalent (Figure 5a, center). With truncated training, however,\nthe effect is absent (Figure 5b, center).\n• Another issue is that the model can remain in “thinking mode” even after the closing tag\n</think> is inserted. This is evident when the model generates the closing tag again after the\nforced insertion, suggesting it does not fully “register” the artificial closure. We evaluate this\nusing “Well-Formedness,” where a well-formed response should contain only a single closing\ntag (either forced by the budget or produced naturally). Figure 5a (right) shows that for short\nbudgets, the percentage of well-formed responses drops sharply. With truncation training,\nhowever, the model consistently produces well-formed responses (Figure 5b, right).\n4. Pruning and Distillation\nIn this section, we describe the pruning and distillation process to compress the aligned 12B model\nto the Nano 2 model with the goal of running longer context (128k sequence length) inference on\nthe NVIDIA A10G GPU. Note that storing just the weights of a 12B parameter model in bfloat16\nprecision requires 22.9 GiB, which is more than the 22 GiB memory capacity of an A10G GPU; this\nclearly indicates the need for compression.\nOur compression strategy builds on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024;\nTaghibakhshi et al., 2025), which is a lightweight model pruning framework for LLMs. While\nMinitron was originally designed for compressing pretrained base models targeting user-defined\nparameter budgets, in this work, we extend it to compress reasoning models while also incorporating\n17\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nthe memory constraints and throughput-based objectives stated above.\n4.1. Importance Estimation\nWe collect importance or sensitivity scores for each model component (e.g., layers, FFN neurons)\nto help decide which components to remove; this is the importance estimation phase. The scores\ncomputed in this phase are used to decide which model components can be pruned. We note\nthat sensitivity analysis based on gradient information is typically impractical at modern LLM\nscale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only forward\npasses. In this work, we use a simplified approach that works well in our ablation studies: a) prune\nlayers, and b) prune FFN hidden dimensions (effectively neurons) and embedding channels. We\nalso experimented with pruning Mamba heads; unfortunately, this axis caused severe accuracy\ndegradation. We now describe how we compute the importance of each layer, embedding channel,\nFFN neuron and Mamba head.\nLayer importance.\nWe compute layer importance in an iterative fashion: for each candidate layer,\nwe temporarily remove it from the model and compute the mean squared error (MSE) between the\noriginal model’s logits and those produced by the pruned model. This MSE reflects the contribution\nof that layer to the model’s predictions: lower values indicate smaller impact. At each pruning step,\nwe remove the layer with the lowest MSE, as it has the least influence on the final output. We repeat\nthis process until the desired depth is reached. This strategy ensures that pruning preferentially\nremoves layers whose absence minimally affects the model’s behavior. For more details on iterative\nMSE-based layer importance, please refer to NVIDIA (2025).\nFFN and embedding channel importance.\nFFN layers internally are composed of two linear\noperators with a non-linear activation in between:\nFFN(X) = 𝛿\n(︂\nX · 𝑊𝑇\n1\n)︂\n· 𝑊2.\nHere, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN\nlayer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN\nhidden dimension respectively. 𝛿(·) refers to the non-linear activation function (squared ReLU in\nthis work).\nFollowing the same procedure as Minitron (Muralidharan et al., 2024), we compute the importance\nof each neuron in the first linear operator of each FFN layer by examining the set of outputs it\nproduces. We use a small calibration dataset of 1024 samples for this purpose. Formally, we compute\neach neuron’s importance score by aggregating its outputs given an input batch 𝑋:\n𝐹(𝑖)\nneuron =\n∑︁\nB,S\n𝛿\n(︂\nX\n(︀𝑊𝑖\n1\n)︀𝑇\n)︂\n.\nHere, 𝑊𝑖\n1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀\nB,S refers to aggregation along the\nbatch and sequence dimensions. We use the mean and l2-norm aggregation functions along the\nbatch and sequence dimensions, following the observations in the Minitron paper. For a sequence of\nscores S, mean aggregation is defined as 1\n𝑛\n∑︀𝑛\n𝑖=1 |S𝑖|, and l2-norm is\n√︁∑︀𝑛\n𝑖=1 S2\n𝑖. Embedding channel\nimportance is computed similarly, by examining the outputs of LayerNorm layers instead; we refer\nthe reader to Muralidharan et al. (2024) for more details.\n18\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba importance.\nMamba layers process inputs through multiple projection matrices (𝑊𝑥,\n𝑊𝑧, 𝑊𝐵, 𝑊𝐶, 𝑊𝑑𝑡) that produce intermediate representations before causal convolution and selective\nstate space model (SSM) updates, followed by gated normalization and an output projection (𝑊𝑂).\nWe follow the methodology described in Taghibakhshi et al. (2025) for importance estimation:\nspecifically, we adopt a nested activation-based scoring strategy over a small calibration dataset of\n1024 samples, similar to FFN importance but adapted to Mamba’s group-aware structure. First,\nwe obtain activation scores from the 𝑊𝑥projection, denoted 𝑠∈R𝑚ℎ×𝑚𝑑, where 𝑚ℎis the number\nof Mamba heads and 𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is\ncomputed as\n𝑠𝑑=\n⃦⃦⃦⃦⃦⃦\n∑︁\nB,S\n𝑠:,𝑑\n⃦⃦⃦⃦⃦⃦\n2\n,\nwhere the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and\nl2-norm metrics. Next, head scores are computed by using the l2-norm over the Mamba head\nchannel set:\n𝑓ℎ= ‖𝑠ℎ,𝑚𝑑‖2 ,\n∀ℎ∈{1, . . . , 𝑚ℎ},\nand heads are ranked within each Mamba group 𝒢𝑔to preserve group-aware computation semantics:\nℛ𝑔= argsortℎ∈𝒢𝑔(𝑓ℎ).\nwhich ensures that pruning decisions respect the model’s structural constraints and SSM’s sequence\nmodeling. The lowest-scoring heads are pruned by trimming the corresponding rows from all affected\nprojection, convolution, and SSM parameter matrices. This strategy preserves the integrity of the\nSSM block while removing less important Mamba heads. As shown in Taghibakhshi et al. (2025),\npruning Mamba heads yields a better accuracy–throughput trade-off than pruning head channels;\nwe consequently focus on head pruning in this work.\n4.2. Lightweight Neural Architecture Search\nWe first define the constraints and objectives for the Nano 2 model, and then describe our lightweight\nNeural Architecture Search (NAS) framework that finds the most promising architectural candidates\nthat meet our objectives and constraints.\nMemory constraints.\nMemory requirements during inference consist of two distinct components\nwith different scaling behaviors.\nThe parameter memory, while substantial, remains constant\nregardless of the input size. In contrast, the key-value cache memory scales linearly with both batch\nsize and sequence length, often becoming the dominant factor in long-sequence scenarios. For the\nNano 2 model, our goal was to be able to perform inference at a sequence length of 128k and a batch\nsize of at least 1 within a memory budget of 19.66 GiB. We obtained the budget as follows: from the\n22.06 GiB available memory on an NVIDIA A10G GPU, we subtract a 5% buffer for frameworks\nsuch as vLLM and TensorRT-LLM and another 1.3 GiB to allow sufficient space for a vision encoder.\nMeasuring throughput.\nFor the experiments below, unless otherwise specified, we measure\nthroughput on an input and output sequence length of 8k and 16k tokens respectively, which we\nbelieve represents a typical reasoning scenario. For this combination of input and output sequence\nlength, we report vLLM output token generation throughput at the maximum batch size that fits on\nthe A10G GPU.\n19\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n4.2.1. Candidate enumeration.\nOur compression strategy explores multiple axes within the 19.66 GiB memory budget through\ncombinatorial pruning. Our search space includes depth reduction (removing 6-10 layers from the\noriginal 62-layer architecture) combined with width pruning of embedding channels (4480-5120),\nFFN dimension (13440-20480), and Mamba heads (112-128). This multi-axis search space results in\nhundreds of candidate architectures meeting the memory constraint.\n4.2.2. Finding the Best Architecture\nSince performing knowledge distillation and throughput benchmarking on the full set of candidates\nwould be prohibitively expensive, we break down the problem into two parts: (1) find the optimal\ndepth for the compressed model, and (2) find the optimal width-pruned architecture given the depth.\nEffect of depth.\nWe compare the accuracy of three depth-pruned candidates obtained from the\n12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all\nthree variants so as to achieve a good balance between KV cache size and long-context performance;\nprior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA,\n2025). We leave the width dimensions untouched for this experiment. Table 9 lists average reasoning\naccuracy at different depths after 6B tokens of distillation; in line with our previous observations on\nthe strong correlation between depth and task performance (Muralidharan et al., 2024; Sreenivas et al.,\n2024), we notice that reducing depth beyond 56 layers results in significant accuracy degradation; as\na result, we fix the depth at 56 for further width pruning.\nAccuracy (Avg)\n52 Layers\n44.92\n54 Layers\n47.35\n56 Layers\n51.48\nTable 9 | Effect of depth on reasoning accuracy. Results are after distilling with 6B tokens.\nCombining depth and width pruning.\nAs described above, we fix the depth of our target\nmodel to 56 layers with 4 attention layers. We perform 60B tokens of distillation on this checkpoint\n(see Section 4.3 for additional details) and perform further width pruning along the embedding, FFN,\nand Mamba axes. We enumerate all candidate pruned architectures that meet our memory budget,\nand sort them in decreasing order of estimated memory consumption at 128k context length and\nbatch size 1. The top 3 candidates from this list are picked for further evaluation: in particular, we\nperform short Knowledge Distillation (KD) on these candidates for 19B tokens after depth+width\npruning; we also benchmark throughput to pick the final architectural candidate. Table 10 lists the\narchitectural details of the top 3 candidates, along with the achieved task performance (post KD)\nand throughput. As shown in the Table, Candidate 2 achieves the best accuracy while still having\nreasonable runtime performance; consequently, we use this architecture for Nano 2.\nFFN vs.\nMamba pruning.\nWe ablate the number of Mamba heads following the recipe\nin Taghibakhshi et al. (2025), considering configurations with 87.5% and 93.75% of the original\nheads. However, due to the relatively smaller compression ratios explored in this work (less than 15%\nafter depth pruning) compared to those in Taghibakhshi et al. (2025) (around 50%), we find that\napplying Mamba head pruning yields limited benefit, and in these cases, pruning only the FFN and\n20\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n#Layers Hidden\nFFN\nMamba #Heads Params. (B) Accuracy Throughput\nCandidate 1\n56\n4480\n17920\n112\n8.92\n59.07\n161.02\nCandidate 2\n56\n4480\n15680\n128\n8.89\n63.02\n156.42\nCandidate 3\n56\n4800\n14400\n120\n8.97\n62.94\n155.86\nTable 10 | Top 3 candidates for architecture selection. Accuracy is the average across reasoning\nbenchmarks after distillation with 19B tokens. The last column shows vLLM output generation\nthroughput (ISL/OSL=8k/16k and batch size=8).\nembedding dimensions—after depth pruning—proves sufficient to achieve the desired compression\nwhile preserving accuracy. Candidates 1 and 2 in Table 10 highlight this difference.\n4.3. Retraining with Distillation\nTo recover the accuracy lost due to pruning, the model undergoes continued training. Recent work\nhas demonstrated that distilling knowledge from the original model to the pruned model outperforms\nconventional fine-tuning (Muralidharan et al., 2024; Sreenivas et al., 2024; Bercovich et al., 2024);\nwe thus adopt logit-based distillation for continued training, employing forward KL divergence loss\nexclusively during the accuracy recovery phase (see §3 of the Minitron paper (Muralidharan et al.,\n2024) for more details on the distillation loss formulation). Building on the candidate selection\nprocess described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below,\nto yield the final Nano 2 reasoning and base models.\n% Reasoning-SFT data\n% Pretraining data\nAccuracy (Avg)\n50\n50\n57.5\n70\n30\n58.5\n90\n10\n57.2\nTable 11 | Effect of varying reasoning data proportion on math accuracy after ∼6B tokens of KD.\nReasoning model.\nThe reasoning model is distilled in stages with increasing sequence lengths to\nstrengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement\nlearning (RL), preference optimization and model merging to retain desired behaviors and ensure\nrobustness across diverse tasks. We now describe these various stages:\n1. Depth pruning to 56 layers; Knowledge Distillation (KD) with ∼60B tokens at 8,192 sequence\nlength.\n2. Width pruning and KD with:\n• ∼50B tokens at 8,192 sequence length.\n• ∼25B tokens at 49,152 sequence length.\n• ∼1B tokens at 262,144 sequence length.\n3. Direct Preference Optimization (DPO).\n4. Group Relative Policy Optimization (GRPO).\n5. KD with ∼0.4B tokens at 262,144 sequence length to recover post-RL drops.\n6. RLHF for alignment with human preferences.\n7. Model merging between steps 5 and 6 via 0.5 linear interpolation.\n21\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMore details on DPO, GRPO and RLHF can be found in Section 3. Figure 6 shows the effects of\nstaged training on model accuracy across different reasoning benchmarks. Here, the 𝑥-axis represents\nthe various stages (starting from Step 2 above), and the 𝑦-axis shows the scores obtained for the\nvarious benchmarks as training progresses. As shown in the Figure, DPO and GRPO are critical for\nenhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the\nlatter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next\nstep (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard)\nbut causes additional benchmark drops, which are then recovered through model merging.\nKD+LCExt\nDPO\nGRPO\nKD\nRLHF\nMerge\nPipeline Stage\n50\n55\n60\n65\n70\n75\n80\n85\n90\nScore (%)\nDistillation Pipeline\nAIME-25\nGPQA-D\nBFCLv3\nIFEval (Pr.)\nMMLU-Pro\nArenaHard\nLiveCodeBench\nFigure 6 | Task accuracy at different stages of the distillation pipeline for Nemotron Nano 2.\nDataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining\n(Section 2.2) data yields the highest accuracy (Table 11). For KD at sequence length 262,144, we\nuse 100% stage 3 post-training data (Section 3.2).\nBase model.\nDistillation proceeds in stages: depth-only pruning and KD on ∼120B tokens,\nfollowed by width pruning and KD on ∼360B tokens (both at sequence length 8,192), and finally\nKD on ∼2.5B tokens at sequence length 524,288 to instill long-context capabilities.\nDataset: Following Sreenivas et al. (2024), we use 100% pretraining data described in sections 2.2\nand 2.6 for distillation of the base model at sequence lengths 8,192 and 524,288, respectively.\n4.4. Results\nWe efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden\nsize, and embedding channels, improving inference throughput and enabling long-context inference on\nan NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally,\nthe number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was\npruned from 20480 to 15680. As shown in Figure 1 and Tables 5 and 6, Nemotron-Nano-9B-v2\nachieves 3×-6× higher throughput than Qwen3-8B for generation-heavy scenarios, while surpassing\n22\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nit in accuracy and remaining comparable to the 12B teacher on most benchmarks.\n5. Conclusion\nIn this report, we introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model\nthat achieves comparable or better accuracies at up to 6× higher throughput than existing state-\nof-the-art models such as Qwen3-8B. To create Nemotron-Nano-9B-v2, we started by pre-training\nNemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and\nsynthetically generated data. We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT,\nGRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy\nto produce the final model. As a result of this compression, Nemotron-Nano-9B-v2 can run inference\non context lengths of up to 128k tokens in bfloat16 precision on a single NVIDIA A10G GPU with\n22 GiB of memory. We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling\nNemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of\nits pre- and post-training data on HuggingFace (links at the bottom of Section 1).\nContributors\nWe thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2.\nData. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick,\nDan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak\nScowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov,\nMehrzad Samadi, Miguel Martinez, Mohammad Shoeybi, Mostofa Patwary, Pavlo Molchanov, Pritam\nGundecha, Rabeeh Karimi Mahabadi, Ranjit Rajan, Rima Shahbazyan, Sanjeev Satheesh, Sarah\nYurick, Sean Narenthiran, Seungju Han, Shizhe Diao, Shrimai Prabhumoye, Shubham Toshniwal,\nSiddhartha Jain, Somshubra Majumdar, Syeda Nahida Akter, Vahid Noroozi, Vineeth Kalluru,\nVitaly Kurin, Wasi Uddin Ahmad, Wei Du, Ximing Lu, Yejin Choi, Ying Lin.\nFP8. Hua Huang, Jinze Xue, Keith Wyss, Kunlun Li, Mike Chrzanowski, Oleg Rybakov, Przemek\nTredak, Tim Moon, Zhongbo Zhu.\nArchitecture. Bita Darvish Rouhani, Brandon Norick, Duncan Riach, Nidhi Bhatia, Roger Waleffe,\nWonmin Byeon, Ritika Borkar, Xin Dong, Yonggan Fu.\nPretraining. Aarti Basant, Abhijit Paithankar, Abhinav Khattar, Deepak Narayanan, Herman\nSahota, Hexin Wang, Jupinder Parmar, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja,\nRoger Waleffe, Russell J. Hewett, Ryan Prenger, Seonmyeong Bak.\nInfrastructure. Alex Kondratenko, Alex Shaposhnikov, Anubhav Mandarwal, Ashwin Poojary,\nDong Ahn, Gargi Prasad, Haim Elisha, Harsh Sharma, Kumar Anik, Maer Rodrigues de Melo, Ruoxi\nZhang, Shelby Thomas, Stefania Alborghetti, Tony Wang.\nLong Context. Deepak Narayanan, Dima Rekesh, Duncan Riach, John Kamalu, Kezhi Kong,\nMarkus Kliegl, Roger Waleffe, Samuel Kriman.\nInference. Daniel Afrimi, Helen Ngo, Keshav Santhanam, Kushan Ahmadian, Lawrence McAfee,\nLuis Vega, Nave Assaf, Peter Dykas, Shanmugam Ramasamy, Siddharth Singh, Tomer Asida, Vijay\nKorthikanti.\nAlignment. Adithya Renduchintala, Alexander Bukharin, Ameya Sunil Mahabaleshwarkar, Banghua\nZhu, Bilal Kartal, Brian Yu, Charles Wang, Christian Munley, David Mosallanezhad, Gerald Shen,\nHaifeng Qian, Hayley Ross, Hoo Chang Shin, Igor Gitman, Jian Zhang, Jiaqi Zeng, Julien Veron\n23\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nVialard, Junkeun Yi, Kezhi Kong, Luis Vega, Makesh Narsimhan Sreedhar, Oleksii Hrinchuk, Oleksii\nKuchaiev, Peter Jin, Prasoon Varshney, Ritu Gala, Shuoyang Ding, Soumye Singhal, Tugrul Konuk,\nVenkat Srinivasan, Vitaly Lavrukhin, Yian Zhang, Yoshi Suhara, Zhen Dong, Zijia Chen.\nCompression. Aditya Malte, Akhiad Bercovich, Akshay Hazare, Ali Taghibakhshi, Ameya Sunil\nMahabaleshwarkar, Ashwath Aithal, Banghua Zhu, Daniel Korzekwa, Deepak Narayanan, Gerald\nShen, Hayley Ross, Julien Veron Vialard, Luis Vega, Marcin Chochowski, Mostofa Patwary, Nima\nTajbakhsh, Oluwatobi Olabiyi, Pavlo Molchanov, Ran El-Yaniv, Roger Waleffe, Saurav Muralidharan,\nSepehr Sameni, Sharath Turuvekere Sreenivas, Tomer Asida, Yashaswi Karnati, Yian Zhang, Yoshi\nSuhara, Zijia Chen.\nSoftware Support. Abhijit Khairnar, Adithya Renduchintala, Ali Taghibakhshi, Anna Shors,\nAshwath Aithal, Balaram Buddharaju, Bobby Chen, Charlie Truong, Deepak Narayanan, Dmytro\nPykhtar, Duncan Riach, Gerald Shen, Helen Ngo, Jared Casper, Jimmy Zhang, Keshav Santhanam,\nKezhi Kong, Lawrence McAfee, Luis Vega, Nima Tajbakhsh, Parth Chadha, Piotr Bialecki, Prashant\nGaikwad, Rajen Patel, Roger Waleffe, Sahil Jain, Terry Kong, Tyler Poon, Vijay Korthikanti, Vikram\nFugro, Yoshi Suhara, Zhiyu Li.\nEvaluations and Safety.\nChristopher Parisien, Dan Su, Daniel Rohrer, Eileen Long, Erick\nGalinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta\nStepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien,\nMichal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona\nGhosh, Tomasz Hliwiak.\nLegal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham,\nKatherine Cheung, Laya Sleiman, Meredith Price, Michael Boone, Nikki Pope, Ria Cheruvu, Saori\nKaji.\nMarketing. Amelia Barton, Chris Alexiuk, Mark Cai, Nirmal Kumar Juluru, Shreya Gopal.\nProject Management. Alejandra Rico, Amy Shen, Ann Guan, Ashton Sharabiani, Elliott Ning,\nKrzysztof Pawelec, Negar Habibi, Twinkle Vashishth.\nProduct. Arun Venkatesan, Chintan Patel, Chris Alexiuk, Joey Conway, Padmavathy Subramanian,\nUdi Karpas.\nLeadership. Andrew Tao, Boris Ginsburg, Bryan Catanzaro, Eric Chung, Jan Kautz, Joey Conway,\nJonathan Cohen, Kari Briski, Mohammad Shoeybi, Mostofa Patwary, Oleksii Kuchaiev, Pavlo\nMolchanov.\nWe also thank Chen Zhang, Michael Goin, Thomas Parnell from the vLLM team for their assistance.\n24\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nReferences\nGretel synthetic safety alignment dataset, 12 2024. URL https://huggingface.co/datasets/\ngretelai/gretel-safety-alignment-en-v1.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical\nreport. arXiv preprint arXiv:2412.08905, 2024.\nWasi Uddin Ahmad, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Vahid Noroozi, Somshubra\nMajumdar, and Boris Ginsburg. Opencodeinstruct: A large-scale instruction tuning dataset for\ncode llms. arXiv preprint arXiv:2504.04030, 2025a.\nWasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha\nJain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data\ndistillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025b.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-\npoints, 2023. URL https://arxiv.org/abs/2305.13245.\nSyeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro. MIND: Math Informed syNthetic Dialogues\nfor Pretraining LLMs, 2024. URL https://arxiv.org/abs/2410.12881.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis\nTunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua\nLochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher,\nHaojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf.\nSmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model, 2025. URL\nhttps://arxiv.org/abs/2502.02737.\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and\nextraction, 2024. URL https://arxiv.org/abs/2309.14316.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with\nLarge Language Models, 2021. URL https://arxiv.org/abs/2108.07732.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL\nhttps://arxiv.org/abs/2502.13923.\nAdrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Dis-\ncovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing: System Demonstrations, pp. 122–131. Association for Computational\nLinguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.\nAkhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah,\nIdo Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi\nKoren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny,\n25\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nRan Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and\nRan El-Yaniv.\nPuzzle: Distillation-Based NAS for Inference-Optimized LLMs, 2024.\nURL\nhttps://arxiv.org/abs/2411.19146.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido\nGalil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas,\nRan Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk,\nGerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia\nChen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei\nJia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander\nFicek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du,\nShubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman,\nEvelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl,\nRabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon\nNorick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav\nKhattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry\nKong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky,\nRobert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen,\nManoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka\nDong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro\nLarroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla,\nMuthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury,\nOmri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon\nDerczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo\nRibalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala\nPrayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan\nCatanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron:\nEfficient reasoning models, 2025a. URL https://arxiv.org/abs/2505.00949.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil,\nZach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models.\narXiv preprint arXiv:2505.00949, 2025b.\nJanek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search\nEngine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi,\nand Benjamin Piwowarski (eds.), Advances in Information Retrieval. 40th European Conference\non IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York,\nMarch 2018. Springer.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about\nPhysical Commonsense in Natural Language, 2019. URL https://arxiv.org/abs/1911.11641.\nAndrei Z Broder. Identifying and filtering near-duplicate documents. In Annual symposium on\ncombinatorial pattern matching, pp. 1–10. Springer, 2000.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, et al. Evaluating Large Language Models Trained on Code, 2021. URL https://arxiv.\norg/abs/2107.03374.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Advances in Neural Information Processing\nSystems (NeurIPS), NIPS ’17, 2017.\n26\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge. ArXiv, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training Verifiers to Solve Math Word Problems, 2021. URL https://arxiv.org/\nabs/2110.14168.\nTri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality, 2024. URL https://arxiv.org/abs/2405.21060.\nGemma Team @ Google DeepMind. Gemma 3 Technical Report, 2025. URL https://arxiv.org/\nabs/2503.19786.\nDeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\n2025a. URL https://arxiv.org/abs/2501.12948.\nDeepSeek-AI. DeepSeek-V3 Technical Report, 2025b. URL https://arxiv.org/abs/2412.19437.\nIstván Endrédy and Attila Novák. More effective boilerplate removal-the goldminer algorithm.\nPolibits, 48:79–83, 12 2013. doi: 10.17562/PB-48-10.\nSteven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, and\nBryan Catanzaro. Maximize Your Data’s Potential: Enhancing LLM Accuracy with Two-Phase\nPretraining, 2024. URL https://arxiv.org/abs/2412.15285.\nShaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian\nRebedea, Jibin Rajan Varghese, and Christopher Parisien. AEGIS2.0: A diverse AI safety dataset\nand risks taxonomy for alignment of LLM guardrails. In Luis Chiruzzo, Alan Ritter, and Lu Wang\n(eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp.\n5992–6026, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics.\nISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.306. URL https://aclanthology.\norg/2025.naacl-long.306/.\nXiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo\nHuang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal\npre-training for enhanced mathematical reasoning, 2024. URL https://arxiv.org/abs/2409.\n12568.\nAdib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak resistance\nin aligned llms without fine-tuning. arXiv preprint arXiv:2401.10862, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring Massive Multitask Language Understanding, 2021a. URL https:\n//arxiv.org/abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, 2021b.\nURL https://arxiv.org/abs/2103.03874.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\nmodels? arXiv preprint arXiv:2404.06654, 2024.\n27\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng,\nYewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao,\nChenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai\nli, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models\nwith scalable training strategies. In First Conference on Language Modeling, 2024. URL https:\n//openreview.net/forum?id=3X2L2TFr0f.\nSiming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu,\nChenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang,\nZili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code\nlarge language models, 2025. URL https://arxiv.org/abs/2411.04905.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of\ndimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,\npp. 604–613, 1998.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion\nStoica. From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline, April 2024a.\nURL https://lmsys.org/blog/2024-04-19-arena-hard/.\nXuehai Li, Zi Ye, Xiaoxin Zhang, Xinshi Lu, Yingqiang Xia, Bairu Wu, Shihan Dong, Qipeng Jin,\nJialu Wang, Heng Ji, et al. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint\narXiv:2405.01470, 2024b.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A Hybrid Transformer-Mamba\nLanguage Model, 2024. URL https://arxiv.org/abs/2403.19887.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050, 2023.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146,\n2017.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A\nchallenge dataset for machine reading comprehension with logical reasoning. arXiv preprint\narXiv:2007.08124, 2020.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by\nChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.\narXiv preprint arXiv:2305.01210, 2023. doi: https://doi.org/10.48550/arXiv.2305.01210. URL\nhttps://arxiv.org/abs/2305.01210.\nZuxin Liu et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920,\n2024.\n28\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane\nTazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis\nKocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil\nPaul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii\nZheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli\nHe, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham\nOblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan\nHui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu,\nTorsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,\nMostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2\nand the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173.\nWeidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv: A benchmark for\nassessing the robustness of multimodal large language models against jailbreak attacks. arXiv\npreprint arXiv:2404.03027, 2024.\nRabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad\nShoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math\npretraining dataset, 2025. URL https://arxiv.org/abs/2508.15096.\nSomshubra Majumdar, Vahid Noroozi, Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek,\nWasi Uddin Ahmad, Jocelyn Huang, Jagadeesh Balam, and Boris Ginsburg. Genetic instruct:\nScaling up synthetic generation of coding instructions for large language models. arXiv preprint\narXiv:2407.21077, 2024.\nLlama Team @ Meta. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.\n21783.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering, 2018. URL https://arxiv.\norg/abs/1809.02789.\nIvan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schif-\nferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical\nreasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa\nPatwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact\nLanguage Models via Pruning and Knowledge Distillation, 2024. URL https://arxiv.org/abs/\n2407.14679.\nNVIDIA. Nemotron-4 340B Technical Report, 2024. URL https://arxiv.org/abs/2406.11704.\nNVIDIA. Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models, 2025.\nURL https://arxiv.org/abs/2504.03624.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\n29\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian,\nDan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa,\nJiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel\nMartinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath\nAithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-\n4 15B Technical Report. arXiv preprint arXiv:2402.16819, 2024. URL https://arxiv.org/abs/\n2402.16819.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An Open\nDataset of High-Quality Mathematical Web Text, 2023.\nGuilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Amir Hossein\nKargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. Fineweb2: One\npipeline to scale them all – adapting pre-training data processing to every language, 2025. URL\nhttps://arxiv.org/abs/2506.20920.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra,\nAdam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry\nDodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes,\nMobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav\nKazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou,\nZihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon,\nYongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala,\nNoah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney,\nAntrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng,\nJennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben\nMcCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui\nLi, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G.\nWillcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward\nVendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow,\nNatanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis\nEfremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod,\nGözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang,\nPaolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin\nImperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel\nLoiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad\nHogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov,\nPhilippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den\nHoute, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust,\nBikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong\nYang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu,\nAriel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner,\nJames Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene,\nJoseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan,\nSergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova,\nDaniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster,\nDaniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes,\nAlexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider,\nZakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias\nKreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger,\n30\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nKaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov,\nVáclav Rozhoň, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Poświata, Josef Tkadlec, Alan\nGoldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan\nStendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek\nShukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow,\nHao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K Wang, Kalyan\nRamakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi,\nEthan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw,\nJP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam,\nHieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael\nKrause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon\nLee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik\nKirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie,\nAnna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani,\nShreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob\nPlatnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson,\nMarco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu,\nHannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William\nAlley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob\nLoader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida\nBosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes,\nJeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane\nDurand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff,\nLynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh\nShah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee,\nRobin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt,\nJiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak\nPradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz\nFirsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones,\nShashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K.\nZhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan,\nAndrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed,\nJulian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska,\nClaudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy\nManik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David\nStap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo\nRodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna\nSamuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough\nMohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez,\nDaniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu,\nMike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira,\nSimon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank\nReidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung\nKim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon\nPark, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael\nKirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan,\nKrzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua\nRobinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin\nWhite, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M.\n31\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin\nYong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo\nAlbani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier,\nLawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel\nObikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara\nPopescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew\nBrooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua\nNewbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover,\nTing Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder\nde Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent\nCheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie\nHausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang,\nDavid Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik,\nAaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul,\nMohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna\nLiakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth\nAnderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh,\nWentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson,\nMohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José\nMoyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier,\nOmid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali\nM. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh\nDucey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu,\nJack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah,\nMarc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark,\nAssaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel\nPoesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam,\nJuan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel\nBugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti,\nAbdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal,\nMohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry\nMalishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume,\nWiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo,\nJakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa\nGonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo\nJiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader\nDendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca\nArnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony\nFruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan,\nInnocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach,\nChris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John\nLai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling\nDuclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith\nKrenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P\nV, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubić,\nSamuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella,\nAlex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang,\nJason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha,\nYinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng\n32\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał\nPerełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa\nNguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario\nAbbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del\nRio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny\nReddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd,\nThom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt,\nSatyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar\nShridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang,\nAdam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena,\nXing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming\nYin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez,\nCostin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin\nBriański, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-\nOrallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen,\nAlexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan\nTodoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther\nYap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi\nWang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira\nArrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam\nBouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas\nSubramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim,\nYushun Chen, Sara Vera Marjanović, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen,\nDawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica\nWeng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony\nGitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin,\nPhilipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan,\nJun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang,\nIsha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao\nZheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen,\nDeepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gonçalves,\nWei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline\nGeirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon\nChristof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac\nPark, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng,\nZhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang,\nBruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang,\nMarc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying\nLiu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe,\nHongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset,\nZishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia\nChernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park,\nHieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui\nPan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W.\nBartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish\nCheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay\nPaek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent\nCheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath,\nViolet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin\n33\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny\nSun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert\nYang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang,\nAndrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David\nSun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan\nZhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks.\nHumanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249.\nAkshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Thai Lan, Shirley Kokane,\nJuntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese,\nJuan Carlos Niebles, Shelby Heinecke, Huan Wang, and et al. Apigen-mt: Agentic pipeline for\nmulti-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601,\n2025.\nQwen. Qwen2.5 Technical Report, 2025. URL https://arxiv.org/abs/2412.15115.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023.\nHayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara. When2Call: When (not) to\ncall tools. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), pp. 3391–3409, Albuquerque, New Mexico, April 2025. Association for Computational\nLinguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.174/.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale, 2019. URL https://arxiv.org/abs/1907.\n10641.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. Deepseekmath: Pushing\nthe limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300,\n2024.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\nare multilingual chain-of-thought reasoners, 2022. URL https://arxiv.org/abs/2210.03057.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al.\nGlobal mmlu: Understanding and addressing cultural and linguistic biases in multilingual evalua-\ntion. arXiv preprint arXiv:2412.03304, 2024a.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond\nNg, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T.\nMartins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and\nSara Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in\nmultilingual evaluation, 2024b. URL https://arxiv.org/abs/2412.03304.\n34\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDavid R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer:\nSearching for Efficient Transformers for Language Modeling, 2022. URL https://arxiv.org/\nabs/2109.08668.\nSharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil\nMahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan\nYu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel\nKorzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, and Bryan\nCatanzaro. LLM Pruning and Distillation in Practice: The Minitron Approach, 2024. URL\nhttps://arxiv.org/abs/2408.11796.\nOlly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, and Bertie Vidgen.\nWorkbench: a benchmark dataset for agents in a realistic workplace setting. arXiv preprint\narXiv:2405.00823, 2024. doi: 10.48550/arXiv.2405.00823.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a\nrefined long-horizon pretraining dataset. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2459–2475, Vienna, Austria, July\n2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.\nacl-long.123. URL https://aclanthology.org/2025.acl-long.123/.\nAli Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi\nKarnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi\nOlabiyi, et al. Efficient hybrid language model compression through group-aware ssm pruning.\narXiv preprint arXiv:2504.11409, 2025.\nMinyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland\nHaas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong,\nKha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu,\nKilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu\nHuerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024. URL\nhttps://arxiv.org/abs/2407.13168.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert\nGu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh,\nJared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An Empirical Study of\nMamba-based Language Models, 2024. URL https://arxiv.org/abs/2406.07887.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024. URL\nhttps://arxiv.org/abs/2212.03533.\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan\nDuan. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 30:2201–2216, 2022.\n35\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nZhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin,\nEllie Evans, Yi Dong, and Oleksii Kuchaiev. Helpsteer3-preference: Open human-annotated\npreference data across diverse tasks and languages. arXiv preprint arXiv:2505.11475, 2025.\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig\nSchmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without\nincreasing inference time, 2022. URL https://arxiv.org/abs/2203.05482.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and\nJoseph E. Gonzalez. Berkeley Function Calling Leaderboard. 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\nbenchmark and contamination for language models with rephrased samples, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine\nReally Finish Your Sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\nBiao Zhang and Rico Sennrich.\nRoot Mean Square Layer Normalization, 2019.\nURL https:\n//arxiv.org/abs/1910.07467.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Yonghao Li, Zhuohan Chen, Zhewei Wong, Siyuan\nZhuang, Yakun Shao, Kai Xu, Zhenyu Zhang, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2309.11998, 2023.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian\nYin, Ming Zhou, and Nan Duan. Analytical reasoning of text. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pp. 2306–2319, 2022.\nFan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong\nLiu, and Eric P Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint\narXiv:2504.02807, 2025.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint\narXiv:2311.07911, 2023.\nA. Permissive Source Code Licenses\nWe remove source code with a license not in the following list:\n36\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3Com Microcode 3com-microcode, 3D Slicer License 1.0 [3dslicer-1.0], 4Suite 1.1 [4suite-1.1], AAL\n[attribution], Abstyles License [abstyles], ACE TAO License [ace-tao], AdaCore Doc License [adacore-\ndoc], ADI BSD [adi-bsd], Adobe Glyph License [adobe-glyph], Adobe Postscript AFM License\n[apafml], Adobe Source Code License 2006 [adobe-scl], AES-128 3.0 License [aes-128-3.0], AFL\n1.1 [afl-1.1], AFL 1.2 [afl-1.2], AFL 2.0 [afl-2.0], AFL 2.1 [afl-2.1], AFL 3.0 [afl-3.0], afmparse\nLicense [afmparse], Agere BSD [agere-bsd], Alexisisaac Freeware License [alexisisaac-freeware],\nAllegro 4 License [allegro-4], Altera License [xnet], Amazon Digital Services License [adsl], AMD\nHistorical License [amd-historical], AMD PLPA License [amdplpa], AMPAS BSD-Style License\n[ampas], AMSFonts license [ams-fonts], Andre Adrian DFS license [adrian], ANTLR-PD [antlr-\npd], ANTLR-PD with fallback [antlr-pd-fallback], ANU License [anu-license], Apache 1.0 [apache-\n1.0], Apache 1.1 [apache-1.1], Apache 2.0 [apache-2.0], Apache Patent Provision Exception Terms\n[apache-patent-exception], App::s2p License [app-s2p], Apple Attribution 1997 [apple-attribution-\n1997], Apple Attribution License [apple-attribution], Apple Example Code License [apple-excl],\nApple MIT License [aml], Apple Sample Source Code License [apple-sscl], Aravindan Premkumar\nLicenase [aravindan-premkumar], ArgoUML License [argouml], ARM LLVM Grant [arm-llvm-sga],\nArray Input Method Public License [array-input-method-pl], Artistic 1.0 [artistic-1.0], Artistic 1.0\nw/clause 8 [artistic-1.0-cl8], Artistic 2.0 [artistic-2.0], Artistic-Perl-1.0 [artistic-perl-1.0], ASMUS\nLicense [asmus], ASN.1 Object Dumping Code License [asn1], Atkinson Hyperlegible Font License\n[atkinson-hyperlegible-font], Baekmuk Fonts License [baekmuk-fonts], Bahyph License [bahyph],\nBaKoMa Fonts Licence 1995 [bakoma-fonts-1995], Barr TeX License [barr-tex], BEA 2.1 [bea-2.1],\nBeal Screamer License [beal-screamer], Beer-Ware License [beerware], BERI Hardware-Software\nLicense v1.0 [beri-hw-sw-1.0], BigDigits License [bigdigits], Bigelow & Holmes Lucida Fonts License\n[bigelow-holmes], Biopython License [biopython], Bitstream Vera Font License [bitstream], Bitzi-PD\n[bitzi-pd], BLAS License 2017 [blas-2017], Blue Oak Model License 1.0.0 [blueoak-1.0.0], BOHL-\n0.2 [bohl-0.2], Boost 1.0 [boost-1.0], Boost Original [boost-original], Borceux License [borceux],\nBoutell libgd declarations 2021 [boutell-libgd-2021], bpmn.io License [bpmn-io], Brent Corkum\nLicense [brent-corkum], Brian Clapper License [brian-clapper], Brian Gladman 3-Clause License\n[brian-gladman-3-clause], Brian Gladman Dual BSD-GPL [brian-gladman-dual], Brian Gladman\nLicense [brian-gladman], Broadcom CFE License [broadcom-cfe], Broadcom Warranty Disclaimer\n[broadcom-linux-timer], Brocade Firmware License [brocade-firmware], Bruno Podetti License [bruno-\npodetti], BSD 1988 [bsd-1988], BSD 3-Clause Devine [bsd-3-clause-devine], BSD 3-Clause FDA\n[bsd-3-clause-fda], BSD 3-Clause jtag [bsd-3-clause-jtag], BSD 3-Clause No Change [bsd-3-clause-no-\nchange], BSD 3-Clause No Nuclear Warranty [bsd-3-clause-no-nuclear-warranty], BSD 3-Clause no\ntrademark [bsd-3-clause-no-trademark], BSD 3-Clause Open MPI variant [bsd-3-clause-open-mpi],\nBSD 3-Clause Sun [bsd-3-clause-sun], BSD 3-Clause with GPL reference [bsd-top-gpl-addition],\nBSD Acknowledgment (Carrot2) License [bsd-ack-carrot2], BSD Acknowledgment License [bsd-\nack], BSD Advertising Acknowledgement License [bsd-advertising-acknowledgement], BSD Artwork\n[bsd-artwork], BSD Atmel License [bsd-atmel], BSD DPT [bsd-dpt], BSD plus modification notice\n[bsd-plus-mod-notice], BSD Simplified Darwin [bsd-simplified-darwin], BSD Source Code Attribution\n[bsd-source-code], BSD Unchanged [bsd-unchanged], BSD Unmodified [bsd-unmodified], BSD Zero\nClause License [bsd-zero], BSD-1-Clause [bsd-1-clause], BSD-1-Clause Build [bsd-1-clause-build],\nBSD-2-Clause [bsd-simplified], BSD-2-Clause no disclaimer [bsd-no-disclaimer], BSD-2-Clause no\ndisclaimer Unmod [bsd-no-disclaimer-unmodified], BSD-2-Clause Plus Patent [bsd-plus-patent], BSD-\n2-Clause-plus-advertizing [bsd-2-clause-plus-advertizing], BSD-2-Clause-Views [bsd-2-clause-views],\nBSD-3-Clause [bsd-new], BSD-3-Clause tcpdump variant [bsd-new-tcpdump], BSD-3-Clause without\nnotice modification [bsd-new-nomod], BSD-3-Clause X11 disclaimer [bsd-x11], BSD-4-Clause with\nVoices [bsd-original-voices], BSD-4-Clause-Shortened [bsd-4-clause-shortened], BSD-Axis without\nmodification [bsd-axis-nomod], BSD-Credit [bsd-credit], BSD-Derivative [bsd-new-derivative], BSD-\nExport [bsd-export], BSD-InnoSys [bsd-innosys], BSD-Mylex [bsd-mylex], BSD-Original [bsd-original],\n37\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nBSD-Original-Muscle [bsd-original-muscle], BSD-Original-UC [bsd-original-uc], BSD-Original-UC-\n1986 [bsd-original-uc-1986], BSD-Simplified Intel [bsd-simplified-intel], BSD-Simplified source [bsd-\nsimplified-source], BSD-Top [bsd-top], BSLA [bsla], BSLA no advertizing [bsla-no-advert], Business\nSource License 1.0 [bsl-1.0], BYTEmark License [bytemark], bzip2 License 2010 [bzip2-libbzip-2010],\nCaldera License [caldera], Careware [careware], Carnegie Mellon Contributors [carnegie-mellon-\ncontributors], Carnegie Mellon License [carnegie-mellon], Cavium malloc License [cavium-malloc],\nCC-BY-1.0 [cc-by-1.0], CC-BY-2.0 [cc-by-2.0], CC-BY-2.0-UK [cc-by-2.0-uk], CC-BY-2.5 [cc-by-\n2.5], CC-BY-3.0 [cc-by-3.0], CC-BY-3.0-AT [cc-by-3.0-at], CC-BY-3.0-US [cc-by-3.0-us], CC-BY-4.0\n[cc-by-4.0], CC-PD [cc-pd], CC-PD Mark 1.0 [cc-pdm-1.0], CC0-1.0 [cc0-1.0], CDLA Permissive\n1.0 [cdla-permissive-1.0], CDLA Permissive 2.0 [cdla-permissive-2.0], CeCILL-B License [cecill-b],\nCeCILL-B License English [cecill-b-en], CERN Attribution 1995 [cern-attribution-1995], CERN\nOpen Hardware Licence v1.2 [cern-ohl-1.2], CERN Open Hardware License v1.1 [cern-ohl-1.1],\nCERN-OHL-P-2.0 [cern-ohl-p-2.0], CFITSIO License [cfitsio], Checkmk License [checkmk], Chicken\nDance License v0.2 [chicken-dl-0.2], Chris Maunder License [chris-maunder], Chris Stoy Attribution\nLicense [chris-stoy], Clarified Artistic License [artistic-clarified], Classic VB License [classic-vb], Clear\nBSD 1-Clause License [clear-bsd-1-clause], Clear BSD License [clear-bsd], Click License [click-license],\nCLIPS License 2017 [clips-2017], CMU Computing Services License [cmu-computing-services], CMU\nLicense [cmu-template], CMU MIT-style [cmu-mit], CMU Simple License [cmu-simple], CMU Style\n[cmu-uc], CNRI Jython License [cnri-jython], CNRI Python 1.6 [cnri-python-1.6], CNRI Python\n1.6.1 [cnri-python-1.6.1], Code Credit License v1.0.1 [code-credit-license-1.0.1], Code Credit License\nv1.1.0 [code-credit-license-1.1.0], CodeGuru Permissions [codeguru-permissions], CodeSourcery 2004\n[codesourcery-2004], COIL-1.0 [coil-1.0], Common Lisp LOOP License [loop], CommonJ Timer\nLicense [commonj-timer], Compass License [compass], ComponentAce JCraft License [componentace-\njcraft], compuphase Linking Exception to Apache 2.0 [compuphase-linking-exception], Condor Public\nLicense 1.1 [condor-1.1], Copyheart [copyheart], Cornell Lossless JPEG License [cornell-lossless-jpeg],\nCougaar Open Source License [cosl], CP/M License 2022 [cpm-2022], CppCoreGuidelines License\n[cpp-core-guidelines], CRCalc license [crcalc], Creative Commons Attribution 2.5 Australia [cc-by-\n2.5-au], Creative Commons Attribution 3.0 Germany [cc-by-3.0-de], Creative Commons Attribution\n3.0 Netherlands [cc-by-3.0-nl], Crossword License [crossword], Crypto++ License [cryptopp], Crystal\nStacker License [crystal-stacker], CSL-1.0 [csl-1.0], CSPRNG [csprng], Cube License [cube], cURL\nLicense [curl], CVE ToU [cve-tou], CWE ToU [cwe-tou], CxImage License [cximage], D Zlib [d-zlib],\nDAMAIL [damail], Dante Treglia License [dante-treglia], DBAD License 1.1 [dbad-1.1], Debian\nreportbug License [reportbug], Delorie Historical License [delorie-historical], dhtmlab Public License\n[dhtmlab-public], diffmark License [diffmark], dl-de/by-1-0-de [dl-de-by-1-0-de], dl-de/by-1-0-en\n[dl-de-by-1-0-en], dl-de/by-2-0-de [dl-de-by-2-0-de], dl-de/by-2-0-en [dl-de-by-2-0-en], dmalloc License\n[dmalloc], DMTF License 2017 [dmtf-2017], Docbook License [docbook], Dom4j License [dom4j],\nDotseqn License [dotseqn], Douglas Young License [douglas-young], DRL-1.0 [drl-1.0], DRL-1.1\n[drl-1.1], Dropbear License [dropbear], Dropbear-2016 [dropbear-2016], DSDP License [dsdp], Dtree\nLicense [dtree], dvipdfm License [dvipdfm], DWTFNMFPL-3.0 [dwtfnmfpl-3.0], Dynamic Drive TOU\n[dynamic-drive-tou], ECL 1.0 [ecl-1.0], ECL 2.0 [ecl-2.0], EFL 1.0 [efl-1.0], EFL 2.0 [efl-2.0], EFL\nMIT-Style License [enlightenment], eGenix Public License 1.0.0 [egenix-1.0.0], eGenix Public License\n1.1.0 [egenix-1.1.0], EllisLab License [ellis-lab], EMX Library License [emx-library], EnergyPlus\nBSD-Style License [energyplus-bsd], Enhanced MIT License [emit], enna License [enna], Entessa 1.0\n[entessa-1.0], ePaperPress License [epaperpress], EPICS Open License [epics], Eric Glass License\n[eric-glass], Errbot exception [errbot-exception], Etalab Open License 2.0 [etalab-2.0], Etalab Open\nLicense 2.0 English [etalab-2.0-en], EU DataGrid Software License [eu-datagrid], Fabien Tassin\nLicense [fabien-tassin], Fair License [fair], FAL 1.3 [free-art-1.3], Far Manager exception to BSD-\n3-Clause [far-manager-exception], FASTBuild License 2012-2020 [fastbuild-2012-2020], FastCGI\nDevKit [fastcgi-devkit], FastCGI License for Spec Implementation [openmarket-fastcgi], FatFs\n38\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nLicense [fatfs], FFTPACK License 2004 [fftpack-2004], Filament Group MIT License [filament-\ngroup-mit], Flex 2.5 [flex-2.5], Flora License v1.1 [flora-1.1], font-alias License [font-alias], FPLOT\nLIcense [fplot], Fraunhofer ISO 14496-10 License [fraunhofer-iso-14496-10], FreeBSD Boot [freebsd-\nboot], FreeBSD Doc License [freebsd-doc], FreeBSD unmodified first lines License [freebsd-first],\nFreeMarker License [freemarker], FreeTTS License [freetts], FreeType Project License [freetype],\nFreeware Public License (FPL) [fpl], FSF All Permissive License [fsf-ap], FSF Free Software License\n[fsf-free], FSF Notice [fsf-notice], FSF Unlimited License No Warranty [fsf-unlimited-no-warranty],\nFSF-Unlimited [fsf-unlimited], Fujion Clinical Exception to Apache 2.0 [fujion-exception-to-apache-\n2.0], Gareth McCaughan License [gareth-mccaughan], Gary S. Brown License [gary-s-brown], GDCL\nLicense [gdcl], Generic patent disclaimer [patent-disclaimer], Geoff Kuenning License 1993 [geoff-\nkuenning-1993], Ghostpdl Permissive [ghostpdl-permissive], Glulxe License [glulxe], GLUT License\n[glut], GLWTPL [glwtpl], Good Boy License [good-boy], Graphics Gems License [graphics-gems],\nGreg Roelofs License [greg-roelofs], Gregory Pietsch Liberal License [gregory-pietsch], GStreamer\nException (2005) [gstreamer-exception-2005], GTPL-v1 [gtpl-v1], GTPL-v2 [gtpl-v2], GTPL-v3 [gtpl-\nv3], Haskell Report License [haskell-report], HDF4 License [hdf4], HDF5License [hdf5], HDPARM\nLicense [hdparm], Henry Spencer License 1999 [henry-spencer-1999], Henry Spencer Regexp License\n[hs-regexp], HIDAPI License [hidapi], Historical Notice - NTP [historical-ntp], Historical Permission\nNotice and Disclaimer [historical], Homebrewed License [homebrewed], HP 1986 License [hp-1986],\nHPND sell variant with MIT disclaimer [hpnd-sell-variant-mit-disclaimer], HTML 5 spec License\n[html5], httpget notice and disclaimer [httpget], Ian Kaplan License [ian-kaplan], Ian Piumarta\nLicense [ian-piumarta], IBM AS-IS License [ibm-as-is], IBM DHCP License [ibm-dhcp], IBM Non-\nWarranted Sample Code License [ibm-nwsc], IBM PowerPC Software [ibm-pibs], IBM Sample License\n[ibm-sample], IBPP License [ibpp], ICANN-Public [icann-public], ICOT Free Software [icot-free],\nICU Composite License [ibm-icu], ICU License 58 and later [unicode-icu-58], IDT License Notice\n[idt-notice], IETF License [ietf], IETF Trust License [ietf-trust], ilmid License [ilmid], ImageMagick\nLicense [imagemagick], Independent JPEG Group License - short [ijg-short], Indiana Extreme\nLicense 1.1.1 [indiana-extreme], Indiana Extreme License 1.2 [indiana-extreme-1.2], Infineon Free\nSoftware License [infineon-free], Info-Zip License 1997-10 [info-zip-1997-10], Info-Zip License 2001-01\n[info-zip-2001-01], Info-Zip License 2002-02 [info-zip-2002-02], Info-Zip License 2003-05 [info-zip-\n2003-05], Info-Zip License 2004-05 [info-zip-2004-05], Info-Zip License 2005-02 [info-zip-2005-02],\nInfo-Zip License 2007-03 [info-zip-2007-03], Info-Zip License 2009-01 [info-zip-2009-01], Info-Zip\nLicense [info-zip], Inno Setup License [inno-setup], Intel ACPI SLA [intel-acpi], Intel BSD - Export\nControl [intel-bsd-export-control], Intel BSD 2 Clause License [intel-bsd-2-clause], Intel BSD License\n[intel-bsd], Intel Limited Patent License [intel], Intel OSL 1989 [intel-osl-1989], Intel OSL 1993\n[intel-osl-1993], Intel Royalty Free License [intel-royalty-free], ISC License [isc], ISO 14496-10 [iso-\n14496-10], ISO 8879 [iso-8879], ITU License [itu], JA-SiG License [ja-sig], Jam License [jam], Jason\nMayes License [jason-mayes], Jasper 1.0 [jasper-1.0], JasPer 2.0 [jasper-2.0], Java App Stub License\n[java-app-stub], JDBM License v1.00 [jdbm-1.00], JDOM License [jdom], Jetty License [jetty], JGraph\nLicense [jgraph], JPEG License [ijg], JPNIC idnkit License [jpnic-idnkit], JPNIC mdnkit License\n[jpnic-mdnkit], JPython 1.1 [jpython-1.1], jQuery-Tools-PD [jquery-pd], Jscheme License [jscheme],\nJSFromHell License [jsfromhell], JSON License [json], JSON-js-PD [json-js-pd], JSON-PD [json-pd],\nJython License [jython], Kalle Kaukonen License [kalle-kaukonen], Kazlib [kazlib], Keith Rule\nLicense [keith-rule], Kerberos License [kerberos], Kevan Stannard License [kevan-stannard], Kevlin\nHenney License [kevlin-henney], Khronos License [khronos], Knuth CTAN License [knuth-ctan],\nKumar Robotics License [kumar-robotics], latex-ec-fonts [ecfonts-1.0], Latex2e License [latex2e],\nLatex2e with translated notice permission [latex2e-translated-notice], LBNL BSD Variant [lbnl-\nbsd], LCS-Telegraphics License [lcs-telegraphics], Leptonica License [leptonica], libgd License 2018\n[libgd-2018], libgeoTiff License [libgeotiff], LibMib License [libmib], libmng License 2007 [libmng-\n2007], Libpng License [libpng], LIbpng License v2 [libpng-v2], libselinux License [libselinux-pd],\n39\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nlibsrv License v1.0.2 [libsrv-1.0.2], Lil License v1 [lil-1], LILO License [lilo], Linux Device Drivers\n[linux-device-drivers], Linux-OpenIB [linux-openib], LinuxBIOS License [linuxbios], linuxhowtos\nLicense [linuxhowtos], LLNL [llnl], LLVM Exception to Apache 2.0 [llvm-exception], Logica OSL 1.0\n[logica-1.0], LPPL 1.3c [lppl-1.3c], Lucent Public License 1.0 [lucent-pl-1.0], Lucent Public License\n1.02 [lucent-pl-1.02], Lucre License [lucre], LZMA SDK License (versions 9.22 and beyond) [lzma-sdk-\n9.22], LZMA SDK Public Domain [lzma-sdk-pd], M+ Fonts license [m-plus], MakeHuman License\n[make-human-exception], Markus Kuhn License [markus-kuhn-license], Martin Bergmeier License\n[martin-birgmeier], Matrix Template Library License [mtll], Matt Gallagher Attribution License\n[matt-gallagher-attribution], Matt Kruse License [mattkruse], Matthew Kwan License [matthew-\nkwan], MediaInfo(Lib) License [mediainfo-lib], metamail License [metamail], MgOpen Font License\n[mgopen-font-license], Michael Barr License [michael-barr], Minpack Copyright Notice [minpack],\nMirOS License [mir-os], MIT (SEI) [vince], MIT 1995 [mit-1995], MIT Acknowledgment License\n[mit-ack], MIT Addition License [mit-addition], MIT License 1998 [mit-license-1998], MIT License\n[mit], MIT Modern Variant [mit-modern], MIT Nagy Variant [mit-nagy], MIT no advertising with\nExport Control [mit-no-advert-export-control], MIT No Commercial Use of Trademarks [mit-no-\ntrademarks], MIT no false attribution License [mit-no-false-attribs], MIT Old Style [mit-old-style],\nMIT Old Style no advertising [mit-old-style-no-advert], MIT Old Style Spare [mit-old-style-sparse],\nMIT README License [mit-readme], MIT Synopsys License [mit-synopsys], MIT Taylor Variant\n[mit-taylor-variant], MIT Veillard Variant [mit-veillard-variant], MIT with Export Control [mit-\nexport-control], MIT with Specification Disclaimer [mit-specification-disclaimer], MIT Xfig Variant\n[mit-xfig], MIT-0-Clause [mit-0], mod_dav License 1.0 [mod-dav-1.0], Modified MIT License for\nPublic Domain software [pd-mit], Motorola Microprocessor License [motorola], Mozilla GC License\n[mozilla-gc], MPEG SSG License [mpeg-ssg], MPEG-2 NBC MPEG-4 Audio ISO [mpeg-iso], MPICH\nLicense [mpich], MS Systems Journal Sample Code License [msj-sample-code], MS WS Routing\nSpecifications License [ms-ws-routing-spec], MS-LPL [ms-lpl], MS-PL [ms-pl], MS-SS-PL [ms-sspl],\nMulan PSL v1 [mulanpsl-1.0], Mulan PSL v1.0 (En) [mulanpsl-1.0-en], Mulan PSL v2 [mulanpsl-2.0],\nMulan PSL v2.0 (En) [mulanpsl-2.0-en], Mulle Kybernetik License [mulle-kybernetik], Multics\nLicense [multics], Mup License [mup], musl attribution exception [musl-exception], MX4J License\n1.0 [mx4j], Nara Institute License 2003 [naist-2003], NASA 1.3 [nasa-1.3], NAUMEN Public License\n[naumen], NBPL-1.0 [nbpl-1.0], NCBI Public Domain Notice [ncbi], NCSA Open Source License\n[uoi-ncsa], Net SNMP License [net-snmp], Netcat License [netcat], NetCDF License [netcdf], Netron\nProject License [netron], Newlib Historical License [newlib-historical], Newran License [newran],\nNewsletr License [newsletr], Nice License [nice], NICTA Public Software Licence 1.0 [nicta-psl],\nNiels Ferguson License [niels-ferguson], Nilsson Historical License [nilsson-historical], NIST Public\nDomain Notice [nist-pd], NIST Public Domain Notice with fallback [nist-pd-fallback], NIST Software\nLicense [nist-software], NIST SRD License [nist-srd], NLOD-1.0 [nlod-1.0], NLOD-2.0 [nlod-2.0],\nNLPL [nlpl], Node License [node-js], Non White Heterosexual Male [nwhm], Nonexclusive License\n[nonexclusive], Nortel DASA License [nortel-dasa], Notre Dame License [notre-dame], NRL License\n[nrl], NRL permission [nrl-permission], NTLM License [ntlm], NTP Origin License [ntpl-origin],\nNTP-0 [ntp-0], NVIDIA 2002 License [nvidia-2002], NVIDIA License [nvidia], NVIDIA License\nwith Government Qualifications [nvidia-gov], NYSL 0.9982 [nysl-0.9982], NYSL 0.9982 JP [nysl-\n0.9982-jp], O Young Jong License [o-young-jong], O’Reilly Code Sample Notice [oreilly-notice],\nO-UDA-1.0 [o-uda-1.0], Oasis WS Security Specification License [oasis-ws-security-spec], Object\nForm Exception to MIT [object-form-exception-to-mit], ODC-By-1.0 [odc-by-1.0], ODMG License\n[odmg], OFFIS License [offis], OFL 1.0 [ofl-1.0], OFL 1.0 no Reserved Font Name [ofl-1.0-no-\nrfn], OFL 1.0 Reserved Font Name [ofl-1.0-rfn], OFL 1.1 no Reserved Font Name [ofl-1.1-no-rfn],\nOGC 1.0 [ogc-1.0], OGC Software Notice [ogc], OGL 1.0a [ogl-1.0a], OGL Alberta 2.1 [can-ogl-\nalberta-2.1], OGL British Columbia 2.0 [can-ogl-british-columbia-2.0], OGL Canada 2.0 [can-ogl-2.0-\nen], OGL Canada 2.0 Francais [ogl-canada-2.0-fr], OGL Nova Scotia 1.0 [can-ogl-nova-scotia-1.0],\n40\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nOGL Ontario 1.0 [can-ogl-ontario-1.0], OGL Toronto 1.0 [can-ogl-toronto-1.0], OGL-UK-1.0 [ogl-\nuk-1.0], OGL-UK-2.0 [ogl-uk-2.0], OGL-UK-3.0 [ogl-uk-3.0], OGL-WPD-3.0 [ogl-wpd-3.0], Open\nDirectory License [odl], Open Group Test Suite License [opengroup], Open Publication License 1.0\n[openpub], OpenLDAP Public License 1.1 [openldap-1.1], OpenLDAP Public License 1.2 [openldap-\n1.2], OpenLDAP Public License 1.3 [openldap-1.3], OpenLDAP Public License 1.4 [openldap-\n1.4], OpenLDAP Public License 2.0 [openldap-2.0], OpenLDAP Public License 2.0.1 [openldap-\n2.0.1], OpenLDAP Public License 2.1 [openldap-2.1], OpenLDAP Public License 2.2 [openldap-2.2],\nOpenLDAP Public License 2.2.1 [openldap-2.2.1], OpenLDAP Public License 2.2.2 [openldap-\n2.2.2], OpenLDAP Public License 2.3 [openldap-2.3], OpenLDAP Public License 2.4 [openldap-\n2.4], OpenLDAP Public License 2.5 [openldap-2.5], OpenLDAP Public License 2.6 [openldap-\n2.6], OpenLDAP Public License 2.7 [openldap-2.7], OpenLDAP Public License 2.8 [openldap-2.8],\nOpenORB Community License 1.0 [openorb-1.0], OpenSAML License v1 [opensaml-1.0], OpenSSH\nLicense [openssh], OpenSSL License [openssl], OpenSSL/SSLeay License [openssl-ssleay], OPML 1.0\n[opml-1.0], OPNL-1.0 [opnl-1.0], OPNL-2.0 [opnl-2.0], Oracle BSD-Style with Nuclear Restrictions\n[oracle-bsd-no-nuclear], Original SSLeay License [ssleay], Original SSLeay License with Windows\nClause [ssleay-windows], Oswego Concurrent License [oswego-concurrent], Other Permissive Licenses\n[other-permissive], OWTChart License [owtchart], OZPLB 1.0 [ozplb-1.0], OZPLB 1.1 [ozplb-1.1],\nPaolo Messina 2000 [paolo-messina-2000], ParaView License 1.2 [paraview-1.2], Paul Mackerras\nBinary License [paul-mackerras-binary], Paul Mackerras License [paul-mackerras], Paul Mackerras\nNew License [paul-mackerras-new], Paul Mackerras Simplified License [paul-mackerras-simplified],\nPaulo Soares License [paulo-soares], PayPal SDK License 2013-2016 [paypal-sdk-2013-2016], PBM\nLibrary License [libpbm], PCRE License [pcre], PD’Programming License [pd-programming], PDDL\n1.0 [pddl-1.0], Perl 1.0 [perl-1.0], Peter Deutsch Document License [peter-deutsch-document], Phil\nBunce License [phil-bunce], Philippe De Muyter License [philippe-de-muyter], Phorum License 2.0\n[phorum-2.0], PHP License 2.0.2 [php-2.0.2], PHP License 3.0 [php-3.0], PHP License 3.01 [php-3.01],\nPine License [pine], PngSuite License [pngsuite], Politepix Public License 1.0 [politepix-pl-1.0],\nPostgreSQL License [postgresql], ppp License [ppp], Protobuf License [protobuf], PS Utilities License\n[psutils], PSF Python License 3.7.2 [psf-3.7.2], PSF-2.0 [psf-2.0], psfrag License [psfrag], Psytec\nFree Software License [psytec-freesoft], Public Domain [public-domain], Public Domain Disclaimer\n[public-domain-disclaimer], Purdue BSD-Style License [purdue-bsd], pybench License [pybench],\nPyCrypto License [pycrypto], PyGres License 2.2 [pygres-2.2], Python CWI License [python-cwi],\nPython License 2.0 [python], Python License 2.0.1 [python-2.0.1], Qhull License [qhull], QLogic\nMicrocode [qlogic-microcode], Qpopper License [qpopper], Qualcomm Turing License [qualcomm-\nturing], Quirksmode Copyright Notice [quirksmode], radvd License [radvd], Rdisc License [rdisc],\nRed Hat Attribution License [red-hat-attribution], Red Hat BSD-Simplified [red-hat-bsd-simplified],\nRegexp License [regexp], Repoze License [repoze], RiceBSD [ricebsd], Richard Black License [richard-\nblack], Robert Hubley License [robert-hubley], RSA 1990 [rsa-1990], RSA Cryptoki License [rsa-\ncryptoki], RSA Demo License [rsa-demo], RSA-MD4 License [rsa-md4], RSA-MD5 License [rsa-md5],\nRTools.Util License [rtools-util], Ruby License [ruby], Runtime Library Exception to Apache 2.0\n[apple-runtime-library-exception], Rute Users Tutorial and Exposition License 0.8.0 [rute], Ryszard\nSzopa License [ryszard-szopa], SaaS MIT License [saas-mit], Sash Notice [sash], SATA License [sata],\nSAX-PD [sax-pd], Saxpath License [saxpath], SBIA Part B [sbia-b], ScanCode acknowledgment\n[scancode-acknowledgment], scanlogd License [scanlogd-license], ScanSoft Public License 1.2 [scansoft-\n1.2], SCEA Shared Source License 1.0 [scea-1.0], Scheme Language Report License [schemereport],\nScheme Widget Library (SWL) Software License [swl], Scintilla License [scintilla], Scribbles Demos\nRecognizer Notice [scribbles], Script Asylum License [script-asylum], Secret Labs License 2011 [secret-\nlabs-2011], selinux-nsa-declaration-1.0 [selinux-nsa-declaration-1.0], Sendmail License [sendmail],\nService Availability Forum License [saf], Service Component Architecture License [service-comp-arch],\nSFL License Agreement [sfl-license], SGI CID Font Code Public License 1.0 [sgi-cid-1.0], SGI Free\n41\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSoftware License B 1.1 [sgi-freeb-1.1], SGI Free Software License B 2.0 [sgi-freeb-2.0], SGI GLX Public\nLicense 1.0 [sgi-glx-1.0], Sglib License [sglib], SGP4 Permission Notice [sgp4], Shital Shah License\n[shital-shah], SIL Open Font License 1.1 with Reserved Font Name [ofl-1.1-rfn], SimPL 1.1 [simpl-1.1],\nSNMP++ License [hp-snmp-pp], snprintf License [snprintf], SoftFloat [softfloat], SoftFloat Legal\nNotice 2.0 [softfloat-2.0], softSurfer License [softsurfer], SolderPad Hardware License v0.5 [shl-0.5],\nSolderpad Hardware License v2.0 [shl-2.0], Solderpad Hardware License v2.1 [shl-2.1], SolderPad\nHardware License, Version 0.51 [shl-0.51], Sparky License [sparky], SpeechWorks Public License\n1.1 [speechworks-1.1], SQLite Blessing [blessing], Standard ML of New Jersey [standard-ml-nj],\nStanford PVRG License [stanford-pvrg], STLport License 2000 [stlport-2000], STLport License 4.5\n[stlport-4.5], STREAM Benchmark License [stream-benchmark], Stu Nicholls License [stu-nicholls],\nSun RPC License [sun-rpc], Sun source code License [sun-source], SunPro Attribution License\n[sunpro], Sunsoft License [sunsoft], Supervisor License [supervisor], svndiff License [svndiff], SWIG\nLibrary License [swig], Symlinks License [symlinks], Symphonysoft [symphonysoft], Synopsys MIT\nLicense [synopsys-mit], Synthesis Toolkit License [synthesis-toolkit], SystemC Open Source License\nAgreement [accellera-systemc], Taiwan Open Government Data License, version 1.0 [ogdl-taiwan-1.0],\nTakao Abe License [takao-abe], Takuya OOURA License [takuya-ooura], Talis Community License\n[ttcl], Tatu Ylonen License [tatu-ylonen], TCG Spec License v1 [tcg-spec-license-v1], TCL/TK\nLicense [tcl], TCP Wrappers License [tcp-wrappers], TekHVC License [tekhvc], Term Readkey\nLicense [term-readkey], Tested Software License [tested-software], TeX Live License [tex-live], Text-\nTabs+Wrap License [ttwl], TFL [tfl], The Happy Bunny License [happy-bunny], Theodore Ts’o license\n[tso-license], Things I Made (TIM) Public License [things-i-made-public-license], Tidy License [tidy],\nTiger Cryptography License [tiger-crypto], Tigra Calendar 3.2 License [tigra-calendar-3.2], Tigra\nCalendar 4.0 License [tigra-calendar-4.0], Tim Janik License 2003 [tim-janik-2003], Time::ParseDate\nLicense [tpdl], Timestamp Picker License [timestamp-picker], TTYP0 License [ttyp0], TU Berlin\nLicense 1.0 [tu-berlin], TU Berlin License 2.0 [tu-berlin-2.0], Tumbolia Public License [tumbolia],\nTwistedSNMP License [twisted-snmp], UCAR License [ucar], UnboundID LDAP SDK Free Use\nLicense [ldap-sdk-free-use], Unicode DFS 2015 [unicode-dfs-2015], Unicode DFS 2016 [unicode-dfs-\n2016], Unicode Inc License Agreement [unicode], Unicode Mappings License [unicode-mappings],\nUniversity of British Columbia License [ubc], University of Michigan OSL [michigan-disclaimer],\nUNIX Network Programming Book License [unpbook], UnixCrypt License [unixcrypt], Unlicense\n[unlicense], Unlimited Binary Use Exception [unlimited-binary-use-exception], UPL 1.0 [upl-1.0], US\nGovernment Public Domain [us-govt-public-domain], US Government Unlimited Rights [us-govt-\nunlimited-rights], USRobotics Permissive License [usrobotics-permissive], Utopia Typeface License\n[utopia], VCalendar License [vcalendar], Vic Metcalfe Public Domain [vic-metcalfe-pd], VIM License\n[vim], Visual Idiot [visual-idiot], Visual Numerics License [visual-numerics], Vixie Cron License [vixie-\ncron], Vovida Software License 1.0 [vsl-1.0], W3C 3-Clause BSD License [w3c-03-bsd-license], W3C\nSoftware Notice and License [w3c], W3C-SOFTWARE-19980720 [w3c-software-19980720], W3C-\nSOFTWARE-DOC-20150513 [w3c-software-doc-20150513], w3m License [w3m], Westhawk License\n[westhawk], Whistle Communications License [whistle], Whitecat License [whitecat], WIDE License\n[wide-license], Wide Open License [wol], Widget Workshop License [widget-workshop], William\nAlexander License [william-alexander], wingo License [wingo], Wordnet License [wordnet], Wrox Press\nLicense [wrox], WS-Addressing Specification License [ws-addressing-spec], WS-Policy Specification\n[ws-policy-specification], WS-Trust Specification [ws-trust-specification], Wsuipa License [wsuipa],\nWTFNMFPL-1.0 [wtfnmfpl-1.0], WTFPL 1.0 [wtfpl-1.0], WTFPL 2.0 [wtfpl-2.0], WTHPL 1.0\n[wthpl-1.0], wxWidgets Licence [wxwidgets], wxWindows Unrestricted Licence 3.0 [wxwindows-u-3.0],\nX11 Documentation License [x11-doc], X11 License [x11], X11-R5 [x11-x11r5], X11-Style (Acer)\n[x11-acer], X11-Style (Adobe) [x11-adobe], X11-Style (Adobe-DEC) [x11-adobe-dec], X11-Style\n(Bitstream Charter) [x11-bitstream], X11-Style (David R. Hanson) [x11-hanson], X11-Style (DEC\n1) [x11-dec1], X11-Style (DEC 2) [x11-dec2], X11-Style (DSC Technologies) [x11-dsc], X11-Style\n42\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(FSF) [x11-fsf], X11-Style (Keith Packard) [x11-keith-packard], X11-Style (Lucent) [x11-lucent],\nX11-Style (Lucent-variant) [x11-lucent-variant], X11-Style (OAR) [x11-oar], X11-Style (Open Group)\n[x11-opengroup], X11-Style (OpenGL) [x11-opengl], X11-Style (Quarterdeck) [x11-quarterdeck],\nX11-Style (Realmode) [x11-realmode], X11-Style (Silicon Graphics) [x11-sg], X11-Style (Stanford\nUniversity) [x11-stanford], X11-Style (Tektronix) [x11-tektronix], X11-Style (Tiff) [x11-tiff], X11-Style\n(X Consortium Veillard) [x11-xconsortium-veillard], X11-Style (X Consortium) [x11-xconsortium],\nXdebug License v 1.03 [xdebug-1.03], XFree86 License 1.0 [xfree86-1.0], XFree86 License 1.1 [xfree86-\n1.1], xinetd License [xinetd], XML:DB Initiative Software License 1.0 [xmldb-1.0], XSkat License\n[xskat], xxd License [xxd], Yale CAS License [yale-cas], Yensdesign License [yensdesign], Zed License\n[zed], Zend Engine License 2.0 [zend-2.0], ZeusBench notice [zeusbench], ZLIB License [zlib], ZLIB\nLicense with Acknowledgment [zlib-acknowledgement], ZPL 1.0 [zpl-1.0], ZPL 1.1 [zpl-1.1], ZPL\n2.0 [zpl-2.0], ZPL 2.1 [zpl-2.1], zsh License [zsh], Zuora Software License [zuora-software], Zveno\nResearch License [zveno-research]\nThe list above gives the short name (or name, if no short name exists) along with the key, in square\nbrackets, from the ScanCode license dataset available at https://github.com/aboutcode-org/\nscancode-toolkit/tree/develop/src/licensedcode/data/licenses.\n43\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://commoncrawl.org/blog/news-dataset-available",
      "full_text": " Common Crawl - Blog - News Dataset Available The Data Overview Web Graphs Latest Crawl Crawl Stats Graph Stats Errata Resources Get Started AIÂ Agent Blog Examples Use Cases CCBot Infra Status FAQ Community Research Papers Mailing List Archive Hugging Face Discord Collaborators About Team Jobs Mission Impact Privacy Policy Terms of Use Search AIÂ Agent Contact Us &lt; Back to Blog October 4, 2016 News Dataset Available Note: this post has been marked as obsolete. We are pleased to announce the release of a new dataset containing news articles from news sites all over the world. Sebastian Nagel Sebastian is a Distinguished Engineer with Common Crawl. We are pleased to announce the release of a new dataset containing news articles from news sites all over the world. The data is available on AWS S3 in the commoncrawl bucket at crawl-data/CC-NEWS/ . WARC files are released on a daily basis, identifiable by file name prefix which includes year and month. We provide lists of the published WARC files , organized by year and month from 2016 to-date. Alternatively, authenticated AWS users can get listings using the AWS Command Line Interface and the command: aws s3 ls --recursive s3://commoncrawl/crawl-data/CC-NEWS/ The listed WARC files (e.g., crawl-data/CC-NEWS/2016/09/CC-NEWS-20160926211809-00000.warc.gz ) may be accessed in the same way as the WARC files from the main dataset; see how to access and process Common Crawl data . Why a new dataset? News is a text genre that is often discussed on our user and developer mailing list . Yet our monthly crawl and release schedule is not well-adapted to this type of content which is based on developing and current events. By decoupling the news from the main dataset, as a smaller sub-dataset, it is feasible to publish the WARC files shortly after they are written. While the main dataset is produced using Apache Nutch , the news crawler is based on StormCrawler , an open source collection of resources for building low-latency, scalable web crawlers on Apache Storm . Using StormCrawler allows us to test and evaluate a different crawler architecture towards the following long-term objectives: continuously release freshly crawled data incorporate new seeds quickly and efficiently reduce computing costs with constant/ongoing use of hardware. The source code of the news crawler is available on our Github account . Please, report issues there and share your suggestions for improvements with us. Note that the news dataset is released at an early stage in its development: with further iteration, we intend to improve it in both coverage and quality in upcoming months. We are grateful to Julien Nioche ( DigitalPebble Ltd ), who, as lead developer of StormCrawler , had the initial idea to start the news crawl project. Julien provided the first news crawler version for free, and volunteered to support initial crawler setup and testing. This release was authored by: No items found. Erratum:Â Content is truncated Originally reported by:Â Permalink Some archived content is truncated due to fetch size limits imposed during crawling. This is necessary to handle infinite or exceptionally large data streams (e.g., radio streams). Prior to March 2025 (CC-MAIN-2025-13), the truncation threshold was 1 MiB. From the March 2025 crawl onwards, this limit has been increased to 5 MiB. For more details, see our truncation analysis notebook . The Data Overview Web Graphs Latest Crawl Crawl Stats Graph Stats Errata Resources Get Started AIÂ Agent Blog Examples Use Cases CCBot Infra Status FAQ Community Research Papers Mailing List Archive Hugging Face Discord Collaborators About Team Jobs Mission Impact Privacy Policy Terms of Use &copy; 2025 Common Crawl ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://openwebtext2.readthedocs.io/en/latest/",
      "full_text": " OpenWebText2 OpenWebText2 Welcome Download Plug and Play Version Download Raw Scrapes Version Using The Data Cite as WebText Background Dataset Replication Licence OpenWebText2 Docs &raquo; Welcome Welcome! OpenWebText2 is an enhanced version of the original OpenWebTextCorpus covering all Reddit submissions from 2005 up until April 2020, with further months becoming available after the corresponding PushShift dump files are released. In case you haven't heard of WebText, the core principle is extracting URLs from reddit submissions, scraping the URLs, then performing filtering &amp; deduplication. See Background for more information. Download Plug and Play Version This version has already been cleaned for you: Deduplicated by URL Filtered by minimum combined reddit score 3 Deduplicated at document level with MinHashLSH. Stats 17,103,059 documents 65.86 GB uncompressed text 28 GB compressed including text and metadata Download Download Raw Scrapes Version Only deduplicated by URL. Stats 69,547,149 documents 193.89gb uncompressed text. 79gb compressed including text and metadata Download Using The Data The data is stored using lm_dataformat . We use a slightly modified version to allow file peeking for tqdm progress bars: utils/archiver.py . Be sure to call read_jsonl with get_meta=True as both versions contain useful metadata for each document, including several original Reddit fields. import glob import os import math import tqdm from utils.archiver import Reader document_count = 0 total_text_size = 0 dataset_directory = &quot;PATH_TO_FILES&quot; files = glob.glob(os.path.join(dataset_directory, &quot;*jsonl.zst&quot;)) for file_path in tqdm.tqdm(files, dynamic_ncols=True): reader = Reader() for document, metadata in reader.read_jsonl(file_path, get_meta=True): document_count += 1 total_text_size += len(document) billion = math.pow(10, 9) print(f&quot;Total Document Count: {document_count:,}&quot;) print(f&quot;Total Uncompressed Text Size: {(total_text_size / billion):.2f} GB&quot;) Alternatively checkout The-Pile , which acts as an aggregator/dataloader for multiple text datasets. It allows you to configure your total data size requirement, along with the desired weighting for each subset. Once configured, you get a randomized stream of documents, allowing easy feeding to your language model. Cite as @article{pile, title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling}, author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor}, journal={arXiv preprint arXiv:2101.00027}, year={2020} } Next Built with MkDocs using a theme provided by Read the Docs . ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://info.arxiv.org/help/bulk_data/index.html",
      "full_text": " Index - arXiv info | arXiv e-print repository Skip to content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. arXiv info Index Initializing search GitHub arXiv info GitHub Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Home Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> About About About arXiv Who we are Governance Governance arXiv Governance Model Bylaws arXiv Advisory Board Science Advisory Council Editorial Advisory Council Institutions Advisory Council Membership Funding Reports Accessibility arXiv Blog Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Help Help arXiv Help Contents Submit an Article Submit an Article Submission Overview Submit TeX/LaTeX Submit a PDF Legacy Submission System FAQ FAQ Frequently Asked Questions LaTeX2e class for Astronomy & Astrophysics AMS LaTeX packages and AMS Journal styles Downloaded .gz Files that are not Gzipped Why do my citations appear in long form Examples of TeX Double Subscripts Papersize/Layout Problems: Margins are Different and/or Text is Truncated Using Feynmf on arXiv Proprietary fonts and/or their free equivalents How to Prepare Pages for Landscape Printout Common Mistakes that cause Automated Processing to Fail Why do my user defined symbols display incorrectly Why are some pages in the generated PDF file rotated? Why are there Problems with my PostScript File? Why is my PostScript File so Large? Using PSTricks on arXiv References to and in arXiv Documents Status of REVTeX 4 and REVTeX 4.1 support Nota Bene (updated 30 Jun '97) Frequently Asked Questions on Public Statistics Notes about arXiv's TeXLive Version Why doesn't my processed TeX submission look the way I expected it? Textures Why Does My Paper Give the Wrong Date? Why doesn't my paper have the arXiv id stamped on the side of the page? Status Information Ancillary Files (data, code, images) Availability of submissions Category cross listing Endorsement Adding Journal Reference and DOI Licenses Text Overlap Metadata for Required and Optional Fields Submit a new version of a work Oversized Submissions Submit a Paper List for Conference Proceedings Creating tar and zip Files for Upload What is TeX Proxy / Third Party Submission Translations Version Availability Why Submit TeX? Withdraw / Retract a Submission Find and Read Articles Find and Read Articles Finding Articles Search for articles Viewing submissions Email subscriptions for new papers arXiv Identifier Configuring your browser RSS Feeds Licenses Permissions and Reuse What is MathJax? Accounts Accounts Index Directions for Author Registration Author Identifiers ORCID identifiers Authority Records Why isn't a Person \"Registered as an Author?\" Bulk Access to Metadata and Full Text Bulk Access to Metadata and Full Text Index arXiv Identifier for Interacting Services Full Text via S3 API for Metadata RSS Feeds Institutional Repository Interoperability Automated DOI and journal reference updates from publishers Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Brand Brand Our Brand Name and logo use Brand pillars Colors Fonts Images Logos Quotes Swag Tagline Typography Voice Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Labs Labs arXivLabs: An invitation to collaborate arXivLabs: Showcase Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Policies Policies arXiv Policies Code of conduct Privacy policy Identity, affiliation, and registration Submission terms and agreement Paper ownership Submission Guidelines Submission schedule and cutoff time Content Moderation License and copyright Withdrawals Cross listing Translations Non-English submissions Text Overlap Requiring TeX when possible Third party submission Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Categories Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> arXiv Usage Stats Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --> Table of contents Bulk Access Overview Bulk Access Overview arXiv is an open access research sharing platform and access to bulk data is also open, with certain stipulations. Thank you in advance for following arXivâs API Terms of Use , brand guidelines, and licenses of content posted to arXiv. The metadata options are: arXiv API DataCite API using provider-id = arxiv Kaggle Full text options are: AWS for PDF and or (La)TeX source files Kaggle for PDF Crawling our export service This is recommended for new content or subset of content. Otherwise the AWS or Kaggle data sets are preferred. At this time, we do not require that commercial projects sign an MOU. We do encourage anyone who benefits financially from arXiv consider becoming an affiliate or a sponsor ; however, doing so is completely optional. Please do let us know when your product launches. About Help Copyright Privacy Policy contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Report an issue Click here to report an issue with arXiv's documentation in github Report a documentation issue Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.softwareheritage.org/devel/swh-export/graph/dataset.html#summary-of-dataset-versions",
      "full_text": " Dataset &#8212; Software Heritage documentation Skip to main content Back to top Ctrl + K Development API reference Usage Infrastructure About GitLab PyPI System Status Software Heritage Homepage Development API reference Usage Infrastructure About GitLab PyPI System Status Software Heritage Homepage Section Navigation swh.alter Using swh-alter Removal algorithm Recovery bundles swh.alter package swh.alter.bech32 module swh.alter.cli module swh.alter.inventory module swh.alter.mirror_notification_watcher module swh.alter.notifications module swh.alter.operations module swh.alter.progressbar module swh.alter.recovery_bundle module swh.alter.removable module swh.alter.subgraph module swh.alter.utils module swh.auth Command-line interface Django components swh.auth package swh.auth.django package swh.auth.django.backends module swh.auth.django.middlewares module swh.auth.django.models module swh.auth.django.utils module swh.auth.django.views module swh.auth.starlette package swh.auth.starlette.backends module swh.auth.cli module swh.auth.keycloak module swh.auth.pytest_plugin module swh.auth.utils module swh.coarnotify Explanations Whatâs COAR Notify ? What are we doing with the metadata ? swh.coarnotify package swh.coarnotify.server package swh.coarnotify.server.management package swh.coarnotify.server.admin module swh.coarnotify.server.apps module swh.coarnotify.server.forms module swh.coarnotify.server.handlers module swh.coarnotify.server.models module swh.coarnotify.server.signals module swh.coarnotify.server.utils module swh.coarnotify.server.views module swh.coarnotify.settings package swh.coarnotify.settings.common module swh.coarnotify.settings.development module swh.coarnotify.settings.production module swh.coarnotify.settings.tests module swh.coarnotify.cli module swh.coarnotify.client module swh.coarnotify.manage module swh.coarnotify.parsers module swh.coarnotify.renderers module swh.coarnotify.urls module How-to guides Install swh-coarnotify Access our inbox Send a mention of a software in a scientific paper References Workflows Specifications Rest API Command Line Interface Notification payloads swh.core Command-line interface Common database utilities swh.core package swh.core.api package swh.core.api.asynchronous module swh.core.api.classes module swh.core.api.gunicorn_config module swh.core.api.negotiation module swh.core.api.serializers module swh.core.cli package swh.core.cli.backend module swh.core.cli.db module swh.core.cli.nar module swh.core.cli.sentry module swh.core.db package swh.core.db.common module swh.core.db.db_utils module swh.core.github package swh.core.github.pytest_plugin module swh.core.github.utils module swh.core.api_async module swh.core.collections module swh.core.config module swh.core.logger module swh.core.logging module swh.core.nar module swh.core.pytest_plugin module swh.core.retry module swh.core.sentry module swh.core.statsd module swh.core.tarball module swh.core.utils module swh.counters swh.counters package swh.counters.api package swh.counters.api.client module swh.counters.api.server module swh.counters.cli module swh.counters.history module swh.counters.in_memory module swh.counters.interface module swh.counters.journal_client module swh.counters.kafka_client module swh.counters.redis module swh.datasets Luigi workflows Command-line interface swh.datasets package swh.datasets.luigi package swh.datasets.luigi.aggregate_datasets module swh.datasets.luigi.blobs_datasets module swh.datasets.luigi.file_names module swh.datasets.luigi.impact module swh.datasets.luigi.origin_contributors module swh.datasets.cli module swh.datasets.download module swh.datasets.shell module swh.deposit Deposit API User Manual API Documentation Service document Create deposit Update content Update metadata Retrieve status Display content Deposit metadata Use cases Register account Deposit internals Running swh-deposit locally Production deployment Authentication Loading workflow Specifications Loading specification Protocol reference The metadata-only deposit swh.deposit package swh.deposit.api package swh.deposit.api.private package swh.deposit.api.collection module swh.deposit.api.common module swh.deposit.api.content module swh.deposit.api.converters module swh.deposit.api.edit module swh.deposit.api.edit_media module swh.deposit.api.service_document module swh.deposit.api.state module swh.deposit.api.sword_edit module swh.deposit.api.urls module swh.deposit.api.utils module swh.deposit.cli package swh.deposit.cli.admin module swh.deposit.cli.client module swh.deposit.fixtures package swh.deposit.loader package swh.deposit.loader.checker module swh.deposit.loader.checks module swh.deposit.loader.tasks module swh.deposit.templates package swh.deposit.templates.deposit package swh.deposit.tests_migration package swh.deposit.apps module swh.deposit.auth module swh.deposit.client module swh.deposit.config module swh.deposit.errors module swh.deposit.exception module swh.deposit.gunicorn_config module swh.deposit.manage module swh.deposit.models module swh.deposit.parsers module swh.deposit.urls module swh.deposit.utils module Command-line interface swh.digestmap Design document for a hash conversion service swh.digestmap module swh.export Software Heritage Graph Dataset Dataset Relational schema Setup on Amazon Athena Setup on Azure Databricks Exporting a dataset Exporting a subdataset swh.export package swh.export.exporters package swh.export.exporters.edges module swh.export.exporters.orc module swh.export.test namespace swh.export.test.test_edges module swh.export.test.test_journal_processor module swh.export.test.test_orc module swh.export.test.test_utils module swh.export.athena module swh.export.cli module swh.export.exporter module swh.export.fullnames module swh.export.journalprocessor module swh.export.luigi module swh.export.relational module swh.export.utils module swh.fuse Tutorial Configuration Advice for parallelization Design notes Command-line interface swh.fuse package swh.fuse.backends package swh.fuse.backends.compressed module swh.fuse.backends.objstorage module swh.fuse.backends.web_api module swh.fuse.fs namespace swh.fuse.fs.artifact module swh.fuse.fs.entry module swh.fuse.fs.mountpoint module swh.fuse.cache module swh.fuse.cli module swh.fuse.fuse module swh.graph Quickstart Graph Querying HTTP API Using the gRPC API Rust API Memory &amp; Performance tuning Graph compression Command-line interface Docker environment git2graph Test graphs Example dataset Provenance index swh.graph package swh.graph.example_dataset package swh.graph.example_dataset.generate_dataset module swh.graph.grpc namespace swh.graph.grpc.swhgraph_pb2 module swh.graph.grpc.swhgraph_pb2_grpc module swh.graph.luigi package swh.graph.luigi.compressed_graph module swh.graph.luigi.subdataset module swh.graph.luigi.topology module swh.graph.luigi.utils module swh.graph.cli module swh.graph.client module swh.graph.config module swh.graph.download module swh.graph.e2e_tests module swh.graph.find_context module swh.graph.grpc_server module swh.graph.http_client module swh.graph.http_naive_client module swh.graph.http_rpc_server module swh.graph.naive_client module swh.graph.pytest_plugin module swh.graph.shell module swh.graph.webgraph module Software Heritage GraphQL API swh.indexer Software Heritage - Indexer Metadata workflow SwhPkg Vocabulary swh.indexer package swh.indexer.metadata_dictionary package swh.indexer.metadata_dictionary.base module swh.indexer.metadata_dictionary.cff module swh.indexer.metadata_dictionary.codemeta module swh.indexer.metadata_dictionary.composer module swh.indexer.metadata_dictionary.dart module swh.indexer.metadata_dictionary.gitea module swh.indexer.metadata_dictionary.github module swh.indexer.metadata_dictionary.maven module swh.indexer.metadata_dictionary.npm module swh.indexer.metadata_dictionary.nuget module swh.indexer.metadata_dictionary.python module swh.indexer.metadata_dictionary.ruby module swh.indexer.metadata_dictionary.utils module swh.indexer.storage package swh.indexer.storage.api package swh.indexer.storage.converters module swh.indexer.storage.db module swh.indexer.storage.exc module swh.indexer.storage.in_memory module swh.indexer.storage.interface module swh.indexer.storage.metrics module swh.indexer.storage.model module swh.indexer.storage.writer module swh.indexer.bibtex module swh.indexer.cli module swh.indexer.codemeta module swh.indexer.fossology_license module swh.indexer.indexer module swh.indexer.metadata module swh.indexer.metadata_detector module swh.indexer.mimetype module swh.indexer.namespaces module swh.indexer.origin_head module swh.indexer.rehash module Command-line interface swh.journal Software Heritage Journal clients swh.journal package swh.journal.writer package swh.journal.writer.inmemory module swh.journal.writer.interface module swh.journal.writer.kafka module swh.journal.writer.stream module swh.journal.client module swh.journal.pytest_plugin module swh.journal.serializers module swh.lister Tutorial: list the content of your favorite forge in just a few steps Tutorial: run a lister within docker-dev in just a few steps Save a forge swh.lister package swh.lister.arch package swh.lister.arch.lister module swh.lister.arch.tasks module swh.lister.aur package swh.lister.aur.lister module swh.lister.aur.tasks module swh.lister.bioconductor package swh.lister.bioconductor.lister module swh.lister.bioconductor.tasks module swh.lister.bitbucket package swh.lister.bitbucket.lister module swh.lister.bitbucket.tasks module swh.lister.bower package swh.lister.bower.lister module swh.lister.bower.tasks module swh.lister.cgit package swh.lister.cgit.lister module swh.lister.cgit.tasks module swh.lister.conda package swh.lister.conda.lister module swh.lister.conda.tasks module swh.lister.cpan package swh.lister.cpan.lister module swh.lister.cpan.tasks module swh.lister.cran package swh.lister.cran.lister module swh.lister.cran.tasks module swh.lister.crates package swh.lister.crates.lister module swh.lister.crates.tasks module swh.lister.debian package swh.lister.debian.lister module swh.lister.debian.tasks module swh.lister.dlang package swh.lister.dlang.lister module swh.lister.dlang.tasks module swh.lister.elm package swh.lister.elm.lister module swh.lister.elm.tasks module swh.lister.gitea package swh.lister.gitea.lister module swh.lister.gitea.tasks module swh.lister.github package swh.lister.github.lister module swh.lister.github.tasks module swh.lister.gitiles package swh.lister.gitiles.lister module swh.lister.gitiles.tasks module swh.lister.gitlab package swh.lister.gitlab.lister module swh.lister.gitlab.tasks module swh.lister.gitweb package swh.lister.gitweb.lister module swh.lister.gitweb.tasks module swh.lister.gnu package swh.lister.gnu.lister module swh.lister.gnu.tasks module swh.lister.gnu.tree module swh.lister.gogs package swh.lister.gogs.lister module swh.lister.gogs.tasks module swh.lister.golang package swh.lister.golang.lister module swh.lister.golang.tasks module swh.lister.hackage package swh.lister.hackage.lister module swh.lister.hackage.tasks module swh.lister.hex package swh.lister.hex.lister module swh.lister.hex.tasks module swh.lister.julia package swh.lister.julia.lister module swh.lister.julia.tasks module swh.lister.launchpad package swh.lister.launchpad.lister module swh.lister.launchpad.tasks module swh.lister.maven package swh.lister.maven.lister module swh.lister.maven.tasks module swh.lister.nixguix package swh.lister.nixguix.lister module swh.lister.nixguix.tasks module swh.lister.npm package swh.lister.npm.lister module swh.lister.npm.tasks module swh.lister.nuget package swh.lister.nuget.lister module swh.lister.nuget.tasks module swh.lister.opam package swh.lister.opam.lister module swh.lister.opam.tasks module swh.lister.packagist package swh.lister.packagist.lister module swh.lister.packagist.tasks module swh.lister.pagure package swh.lister.pagure.lister module swh.lister.pagure.tasks module swh.lister.phabricator package swh.lister.phabricator.lister module swh.lister.phabricator.tasks module swh.lister.pubdev package swh.lister.pubdev.lister module swh.lister.pubdev.tasks module swh.lister.puppet package swh.lister.puppet.lister module swh.lister.puppet.tasks module swh.lister.pypi package swh.lister.pypi.lister module swh.lister.pypi.tasks module swh.lister.rpm package swh.lister.rpm.lister module swh.lister.rpm.tasks module swh.lister.rubygems package swh.lister.rubygems.lister module swh.lister.rubygems.tasks module swh.lister.save_bulk package swh.lister.save_bulk.lister module swh.lister.save_bulk.tasks module swh.lister.sourceforge package swh.lister.sourceforge.lister module swh.lister.sourceforge.tasks module swh.lister.stagit package swh.lister.stagit.lister module swh.lister.stagit.tasks module swh.lister.tuleap package swh.lister.tuleap.lister module swh.lister.tuleap.tasks module swh.lister.cli module swh.lister.pattern module swh.lister.utils module Command-line interface swh.loader swh.loader.core VCS Loader Overview Package Loader Tutorial Package loader specifications swh.loader.core package swh.loader.core.converters module swh.loader.core.loader module swh.loader.core.metadata_fetchers module swh.loader.core.tasks module swh.loader.core.utils module Command-line interface swh.loader.bzr Software Heritage - How Bazaar/Breezy works swh.loader.bzr package swh.loader.bzr.loader module swh.loader.bzr.tasks module Software Heritage - CVS loader swh.loader.git swh.loader.git package swh.loader.git.base module swh.loader.git.converters module swh.loader.git.directory module swh.loader.git.dumb module swh.loader.git.from_disk module swh.loader.git.loader module swh.loader.git.tasks module swh.loader.git.utils module swh.loader.mercurial swh.loader.mercurial package swh.loader.mercurial.archive_extract module swh.loader.mercurial.converters module swh.loader.mercurial.directory module swh.loader.mercurial.hgutil module swh.loader.mercurial.identify module swh.loader.mercurial.loader module swh.loader.mercurial.tasks module swh.loader.mercurial.utils module swh.loader.metadata swh.loader.metadata package swh.loader.metadata.base module swh.loader.metadata.cli module swh.loader.metadata.gitea module swh.loader.metadata.github module swh.loader.metadata.journal_client module swh.loader.svn swh.loader.svn package swh.loader.svn.converters module swh.loader.svn.directory module swh.loader.svn.exception module swh.loader.svn.fast_crawler module swh.loader.svn.loader module swh.loader.svn.replay module swh.loader.svn.svn_repo module swh.loader.svn.svn_retry module swh.loader.svn.tasks module swh.loader.svn.utils module swh.model Data model SoftWare Heritage persistent IDentifiers (SWHIDs) Command-line interface swh.model package swh.model.fields package swh.model.fields.compound module swh.model.fields.hashes module swh.model.fields.simple module swh.model.cli module swh.model.collections module swh.model.discovery module swh.model.exceptions module swh.model.from_disk module swh.model.git_objects module swh.model.hashutil module swh.model.hypothesis_strategies module swh.model.merkle module swh.model.model module swh.model.swhids module swh.model.toposort module swh.model.validators module swh.objstorage Command-line interface Winery backend swh.objstorage namespace swh.objstorage.api namespace swh.objstorage.api.client module swh.objstorage.api.server module swh.objstorage.backends package swh.objstorage.backends.seaweedfs package swh.objstorage.backends.winery package swh.objstorage.backends.azure module swh.objstorage.backends.http module swh.objstorage.backends.in_memory module swh.objstorage.backends.libcloud module swh.objstorage.backends.noop module swh.objstorage.backends.pathslicing module swh.objstorage.proxies namespace swh.objstorage.proxies.readonly module swh.objstorage.replayer package swh.objstorage.replayer.cli module swh.objstorage.replayer.replay module swh.objstorage.cli module swh.objstorage.constants module swh.objstorage.exc module swh.objstorage.factory module swh.objstorage.interface module swh.objstorage.multiplexer module swh.objstorage.objstorage module swh.objstorage.pytest_plugin module swh.objstorage.utils module swh.objstorage.replayer Command-line interface swh.objstorage.replayer package swh.objstorage.replayer.cli module swh.objstorage.replayer.replay module Software Heritage - Provenance swh.scanner Command-line interface swh.scanner package swh.scanner.dashboard package swh.scanner.dashboard.dashboard module swh.scanner.cli module swh.scanner.config module swh.scanner.data module swh.scanner.exceptions module swh.scanner.output module swh.scanner.policy module swh.scanner.scanner module swh.scanner.setup_wizard module swh.scheduler Command-line interface Software Heritage Scheduler Simulator swh.scheduler package swh.scheduler.api package swh.scheduler.api.client module swh.scheduler.api.serializers module swh.scheduler.api.server module swh.scheduler.celery_backend package swh.scheduler.celery_backend.config module swh.scheduler.celery_backend.first_visits module swh.scheduler.celery_backend.pika_listener module swh.scheduler.celery_backend.recurrent_visits module swh.scheduler.celery_backend.runner module swh.scheduler.celery_backend.utils module swh.scheduler.cli package swh.scheduler.cli.add_forge_now module swh.scheduler.cli.admin module swh.scheduler.cli.celery_monitor module swh.scheduler.cli.config module swh.scheduler.cli.journal module swh.scheduler.cli.origin module swh.scheduler.cli.origin_utils module swh.scheduler.cli.simulator module swh.scheduler.cli.task module swh.scheduler.cli.task_type module swh.scheduler.cli.utils module swh.scheduler.simulator package swh.scheduler.simulator.common module swh.scheduler.simulator.origin_scheduler module swh.scheduler.simulator.origins module swh.scheduler.simulator.task_scheduler module swh.scheduler.sql package swh.scheduler.sql.upgrades package swh.scheduler.backend module swh.scheduler.cli_utils module swh.scheduler.exc module swh.scheduler.in_memory module swh.scheduler.interface module swh.scheduler.journal_client module swh.scheduler.model module swh.scheduler.pytest_plugin module swh.scheduler.task module swh.scheduler.utils module swh.scrubber Command-line interface swh.scrubber package swh.scrubber.base_checker module swh.scrubber.cli module swh.scrubber.db module swh.scrubber.fixer module swh.scrubber.journal_checker module swh.scrubber.objstorage_checker module swh.scrubber.origin_locator module swh.scrubber.storage_checker module swh.scrubber.utils module swh.search Command-line interface Search Query Language swh.search package swh.search.api package swh.search.api.client module swh.search.api.server module swh.search.cli module swh.search.elasticsearch module swh.search.exc module swh.search.in_memory module swh.search.interface module swh.search.journal_client module swh.search.metrics module swh.search.translator module swh.search.utils module swh.shard Read Shard format Benchmarks swh.shard package swh.shard.cli module swh.storage Extrinsic metadata specification Object Masking swh.storage package swh.storage.algos package swh.storage.algos.diff module swh.storage.algos.dir_iterators module swh.storage.algos.directory module swh.storage.algos.discovery module swh.storage.algos.origin module swh.storage.algos.revisions_walker module swh.storage.algos.snapshot module swh.storage.api package swh.storage.api.client module swh.storage.api.serializers module swh.storage.api.server module swh.storage.cassandra package swh.storage.cassandra.common module swh.storage.cassandra.converters module swh.storage.cassandra.cql module swh.storage.cassandra.diagram module swh.storage.cassandra.migrations module swh.storage.cassandra.model module swh.storage.cassandra.schema module swh.storage.cassandra.storage module swh.storage.postgresql package swh.storage.postgresql.converters module swh.storage.postgresql.db module swh.storage.postgresql.storage module swh.storage.proxies package swh.storage.proxies.blocking package swh.storage.proxies.masking package swh.storage.proxies.buffer module swh.storage.proxies.filter module swh.storage.proxies.record_references module swh.storage.proxies.retry module swh.storage.proxies.tenacious module swh.storage.proxies.validate module swh.storage.backfill module swh.storage.cli module swh.storage.common module swh.storage.exc module swh.storage.fixer module swh.storage.in_memory module swh.storage.interface module swh.storage.metrics module swh.storage.migrate_extrinsic_metadata module swh.storage.objstorage module swh.storage.pytest_plugin module swh.storage.replay module swh.storage.utils module swh.storage.writer module Database schema Cassandra migrations Command-line interface swh.vault Getting started Vault API Reference swh.vault package swh.vault.api package swh.vault.api.client module swh.vault.api.serializers module swh.vault.api.server module swh.vault.cookers package swh.vault.cookers.base module swh.vault.cookers.directory module swh.vault.cookers.git_bare module swh.vault.cookers.revision_flat module swh.vault.cookers.revision_gitfast module swh.vault.cookers.utils module swh.vault.backend module swh.vault.cache module swh.vault.cli module swh.vault.cooking_tasks module swh.vault.exc module swh.vault.in_memory_backend module swh.vault.interface module swh.vault.to_disk module Command-line interface swh.web Developers Information swh-web API URLs URI scheme for swh-web Browse application URI scheme for SoftWare Hash IDentifiers (SWHIDs) Miscellaneous URLs swh.web package swh.web.add_forge_now package swh.web.add_forge_now.admin_views module swh.web.add_forge_now.api_views module swh.web.add_forge_now.apps module swh.web.add_forge_now.models module swh.web.add_forge_now.signal_receivers module swh.web.add_forge_now.urls module swh.web.add_forge_now.utils module swh.web.add_forge_now.views module swh.web.admin package swh.web.admin.apps module swh.web.admin.urls module swh.web.alter package swh.web.alter.templatetags package swh.web.alter.apps module swh.web.alter.emails module swh.web.alter.forms module swh.web.alter.models module swh.web.alter.urls module swh.web.alter.utils module swh.web.alter.views module swh.web.api package swh.web.api.views package swh.web.api.apidoc module swh.web.api.apiresponse module swh.web.api.apiurls module swh.web.api.parsers module swh.web.api.renderers module swh.web.api.serializers module swh.web.api.throttling module swh.web.api.urls module swh.web.api.utils module swh.web.archive_coverage package swh.web.archive_coverage.urls module swh.web.archive_coverage.views module swh.web.auth package swh.web.auth.apps module swh.web.auth.models module swh.web.auth.urls module swh.web.auth.utils module swh.web.auth.views module swh.web.badges package swh.web.badges.urls module swh.web.banners package swh.web.banners.urls module swh.web.banners.views module swh.web.browse package swh.web.browse.views package swh.web.browse.browseurls module swh.web.browse.identifiers module swh.web.browse.snapshot_context module swh.web.browse.urls module swh.web.browse.utils module swh.web.client package swh.web.client.cli module swh.web.client.client module swh.web.deposit package swh.web.deposit.urls module swh.web.inbound_email package swh.web.inbound_email.management package swh.web.inbound_email.apps module swh.web.inbound_email.handle_message module swh.web.inbound_email.signals module swh.web.inbound_email.urls module swh.web.inbound_email.utils module swh.web.inbound_email.views module swh.web.jslicenses package swh.web.jslicenses.urls module swh.web.mailmap package swh.web.mailmap.management package swh.web.mailmap.apps module swh.web.mailmap.models module swh.web.mailmap.urls module swh.web.mailmap.views module swh.web.metrics package swh.web.metrics.prometheus module swh.web.metrics.urls module swh.web.metrics.views module swh.web.provenance package swh.web.provenance.api_views module swh.web.provenance.apps module swh.web.provenance.urls module swh.web.save_bulk package swh.web.save_bulk.api_views module swh.web.save_bulk.apps module swh.web.save_bulk.models module swh.web.save_bulk.urls module swh.web.save_bulk.views module swh.web.save_code_now package swh.web.save_code_now.management package swh.web.save_code_now.admin_views module swh.web.save_code_now.api_views module swh.web.save_code_now.apps module swh.web.save_code_now.models module swh.web.save_code_now.origin_save module swh.web.save_code_now.origin_visit_webhook module swh.web.save_code_now.urls module swh.web.save_code_now.views module swh.web.save_origin_webhooks package swh.web.save_origin_webhooks.bitbucket module swh.web.save_origin_webhooks.generic_receiver module swh.web.save_origin_webhooks.gitea module swh.web.save_origin_webhooks.github module swh.web.save_origin_webhooks.gitlab module swh.web.save_origin_webhooks.sourceforge module swh.web.save_origin_webhooks.urls module swh.web.utils package swh.web.utils.management package swh.web.utils.archive module swh.web.utils.citation module swh.web.utils.converters module swh.web.utils.exc module swh.web.utils.highlightjs module swh.web.utils.identifiers module swh.web.utils.middlewares module swh.web.utils.origin_visits module swh.web.utils.query module swh.web.utils.swh_templatetags module swh.web.utils.typing module swh.web.utils.url_path_converters module swh.web.utils.urlsindex module swh.web.vault package swh.web.vault.api_views module swh.web.vault.urls module swh.web.webapp package swh.web.webapp.urls module swh.web.config module swh.web.gunicorn_config module swh.web.manage module swh.web.urls module swh.web.client swh.webhooks swh.webhooks package swh.webhooks.cli module swh.webhooks.interface module swh.webhooks.journal_client module swh.webhooks.svix_retry module swh.webhooks.utils module Development API reference Software Heritage Datasets Software Heritage Graph Dataset Dataset Dataset # We aim to provide regular exports of the Software Heritage graph in two different formats: Columnar data storage : a set of relational tables stored in a columnar format such as Apache ORC , which is particularly suited for scale-out analyses on data lakes and big data processing ecosystems such as the Hadoop environment. Compressed graph : a compact and highly-efficient representation of the graph dataset, suited for scale-up analysis on high-end machines with large amounts of memory. The graph is compressed in Boldi-Vigna representation , designed to be loaded by the WebGraph framework , specifically using our swh-graph library . See also Using Software Heritage data . Terms of Use Usage of the datasets from the Software Heritage archive is covered by our Ethical Charter and the Terms of use for bulk access . Downloading the datasets # All datasets below are available publicly and with no login required, subject to the terms of use above. After installing awscli , datasets hosted on Amazon S3 can be downloaded with this command: aws s3 cp s3 : // softwareheritage / graph /... ./ target / path / -- recursive -- no - sign - request The latest compressed graphs contain some .zst files, which must be decompressed with unzstd before they can be used with swh-graph. Summary of dataset versions # Full graph : Name # Nodes # Edges Columnar Compressed 2025-05-18 49,903,891,086 905,462,853,965 â â 2024-12-06 44,573,066,306 769,494,968,843 â â 2024-08-23 41,074,031,225 644,153,760,912 â â 2024-05-16 38,977,225,252 604,179,689,399 â â 2023-09-06 34,121,566,250 517,399,308,984 â â 2022-12-07 27,397,574,122 416,565,871,870 â â 2022-04-25 25,340,003,875 375,867,687,011 â â 2021-03-23 20,667,308,808 232,748,148,441 â â 2020-12-15 19,330,739,526 213,848,749,638 â â 2020-05-20 17,075,708,289 203,351,589,619 â â 2019-01-28 11,683,687,950 159,578,271,511 â â Teaser datasets : Name # Nodes # Edges Columnar Compressed 2025-05-18-popular-1k 328,715,950 11,785,152,130 â â 2024-08-23-popular-500-python 60,286,526 1,630,768,493 â â 2023-09-06-popular-1k 176,569,127 11,322,432,687 â â 2021-03-23-popular-3k-python 45,691,499 1,221,283,907 â â 2020-12-15-gitlab-all 1,083,011,764 27,919,670,049 â â 2020-12-15-gitlab-100k 304,037,235 9,516,984,175 â â 2019-01-28-popular-4k ? ? â â 2019-01-28-popular-3k-python 27,363,226 346,413,337 â â Full graph datasets # Because of their size, some of the latest datasets are only available for download from Amazon S3. 2025-05-18 # A full export of the graph dated from May 2025 Columnar tables (Apache ORC) : Total size : 27 TiB S3 : s3://softwareheritage/graph/2025-05-18/orc Compressed graph : Total size : 14 TiB S3 : s3://softwareheritage/graph/2025-05-18/compressed âHistory and hostingâ Compressed graph : This is a compressed graph of only the âhistory and hostingâ layer (origins, snapshots, releases, revisions) and the root directory (or rarely content) of every revision/release; but most directories and contents are excluded Total size : 1.5 TiB S3 : s3://softwareheritage/graph/2025-05-18-history-hosting/compressed 2024-12-06 # A full export of the graph dated from December 2024 Columnar tables (Apache ORC) : Total size : 23 TiB S3 : s3://softwareheritage/graph/2024-12-06/orc Compressed graph : Total size : 12 TiB S3 : s3://softwareheritage/graph/2024-12-06/compressed âHistory and hostingâ Compressed graph : This is a compressed graph of only the âhistory and hostingâ layer (origins, snapshots, releases, revisions) and the root directory (or rarely content) of every revision/release; but most directories and contents are excluded Total size : 1.4 TiB S3 : s3://softwareheritage/graph/2024-12-06-history-hosting/compressed 2024-08-23 # A full export of the graph dated from August 2024 Columnar tables (Apache ORC) : Total size : 19 TiB S3 : s3://softwareheritage/graph/2024-08-23/orc Compressed graph : Total size : 11 TiB S3 : s3://softwareheritage/graph/2024-08-23/compressed This graph changed the MPH from GOV/Cmph to PTHash; Rust code hardcoding GOVMPH needs to replace it with DynMph or SwhidPthash . Java is no longer supported to read this graph. 2024-05-16 # A full export of the graph dated from May 2024 Columnar tables (Apache ORC) : Total size : 18 TiB S3 : s3://softwareheritage/graph/2024-05-16/orc Compressed graph : Total size : 11 TiB S3 : s3://softwareheritage/graph/2024-05-16/compressed This graph export contains all files needed by the Rust implementation of swh-graph, so running swh-graph/tools/swh-graph-java2rust.sh is no longer necessary. âHistory and hostingâ Compressed graph : This is a compressed graph of only the âhistory and hostingâ layer (origins, snapshots, releases, revisions) and the root directory (or rarely content) of every revision/release; but most directories and contents are excluded S3 : s3://softwareheritage/graph/2024-05-16-history-hosting/compressed 2023-09-06 # A full export of the graph dated from September 2023 Columnar tables (Apache ORC) : Total size : 15 TiB S3 : s3://softwareheritage/graph/2023-09-06/orc Compressed graph : Total size : 8.8 TiB S3 : s3://softwareheritage/graph/2023-09-06/compressed âHistory and hostingâ Compressed graph : This is a compressed graph of only the âhistory and hostingâ layer (origins, snapshots, releases, revisions) and the root directory (or rarely content) of every revision/release; but most directories and contents are excluded S3 : s3://softwareheritage/graph/2023-09-06-history-hosting/compressed 2022-12-07 # A full export of the graph dated from December 2022 Columnar tables (Apache ORC) : Total size : 13 TiB S3 : s3://softwareheritage/graph/2022-12-07/orc Compressed graph : Total size : 7.1 TiB S3 : s3://softwareheritage/graph/2022-12-07/compressed âHistory and hostingâ Compressed graph : This is a compressed graph of only the âhistory and hostingâ layer (origins, snapshots, releases, revisions) and the root directory (or rarely content) of every revision/release; but most directories and contents are excluded Total size : 1.0 TiB S3 : s3://softwareheritage/graph/2022-12-07-history-hosting/compressed Erratum : author and committer timestamps were shifted back 1 or 2 hours, based on the Europe/Paris timezone 2022-04-25 # A full export of the graph dated from April 2022 Columnar tables (Apache ORC) : Total size : 11 TiB S3 : s3://softwareheritage/graph/2022-04-25/orc Compressed graph : Total size : 6.5 TiB S3 : s3://softwareheritage/graph/2022-04-25/compressed 2021-03-23 # A full export of the graph dated from March 2021. Columnar tables (Apache ORC) : Total size : 8.4 TiB URL : /graph/2021-03-23/orc/ S3 : s3://softwareheritage/graph/2021-03-23/orc Compressed graph : S3 : s3://softwareheritage/graph/2021-03-23/compressed 2020-12-15 # A full export of the graph dated from December 2020. This export has a CSV representation of nodes and edges instead of columnar: edges as graph.edges. cnt,ori,rel,rev,snp .csv.zst and graph.edges.dir. 00..21 .csv.zst nodes as graph.nodes.csv.zst deduplicated labels as graph.labels.csv.zst statistics as graph.edges.count.txt , graph.edges.stats.txt , graph.labels.count.txt , graph.nodes.count.txt , and graph.nodes.stats.txt Compressed graph : URL : /graph/2020-12-15/compressed/ S3 : s3://softwareheritage/graph/2020-12-15/compressed Edges : - S3 : s3://softwareheritage/graph/2020-12-15/edges 2020-05-20 # A full export of the graph dated from May 2020. Only available in compressed representation. (DEPRECATED: known issue with missing snapshot edges.) Compressed graph : URL : /graph/2020-05-20/compressed/ 2019-01-28 # A full export of the graph dated from January 2019. The export was done in two phases, one of them called â2018-09-25â and the other â2019-01-28â. They both refer to the same dataset, but the different formats have various inconsistencies between them. (DEPRECATED: early export pipeline, various inconsistencies). Columnar tables (Apache Parquet) : Total size : 1.2 TiB URL : /graph/2019-01-28/parquet/ S3 : s3://softwareheritage/graph/2018-09-25/parquet Compressed graph : URL : /graph/2019-01-28/compressed/ Teaser datasets # If the above datasets are too big, we also provide âteaserâ datasets that can get you started and have a smaller size fingerprint. 2025-05-18-popular-1k # This is a subgraph of the 2025-05-18 export, filtered by rooting from 1000 popular origins: 900 among the most starred Github repositories (as of July 1st 2025) 100 among the most frequently installed Debian packages (according to the Debian Popularity Contest database published on Sept 3rd 2025). The corresponding origins list is in s3://softwareheritage/graph/2025-05-18-popular-1k/origins.txt . Columnar (Apache ORC) : Total size : 349 GiB S3 : s3://softwareheritage/graph/2025-05-18-popular-1k/orc/ Compressed graph : Total size : 202 GiB S3 : s3://softwareheritage/graph/2025-05-18-popular-1k/compressed/ 2024-08-23-popular-500-python # The 2024-08-23-popular-500-python teaser contains a subset of the 443 repositories archived by Software Heritage as of 2024-08-23, among the 700 GitHub repositories tagged as being written in Python with the most stars. Columnar (Apache ORC) : Total size : 36 GiB S3 : s3://softwareheritage/graph/2024-08-23-popular-500-python/orc/ Compressed graph : Total size : 23 GiB S3 : s3://softwareheritage/graph/2024-08-23-popular-500-python/compressed/ 2023-09-06-popular-1k # The popular-1k teaser contains a subset of 1120 popular repositories tagged as being written in one of the 10 most popular languages (Javascript, Python, Java, Typescript, C#, C++, PHP, Shell, C, Ruby), from GitHub, Gitlab.com, Packagist, PyPI and Debian. The selection criteria to pick the software origins for each language was the following: the 50 most popular Gitlab.com projects written in that language that have 2 stars or more, for Python, the 50 most popular PyPI projects (by usage statistics, according to the Top PyPI Packages database), for PHP, the 50 most popular Packagist projects (by usage statistics, according to Packagistâs API ), the 50 most popular Debian packages with the relevant implemented-in:: debtag (by âinstallsâ according to the Debian Popularity Contest database). most popular GitHub projects written in Python (by number of stars), until the total number of origins for that language reaches 200 removing origins not archived by Software Heritage by 2023-09-06 Columnar (Apache ORC) : Total size : 280 GiB S3 : s3://softwareheritage/graph/2023-09-06-popular-1k/orc/ Compressed graph : Total size : 42 GiB S3 : s3://softwareheritage/graph/2023-09-06-popular-1k/compressed/ 2021-03-23-popular-3k-python # The popular-3k-python teaser contains a subset of 2197 popular repositories tagged as being written in the Python language , from GitHub, Gitlab.com, PyPI and Debian. The selection criteria to pick the software origins was the following: the 580 most popular GitHub projects written in Python (by number of stars), the 135 Gitlab.com projects written in Python that have 2 stars or more, the 827 most popular PyPI projects (by usage statistics, according to the Top PyPI Packages database), the 655 most popular Debian packages with the debtag implemented-in::python (by âvotesâ according to the Debian Popularity Contest database). Columnar (Apache ORC) : Total size : 36 GiB S3 : s3://softwareheritage/graph/2021-03-23-popular-3k-python/orc/ Compressed graph : Total size : 15 GiB S3 : s3://softwareheritage/graph/2021-03-23-popular-3k-python/compressed/ 2020-12-15-gitlab-all # A teaser dataset containing the entirety of Gitlab.com, exported in December 2020. Available in compressed graph format. Compressed graph : URL : /graph/2020-12-15-gitlab-all/compressed/ 2020-12-15-gitlab-100k # A teaser dataset containing the 100k most popular Gitlab.com repositories, exported in December 2020. Available in compressed graph format. Compressed graph : URL : /graph/2020-12-15-gitlab-100k/compressed/ 2019-01-28-popular-4k # This teaser dataset contains a subset of 4000 popular repositories from GitHub, Gitlab.com, PyPI and Debian. The selection criteria to pick the software origins was the following: The 1000 most popular GitHub projects (by number of stars) The 1000 most popular Gitlab.com projects (by number of stars) The 1000 most popular PyPI projects (by usage statistics, according to the Top PyPI Packages database), The 1000 most popular Debian packages (by âvotesâ according to the Debian Popularity Contest database) Columnar (Apache Parquet) : Total size : 27 GiB URL : /graph/2019-01-28-popular-4k/parquet/ S3 : s3://softwareheritage/graph/2019-01-28-popular-4k/parquet/ 2019-01-28-popular-3k-python # The popular-3k-python teaser contains a subset of 3052 popular repositories tagged as being written in the Python language , from GitHub, Gitlab.com, PyPI and Debian. The selection criteria to pick the software origins was the following, similar to popular-4k : the 1000 most popular GitHub projects written in Python (by number of stars), the 131 Gitlab.com projects written in Python that have 2 stars or more, the 1000 most popular PyPI projects (by usage statistics, according to the Top PyPI Packages database), the 1000 most popular Debian packages with the debtag implemented-in::python (by âvotesâ according to the Debian Popularity Contest database). Columnar (Apache Parquet) : Total size : 5.3 GiB URL : /graph/2019-01-28-popular-3k-python/parquet/ S3 : s3://softwareheritage/graph/2019-01-28-popular-3k-python/parquet/ previous Software Heritage Graph Dataset next Relational schema On this page Downloading the datasets Summary of dataset versions Full graph datasets 2025-05-18 2024-12-06 2024-08-23 2024-05-16 2023-09-06 2022-12-07 2022-04-25 2021-03-23 2020-12-15 2020-05-20 2019-01-28 Teaser datasets 2025-05-18-popular-1k 2024-08-23-popular-500-python 2023-09-06-popular-1k 2021-03-23-popular-3k-python 2020-12-15-gitlab-all 2020-12-15-gitlab-100k 2019-01-28-popular-4k 2019-01-28-popular-3k-python Edit This Page Show Source so the DOM is not blocked --> Â© Copyright 2015-2024 The Software Heritage developers. Created using Sphinx 7.4.7. Built with the PyData Sphinx Theme 0.16.1. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2412.02595",
      "full_text": " [2412.02595] Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2412.02595 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2412.02595 (cs) [Submitted on 3 Dec 2024 ( v1 ), last revised 30 May 2025 (this version, v2)] Title: Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset Authors: Dan Su , Kezhi Kong , Ying Lin , Joseph Jennings , Brandon Norick , Markus Kliegl , Mostofa Patwary , Mohammad Shoeybi , Bryan Catanzaro View a PDF of the paper titled Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset, by Dan Su and 8 other authors View PDF HTML (experimental) Abstract: Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at this https URL Comments: ACL 2025 Subjects: Computation and Language (cs.CL) Cite as: arXiv:2412.02595 [cs.CL] &nbsp; (or arXiv:2412.02595v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2412.02595 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Markus Kliegl [ view email ] [v1] Tue, 3 Dec 2024 17:28:50 UTC (98 KB) [v2] Fri, 30 May 2025 20:49:42 UTC (81 KB) Full-text links: Access Paper: View a PDF of the paper titled Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset, by Dan Su and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2508.14444},",
      "full_text": "Title: [2508.14444},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2508.14444%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2508.14444},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2508.14444%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2508.14444},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat",
      "full_text": " docs: latest docs docs latest latest ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.nvidia.com/Megatron-Core/developer-guide/latest/index.html",
      "full_text": " Megatron Core User Guide - NVIDIA Docs Topics Topics AR / VR Cybersecurity Edge Computing Recommenders / Personalization Computer Vision / Video Analytics Data Center / Cloud Generative AI / LLMs Robotics Content Creation / Rendering Data Science Networking Simulation / Modeling / Design Conversational AI NVIDIA Developer Blog Forums Sign In Menu Docs Hub Topics Topics AR / VR Cybersecurity Edge Computing Recommenders / Personalization Computer Vision / Video Analytics Data Center / Cloud Generative AI / LLMs Robotics Content Creation / Rendering Data Science Networking Simulation / Modeling / Design Conversational AI NVIDIA Developer Blog Forums Sign In Megatron Core User Guide Submit Search Submit Search NVIDIA Docs Hub Homepage &nbsp; &nbsp; NVIDIA Megatron-Core &nbsp; &nbsp; Megatron Core User Guide Megatron Core is a Python library that has the core components required to build your language models. A reference implementation of Megatron Core can be found in NeMo It offers a simple and intuitive API. User Guide User Guide Quick Start API Guide API Guide models package Subpackages Module contents tensor_parallel package Submodules tensor_parallel.cross_entropy module tensor_parallel.data module tensor_parallel.layers module tensor_parallel.mappings module tensor_parallel.random module tensor_parallel.utils module Module contents context_parallel package Context parallelism overview Context parallelism benefits Enabling context parallelism pipeline_parallel package Submodules pipeline_parallel.p2p_communication module pipeline_parallel.schedules module Module contents fusions package Submodules fusions.fused_bias_dropout module fusions.fused_bias_gelu module fusions.fused_layer_norm module fusions.fused_softmax module fusions.fused_cross_entropy_loss module transformer package Submodules transformer.attention module transformer.dot_product_attention module transformer.enums module transformer.identity_op module transformer.mlp module transformer.module module transformer.transformer_block module transformer.transformer_config module transformer.transformer_layer module transformer.utils module Module contents Mixture of Experts package Megatron Core MoE Key Features User Guide Performance Best Practice dist_checkpointing package Subpackages Submodules dist_checkpointing.serialization module dist_checkpointing.mapping module dist_checkpointing.optimizer module dist_checkpointing.core module dist_checkpointing.dict_utils module dist_checkpointing.utils module Module contents Distributed Optimizer Data flow Sharding scheme Key steps distributed package Submodules distributed.distributed_data_parallel distributed.finalize_model_grads Module contents datasets package Data Pipeline Submodules datasets.blended_megatron_dataset_config module datasets.blended_megatron_dataset_builder module datasets.megatron_tokenizer module datasets.indexed_dataset module datasets.megatron_dataset module datasets.gpt_dataset module datasets.masked_dataset module datasets.bert_dataset module datasets.t5_dataset module datasets.blended_dataset module datasets.utils module Module contents Microbatches Calculator Module contents Optimizer Parameters Scheduler Module contents encoder-decoder-parallelism package Submodules Encoder Pipeline Parallelism Encoder Tensor Parallelism Number of GPUs Required © Copyright 2022-2025, NVIDIA. Last updated on Jan 14, 2025. Topics Megatron Core User Guide User Guide User Guide Quick Start Environment Setup Writing Your First Training Loop Extending Further API Guide API Guide models package Subpackages models.gpt package Submodules models.gpt.gpt_model module Module contents models.t5 package Submodules models.t5.t5_model module Module contents models.bert package Submodules models.bert.bert_model module Module contents Module contents tensor_parallel package Submodules tensor_parallel.cross_entropy module tensor_parallel.data module tensor_parallel.layers module tensor_parallel.mappings module tensor_parallel.random module tensor_parallel.utils module Module contents context_parallel package Context parallelism overview Context parallelism benefits Enabling context parallelism pipeline_parallel package Submodules pipeline_parallel.p2p_communication module pipeline_parallel.schedules module Module contents fusions package Submodules fusions.fused_bias_dropout module fusions.fused_bias_gelu module fusions.fused_layer_norm module fusions.fused_softmax module fusions.fused_cross_entropy_loss module transformer package Submodules transformer.attention module transformer.dot_product_attention module transformer.enums module transformer.identity_op module transformer.mlp module transformer.module module transformer.transformer_block module transformer.transformer_config module transformer.transformer_layer module transformer.utils module Module contents Mixture of Experts package Megatron Core MoE Key Features Parallelism Router and Load Balancing Performance Optimizations Token Dispatch Mechanism Ease of use Upcoming features User Guide MoE Related Arguments Usage Quick Start Fine-tuning Mixtral Models Distributed Checkpointing Shared Experts Upcycling MoE training example: Performance Best Practice Tuning Guide of Parallel Mappings MoE Parallel Folding Advantages of MoE Parallel Folding End-to-End Training Practice Reference Best Parallel Mapping dist_checkpointing package Subpackages dist_checkpointing.strategies package Submodules dist_checkpointing.strategies.base module dist_checkpointing.strategies.tensorstore module dist_checkpointing.strategies.two_stage module dist_checkpointing.strategies.zarr module Module contents Submodules dist_checkpointing.serialization module dist_checkpointing.mapping module dist_checkpointing.optimizer module dist_checkpointing.core module dist_checkpointing.dict_utils module dist_checkpointing.utils module Module contents Distributed Optimizer Data flow Sharding scheme Key steps distributed package Submodules distributed.distributed_data_parallel distributed.finalize_model_grads Module contents datasets package Data Pipeline Data pre-processing IndexedDatasetBuilder IndexedDataset Data loading: construction BlendedMegatronDatasetConfig (extendable) BlendedMegatronDatasetBuilder IndexedDataset MegatronDataset (extendable) BlendedDataset Data loading: implementation GPTDataset BlendedDataset Submodules datasets.blended_megatron_dataset_config module datasets.blended_megatron_dataset_builder module datasets.megatron_tokenizer module datasets.indexed_dataset module datasets.megatron_dataset module datasets.gpt_dataset module datasets.masked_dataset module datasets.bert_dataset module datasets.t5_dataset module datasets.blended_dataset module datasets.utils module Module contents Microbatches Calculator Module contents Optimizer Parameters Scheduler Module contents encoder-decoder-parallelism package Submodules Encoder Pipeline Parallelism Encoder Tensor Parallelism Number of GPUs Required Corporate Info NVIDIA.com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2025 NVIDIA Corporation Close content here ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/",
      "full_text": " Turbocharge LLM Training Across Long&#x2d;Haul Data Center Networks with NVIDIA Nemo Framework | NVIDIA Technical Blog DEVELOPER Home Blog Forums Docs Downloads Training Join Technical Blog Subscribe Related Resources Data Center / Cloud English 中文 Turbocharge LLM Training Across Long-Haul Data Center Networks with NVIDIA Nemo Framework May 08, 2025 By Kyle Aubrey , Hao Wu , Dheevatsa Mudigere , Selvaraj Anandaraj and Wenwen Gao Like Discuss (0) L T F R E AI-Generated Summary Like Dislike The latest NVIDIA NeMo Framework 25.02 and NVIDIA Megatron-Core 0.11.0 releases enable multi-data center large language model training by introducing key innovations such as adaptive resource orchestration, Hierarchical AllReduce, distributed optimizer architecture, and chunked inter-data center communications. These advancements allow for high-efficiency training across geographically separated data centers, achieving 96% scaling efficiency when training a 340B parameter model across two data centers approximately 1,000 kilometers apart using thousands of NVIDIA GPUs. By leveraging NVIDIA GPU data center platforms and AI software stack, multi-data center training is poised to enable the creation of supercomputers that can harness over 500,000 GPUs across multiple facilities, enhancing reliability, flexibility, and energy efficiency. AI-generated content may summarize information incompletely. Verify important information. Learn more Multi-data center training is becoming essential for AI factories as pretraining scaling fuels the creation of even larger models, leading the demand for computing performance to outpace the capabilities of a single facility. By distributing workloads across multiple data centers, organizations can overcome limitations in power, cooling, and space, enabling the training of even larger, more accurate models with better efficiency.&nbsp; The latest release of NVIDIA NeMo Framework 25.02 and NVIDIA Megatron-Core 0.11.0 brings new capabilities for multi-data center large language model (LLM) training. This update enables users to scale training beyond the physical and operational limits of a single data center, unlocking unprecedented efficiency and performance by harnessing the combined power of multiple sites.&nbsp; In this post, we&#8217;ll cover how NeMo Framework and Megatron-Core are revolutionizing multi-data center training with these key advances: High efficiency across sites: Achieving 96% scaling efficiency by effectively distributing training across thousands of NVIDIA GPUs in geographically separated data centers. Advanced communication strategies: Overcoming inter-data center latency using hierarchical orchestration and gradient synchronization. Real-world success: Validating these innovations through the efficient training of a LLM with 340B parameters, paving the way for next-generation AI supercomputing. Why multi-data center training is hard Training trillion-parameter models isn’t just about adding more GPUs—it’s about overcoming key infrastructure challenges that influence cost and performance. When managing compute across multiple data centers, developers must contend with high inter-region latency (often 20 milliseconds or more) that can introduce performance bottlenecks during gradient updates and model synchronization during large-scale LLM training. Addressing these issues enables distributed LLM training architectures that boost performance, maximize hardware and energy efficiency, reduce infrastructure strain, and enable AI projects to scale across geographic regions without centralized resource bottlenecks. Key challenges include: High-latency and bandwidth limitations: Communication between data centers can be slow and constrained, reducing training efficiency.&nbsp;&nbsp; Synchronization: Keeping distributed data centers aligned demands sophisticated protocols and techniques. Traffic management: Minimizing data flow over long-haul networks is essential to maintaining low-latency and high-throughput. Enabling high-efficiency multi-data center training To overcome the challenges of multi-data center LLM training, NeMo Framework 25.02 and Megatron-Core 0.11.0 introduce four key innovations: Adaptive resource orchestration Hierarchical AllReduce (HAR) Distributed optimizer architecture Chunked inter-data center communications These capabilities optimize communication, orchestration, and compute efficiency across geographically separated sites, ensuring scalable, high-performance training of the world’s largest AI models. Adaptive resource orchestration Adaptive resource orchestration is a distributed training strategy that leverages the hierarchy of latency and bandwidth between various GPUs within and across clusters. It selects and prioritizes parallelism methods, which are resilient to communication delays and bandwidth constraints, making it ideal for cross-data center development deployments. In these setups, model-parallel techniques —such as tensor, context, and expert parallelism—demand frequent, high-bandwidth synchronization that doesn&#8217;t suit the high-latency environment between data centers. Instead, data parallelism and pipeline parallelism techniques are favored due to: Latency tolerance: Data parallelism’s batched gradient aggregation accommodates the larger delays inherent in inter-data center communications. Bandwidth efficiency: Hierarchical reduction patterns in data parallelism consolidate cross-data center traffic, significantly lowering bandwidth requirements. Hardware agnosticism: Both methods abstract hardware differences across sites through standardized sharding. By aligning the choice of parallelism technique with the network&#8217;s constraints, adaptive resource orchestration reduces the inter-data center bandwidth requirement per GPU to roughly 1/N of the intra-data center demand, yielding substantial efficiency gains over traditional flat approaches. Hierarchical all-reduce HAR involves synchronizing gradients in three steps:&nbsp; ReduceScatter within each group or data center,&nbsp; AllReduce across data centers. AllGather within each data center.&nbsp; This method minimizes traffic over long-haul networks by first optimizing inter-data center communication, ensuring high throughput and low latency.&nbsp; Figure 1 explains how HAR works. Figure 1. HAR explained Distributed optimizer architecture The partial-data parallel distributed optimizer enhances efficiency through localized weight updates and gradient reductions within each data center, followed by a single synchronized gradient reduction across sites, eliminating redundant optimizer state duplication while minimizing cross-data center communication. By sharding optimizer states within data centers (rather than globally) and replicating optimizer instances across sites, the architecture preserves memory efficiency at scale while reducing inter-data center traffic. Chunked inter-data center communications By splitting communications into chunks and overlapping those chunks with computation, inter-data center communication can be hidden behind intra-data center operations. This technique ensures that the training process remains efficient even at large scales, enabling high tolerance to latency between sites.&nbsp; Multi-data center training of NVIDIA Nemotron-4 340B&nbsp; Recently, we had an opportunity to run a large-scale training of Nemotron-4 340B .&nbsp;To set the baseline, the LLM was trained using a single data center with 3,072 NVIDIA GPUs.&nbsp;&nbsp; Next, the model was trained across two data centers, located approximately 1,000 kilometers apart, to demonstrate the effectiveness of these new features. As shown in Table 1 , the setup achieved over 96% of baseline throughput at a 3,072 GPU scale (1,500 GPUs in each data center), with independent inter- and intra-data center communications overlapping to maximize efficiency. By leveraging the capabilities of NeMo Framework and Megatron-Core, the training process achieved remarkable efficiency and scalability, setting a new standard for LLM development. Metric Single Data Center (ORD) Multi-Data Center (ORD + IAD) Total GPUs 3,072 GPUs 3,072 GPUs (1,536 in ORD, 1,536 in IAD) GPU Nodes 375 nodes (8 GPUs per node) 375 nodes (8 GPUs per node) Data Center Locations Oracle Cloud Infrastructure (OCI) – Chicago, IL (ORD) OCI – Chicago, IL (ORD) and Ashburn, VA (IAD) Distance Between Data Centers N/A Approximately 1,000 km Measured Round-Trip Latency N/A 21 milliseconds Scaling Efficiency Baseline (100%) Over 96% compared to single-site baseline Model FLOPS Utilization (MFU) 51% 49% Training Model Nemotron-4 340B Nemotron-4 340B Table 1. A comparison of the baseline compared to multi-data center training of Nemotron-4 340B Unleashing supercomputing across multiple facilities Multi-data center training is emerging as a transformative approach in AI factories, laying the groundwork for distributed systems that span several buildings and even regions. By integrating advanced networking and synchronization technologies, this approach coordinates vast arrays of GPUs across distinct facilities, ensuring that complex training tasks can run concurrently and seamlessly.&nbsp; NVIDIA GPU data center platforms , including low-latency networking solutions , and AI software stack, enable unprecedented parallelism. This full-stack platform paves the way for supercomputers that could eventually harness more than 500,000 GPUs across multiple data centers. This architecture not only scales computational power but also enhances reliability and flexibility by dynamically balancing workloads across multiple sites, reducing bottlenecks and optimizing energy efficiency.&nbsp; Get started today Support for training LLMs across multiple data centers is built into Megatron-Core , which is deeply integrated into NVIDIA NeMo Framework, an end-to-end platform for developing custom generative AI, including large language models (LLMs), vision language models (VLMs), retrieval models, video models, and speech AI—anywhere. It incorporates Megatron-Core for large-scale LLM training and provides a more extensive set of tools for building state-of-the-art custom generative AI, multimodal, and speech AI agentic systems. To learn more, see the NVIDIA NeMo Framework documentation and the GitHub examples repository . Discuss (0) Like Tags Data Center / Cloud | Models / Libraries / Frameworks | Cloud Services | NeMo | Nemotron | Intermediate Technical | featured | LLMs About the Authors About Kyle Aubrey Kyle Aubrey is the director of Technical Marketing at NVIDIA, where he leads initiatives in AI inference and training across NVIDIA accelerated computing platforms, including Hopper, Blackwell, Rubin, and beyond. With a passion for demystifying complex technologies, he empowers diverse audiences to harness the full potential of NVIDIA's cutting-edge solutions. Kyle holds a bachelor’s degree in Electrical Engineering from Rose-Hulman Institute of Technology and an MBA from Pepperdine University. View all posts by Kyle Aubrey About Hao Wu Hao Wu is a distinguished GPU architect at NVIDIA. He joined the NVIDIA Compute Architecture group in 2011 after finishing his Ph.D. at the Chinese Academy of Science. Hao’s technical focus has been low precision training, inference, and recently extreme scale LLM training. View all posts by Hao Wu About Dheevatsa Mudigere Dheevatsa Mudigere is a Distinguished Engineer in the NVIDIA Compute Architecture group, focusing on the application-driven co-design of large-scale AI systems. He and his team work on understanding current and future AI applications and developing HW/SW technology to enable more capable and efficient AI systems. Before NVIDIA, he worked on designing, building, and deploying production hyperscale AI systems. View all posts by Dheevatsa Mudigere About Selvaraj Anandaraj Selvaraj Anandaraj is a Deep Learning Performance Engineer working on accelerating Deep Learning workloads using NVIDIA hardware and software stacks. His recent work is focused on having a highly performant software stack to train and infer large language models at scale. He earned a Master’s degree from the University of Wisconsin-Madison with a specialization in Machine Learning systems. View all posts by Selvaraj Anandaraj About Wenwen Gao Wenwen Gao is a senior product manager for NeMo at NVIDIA, focusing on LLM training framework and microservices. Her past experience include LLM inference (NIM) and recommender systems (Merlin). She holds a B.S. in computer science from the University of Toronto and an M.B.A. from the MIT Sloan School of Management. View all posts by Wenwen Gao Comments Related posts NCCL Deep Dive: Cross Data Center Communication and Network Topology Awareness NCCL Deep Dive: Cross Data Center Communication and Network Topology Awareness Ensuring Reliable Model Training on NVIDIA DGX Cloud Ensuring Reliable Model Training on NVIDIA DGX Cloud New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility Networking for Data Centers and the Era of AI Networking for Data Centers and the Era of AI New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More Related posts Modeling Attacks on AI-Powered Apps with the AI Kill Chain Framework Modeling Attacks on AI-Powered Apps with the AI Kill Chain Framework How Quantization Aware Training Enables Low-Precision Accuracy Recovery How Quantization Aware Training Enables Low-Precision Accuracy Recovery Deploy Scalable AI Inference with NVIDIA NIM Operator 3.0.0 Deploy Scalable AI Inference with NVIDIA NIM Operator 3.0.0 How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M+ Token Context Workloads NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M+ Token Context Workloads L T F R E ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/",
      "full_text": " Train Generative AI Models More Efficiently with New NVIDIA Megatron&#x2d;Core Functionalities | NVIDIA Technical Blog DEVELOPER Home Blog Forums Docs Downloads Training Join Technical Blog Subscribe Related Resources Conversational AI English 中文 Train Generative AI Models More Efficiently with New NVIDIA Megatron-Core Functionalities Jul 12, 2024 By Erin Ho , Deepak Narayanan , Seonmyeong Bak , Zijie Yan , Shiqing Fan and Mikolaj Blaz Like Discuss (0) L T F R E AI-Generated Summary Like Dislike NVIDIA Megatron-Core is an open-source PyTorch-based library that provides GPU-optimized techniques and modular APIs for training large language models at scale, and has been used by companies like Reka AI and Codeium to train models efficiently. The latest version, Megatron-Core v0.7, supports multimodal training with the Large Language and Vision Assistant (LLaVA) pipeline, allowing model developers to blend multimodal datasets with determinism and reproducibility. Megatron-Core v0.7 also introduces improvements such as fast distributed checkpointing, training throughput optimization for mixture of experts models, and enhanced scalability features like fine-grained overlapping of data parallelism gradient all-reduce with the backward pass. AI-generated content may summarize information incompletely. Verify important information. Learn more First introduced in 2019 , NVIDIA Megatron-LM sparked a wave of innovation in the AI community, enabling researchers and developers to use the underpinnings of this open-source library to further large language model (LLM) advancements. Today, many of the most popular LLM developer frameworks have been inspired by and built using the Megatron-LM library, spurring a wave of foundation models and AI startups. Some of the most popular LLM frameworks built on top of Megatron-LM include Colossal-AI , Hugging Face Accelerate , and NVIDIA NeMo . To facilitate easy migration and enable researchers and model developers to access the latest research in distributed training, NVIDIA has recently revamped Megatron-LM . This resulted in NVIDIA Megatron-Core , an open-source PyTorch-based library with a collection of GPU-optimized techniques, cutting-edge system-level innovations, and modular APIs for training models at large scale. Megatron-Core continues to advance large-scale distributed training. This post highlights some of the recent advancements, including the new Large Language and Vision Assistant (LLaVA) pipeline for multimodal training. NVIDIA Megatron-Core Megatron-Core contains GPU-optimized techniques with cutting-edge system-level innovations. It abstracts these techniques into composable and modular APIs, providing full flexibility for framework developers and researchers to train custom transformers at scale on NVIDIA accelerated computing infrastructure. The Megatron-Core library offers the core building blocks for transformer models, such as attention mechanisms, transformer blocks and layers, normalization layers, and embedding techniques. Additional functionality, including activation recomputation and distributed checkpointing, is also natively built into this library.&nbsp; Popular LLM architectures such as GPT, BERT, T5, and RETRO can be efficiently built at large compute scales using Megatron-Core. Furthermore, Megatron-Core is compatible with all NVIDIA Tensor Core GPUs and can take advantage of the FP8 data format supported by the NVIDIA Hopper architecture to further boost compute throughput and reduce memory footprint. Megatron-Core has enabled customers like Reka AI and Codeium to train models at large scale. &#8220;Megatron-Core’s modular, composable design seamlessly integrates into our multimodal LLM architecture,” said Deyu Fu, a technical staff member at Reka AI. “With optimized GPU kernels and parallelism techniques, it helps us handle very large models and extensive contexts with ease, all while enabling dense and sparse training to scale efficiently at cluster levels.” “By using Megatron-Core, we&#8217;re able to stay on the frontier of techniques for training large language models by simply turning on a flag in the library,” said Devin Chotzen-Hartzell, a machine learning engineer at Codeium. “This enables us to focus on our differentiators in data and alignment.&#8221; Multimodal training is now supported in Megatron-Core&nbsp;&nbsp; With the introduction of visual instruction tuning , large multimodal models have garnered widespread interest from both researchers and industries. These models leverage various types of data to generate comprehensive and context-aware responses, using multiple sensory inputs to understand and interact with their environment. This advancement brings generative AI models closer to how humans process the world.&nbsp; We&#8217;re excited to announce that Megatron-Core v0.7 now supports multimodality. For a complete multimodality reference pipeline with LLaVA, visit NVIDIA/Megatron-LM on GitHub. Model developers can easily blend multimodal datasets with determinism and reproducibility using the open-source multimodal data loader under Megatron. This also works across checkpoint saves and loads. The LLaVA pipeline example walks you through how to:&nbsp; Prepare pretraining and supervised fine-tuning (SFT) datasets for Megatron webdataset-based formats. Leverage Megatron Core parallelism and memory-saving techniques to train a LLaVA architecture model initialized from Mistral and CLIP. Evaluate with different tasks like COCO captioning and VQAv2.&nbsp; The Megatron-Core v0.7 release focuses on functional aspects of the LLaVA pipeline, with a Massive Multidiscipline Multimodal Understanding (MMMU) score of 38, which is in the expected range for a 7B-parameter LLM-based LLaVA architecture. Additionally, with the Megatron-Core (Mcore) spec system , researchers can easily customize submodules in the PyTorch model definition. Within the next few releases, Megatron-Core will enable the use of heterogeneous parallelism strategies for different models. This approach is particularly beneficial because vision models, which are often smaller, typically require less complex sharding techniques compared to large language models in multimodal training. All Megatron-Core multimodal training capabilities, including the multimodal data loader , will soon be integrated into NVIDIA NeMo , enhancing the current multimodal features in NeMo for models like NeVa .&nbsp; Training throughput optimization for mixture of experts&nbsp; In the rapidly evolving landscape of generative AI, mixture of experts (MoE) models have become an attractive option, as they can be pretrained to achieve better accuracy without increasing the number of floating-point operations. In MoEs, the dense FFN layer is replaced with an MoE layer where each token is routed to a few experts, chosen by a router. Megatron-Core v0.7 expands MoE functionality and adds various training speed and memory optimizations, making Megatron-Core the most comprehensive solution for training MoEs at large scale. Specifically, Megatron-Core now supports MoE training with token dropping as used in GShard , and has training speed optimizations such as an enhanced GroupedGEMM with multi-CUDA stream computation and gradient accumulation fusion. Table 1 shows that Megatron-Core achieves throughput of over 400 TFLOP/s per-GPU throughput when training in BF16 precision, with an all-to-all dispatcher and sequence length = 4096. Each token is routed to (&#8211;moe-router-topk) two experts. We continue to optimize our FP8 recipes for MoE and will make them available in upcoming Megatron-Core releases. Megatron-Core also supports expert parallelism for MoEs, which can be combined with other parallelism techniques such as tensor, data, sequence, and pipeline parallelism already supported by Megatron-Core. For more details, see the User Guide . Model Precision # of GPUs MBS GBS TP EP PP Gradient accumulation Per-GPU throughput (TFLOP/s/GPU) Mistral 7B (Dense model baseline) BF16 128 4 256 2 N/A 1 1 492 Mixtral 8x7B BF16 128 1 256 1 8 4 8 402 Table 1. Per-GPU throughput for Mixtral 8x7B on NVIDIA H100 GPUs with dropless-token implementation in Megatron-Core v0.7 Fast distributed checkpointing for better training resiliency Distributed checkpointing is crucial for maintaining resiliency in large-scale training. The PyTorch native solution torch.save often lacks efficiency and scalability, leading to the development of more efficient solutions. For example, Azure Nebula and AWS Gemini offer asynchronous checkpointing, and the PyTorch Distributed Checkpoint (DCP) saves checkpoints per rank using threads. While these methods are faster than the vanilla `torch.save` by leveraging parallelism and asynchrony, challenges remain in achieving efficient asynchronous checkpointing.&nbsp; Specifically, these solutions perform asynchronous checkpointing either without parallelism within a multi-GPU server or by using Python threads (which can be inefficient due to the Python Global Interpreter Lock), leading to increased checkpointing times and lower training speed.&nbsp; These solutions also force users to load a checkpoint with the same parallelism configuration (PP and TP size, for example) used to store the checkpoint, preventing easy dynamic parallelism reconfiguration during a long training run. Megatron-Core v0.7 addresses these issues by introducing fully parallel and asynchronous saving capabilities. With fully parallel saving (FPS), data-parallel replicas perform parallel writes, enabling better utilization of the available file system bandwidth. Asynchronous parallel saving further speeds up distributed checkpointing by copying model parameters to the CPU (or local storage in the future) first before persisting the checkpoint to stable storage in the background, with minimal interruption to the main training process. Figure 1. Fully parallel saving in Megatron-Core uses the data-parallel replicas for parallel writing across nodes Most importantly, Megatron-Core enables users to resume training from a checkpoint saved with different tensor and pipeline parallelism degrees, providing the flexibility to change training configurations as needed during training.&nbsp; The save and load APIs in Megatron-Core are designed to be highly similar to PyTorch native APIs, making it easy to adopt Megatron-Core distributed checkpointing. When not using a distributed optimizer, this improvement reduces checkpointing overhead by 26x for Nemotron-4 340B compared to the native PyTorch solution, and by 50x for Nemotron-4 15B . With the distributed optimizer, users can achieve a 42x reduction in checkpoint overhead for Nemotron-4 340B (Figure 2).&nbsp; Figure 2. Checkpoint overhead comparison for a Nemotron-4 340B model across PyTorch native and the recent Megatron-Core releases&nbsp; Improved scalability Since the v0.5 release, Megatron-Core supports fine-grained overlapping of the data parallelism gradient all-reduce with the backward pass. This happens by grouping parameters into buckets and initiating asynchronous communication collectives for a bucket when all gradients for the bucket’s parameters are ready. This improves Megatron-Core throughput by reducing the amount of exposed data-parallel communication, and is especially useful when running configurations with a small batch size per GPU (and less gradient accumulation).&nbsp; Figure 3 shows per-GPU throughput for the Nemotron-4 15B model in a weak scaling experiment, where the batch size is increased as the data-parallel size is also increased (global batch size is 3*data_parallel_size), and a tensor-parallel size of 8. We observe that this optimization improves throughput by 34% with data-parallel size 32 and a batch size of 96. The --overlap-grad-reduce flag can enable this overlapping technique when using data parallelism. For more details, see the Megatron-Core documentation . Figure 3. Effect of the --overlap-grad-reduce optimization for Nemotron-4 15B using NVIDIA H100 GPUs and BF16 precision In the Megatron-Core v0.6 release, we introduced distributed optimizer support where the optimizer state is split over data-parallel replicas, reducing peak memory footprint. The distributed optimizer also breaks up the gradient all-reduce that was previously required into a gradient reduce-scatter (RS) and parameter all-gather (AG). Megatron-Core overlaps the reduce-scatter with the backward pass computation and the all-gather with the forward pass computation. These optimizations facilitate near-linear scaling for Nemotron-4 15B. Figure 4 shows per-GPU throughput for the 15B model with these optimizations enabled using a similar experiment setup to Figure 3. Figure 4. Effect of overlapping the reduce-scatter (RS) collective with the backward pass and all-gather (AG) collective with the forward pass for the Nemotron-4 15B model using NVIDIA H100 GPUs In addition, the v0.7 release further improves the speed of Megatron-Core at large data-parallel sizes by providing better in-built heuristics on how to size buckets. This enables communication to remain bandwidth-bound and not latency-bound even at DP sizes of &gt;300 (GPU count of &gt;3,000). Figure 5 shows per-GPU throughput for the 15B model up to a data-parallel size of 384 with the same weak scaling experiment setup and all optimizations enabled (distributed optimizer with both reduce-scatter and all-gather overlapping). Figure 5. Comparison of Megatron-Core 0.6 and 0.7 releases on Nemotron-4 15B using NVIDIA H100 GPUs up to a data-parallel size of 384 Megatron-Core optimizations also work out of the box for larger models that require other parallelism dimensions, including pipeline parallelism. The Nemotron-4 340B model uses these optimizations in Megatron-Core to achieve high training throughput at large GPU scales with BF16. Table 2 shows per-GPU throughput of Megatron-Core on the Nemotron-4 340B base model with different batch sizes. TP size is 8, PP size is 12, the number of virtual pipeline stages is 8, and sequence length is 4096. For more details, see the Nemotron-4 340B Technical Report . Note that other parallelism configurations can result in slightly higher throughputs. Precision # of GPUs (H100) Data-parallel size Batch size Per-GPU throughput (TFLOP/s/GPU) BF16 1536 16 768 419.3 BF16 3072 32 1536 418.3 BF16 6144 64 2304 405.0 Table 2. Per-GPU throughput of Megatron-Core on the Nemotron-4 340B base model with different batch sizes&nbsp; Get started Megatron-Core is available as open source in the NVIDIA/Megatron-LM repository on GitHub and can be used with Megatron-LM or NVIDIA NeMo . Megatron-LM, a lightweight training framework, offers a customizable native PyTorch training loop, ideal for users preferring fewer abstraction layers. It serves as a straightforward entry point for exploring Megatron-Core. For more details, see the Megatron-Core documentation . Megatron-Core is deeply integrated into NVIDIA NeMo , an enterprise-grade AI software platform with security, stability, manageability, and support. It incorporates Megatron-Core for LLM capabilities and provides a more extensive set of tools for multimodal and speech AI. To learn more, see the NVIDIA NeMo framework documentation . Discuss (0) Like Tags Conversational AI | Generative AI | HPC / Scientific Computing | NeMo | Intermediate Technical | Deep dive | featured | LLMs About the Authors About Erin Ho Erin Ho is the product manager for TensorRT quantization and Megatron-Core at NVIDIA, where her experience spans both training and inference. Her current focus is shaping the direction of NVIDIA's AI software to better serve the community. She holds an M.S. in computer science from National Tsing Hua University, complemented by a business degree from Carnegie Mellon University. View all posts by Erin Ho About Deepak Narayanan Deepak Narayanan is a senior applied deep learning research scientist in the ADLR group at NVIDIA, where he looks at making the training and inference of LLMs faster and more reliable. He holds a PhD in Computer Science from Stanford University. View all posts by Deepak Narayanan About Seonmyeong Bak Seonmyeong Bak is a senior system software engineer at NVIDIA, specializing in optimizing large-scale deep learning (DL) training performance on supercomputers. His expertise lies in analyzing and improving the communication and storage performance of DL training workloads. Before joining NVIDIA, Seonmyeong researched task-level parallel programming models for supercomputing. He holds a PhD in Computer Science from Georgia Institute of Technology and a bachelor’s degree in Computer Science from Sungkyunkwan University. View all posts by Seonmyeong Bak About Zijie Yan Zijie Yan is a senior DevTech engineer at NVIDIA, having joined the DevTech team in 2021. He specializes in improving the efficiency and scalability of large language model (LLM) training systems. Currently, Zijie drives the engineering initiatives for MoE support in Megatron-Core, where he collaborates closely with the team on the engineering development and performance enhancement of the MoE training system. Before joining NVIDIA, Zijie conducted research on communication optimization for distributed deep learning during his master's studies at Sun Yat-sen University. View all posts by Zijie Yan About Shiqing Fan Shiqing Fan is a senior architect in Compute Architecture at NVIDIA, where he works on improving the end-to-end performance of neural network training both at single-node scale and supercomputer scale. He received his M.S. and B.S. from Nanjing University. View all posts by Shiqing Fan About Mikolaj Blaz Mikolaj Blaz is a senior software engineer working on the efficiency of DL workloads at NVIDIA. He specializes in checkpointing, improving resiliency for large-scale LLM training. He received his M.Sc. in computer science from the University of Warsaw. View all posts by Mikolaj Blaz Comments Related posts NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Framework Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Framework New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More NVIDIA AI Platform Delivers Big Gains for Large Language Models NVIDIA AI Platform Delivers Big Gains for Large Language Models Related posts How to Scale Your LangGraph Agents in Production From A Single User to 1,000 Coworkers How to Scale Your LangGraph Agents in Production From A Single User to 1,000 Coworkers Upcoming Livestream: Building Cross-Framework Agent Ecosystems Upcoming Livestream: Building Cross-Framework Agent Ecosystems Upcoming Livestream: Techniques for Building High-Performance RAG Applications Upcoming Livestream: Techniques for Building High-Performance RAG Applications New Video: Build Self-Improving AI Agents with the NVIDIA Data Flywheel Blueprint New Video: Build Self-Improving AI Agents with the NVIDIA Data Flywheel Blueprint Best-in-Class Multimodal RAG: How the Llama 3.2 NeMo Retriever Embedding Model Boosts Pipeline Accuracy Best-in-Class Multimodal RAG: How the Llama 3.2 NeMo Retriever Embedding Model Boosts Pipeline Accuracy L T F R E ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2406.07887",
      "full_text": "An Empirical Study of Mamba-based Language Models\nRoger Waleffe1,2∗Wonmin Byeon1 Duncan Riach1 Brandon Norick1†\nVijay Korthikanti1 Tri Dao3,4 Albert Gu5,6 Ali Hatamizadeh1 Sudhakar Singh1\nDeepak Narayanan1 Garvit Kulshreshtha1 Vartika Singh1 Jared Casper1\nJan Kautz1 Mohammad Shoeybi1 Bryan Catanzaro1\n1NVIDIA\n2University of Wisconsin-Madison\n3Princeton University\n4Together AI\n5Carnegie Mellon University\n6Cartesia AI\nAbstract\nSelective state-space models (SSMs) like Mamba (Gu and Dao 2023) overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with sequence length\nand large inference-time memory requirements from the key-value cache. Moreover, recent studies\nhave shown that SSMs can match or exceed the language modeling capabilities of Transformers,\nmaking them an attractive alternative. In a controlled setting (e.g., same training data), however,\nstudies so far have only presented small scale experiments (training with <3B parameters and <1T\ntokens) comparing SSMs to equivalent Transformers. To understand the strengths and weaknesses\nof these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba,\nMamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to an 8B-parameter hybrid architecture consisting of 43% Mamba-2, 7%\nself-attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of natural language\ntasks, we answer the important question of whether Mamba models can match their Transformer\ncounterparts at larger training budgets. Our results show that while pure SSM-based models match\nor exceed Transformers on many tasks, both Mamba and Mamba-2 models lag behind Transformer\nmodels on tasks which require strong copying or in-context learning abilities (e.g., five-shot MMLU,\nPhonebook Lookup) or long-context reasoning. In contrast, we find that the 8B-parameter Mamba-\n2-Hybrid exceeds the 8B-parameter Transformer on all 12 standard tasks we evaluated (+2.65\npoints on average) and is predicted to be up to 8× faster when generating tokens at inference\ntime. To validate long-context capabilities, we provide additional experiments evaluating variants\nof the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequence\nlengths. On an additional 23 long-context tasks, the hybrid model continues to closely match\nor exceed the Transformer on average. To enable further study, we release the checkpoints as\nwell as the code used to train our SSM-based models as part of NVIDIA’s Megatron-LM project\n(https://github.com/NVIDIA/Megatron-LM)1.\n1\nIntroduction\nTransformer-based large language models (LLMs) (Vaswani et al. 2017) have become the dominant\nneural network architecture for natural language processing and have achieved impressive results across\na wide array of tasks (Achiam et al. 2023; Touvron et al. 2023). Much of the success of these models can\nbe attributed to their self-attention layers (Bahdanau, Cho, and Bengio 2014), which enable all-to-all\ninformation routing between tokens in a sequence, and their ability to improve with scaling model\nand dataset sizes. However, self-attention layers suffer from some drawbacks that make training and\ndeploying these models on long sequences challenging. At training time, the computation required for\nself-attention layers scales quadratically with the sequence length. At inference time, generating one\ntoken requires a memory capacity that scales linearly with the number of preceding tokens, necessitating\na large key-value cache to store the required state. Many recent works have attempted to address the\n∗Work done as an intern at NVIDIA\n†Correspondence to bnorick@nvidia.com\n1A fixed snapshot of the code used in this technical report is available at\nhttps://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba.\n1\narXiv:2406.07887v1  [cs.LG]  12 Jun 2024\n\nefficiency issues with self-attention layers (Tay et al. 2022); these works however have yet to match\nself-attention’s language modeling capabilities.\nStructured state space models (Gu, Goel, and Re 2021), in particular Mamba (Gu and Dao 2023)\nand more recently Mamba-2 (Dao and Gu 2024), have been proposed as a promising alternative to\nself-attention layers and Transformers. These models use constant computation and memory to generate\na single token at inference time (after initializing the SSM states based on the context) and can be\ncomputed efficiently using hardware-aware algorithms during training. They have been shown to match\nor exceed the downstream accuracy of Transformers on standard language modeling tasks for models up\nto 2.8B parameters (Dao and Gu 2024; Gu and Dao 2023). Follow up work has sought to further probe\nthe in-context learning abilities of these models at small scale (Park et al. 2024), and some recent work\nhas investigated combining Mamba layers with attention layers to form hybrid models (Glorioso et al.\n2024; Lieber et al. 2024). These works scale Mamba-based hybrid models beyond 7B parameters and\nshow that doing so can result in high quality models. However, in these studies the larger models were\nnot compared with equivalent Transformers in a controlled setting (i.e., same training data, parameter\ncount). Such controlled comparisons have been limited to small-scale experiments and larger-scale\nstudies of Mamba-2 models are still lacking.\nIn this technical report, we present a direct comparison between Mamba-based and Transformer-based\nLLMs trained on large datasets. In particular, our primary goal is to provide a rigorous apples-to-apples\ncomparison between Mamba, Mamba-2, Mamba-2-Hybrid (containing Mamba-2, attention, and MLP\nlayers), and Transformers for 8B-parameter models trained on up to 3.5T tokens, with the same\nhyperparameters. Using a diverse set of natural language tasks, we answer the important question\nof whether Mamba models can match their Transformer counterparts at larger training budgets. We\nevaluate these models on 35 popular downstream language modeling tasks and use the exact same\nevaluation setup for Mamba-based and Transformer models. To ensure our evaluations are standard\nand reproducible, we provide details about the specific open-source benchmark suites and versions used\nin our experiments in Section 2. Overall, our experiments eliminate the common difficulty of comparing\nLLMs, where it is often the case that both the model architecture but also the training data, tokenizer,\nand evaluation pipeline have changed.\nOur experiments show that while Mamba and Mamba-2 models are good at modeling language (e.g.,\nthey match or exceed Transformers on many downstream tasks), they lag behind Transformer models\nwhen it comes to in-context learning and recalling information from the context. This confirms recent\nfindings at smaller scales (Park et al. 2024). In particular, we highlight the difficulty pure SSM models\nface with the standard five-shot MMLU (Hendrycks, Burns, et al. 2020) and two-shot Phonebook tasks.\nFor the former, after training for 1.1T tokens, both Mamba and Mamba-2 models produce nearly 15\npoints lower accuracy when compared to a Transformer model on this task. While the MMLU accuracy\ngap is partially addressed by training with more tokens (e.g., 3.5T tokens), SSM models still lag behind\nTransformer models for this common benchmark. We find that Phonebook and standard long-context\nbenchmark tasks remain challenging for SSM models regardless of the number of training tokens.\nBased on the above findings, we study in detail the potential for hybrid SSM-Transformer models to\novercome the challenges faced by pure SSM architectures while retaining (some of) their inference-\ntime benefits. Similar to Lieber et al. 2024, we focus on LLMs consisting of a mixture of Mamba-2,\nself-attention, and MLP layers. Our ablation experiments aiming to identify the best hybrid model\narchitecture lead us to design an 8B-parameter Mamba-2-Hybrid with 24 Mamba-2 layers, 4 self-attention\nlayers, and 28 MLP layers. The self-attention and MLP layers are evenly distributed throughout the\nmodel. Extensive evaluations of this architecture show that it matches or exceeds Transformers on\ncommon natural language evaluations. When training for 3.5T tokens, a Mamba-2-Hybrid model\nexceeds a corresponding Transformer on all 12 short-context benchmarks we evaluated. On MMLU,\nthe hybrid model reaches a five-shot accuracy 3.5 points higher than the Transformer.\nWe also study long-context extensions of Mamba-2-Hybrid and the corresponding Transformer to\nsupport 16K and 32K context lengths. On 23 long-context evaluations, the 16K and 32K models closely\nmatch or exceed the Transformer baselines on average. Our results show that the hybrid models are\nparticularly good at retrieving, tracking, and aggregating information over long contexts. We highlight\nthree multi-document question answering tasks, however, which challenged the long-context hybrid\nmodels. We discuss potential reasons for these results and highlight areas of future work related to\n2\n\nX\nY\nSSM\ngroup norm\n×\nall reduce\ngroup norm\n×\nX\nY\nall reduce\nB, C\nconv1d\nprojection\nall reduce\nA\nB, C\nA\nconv1d\nSSM\nA\nX, B, C\nσ\nσ\nconv1d\nσ\nσ\nσ\nσ\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nSSM×\n×\nSSM\nprojection\nconv1d\nprojection\nprojection\nMamba\nMamba-2\nA\nB, C\nA\nB, C\nA\nX, B, C\nX\nX\nσ\nσ\nFigure 1: Mamba and Mamba-2 blocks with tensor model parallel size two. Mamba requires two\nall-reduces per layer while Mamba-2 requires only one. More details can be found in Dao and Gu 2024.\nextending hybrid SSM-Transformer models to long sequence lengths.\nFinally we highlight that, due to our use of global attention without any explicit position encoding\nin these models, long-context Mamba-2-Hybrid models can generalize beyond their trained sequence\nlength. This is in contrast with recent hybrid models that use windowed attention and exhibit accuracy\ndegradation on contexts larger than the window size but less than the pretraining sequence length (De\net al. 2024). We find that a Mamba-2-Hybrid extended to support 128K contexts can perform the\nPhonebook lookup task perfectly even when the phone book contains more than 150K tokens.\nWe present our findings above to highlight the promise for larger-scale SSM-based models to provide\nfaster, more efficient language model inference without compromising training efficiency or model\naccuracy compared to Transformers. We hope that by releasing these results, the community is further\nexcited by the potential of Mamba-based LLMs. To help enable further adoption, we release the\ncode used to train our Mamba, Mamba-2, and Mamba-2-Hybrid hybrid models as part of NVIDIA’s\nMegatron-LM library (https://github.com/NVIDIA/Megatron-LM). We also release the model weights\nfor our Mamba-2 8B and Mamba-2-Hybrid 8B on Hugging Face.\n2\nPreliminaries\nIn this section, we discuss briefly our implementation of SSM layers in Megatron-LM and discuss the\ntraining data and evaluations used throughout this report.\n2.1\nModel Implementation\nTo support efficient large-scale training, we implement Mamba and Mamba-2 layers with support for\ntensor (Shoeybi et al. 2019), sequence (Korthikanti et al. 2022), and pipeline parallelism (Narayanan et al.\n2021) (only for Mamba-2). As described in Dao and Gu 2024, tensor-parallel support for Mamba layers\nrequires two all-reduces per block compared to just one all-reduce for Transformer layers (Figure 1),\nleading to increased communication overheads for training larger-scale Mamba models. Mamba-2 tensor\nparallel support, on the other hand, requires only one all-reduce per layer, but requires the use of\nGroupNorm rather than LayerNorm for the internal block normalization (see Figure 1).\nWe found that using GroupNorm lead to no difference in validation loss when compared to using full\nLayerNorm as long as the group size (the model hidden dimension divided by the number of groups)\nis sufficiently large to allow for accurate calculations of the per-group normalization statistics (in our\n3\n\nexperience this meant a group size greater than 256). To implement SSM-Transformer hybrid models,\nwe combine our Mamba or Mamba-2 layers with the existing self-attention and MLP layers supported\nin Megatron-LM. These layers support all the previously mentioned parallelization strategies enabling\nus to immediately train hybrid models with tensor, sequence, and pipeline parallelism.\n2.2\nTraining Data\nWe train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are\npredecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English,\nand 15% code. For additional details, refer to the discussion included in the Nemotron-4 technical\nreport (Parmar et al. 2024). We use a vocabulary of 256K tokens trained with SentencePiece (Kudo\nand Richardson 2018).\n2.3\nEvaluation Tasks and Setup\nWe now discuss the evaluations used throughout the paper. Wherever possible, we use open-source\nLLM benchmark suites to ensure our evaluations are standard and reproducible. We report results\nusing a large number of common tasks:\n• Standard Short-Context Tasks: We use the open-source LM Evaluation Harness library\n(commit 94cc1850) (L. Gao et al. 2023) to evaluate the following 12 tasks (metric used for\nevaluation reported in parentheses): WinoGrande (accuracy) (Sakaguchi et al. 2021), PIQA\n(accuracy) (Bisk et al. 2020), HellaSwag (normalized accuracy) (Zellers et al. 2019), ARC-\nEasy and ARC-Challenge (accuracy and normalized accuracy) (Clark et al. 2018), MMLU\n(accuracy) (Hendrycks, Burns, et al. 2020), OpenBookQA (normalized accuracy) (Mihaylov et al.\n2018), TruthFulQA (accuracy) (Lin, Hilton, and Evans 2021), PubMedQA (accuracy) (Jin et al.\n2019), and RACE (accuracy) (Lai et al. 2017). Each of the proceeding tasks are evaluated by\nmeasuring the probability returned by the model for each possible answer choice. We also use the\ngeneration-based tasks Natural Questions (NQ) (exact match) (Lee, Chang, and Toutanova 2019)\nand SquadV2 (F1) (Rajpurkar, Jia, and Liang 2018).\n• Natural Long-Context Tasks: To evaluate long-context models, as above, we use three tasks\nfrom LM Evaluation Harness: NarrativeQA (F1) (Kočisk`y et al. 2018), Qasper (F1) (Dasigi\net al. 2021), and QuALITY (normalized accuracy) (Shaham et al. 2022). The first two tasks are\ngeneration-based, while the latter uses continuation probabilities returned by the model for each\nanswer. Each of these three tasks requires the model to answer a given question based on a long\ninput document.\nWe also use six tasks from the LongBench (Bai et al. 2023) long-context evaluation benchmark\n(commit 48798083): MultiFieldQA-English (F1), HotpotQA (F1) (Yang et al. 2018), 2WikiMQA\n(F1) (Ho et al. 2020), Musique (F1) (Trivedi et al. 2022), TREC (accuracy) (Li and Roth 2002),\nand TriviaQA (F1) (Joshi et al. 2017). Each of these six tasks requires the model to generate the\nanswer. MultiFieldQA tests a model’s ability to perform single-document question answering\nwhile HotpotQA, 2WikiMQA, and Musique measure multi-document question answer capabilities.\nTREC and TriviaQA are used to measure a model’s ability to perform in-context learning over\nlong inputs.\n• Synthetic Long-Context Tasks: Finally, we also evaluate our models using synthetic tasks\nthat aim to measure a model’s ability to retrieve, track, and aggregate information across long\ninput texts. For these evaluations, we use the Phonebook task introduced in Jelassi et al. 2024\n(illustrated in Figure 2b) and 13 open-source, generation-based tasks in the RULER benchmark,\ndescribed explicitly in Appendix B of Hsieh et al. 2024. The RULER tasks consist of eight Needle\nIn A Haystack (NIAH) variants, one multi-hop tracing task called Variable Tracking (VT), two\nlong-context aggregation tasks (Common Words Extraction (CWE) and Keywords Extraction\n(KWE)), one single-document question answer task (SquadQA), and one multi-document question\nanswer task (HotpotQA). For all tasks, we report the accuracy on 400 synthetic samples generated\nby RULER.\n4\n\nTable 1: 8B-parameter Mamba, Mamba-2, and Transformer architectures used in the experiments.\nModel\nParams (B)\n# Layers\nModel Dim\nAttn. Heads\nState Dim.\n# Groups\nPos. Emb.\nSeq. Len\nTransformer\n8.53\n32\n4096\n32\n-\n-\nRope\n4096\nMamba\n8.15\n56\n4096\n-\n128\n-\nNone\n4096\nMamba-2\n8.24\n56\n4096\n-\n128\n8\nNone\n4096\n3\nMamba and Mamba-2 Compared to Transformers\nIn this section we discuss our observations and experimental results training 8 billion (8B) parameter\nMamba and Mamba-2 models and compare with 8B-parameter Transformer models. We find that\nMamba and Mamba-2 can match or exceed Transformers on standard zero-shot tasks (Section 3.3.1) but\nlag behind on MMLU and copying tasks, which we discuss in details in Sections 3.3.2 and 3.3.3.\n3.1\nModel Architectures\nWe train Mamba, Mamba-2, and Transformer models with the architectures summarized in Table 1.\nWe discuss the architectures in more detail next. Additional details can be found in the released model\ncheckpoints and open-sourced code in Megatron-LM.\nTransformer.\nOur 8B Transformer model follows the style of GPT3 (Brown et al. 2020) and\nconsists of 32 layers (each Multi-Head Attention + MLP) with a hidden dimension of 4096. We use 32\nattention heads, 128 KV-channels, a 4× expansion for the MLPs, SwiGLU activation (Shazeer 2020),\nLayerNorm (Ba, Kiros, and Hinton 2016), and RoPE (J. Su et al. 2024) for position embeddings. We\ndo not use bias weights for linear layers or Dropout. Additionally, we use seperate parameters for model\nembeddings and output layer weights (which we refer to as untied embeddings).\nMamba.\nWe train an 8B-parameter Mamba model with hidden dimension 4096 and 56 layers\n(typically 2 Mamba layers have around the same parameters as one block of attention + MLP). The state\ndimension for each Mamba layer is set to 128 and we use GELU (Hendrycks and Gimpel 2016) activation.\nFollowing (Gu and Dao 2023), we do not use any explicit position encoding and for normalization we\nuse RMSNorm (B. Zhang and Sennrich 2019). As for the Transformer, we do not use bias weights for\nlinear layers or Dropout and we use untied embeddings.\nMamba-2.\nFor Mamba-2 models, we use the same architecture as above for Mamba except replace\neach layer with the updated Mamba-2 block (Dao and Gu 2024). We set the internal Mamba-2 state\ndimension to 128 and use eight groups. We retain the default values from Dao and Gu 2024 and use a\nhead dimension of 64, expansion factor of two, and window size of four for convolution.\n3.2\nTraining Hyperparameters\nWe train the above models on 1.1T and 3.5T token datasets (see details in Section 2) using the following\nhyperparameters: On the smaller dataset, we use a batch size of 256, peak learning rate of 1e-4 and\nminimum learning rate of 1e-5. On the larger dataset we increase the batch size to 1024 and use higher\nlearning rates: a peak of 3e-4 and minimum of 3e-5. On both datasets we use learning rate warm up\nover 122K samples, a cosine learning rate schedule, weight decay of 0.1, 0.9 and 0.95 for Adam β1 and\nβ2 parameters respectively, and train using BF16. We performed some studies at smaller scale and\nfound that Mamba network hyperparameters are similar to that of Transformers and as a result we use\nthe same hyperparameters across models to make a rigorous direct comparison.\n3.3\nEmpirical Evaluation of Mamba and Mamba-2\n3.3.1\nDownstream Language Modeling Tasks\nIn Table 2 and 3 we report the results of training our 8B-parameter Mamba, Mamba-2, and Transformer\nmodels on 1.1T and 3.5T tokens respectively, using six standard tasks for measuring natural language\nunderstanding. On the 3.5T dataset, we train only a pure Mamba-2 model (and not a Mamba model)\n5\n\nTable 2: Evaluation results for 8B-parameter models trained on 1.1T tokens. Pure SSM models (Mamba\nand Mamba-2) match or exceed Transformers on many natural language tasks, but fall short on others\n(e.g., MMLU) (see Section 3.3).\nModel\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nAvg. w/o MMLU\nAvg\n0-Shot\n5-Shot\nTransformer\n69.22\n78.29\n75.6\n73.15\n43.09\n38.32\n46.28\n67.87\n60.56\nMamba\n68.27\n78.89\n75.63\n75.42\n42.15\n28.63\n28.00\n68.07\n56.71\nMamba-2\n70.8\n78.35\n75.54\n75.13\n43.00\n28.94\n29.19\n68.56\n57.28\nfor efficiency reasons—the pure Mamba model on the 1.1T dataset was almost 3× slower than the pure\nMamba-2 model due to the large state dimension.\nOur results confirm those of prior works (Dao and Gu 2024); both Mamba and Mamba-2 models can\nmatch or exceed Transformer models on common tasks. On both datasets, pure SSM models achieve\nhigher accuracy than Transformers when averaged over the WinoGrande, PIQA, HellaSwag, ARC-Easy,\nand ARC-Challenge evaluation tasks. The results on the 1.1T dataset also highlight that pure Mamba-2\nmodels are equal or better than Mamba models on average. The most interesting observation from these\nresults, however, is that the accuracy on MMLU is significantly worse for pure SSM models compared\nto the Transformer when training for the shorter token horizon (1.1T tokens). For example Mamba-2\nfive-shot accuracy is 17 points lower than that of the Transformer in this setting. Table 3 shows that\ntraining for more tokens helps the Mamba-2 model improve on MMLU (visualized in Figure 6), closing\nthe gap to the Transformer to just 1.37 points. We discuss the MMLU result in more detail in the\nfollowing section.\nTable 3: Evaluation results for 8B-parameter models trained on 3.5T tokens. We train only pure\nMamba-2 and Transformer models in this setting due to efficiency issues with training larger-scale\nMamba models. Training for 3.5T tokens instead of 1.1T (Table 2) allows Mamba-2 to approach\nTransformer-level accuracy on MMLU and produces a pure SSM model that exceeds the Transfomer in\nterms of average task accuracy.\nModel\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nAvg. w/o MMLU\nAvg\n0-Shot\n5-Shot\nTransformer\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n68.14\n62.35\nMamba-2\n71.59\n79.82\n77.69\n75.93\n48.12\n47.25\n48.7\n70.63\n64.16\n3.3.2\nA Closer Look at MMLU\nWe investigate the gap in MMLU accuracy between pure SSM models and Transformers by evaluating\nour 1.1T Mamba, Mamba-2, and Transformer models (where the gap is largest) on different instances\nof this task. Generally, MMLU accuracy is calculated by prompting the model with a question that\nincludes four answer choices labeled with the letters A, B, C, and D. The model is then shown each of\nthe four letters A, B, C, and D and the letter most likely to follow the prompt (measured by probabilities\noutput from the model) is taken as the model’s answer (Figure 2a). MMLU accuracy, however, can also\nbe measured by calculating the probability of the full answer choice following the prompt (which we\ncall a choice-text-in-targets variant) or using a cloze format. In the latter case, the model is prompted\nwith only the question (no answer choices are provided) and the text of each answer is used to calculate\nprobabilities.\nWe show the results of evaluating our 8B-parameter pure SSM and Transformer models trained on 1.1T\ntokens on the three formulations of MMLU described above in Table 4. While the pure SSM models\nstruggle with the standard and choice-text-in-targets formulations, they actually exceed the accuracy of\nthe Transformer in the cloze setting. This experiment, together with the MMLU results for Mamba-2\ntrained on 3.5T tokens (Table 3, Figure 6), highlight that the pure SSM models contain the\nsame knowledge as the Transformer, but that they require substantially more training to\nunderstand the format of the multiple choice questions in the first two settings. We hypothesize\n6\n\nQuestion\nWhich is a valid expression in Python 3.5?\nStandard\nChoices\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nTargets\nA\nB\nC\nD\nChoice text in targets\nChoices\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nTargets\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nCloze\nChoices\nTargets\nsort('ab')\nsorted('ab')\n\"ab\".sort()\n1/0\n(a) Three different formats for MMLU multiple choice\nquestions. In all cases the model is first prompted with\na question. In the standard and ‘choice text in targets’\nvariants (but not ‘cloze’ variant), the prompt includes\nfour multiple choice answers following the question.\nThe correct answer is then calculated by measuring\nwhich of the four answers, represented in the target\nformat, is predicted to have the highest probability of\nfollowing the given prompt.\nStandard\nPhonebook\nRamona Thornton: 333-848-7744\nLillie Acevedo: 254-992-9261\nJake Barron: 130-894-2531\nConnor Savage: 783-474-346\nEmilio Burton: 258-542-2658\nFew-shot examples\nWhat is the phone number for Lillie Acevedo?\nAnswer: 254-992-9261\nWhat is the phone number for Emilio Burton?\nAnswer: 258-542-2658\nSuffix\nWhat is the phone number for Jake Barron?\nAnswer:\nAnswer\n130-894-2531\nReversed\nInstructions\nGiven a phone book with entries of the form:\nLillie Acevedo: 254-992-9261\nRamona Thornton: 333-848-7744\n... and so on.\nPlease find the phone number for Jake Barron.\nI will ask you for this phone number later.\nMemorize it so you can respond correctly.\nRemember Jake Barron.\nHere is the phonebook:\nPhonebook\nRamona Thornton: 333-848-7744\nLillie Acevedo: 254-992-9261\nJake Barron: 130-894-2531\nConnor Savage: 783-474-346\nEmilio Burton: 258-542-2658\nSuffix\nOkay, that is the end of the phonebook.\nRemember when I asked you to memorize the\nphone number for Jake Barron?\nWhat is the phone number for Jake Barron?\nAnswer\n130-894-2531\n(b) Two different versions of the synthetic Phonebook\ntask. In the standard configuration (left) the model is\nfirst prompted with a phone book. It is then shown two\nin-context example questions before the final question\nwhich asks it to recall the phone number for a specific\nperson.\nIn the ‘reversed’ setting (right) the model\nis told which phone number to look for before being\nshown the phone book and then later asked to recall\nthis number.\nFigure 2: Illustrations of the MMLU and Phonebook tasks used in the experiments.\nTable 4: MMLU accuracy for pure SSM and Transformer models according to the three task variants\ndescribed in Figure 2a. Results are for 8B-parameter models trained on 1.1T tokens. While pure SSM\nmodels struggle with the standard and choice-text-in-targets formulations on this token horizon, they\nmatch or exceed the Transformer model in the cloze setting. Together with the results for Mamba-2\ntrained on 3.5T tokens (Table 3, Figure 6), this suggests that SSM models contain the same knowledge\nas Transformers, but require more training to understand the MMLU task formatting.\nModel\nMMLU-Standard\nMMLU-W/Choice\nMMLU-Cloze\n0-Shot\n5-Shot\n0-Shot\n5-Shot\n0-Shot\n5-Shot\nTransformer\n38.32\n46.28\n33.54\n46.64\n37.26\n39.24\nMamba\n28.63\n28.00\n27.42\n29.17\n38.26\n39.28\nMamba-2\n28.94\n29.19\n28.54\n30.68\n37.68\n38.17\nthat the reason for this confusion, especially in the standard MMLU setting, is that pure SSM models\nare unable to directly route the knowledge of each answer into a single answer token. In contrast, the\nself-attention layers in the Transformer are particularly good at that task, especially when they are\nshown several in-context examples that teach them to do such routing (e.g., 5-Shot MMLU in the\nstandard formulation). Finally, we note that while the Mamba-2 hybrid model trained for 3.5T tokens\ncloses the MMLU gap to the Transformer, it sees an accuracy improvement on standard MMLU of\nonly 1.45 points when moving from 0- to 5- shot, compared with 4.38 for the Transformer, providing\nadditional evidence that Transformers may have superior in-context learning capabilities.\n3.3.3\nCopying Tasks\nBeyond downstream language modeling tasks, we also evaluate pure SSM-based models and compare\nto Transformers on the synthetic Phonebook task (Jelassi et al. 2024) that aims to measure a model’s\nability to perform in-context learning (through few shot examples) and copying from earlier in the\ncontext. We illustrate an example Phonebook prompt in Figure 2b. The model is first prompted with a\nlist of (name, phone number) pairs, and then asked ‘What is the phone number for {name}?’ with two\n7\n\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba\nMamba-2\nTransformer\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0\n2\n4\n6\n8\n10\n12\nAvg. # of Correct Tokens\nMamba\nMamba-2\n(b)\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba\nMamba-2\nTransformer\n(c)\nFigure 3: Evaluation results for pure SSM and Transformer models (trained for 1.1T tokens) on the\nPhonebook task illustrated in Figure 2b. (a) On the standard Phonebook task, Transformers are\ncapable of in-context learning and answering questions that require copying from the input, but SSM\nmodels struggle with this task. (b) In the standard Phonebook setting (i.e., (a)), SSM models exhibit\nfuzzy memory—while they are unable to correctly predict the phone number, they predict phone\nnumbers that share multiple digits (in the right locations) with the correct answer (see Section 3.3.3).\n(c) On the Reversed Phonebook formulation, even when notified at the beginning of the context which\nphone number they will be asked to recall, SSM models still lag behind Transformer models.\nexample question answer pairs before the actual question used for testing. For each trial, we randomly\ngenerate names and phone numbers to create the phone book and randomly select which names are\nused for the two examples and the final query. Accuracy on this task is then measured by whether the\nmodel generates the correct phone number or not.\nWe vary the length of the phone book (the number of (name, phone number) pairs) and plot the\naccuracy for each phone book length averaged over 20 different random initializations in Figure 3a. The\n8B Transformer model can respond correctly with near 100% accuracy for phone book lengths up to its\npretraining context length (4096). In contrast, both Mamba and Mamba-2 models begin to respond\nincorrectly for input sequence lengths beyond approximately 500 tokens. In contrast to MMLU, this\nbehavior persists for Mamba-2 even when training for 3.5T tokens (Figure 7a).\nA closer look at the SSM model predictions shows that while they cannot perfectly recall the correct\nphone number, these models have compressed information about each phone book entry into their\nrunning states—we show in Figure 3b the average number of correct tokens predicted by Mamba and\nMamba-2 on Phonebook by comparing the predicted answer to the true answer. Figure 3b shows that\npure SSM-based models have fuzzy memory. That is, while they cannot predict the phone number\nexactly, they do generally respond with phone numbers that are similar to the correct answer.\nFinally, we evaluate whether changing the Phonebook prompt allows for SSM models to achieve better\nresults. In particular, we prompt the model with the name of the person whose phone number it will\nbe asked to recall before showing it the phone book (the Reversed formulation in Figure 2b). Figure 3c\nshows the results of the 8B Mamba, Mamab-2, and Transformer models in this modified Phonebook\nsetting. Interestingly, while the SSM models achieve better accuracy as a function of phone book\nlength using this prompt, the accuracy still degrades for phone books with lengths shorter than 4096\n(the sequence length used for pretraining). Even with the modified Phonebook prompt, it remains\nchallenging for the SSM to decide which information to store exactly and which information to forget\non this task. We hypothesize that finetuning Mamba and Mamba-2 on the Phonebook task would lead\nto improved accuracy.\n3.3.4\nTakeaway\nOur experiments training 8B-parameter Mamba and Mamba-2 models showed that while these models\nachieve comparable or better accuracy than Transformers on many standard natural language modeling\ntasks, they achieve lower accuracy on others. In particular, we identified MMLU (with smaller token\nhorizons) and Phonebook as challenging tasks for pure SSM-based models and hypothesize that this is\nbecause these tasks require in-context learning, information routing between tokens, and copying from\nthe context.\n8\n\n4\nHybrid Mamba-Transformer Models\nMotivated by the difficulties pure SSM models face with retrieving information from the context and\nin-context learning, we now study the hypothesis that adding a few Transformer layers (made of\nself-attention and MLP layers) back into the architecture enables the model to overcome these issues.\nIn this section we consider hybrid models containing a combination of Mamba/Mamba-2, self-attention,\nand MLP layers.\n4.1\nDesigning a Hybrid SSM-Transformer Model\nWe begin by discussing the ablation studies that led us to design our final hybrid model architecture.\nFor the experiments reported in this section, all model variants have the same number of parameters\nper layer. This ensures that model quality changes are not due to an increase or decrease in the overall\nnumber of parameters, and also that we can control the ratio of parameters by controlling the ratio of\nlayers. To do so, we adjust both the number of attention heads (while keeping the head size constant)\nand the MLP expansion factor such that self-attention and MLP layers have approximately the same\nnumber of parameters as Mamba layers.\n0\n20\n40\n60\n80\n100\nPercentage of Attention Layers\n2.56\n2.58\n2.60\n2.62\nValidation Loss\nFigure 4:\nValidation loss versus per-\ncentage of attention layers for 130M-\nparameter hybrid Mamba-Transformer\nmodels (24 total layers).\nNumber of Attention and MLP Layers.\nWe first\nstudy how the number of self-attention and MLP layers in a\nhybrid model impacts model quality. For these experiments,\nwe train 130M parameter hybrid models with 24 layers, and\nvary the percentage of the those layers that are attention and\nthat are MLP. As we increase the percentage of these layer\ntypes, we evenly distribute them throughout the model, as\ndescribed in Appendix A. We report the validation loss as a\nfunction of the attention layer ratio in Figure 4. From these\nexperiments, we discover that validation loss is minimized\nwhen roughly 8% of the layers are self-attention layers. Ex-\nperiments with 840M parameter models confirm that these\nfindings scale across model sizes. These results are also\nconsistent with those reported by Dao and Gu 2024. After\nfixing the percentage of attention layers to 8, we vary the percentage of MLP layers between 5 and\n50. We conclude that 30%-50% of the layers can be MLPs without increasing model loss. In general,\nwe prefer larger MLP layer ratios from an efficiency perspective—with 50% of the layers set as MLPs,\ntraining is 20% faster than when MLP layers make up only 5% of the model.\nPosition Embeddings.\nWe next evaluate whether or not to add Rotary Position Embeddings\n(RoPE) (J. Su et al. 2024) to every self-attention layer in a hybrid model. For these experiments, we\ntrain an 840M-parameter Mamba-Hybrid model on the 1.1T token dataset with and without RoPE,\neach with an attention layer ratio of 8%. We use a 4096 sequence length. We then extend these base\nmodels to a context length of 16384 through continued pretraining on the longer sequence length\nfor an additional 16B tokens. We experiment with and without adjusting the RoPE base frequency\nduring continued pretraining (continued pretraining with an increase base frequency was introduced\nby Xiong et al. (2023)). Results are reported in Table 5. The base 840M model trained with RoPE\nprovides a similar accuracy to the model without RoPE, but achieves a lower average accuracy after\nlong context extension (regardless of whether the RoPE base frequency is modified or not). Based on\nthese experiments, as in recent work (Lieber et al. 2024), we opt to ignore RoPE position embeddings\nfor larger-scale hybrid models.\nAdditional Ablation Experiments.\nWe also evaluated how the ordering of Mamba/Mamba-2,\nself-attention, and MLP layers affects the resulting model’s natural language abilities (measured with\nvalidation loss). When testing, following Park et al. (2024), we made certain that a Mamba layer\nappears at the beginning of the architecture—this ensures that the hybrid model can operate without\nposition embeddings, as the first Mamba layer naturally learns to encode the positional information.\nOur experiments found no significantly better configuration than to evenly distribute self-attention\nand MLP layers throughout the model, as described in Appendix A. We did not find it necessary to\n9\n\nTable 5: Comparison of 840M parameter hybrid Mamba-Transformer models with and without RoPE\nposition embeddings when using an attention layer ratio of 8% and training on 1.1T tokens. We train a\nbase model with a sequence length of 4096. We extend this base model to a sequence length of 16384\nwith continued pretraining for an additional 16B tokens. We find that hybrid models do not need\nposition embeddings, and may perform better without them on long contexts.\nSeq.\nRoPE\nRoPE Freq. Base\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nAvg\n4K\nno\n-\n56.2\n70.84\n52.13\n55.01\n26.79\n52.19\nyes\n10K\n55.8\n71.71\n52.3\n55.43\n26.71\n52.39\n16K\nno\n-\n56.83\n72.31\n53.86\n56.52\n27.65\n53.43\nyes\n10K\n54.62\n71.49\n53.17\n56.23\n27.56\n52.61\nyes\n500K\n54.7\n71.38\n50.42\n54.29\n26.79\n51.52\nTable 6: Summary of the 8B-parameter Mamba-2-Hybrid architecture used throughout the experi-\nments. We design the layer pattern to spread attention and MLP layers evenly throughout the model\n(Appendix A).\nModel\nParams (B)\n# Layers\nModel Dim\nAttn. Heads\nState Dim.\n# Groups\nPos. Emb.\nSeq. Len\nMamba-2-Hybrid\n8.66\n56\n4096\n32\n128\n8\nNone\n4096\nHybrid Layer Pattern (M=Mamba-2, *=Self-Attention, +=MLP)\nM+M+M++M+M*+M+M+M+M++M*+M+M+M+M+M*++M+M+M+M+M*+M++M+M+M+\nconstruct hybrid model architectures using a repeated block pattern. We also found that hybrid models\ncan use self-attention layers with Group-Query Attention (GQA) (Ainslie et al. 2023) rather than\nMulti-Head Attention (MHA) with little degradation in model quality (validation perplexity increases\n≈0.04%). Given the decrease in the amount of computation and memory required for inference with\nGQA compared to MHA, we thus opt to use GQA when training larger-scale hybrid models.\n4.2\nMamba-2-Hybrid 8B\nModel Architecture and Hyperparameters.\nBased on the study described in Section 4.1, we\ntrain an 8B-parameter hybrid SSM-Transformer model with the architecture summarized in Table 6.\nOut of 56 total layers, the hybrid model has 4 (7.1%) self-attention layers, 24 (42.9%) Mamba-2 layers,\nand 28 (50%) MLP layers. Rather than using a single repeated hybrid block structure, to construct our\nmodel we allocate the layers such that 1) a Mamba-2 layer comes first and 2) the attention and MLP\nlayers are evenly distributed throughout the model, as described in Appendix A. We use Mamba-2 for\nthe SSM-layers rather than Mamba, as the SSM scan used by Mamba-2 is up to 8× faster than that\nof Mamba (Dao and Gu 2024). Moreover, our experiments in Section 3.3, showed that 8B-parameter\nMamba-2 models match or exceed 8B-parameter Mamba models on common downstream natural\nlanguage tasks. For the Mamba-2 layers, we use the same parameters as for our pure Mamba-2 model\n(Section 3.1). That is, we use an internal state dimension of 128, eight groups, a head dimension of\n64, expansion factor two, and window size of four for convolution. For the attention layers, we use\nGroup Query Attention with eight groups, 32 attention heads, and 128 KV-Channels. For MLP layers,\nwe use a 4× expansion ratio. Throughout the model, we use a hidden dimension of 4096 and GELU\nactivation. We opt to use no explicit position embeddings. For each layer, we include a residual skip\nconnection and RMSNorm before the Mamba-2, self-attention, or MLP block. As for the pure SSM and\nTransformer models, we do not use Dropout, biases for linear layers, and we use separate parameters for\nmodel embeddings and output layer weights (i.e., untied embeddings). We train our Mamba-2-Hybrid\n8B on the 1.1T token and 3.5T token datasets using the hyperparameters described in Section 3.1 (i.e.,\nthe exact same ones as for the Transformer models and pure SSM models).\nTraining Efficiency.\nWe highlight that our Mamba-2-Hybrid model implemented in Megatron-LM\ncan be trained efficiently on thousands of GPUs. To do so, we compare our measured Model Flop\nUtilization (MFU) with that of Transformers. As in prior work (Korthikanti et al. 2022), we define the\nMFU as follows: First we define the model FLOPs per second to be the number of FLOPs required to\n10\n\nTable 7: Detailed evaluation results on 12 common natural language tasks comparing an 8B-parameter\nhybrid model (Mamba-2-Hybrid) with the pure Mamba-2 SSM and Transformer models from Section 3.3\nwhen training for 3.5T tokens. The Mamba-2-Hybrid model achieves the highest overall accuracy and\nis 2.65 points better than the Transformer on average.\nModel\nWG\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nOpenBook\nTruthFul\nPubMed\nRACE\nNQ\nSquadV2\nAvg\n0-Shot\n5-Shot\nTransformer\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n42.00\n35.48\n69.20\n39.52\n15.15\n53.4\n53.17\nMamba-2\n71.59\n79.82\n77.69\n75.93\n48.12\n47.25\n48.7\n44.2\n35.66\n75.2\n37.7\n17.17\n51.9\n54.69\nMamba-2-Hybrid\n71.27\n79.65\n77.68\n77.23\n47.7\n51.46\n53.60\n42.80\n38.72\n69.80\n39.71\n17.34\n58.67\n55.82\nperform a model forward and backward pass divided by the iteration time. We can then define the\nMFU to be the model FLOPs per second divided by the peak theoretical FLOPs per second of the\nGPUs used for training. When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel\nsize of four and data-parallel size of 256 (1024 total GPUs) (micro batch size 4, global batch size\n1024), our Mamba-2-Hybrid achieves an MFU of 29.9%. This can be compared to the 30.7% MFU of a\ncorresponding 8B parameter Transformer implemented in Megatron-LM and trained with the same\nparallelization configuration.\n0\n20\n40\n60\n80\n100\n120\nInput Context Length (Thousand Tokens)\n1\n2\n3\n4\n5\n6\n7\nInference Speedup (×)\nFigure 5: Predicted speedup to generate\none token for an 8B-parameter Mamba-2-\nHybrid model compared to a Transformer.\nInference Speed.\nWe also highlight that the hybrid\nmodel benefits from the inference-time speedups expected of\na pure SSM model compared to a pure Transformer model.\nIn Figure 5, we plot the predicted time to generate one\ntoken for the 8B Transformer model over the time for the 8B\nMamba-2-Hybrid model using a batch size of 32. For short\ninput context lengths, both models can generate the next\ntoken in roughly equivalent time. For long context lengths,\nhowever, the hybrid model benefits from its many SSM\nlayers and generates the content nearly 8× faster than the\nTransformer. We expect additional inference-time benefits\nfor the hybrid model due to a reduced key-value cache size\nthat should enable Mamba-2-Hybrid to use larger batch\nsizes than possible with the Transformer model.\n4.3\nEmpirical Evaluation of Mamba-2-Hybrid\n4.3.1\nDownstream Language Modeling Tasks\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTraining Completion (tokens)\n1e12\n30\n35\n40\n45\n50\n55\nMMLU Five Shot Avg Accuracy\nMamba-2-Hybrid\nMamba-2\nTransformer\nFigure 6:\nFive-shot MMLU accuracy\n(standard formulation) for 8B-parameter\nmodels trained on 3.5T tokens as a func-\ntion of training completion.\nWe evaluate the Mamba-2-Hybrid 8B model trained on the\n3.5T token dataset using downstream natural language tasks\nin Table 7. We include comparisons with the pure SSM and\nTransformer models discussed in Section 3.3. Remarkably,\nMamba-2-Hybrid achieves higher accuracy than the corre-\nsponding Transformer on all 12 common natural language\ntasks we evaluated (Table 7). The average improvement\non these tasks compared to the Transformer model is 2.65\npoints. Figure 6 shows the behavior of Mamba-2-Hybrid,\nMamba-2, and the corresponding Transformer for MMLU\naccuracy as training progresses. While Mamba-2-Hybrid\nalways leads pure Mamba-2, early in training the hybrid is\ninferior in this regard to the Transformer. However, once the\nhybrid matches the Transformer’s accuracy at 1.5T tokens it\nquickly gains and maintains a strong advantage. We believe\nthis motivates further study into the data efficiency and\nsaturation behavior of Mamba-based models.\nTable 7 and Figure 6 show that when training on sufficiently long token horizons (in our case 3.5T\ntokens), a hybrid model containing Mamba-2, self-attention, and MLP layers can exceed the accuracy\n11\n\nTable 8: Standard task evaluation results of 16K and 32K sequence length variants of the 8B-parameter\nMamba-2-Hybrid and Transformer model trained on 3.5T tokens. These tasks do not require long-\ncontext abilities but they show that the accuracy on these tasks does not degrade for the long-context\nmodels when compared to the base 4K models.\nModel\nWG\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nOpenBook\nTruthFul\nPubMed\nRACE\nNQ\nSquadV2\nAvg\n0-Shot\n5-Shot\nTransformer-4K\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n42.00\n35.48\n69.20\n39.52\n15.15\n53.4\n53.17\nMamba-2-Hybrid-4K\n71.27\n79.65\n77.68\n77.23\n47.7\n51.46\n53.60\n42.80\n38.72\n69.80\n39.71\n17.34\n58.67\n55.82\nTransformer-16K\n70.4\n78.67\n76.4\n74.45\n44.37\n47.48\n51.22\n42.2\n36.33\n69.8\n38.95\n14.29\n55.38\n53.84\nMamba-2-Hybrid-16k\n71.67\n79.92\n78.24\n77.95\n48.12\n52.01\n54.92\n44.60\n37.23\n70.00\n39.33\n18.50\n58.99\n56.27\nTransformer-32K\n69.22\n78.51\n76.01\n73.74\n43.09\n47.80\n50.42\n41.60\n36.28\n69.40\n38.66\n15.79\n54.93\n53.50\nMamba-2-Hybrid-32K\n71.43\n79.54\n78.08\n78.07\n47.70\n52.41\n55.09\n45.40\n37.86\n71.00\n40.10\n18.64\n57.93\n56.4\nof a pure Mamba-2 and a pure Transformer model when averaged over a wide variety of downstream\nnatural language tasks. These results provide exciting evidence for the capability of hybrid models to\nprovide faster LLM inference and greater model quality when compared to Transformers.\n4.3.2\nLong-Context Evaluation\nIn this section, we evaluate the long-context ability of hybrid SSM-Transformer models by training\ntwo Mamba-2-Hybrid 8B extensions—a 16386 (16K) and 32768 (32K) variant—and compare to\ncorresponding extensions of the 8B Transformer. We extend the base models (pretrained using sequence\nlengths of 4096) to 16K and 32K versions through continued pretraining on the respective larger context\nlengths. We use full global attention in the four self-attention layers. In this initial study, we use\nthe same underlying data as in our 3.5T dataset. That is, we do not explicitly select a data subset\nconsisting of long documents, but rather use packed sequences to generate 16K and 32K inputs for\nthe model. All long context models are trained for an additional 50B tokens with a learning rate that\nincreases linearly over the first 1.7B tokens and then decays according to cosine annealing thereafter.\nWe use a max learning rate of 3e-5 and minimum learning rate of 3e-6. For the Transformer extensions,\nwe automatically adapt the RoPE base frequency to the longer context lengths using the dynamic NTK\nscaling described in bloc97 (2023).\nResults on Standard Short-Context Tasks.\nWe first evaluate the 16K and 32K Mamba-2-Hybrid\nand Transformer models on the 12 standard natural language tasks used above. While these tasks\ndo not require long-context abilities, we aim to check whether model accuracy degrades on common\ntasks as a result of extending our models to long-context variants. Results are reported in Table 8. On\naverage, we find no accuracy decrease on these tasks for the long-context variants. In fact, the 16K and\n32K models slightly improve compared to the base models which is due to the 16K and 32K models\nseeing 1.4% more data. As for the original 4K evaluations, the 16K and 32K Mamba-2-Hybrid is more\nthan 2 points better than the corresponding Transformer models on average.\nResults on Natural Long-Context Tasks.\nWe now focus on evaluating the 16K and 32K model\nextensions on tasks which require natural language reasoning across long contexts. Results when\nevaluating these models on nine common long-context tasks are shown in Table 9. In this setting,\nthe base (4K) Mamba-2-Hybrid and Transformer models achieve similar accuracy on most tasks and\nare more than 6 points better than the pure Mamba-2 model. For both architectures, the 16K and\n32K variants improve over the base models by an average of roughly 4 points. This is particularly\ndue to a large accuracy increase on tasks with many long inputs (e.g., NarrativeQA). Comparing the\n16K and 32K Mamba-2-Hybrid to the corresponding 16K and 32K Transformer, we observe that the\nTransformer models separate from the hybrid models on some tasks, particularly Multi-Document\nQuestion Answering tasks (e.g., HotpotQA). This leads the 16K and 32K Transformer models to reach\napproximately one point higher average accuracy than the 16K and 32K Mamba-2-Hybrid models\nrespectively.\nWe hypothesize that the hybrid model reaches lower accuracy than the Transformer on these tasks\nbecause the SSM layer states are sometimes confused by documents irrelevant to the question (which is\nunknown until the end of the sequence)—The Muti-Document Question Answering tasks in Table 9 are\ntaken from the LongBench evaluation suite which generates long-context inputs by concatenating the\n12\n\nTable 9: Natural long-context evaluation results for 8B-parameter base and long-context extensions of\nMamba-2-Hybrid and Transformer models trained on 3.5T tokens. While the 4K models are comparable\non these tasks, the 16K and 32K Transformer is roughly 1 point better on average when compared to\nthe 16K and 32K Mamba-2-Hybrid. These improvements come largely on Multi-Document Question\nAnswering tasks from LongBench (Bai et al. 2023).\nModel\nSingle Doc. QA\nMulti-Doc. QA\nFew-Shot Learning\nNarrativeQA\nQasper\nMultiFieldQA\nQuALITY\nHotpotQA\n2WikiMQA\nMusique\nTREC\nTriviaQA\nAvg\n(avg, max) Ctx. Len:\n(86K, 506K)\n(4.9K, 22K)\n(7.2K, 17K)\n(6.5K, 9.8K)\n(13.3K, 19K)\n(7.5K, 17K)\n(16.3K, 18K)\n(7.1K, 11.9K)\n(12.3K, 25K)\nMamba-2\n22.53\n25.74\n29.26\n33.84\n29.99\n24.6\n11.1\n54\n77.77\n34.31\nTransformer-4K\n23.44\n28.51\n38.39\n36.48\n36.28\n33.48\n17.68\n68\n83.65\n40.66\nMamba-2-Hybrid-4k\n24.53\n28.75\n39.01\n35.6\n36.24\n32.91\n15.24\n68.5\n86.7\n40.83\nTransformer-16K\n27.51\n29.71\n41.13\n39.02\n48.61\n34.87\n21.42\n78.5\n86.39\n45.24\nMamba-2-Hybrid-16k\n29.76\n30.93\n40.9\n38.93\n42.17\n31.34\n23.32\n74\n90.05\n44.6\nTransformer-32K\n30.06\n29.09\n40.61\n39.17\n48.2\n35.52\n24.55\n76.5\n86.43\n45.57\nMamba-2-Hybrid-32K\n31.56\n30.55\n40.69\n38.88\n41.9\n29.06\n21.33\n74.5\n89\n44.16\nfew documents from each task needed to answer the question (e.g., HotpotQA questions contain two\nparagraphs and then a question which requires knowledge from both) with many random paragraphs\nsampled from Wikipedia. This confusion could be due to our continued pretraining recipe which simply\npacks unrelated sequences together to make 16K and 32K inputs, potentially leading the SSM layers\nto believe separate documents are related when they are not. While this recipe is widely used for\nTransformers, it may not directly apply to hybrid models.\nBased on our experience evaluating these tasks, we also note that hybrid models may be more sensitive\nto prompt formatting than Transformer models. As evidence to support this hypothesis, we found\nthat minor prompt modifications could change the results for both models, but more so for the hybrid\nmodel. For example, on Musique, prompt modifications led the accuracy for the Mamba-2-Hybrid-4K\nmodel to fall in the range [10.63, 16.16]. In contrast, the accuracy for the Transformer was relatively\nsteady, remaining in the range [15.25, 17.68]. We highlight, however, that the prompt format for the\nmajority of the tasks in Table 9 (e.g., the Multi-Document QA tasks, see Section 2) are taken from the\nLongBench evaluation suite (Bai et al. 2023) and have been optimized for Transformer models. As a\nresult of these observations, we believe interesting areas of future work involve further study on the\nprompt robustness of hybrid models and comparing aligned and instruction-tuned hybrid models to\ntheir Transformer counterparts.\nResults on Synthetic Long-Context Tasks.\nBeyond the natural long-context tasks discussed\nabove, we also evaluate the 16K and 32K hybrid and Transformer extensions on the synthetic tasks\nin the RULER (Hsieh et al. 2024) benchmark suite. These tasks expand upon the basic Needle In\nA Haystack (NIAH) problem where the model is asked to recall information (the needle) from long\ninputs of otherwise irrelevant text. RULER also includes tasks which require tracing and aggregating\ninformation across the context. For these evaluations, the task context lengths are set to 4K for the\nbase models, 16K for the 16K extensions, and 32K for the 32K models.\nResults on the 13 RULER tasks are shown in Table 10. Overall, the Mamba-2-Hybrid models show\nsignificantly improved NIAH abilities compared to the Transformer models and pure Mamba-2 model.\nFor example, The 16K hybrid model achieves 13 points higher average accuracy on these tasks compared\nto the 16K Transformer. The long-context Transformer models are particularly challenged by the\nVariable Tracking (VT) task. This task includes a one shot demonstration of the task in the context\nand a closer inspection of the model predictions shows that the Transformer tends to directly copy\nthe answer for the in-context example instead of predicting the output of the actual question. This\nbehavior is consistent with prior observations for LWM-7B and Yi-34B models on these tasks (Hsieh\net al. 2024). Interestingly, while the hybrid model is generally better on most tasks, the Transformer\nconsistently reaches higher accuracy on Keywords Extraction (KWE).\nWe also observe that the hybrid model reaches higher accuracy than the Transformer on the HotpotQA\ntask, which contrasts the behavior in Table 9 when running HotpotQA using the LongBench evaluation\nsuite. As described above, while the latter benchmark constructs long context HotpotQA questions by\nadding random Wikipedia passages to the relevant information, RULER extends the context length\nof HotpotQA by adding paragraphs randomly sampled from HotpotQA itself. This slight difference\n13\n\nTable 10: Evaluation results of 8B-parameter Mamba-2-Hybrid and Transformer models trained for\n3.5T tokens, plus their long-context extensions, on the synthetic RULER long-context benchmark\nsuite (Hsieh et al. 2024). Mamba-2-Hybrid models achieve higher accuracy than Transformers on these\ntasks, highlighting their improved ability to recall, trace, and aggregate information across long inputs.\nModel\nSynthetic Tasks\nNeedle-In-A-Haystack Tasks (NIAH)\nHotpotQA\nSquadQA\nCWE\nVT\nKWE\nNIAH-1\nNIAH-2\nNIAH-3\nMK-NIAH-1\nMK-NIAH-2\nMK-NIAH-3\nMV-NIAH\nMQ-NIAH\nAvg\nMamba-2\n31.75\n35.5\n32.87\n76.45\n76.58\n100\n98\n83\n40.25\n13.75\n5.5\n35\n49.12\n52.14\nTransformer-4K\n42.25\n56.5\n36.42\n79.2\n76.33\n100\n100\n75.25\n99.5\n94\n58\n96.56\n95.06\n77.62\nMamba-2-Hybrid-4k\n48.75\n56.5\n32.2\n90.55\n65.58\n100\n100\n95.75\n89.5\n95.5\n96\n97.94\n97.62\n81.99\nTransformer-16K\n37.25\n45\n3\n7.25\n58.33\n100\n99.5\n75.75\n93.75\n56.5\n57.5\n88.5\n87.94\n62.33\nMamba-2-Hybrid-16k\n44\n48.75\n12.88\n83.2\n46.83\n100\n100\n81.5\n92\n92.25\n83\n89.81\n90.19\n74.19\nTransformer-32K\n35\n41.5\n4.42\n0.35\n53.5\n100\n99\n76.5\n70.75\n57.75\n41.25\n69.69\n84.69\n56.49\nMamba-2-Hybrid-32K\n38.5\n41.75\n8.4\n79.9\n36.5\n100\n100\n96.75\n84\n76.5\n81.5\n84.31\n80.94\n69.93\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nMamba-2\nTransformer\n(a) 4K base models\n0\n5\n10\n15\n20\n25\n30\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nTransformer\n(b) 16K models\n0\n10\n20\n30\n40\n50\n60\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nTransformer\n(c) 32K models\nFigure 7: Phonebook evaluation results for 8B-parameter models trained on 3.5T tokens and their\nlong-context extensions. We use the standard Phonebook formulation (Figure 2b). Mamba-2-Hybrid\nmodels can generalize beyond their pretraining sequence length and perform the Phonebook task on\ncontexts longer than pure SSM or Transformer models. (a) Base 4K Mamba-2, Mamba-2-Hybrid, and\nTransformer model Phonebook evaluations. (b) 16K long-context extensions of the Mamba-2-Hybrid\nand Transformer model evaluated on Phonebook. (c) Phonebook evaluations for the Mamba-2-Hybrid\nand Transformer models extended to support 32K sequence lengths.\nin the distribution used for context creation seems to confuse the hybrid model in one case (i.e.,\nLongBench HotpotQA (Table 9), but not in the other (i.e., RULER HotpotQA (Table 10) and provides\nan interesting area of future study.\nResults on Copying Tasks: Phonebook.\nFinally, we evaluate the long-context hybrid and\nTransformer models on the synthetic Phonebook task (Section 3.3.3, Figure 2b). We use the standard\nformulation which tests a model’s ability to perform in-context learning and copying from the context.\nResults for the base 4K models trained on 3.5T tokens are shown in Figure 7a. In this Figure, we also\ninclude the results for the pure Mamba-2 model trained on 3.5T tokens. As highlighted for pure Mamba\nmodels trained on 1.1T tokens (Section 3.3.3), the Mamba-2 model is unable to accurately predict the\nrequired phone numbers for sequences >1000 tokens. In contrast, the Transformer and Mamba-2-Hybrid\ncan do the Phonebook task with near perfect accuracy up to the pretraining context length (4K).\nIn fact, the hybrid model can generalize slightly beyond this sequence length, achieving 100 percent\naccuracy on Phonebook up to 5.5K tokens. Similar results hold for the long-context models (Figure 7b\nand Figure 7c). Both the 16K and 32K Mamba-2-Hybrid extensions can perform the Phonebook task\nperfectly beyond their trained context length. The long-context Transformer models, however, start to\nmake mistakes as the phone book length approaches their trained context lengths. In Griffin (De et al.\n2024), the authors make similar observations, finding that their Transformer baseline slowly degrades\nas it approaches the training context length and that hybrid architectures show near-perfect accuracy\nup to their attention window size. As with the RULER evaluations above, these experiments highlight\nagain the strong ability for hybrid models to perform in-context learning and to retrieve information\nfrom a long context.\nA 128K Mamba-2-Hybrid Model.\nWhile we focused in this section on evaluating 16K and 32K\nMamba-2-Hybrid long-context extensions and comparing them to corresponding Transformer models,\n14\n\nwe now show that the hybrid architecture can extend to context lengths well beyond 32K. We extend\nthe base 4K Mamba-2-Hybrid model to a sequence length of 128K through continued pretraining as\ndescribed above, using full global attention for the four self-attention layers. This training required only\ntensor and pipeline parallelism in Megatron-LM to prevent out-of-memory issues. We report results for\nthis model on the Phonebook task in Figure 8. As for the 4K, 16K, and 32K Mamba-2-Hybrid models,\nthe 128K model is able to do this task perfectly up to and beyond the sequence length it was trained\non. This experiment highlights the promising potential for extending hybrid models to long context\nlengths.\n0\n50\n100\n150\n200\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba2-Hybrid\nFigure 8:\nPhonebook accuracy (stan-\ndard setting) for an 8B Mamba-2-Hybrid\ntrained on 3.5T tokens and extended to\n128K sequence length through continued\npretraining for 50B tokens.\nTakeaway.\nWe have presented a detailed evaluation of\nlong-context 8B-parameter Mamba-2-Hybrid models and\ncompared them with their Transformer counterparts. Over-\nall, the hybrid models match or exceed the long-context\ncapabilities of the Transformers in most tasks. This is par-\nticularly true for tasks like Phonebook and the Needle In A\nHaystack (NIAH) present in the synthetic RULER bench-\nmark. We have identified, however, a few tasks where the\nhybrid models failed to reach Transformer-level accuracy\n(e.g., Multi-Document Question Answering in the Long-\nBench evaluation suite). We encourage further research\ninto these settings and into long-context versions of hybrid\nSSM-Transformer architectures.\n5\nRelated Work\nRecent work has also introduced Mamba-Attention hybrid models to improve accuracy and efficiency\ncompared to pure Mamba and Transformers. Park et al. (2024) show the limitations of Mamba on\nin-context learning (ICL) tasks and propose a hybrid model to improve the ICL accuracy. Their\nexperiments, however, are isolated to ICL tasks, and the model size is small (up to 77M parameters).\nJamba (Lieber et al. 2024) and Zamba (Glorioso et al. 2024) train Mamba-Attention hybrid models\nat 7B scale. Both show that their hybrid models significantly improve inference speed and GPU\nmemory compared to other models including Llama (Touvron et al. 2023) and Mistral-7B (Jiang\net al. 2023). Jamba improves the model accuracy and efficiency by adding Mixture-of-Experts (MoE),\nwhich increases the total model capacity (52B total parameters) but not its active parameters. They\ncompare their hybrid architecture with pure Mamba and Transformer models on four standard and three\nlong-context tasks, but only using 1.3B parameter models trained for 250B tokens, or 7B parameter\nmodels trained for 50B tokens. Zamba introduces a shared attention module and uses an annealing\nphase during training with high-quality datasets, which boosts the quality of their hybrid model. We\nfocus on combining Mamba, attention, and MLP layers into hybrid models for direct comparison with\nTransformer baselines at larger scales (>7B parameters and >1T tokens).\nOther recent work introduces hybrid models that mix either linear RNNs or convolutions with attention.\nDe et al. (2024) introduce a hybrid model that blends gated linear recurrences with local (sliding\nwindow) attention and show that the hybrid model can improve next token prediction latency with\nincreasing context length. They train 1B parameter models on 8K sequence lengths for long-context\nmodeling. Arora et al. (2023) report that a simple convolution-attention hybrid model outperforms pure\nattention in multi-query associative recall problems while reducing total FLOPs. Several additional\nworks add SSM layers to the Transformer architecture to increase accuracy: Saon, Gupta, and Cui\n(2023) use SSM layers together with Transformers to improve speech recognition quality. Pilault et al.\n(2024) combine an SSM and a block-wise Transformer at every layer. They show improved perplexity\nand generalization capabilities for longer sequences (up to 65K). Their model is scaled up to 1.3B\nparameters. We note that all the hybrid models mentioned above have manually designed architectures\nand place an MLP layer (if they use MLP) after each attention layer, similar to Transformers. Our\nstudy, however, finds that a specific hybrid architecture design or pattern is not required. Instead, the\nrelative proportions of each type of hybrid component appears to be the key factor that determines the\nquality of the model.\n15\n\n6\nConclusion\nTo address the question of whether SSM models can match the accuracy of Transformers at larger training\nbudgets, in this report we presented a direct experimental comparison between 8B-parameter Mamba,\nMamba-2, Mamba-2-Hybrid, and Transformer models trained on up to 3.5T tokens. Our experiments\nshowed that pure SSM models match or exceed the capabilities of their Transformer counterparts on\nmost downstream tasks but are challenged by tasks that require context-based information retrieval\n(e.g., copying) and in-context learning. We also showed that hybrid SSM-Transformer models (Mamba-\n2-Hybrid) reach higher accuracy than Transformers on all common benchmarks we evaluated. Further,\nthese hybrid models continue to show strong capabilities compared to Transformers when extended\nto 16K and 32K contexts. Based on these results, we are encouraged by the potential for SSM-based\nmodels to deliver inference-time speedups without accuracy degradation compared to Transformer\nmodels. We look forward to future work focusing on how hybrid models can make use of the large\necosystem of frameworks, methods, and libraries currently tailored to the large-scale training and\ninference of Transformers.\nReferences\n[1]\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. “GPT-4\nTechnical Report”. In: arXiv preprint arXiv:2303.08774 (2023).\n[2]\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and\nSumit Sanghai. “GQA: Training Generalized Multi-Query Transformer Models from Multi-head\nCheckpoints”. In: arXiv preprint arXiv:2305.13245 (2023).\n[3]\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri\nRudra, and Christopher Ré. “Zoology: Measuring and Improving Recall in Efficient Language\nModels”. In: arXiv preprint arXiv:2312.04927 (2023).\n[4]\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. “Layer Normalization”. In: arXiv\npreprint arXiv:1607.06450 (2016).\n[5]\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly\nLearning to Align and Translate”. In: arXiv preprint arXiv:1409.0473 (2014).\n[6]\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. “LongBench: A Bilingual,\nMultitask Benchmark for Long Context Understanding”. In: arXiv preprint arXiv:2308.14508\n(2023).\n[7]\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. “PIQA: Reasoning about Physical\nCommonsense in Natural Language”. In: Proceedings of the AAAI Conference on Artificial\nIntelligence. Vol. 34. 05. 2020, pp. 7432–7439.\n[8]\nbloc97. “NTK-aware Scaled RoPE allows LLaMA models to have Extended (8k+) Context\nSize Without any Fine-tuning and Minimal Perplexity Degradation”. In: (2023). url: https:\n//www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_\nllama_models_to_have.\n[9]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are\nFew-shot Learners”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 1877–\n1901.\n[10]\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge”. In: arXiv preprint arXiv:1803.05457 (2018).\n[11]\nTri Dao and Albert Gu. “Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality”. In: International Conference on Machine Learning\n(ICML). 2024.\n[12]\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. “A Dataset\nof Information-Seeking Questions and Answers Anchored in Research Papers”. In: Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. 2021, pp. 4599–4610.\n16\n\n[13]\nSoham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,\nAlbert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. “Griffin:\nMixing Gated Linear Recurrences with Local Attention for Efficient Language Models”. In: arXiv\npreprint arXiv:2402.19427 (2024).\n[14]\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas\nMuennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A Framework\nfor Few-shot Language Model Evaluation. Version v0.4.0. Dec. 2023. url: https://zenodo.org/\nrecords/10256836.\n[15]\nPaolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam\nIbrahim, and Beren Millidge. “Zamba: A Compact 7B SSM Hybrid Model”. In: arXiv preprint\narXiv:2405.16712 (2024).\n[16]\nAlbert Gu and Tri Dao. “Mamba: Linear-time Sequence Modeling with Selective State Spaces”.\nIn: arXiv preprint arXiv:2312.00752 (2023).\n[17]\nAlbert Gu, Karan Goel, and Christopher Re. “Efficiently Modeling Long Sequences with Structured\nState Spaces”. In: International Conference on Learning Representations. 2021.\n[18]\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. “Measuring Massive Multitask Language Understanding”. In: International\nConference on Learning Representations. 2020.\n[19]\nDan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: arXiv preprint\narXiv:1606.08415 (2016).\n[20]\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. “Constructing A Multi-\nhop QA Dataset for Comprehensive Evaluation of Reasoning Steps”. In: Proceedings of the 28th\nInternational Conference on Computational Linguistics. 2020, pp. 6609–6625.\n[21]\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and\nBoris Ginsburg. “RULER: What’s the Real Context Size of Your Long-Context Language Models?”\nIn: arXiv preprint arXiv:2404.06654 (2024).\n[22]\nSamy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. “Repeat After Me:\nTransformers are Better than State Space Models at Copying”. In: arXiv preprint arXiv:2402.01032\n(2024).\n[23]\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\n“Mistral 7B”. In: arXiv preprint arXiv:2310.06825 (2023).\n[24]\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. “PubMedQA:\nA Dataset for Biomedical Research Question Answering”. In: arXiv preprint arXiv:1909.06146\n(2019).\n[25]\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. “TriviaQA: A Large Scale\nDistantly Supervised Challenge Dataset for Reading Comprehension”. In: Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017,\npp. 1601–1611.\n[26]\nTomáš Kočisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor\nMelis, and Edward Grefenstette. “The NarrativeQA Reading Comprehension Challenge”. In:\nTransactions of the Association for Computational Linguistics 6 (2018), pp. 317–328.\n[27]\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. “Reducing Activation Recomputation in Large Transformer\nModels”. In: arXiv preprint arXiv:2205.05198 (2022).\n[28]\nTaku Kudo and John Richardson. “Sentencepiece: A Simple and Language Independent Subword\nTokenizer and Detokenizer for Neural Text Processing”. In: arXiv preprint arXiv:1808.06226\n(2018).\n[29]\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. “RACE: Large-scale\nReAding Comprehension Dataset From Examinations”. In: arXiv preprint arXiv:1704.04683\n(2017).\n[30]\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. “Latent Retrieval for Weakly Supervised\nOpen Domain Question Answering”. In: arXiv preprint arXiv:1906.00300 (2019).\n17\n\n[31]\nXin Li and Dan Roth. “Learning Question Classifiers”. In: COLING 2002: The 19th International\nConference on Computational Linguistics. 2002.\n[32]\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. “Jamba: A Hybrid Transformer-\nmamba Language Model”. In: arXiv preprint arXiv:2403.19887 (2024).\n[33]\nStephanie Lin, Jacob Hilton, and Owain Evans. “TruthfulQA: Measuring How Models Mimic\nHuman Falsehoods”. In: arXiv preprint arXiv:2109.07958 (2021).\n[34]\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. “Can a Suit of Armor\nConduct Electricity? A New Dataset for Open Book Question Answering”. In: arXiv preprint\narXiv:1809.02789 (2018).\n[35]\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,\nVijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro,\net al. “Efficient Large-scale Language Model Training on GPU Clusters using Megatron-LM”.\nIn: Proceedings of the International Conference for High Performance Computing, Networking,\nStorage and Analysis. 2021.\n[36]\nNVIDIA. NVIDIA H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/\nh100/. 2023.\n[37]\nJongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,\nKangwook Lee, and Dimitris Papailiopoulos. “Can Mamba Learn How to Learn? A Comparative\nStudy on In-Context Learning Tasks”. In: arXiv preprint arXiv:2402.04248 (2024).\n[38]\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subrama-\nnian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al.\n“Nemotron-4 15B Technical Report”. In: arXiv preprint arXiv:2402.16819 (2024).\n[39]\nJonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin.\n“Block-state Transformers”. In: Advances in Neural Information Processing Systems 36 (2024).\n[40]\nPranav Rajpurkar, Robin Jia, and Percy Liang. “Know what you don’t Know: Unanswerable\nQuestions for SQuAD”. In: arXiv preprint arXiv:1806.03822 (2018).\n[41]\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. “WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale”. In: Communications of the ACM 64.9 (2021),\npp. 99–106.\n[42]\nGeorge Saon, Ankit Gupta, and Xiaodong Cui. “Diagonal State Space Augmented Transformers\nfor Speech Recognition”. In: ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE. 2023, pp. 1–5.\n[43]\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan\nXiong, Mor Geva, Jonathan Berant, et al. “Scrolls: Standardized Comparison over Long Language\nSequences”. In: arXiv preprint arXiv:2201.03533 (2022).\n[44]\nNoam Shazeer. “GLU Variants Improve Transformer”. In: arXiv preprint arXiv:2002.05202 (2020).\n[45]\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. “Megatron-LM: Training Multi-billion Parameter Language Models using Model\nParallelism”. In: arXiv preprint arXiv:1909.08053 (2019).\n[46]\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. “Roformer: En-\nhanced Transformer with Rotary Position Embedding”. In: Neurocomputing 568 (2024), p. 127063.\n[47]\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efficient Transformers: A Survey”.\nIn: ACM Computing Surveys 55.6 (2022), pp. 1–28.\n[48]\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. “Llama 2: Open\nFoundation and Fine-tuned Chat Models”. In: arXiv preprint arXiv:2307.09288 (2023).\n[49]\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. “MuSiQue:\nMultihop Questions via Single-hop Question Composition”. In: Transactions of the Association\nfor Computational Linguistics 10 (2022), pp. 539–554.\n[50]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. “Attention is All You Need”. In: Advances in Neural Infor-\nmation Processing Systems 30 (2017).\n[51]\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. “Effective Long-context\nScaling of Foundation Models”. In: arXiv preprint arXiv:2309.16039 (2023).\n18\n\n[52]\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\nAnswering”. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. 2018, pp. 2369–2380.\n[53]\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a\nMachine Really Finish your Sentence?” In: arXiv preprint arXiv:1905.07830 (2019).\n[54]\nBiao Zhang and Rico Sennrich. “Root Mean Square Layer Normalization”. In: Advances in Neural\nInformation Processing Systems 32 (2019).\nA\nHybrid Layer Allocation Algorithm\nAlthough we are able to specify, and experiment with, an arbitrary sequence of Mamba, self-attention,\nand MLP layers in our hybrid models, by default we use the allocation algorithm described in\nAlgorithm 1. This algorithm first attempts to place any self-attention layers such that the intervening\nruns of contiguous Mamba layers are as equal in length as possible, while also beginning and ending the\nlayer sequence with a run of Mamba layers. Then, any MLP layers are evenly distributed throughout\nthe sequence while not replacing any self-attention layers. The MLP layers are biased away from the\nstart of the sequence so that the layer sequence begins with a Mamba layer (if there are any Mamba\nlayers) and ends with an MLP layer (if there are any MLP layers).\nAlgorithm 1 Hybrid Layer Allocation\n1: Input: total_layers_count, target_attention_ratio, target_mlp_ratio\n2: Output: layer_type_list\n3: attention_layers_count ←round(total_layers_count * target_attention_ratio)\n4: mamba_layers_count ←total_layers_count - attention_layers_count\n5: mamba_sections_count ←attention_layers_count + 1\n6: mamba_section_length ←mamba_layers_count / mamba_sections_count\n7: layer_type_list ←array of Symbols.MAMBA of size total_layers_count\n8: x ←mamba_section_length\n9: for l in 0 to total_layers_count - 1 do\n10:\nif x < 0.5 then\n11:\nlayer_type_list[l] ←Symbols.ATTENTION\n12:\nx ←x + mamba_section_length\n13:\nelse\n14:\nx ←x - 1\n15: mlp_layers_count ←round(total_layers_count * target_mlp_ratio)\n16: if mlp_layers_count > 0 then\n17:\nmamba_layers_count ←mamba_layers_count - mlp_layers_count\n18:\nmamba_to_mlp_ratio ←mamba_layers_count / mlp_layers_count\n19:\nx ←mamba_to_mlp_ratio\n20:\nfor l in 0 to total_layers_count - 1 do\n21:\nif layer_type_list[l] == Symbols.MAMBA then\n22:\nif x < 0.5 then\n23:\nlayer_type_list[l] ←Symbols.MLP\n24:\nx ←x + mamba_to_mlp_ratio\n25:\nelse\n26:\nx ←x - 1\nTable 11 provides examples of some layer patterns generated by Algorithm 1.\n19\n\nTLC\nATT\nMLP\nPattern\n24\n0.00\n0.00\nMMMMMMMMMMMMMMMMMMMMMMMM\n24\n0.08\n0.00\nMMMMMMM*MMMMMMMM*MMMMMMM\n24\n0.17\n0.00\nMMMM*MMMM*MMMM*MMMM*MMMM\n24\n0.08\n0.30\nMM+MM+M*M+MMM+MM*+MM+MM+\n24\n0.08\n0.50\nM+M+M++*M+M+M+M+*M++M+M+\n24\n0.50\n0.50\n+*+*+*+*+*+**+*+*+*+*+*+\n48\n0.08\n0.50\nM+M+M++M+*M+M+M+M++*M+M+M+M+*M++M+M+M+*M+M++M+M+\n56\n0.08\n0.50\nM+M+M++M+M*+M+M+M+M++M*+M+M+M+M+M*++M+M+M+M+M*+M++M+M+M+\nTable 11: Some examples of hybrid layer patterns generated by Algorithm 1. TLC=total_layer_count,\nATT=target_attention_ratio, MLP=target_mlp_ratio. In the pattern, M=Mamba, *=Self-Attention,\nand +=MLP.\n20\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://docs.nvidia.com/Megatron-Core/",
      "full_text": " NVIDIA Megatron-Core - NVIDIA Docs Topics Topics AR / VR Cybersecurity Edge Computing Recommenders / Personalization Computer Vision / Video Analytics Data Center / Cloud Generative AI / LLMs Robotics Content Creation / Rendering Data Science Networking Simulation / Modeling / Design Conversational AI NVIDIA Developer Blog Forums Sign In Menu Docs Hub Topics Topics AR / VR Cybersecurity Edge Computing Recommenders / Personalization Computer Vision / Video Analytics Data Center / Cloud Generative AI / LLMs Robotics Content Creation / Rendering Data Science Networking Simulation / Modeling / Design Conversational AI NVIDIA Developer Blog Forums Sign In Submit Search NVIDIA Docs Hub Homepage &nbsp; &nbsp; NVIDIA Megatron-Core Megatron-Core is a self contained, light weight PyTorch library that packages everything essential for training large scale transformer. It offer rich collection of GPU techniques to optimize memory, compute and communication inherited from Megatron-LM and Transformer Engine with cutting-edge innovations on system-level efficiency. By abstracting these GPU optimized techniques into composable and modular APIs, Megatron Core allows full flexibility for developers and model researchers to train custom transformers at-scale and easily facilitate developing their own LLM framework on NVIDIA accelerated computing infrastructure. Megatron Core User Guide Developer documentation for Megatron Core covers API documentation, quickstart guide as well as deep dives into advanced GPU techniques needed to optimize LLM performance at scale. Browse Corporate Info NVIDIA.com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2025 NVIDIA Corporation ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html",
      "full_text": " Overview &#8212; NVIDIA NeMo Framework User Guide Skip to main content Back to top Ctrl + K You are viewing the NeMo 2.0 documentation. This release introduces significant changes to the API and a new library, NeMo Run . We are currently porting all features from NeMo 1.0 to 2.0. For documentation on previous versions or features not yet available in 2.0, please refer to the NeMo 24.07 documentation . NVIDIA NeMo Framework User Guide NVIDIA NeMo Framework User Guide Table of Contents NeMo Framework Overview Install NeMo Framework Performance Why NeMo Framework? Getting Started Quickstart with NeMo-Run Quickstart with NeMo 2.0 API Tutorials Developer Guides Migration Guide Pre-Training SFT Training and Inference PEFT Training and Inference Trainer Configuration Precision Configuration Parallelisms Experiment Manager Checkpointing Configurations Optimizer Configuration Data Configuration Nsys Profiling Tokenizers Feature Guide The Bridge Between Lightning and Megatron Core Logging and Checkpointing Serialization Parameter Efficient Fine-Tuning (PEFT) Hugging Face Integration Profiling Best Practices Performance Tuning Guide Training and Customization Long Context Training Context Parallelism Optimal Configuration with Auto Configurator Parameter-Efficient Fine-tuning (PEFT) Supported PEFT Methods A Comparison of Performant and Canonical LoRA Variants Sequence Packing Resiliency Continual Training Custom Datasets Pre-Training Data Module Fine-Tuning Data Module Model Optimization Quantization Pruning Distillation Speculative Decoding Models Large Language Models Baichuan 2 ChatGLM 3 DeepSeek V2 DeepSeek V3 Gemma Gemma 2 GPT-OSS Hyena Llama 3 Llama Nemotron Mamba 2 Mixtral Nemotron Phi 3 Qwen2/2.5 Qwen3 Starcoder Starcoder 2 T5 BERT Vision Language Models NeVA (LLaVA) LLaVA-Next Llama 3.2 Vision Models Llama 4 Models Qwen2-VL Gemma 3 Models Data Preparation to Use Megatron-Energon Dataloader CLIP Llama Nemotron Nano VL 8B Audio-Vision Language Model Speech AI Models Diffusion Models Flux Diffusion Training Framework Embedding Models SBERT Llama Embedding Exporting Llama Embedding To ONNX and TensorRT Reranker Models Llama Reranker Library Documentation Overview NeMo Introduction NeMo Fundamentals Tutorials Mixed Precision Training Parallelisms Mixture of Experts Optimizations Attention Optimizations Activation Recomputation Communication Overlap CPU Offloading Checkpoints NeMo Distributed Checkpoint User Guide Converting from Megatron-LM Evaluate NeMo 2.0 Checkpoints Evaluation Adapters NeMo APIs NeMo Models Neural Modules Experiment Manager Neural Types Adapters Adapter Components Adapters API NeMo Core APIs NeMo Common Collection API Callbacks Losses Metrics Tokenizers Data S3 Checkpointing NeMo ASR API NeMo TTS API NeMo Collections Large Language Models GPT Model Training Batching Positional embeddings Megatron Core Customization Reset Learning Rate Ramp Up Batch Size Machine Translation Models Automatic Speech Recognition (ASR) Models Datasets ASR Language Modeling and Customization Checkpoints Scores NeMo ASR Configuration Files NeMo ASR API All Checkpoints Example With MCV Speech Classification Models Datasets Checkpoints NeMo Speech Classification Configuration Files Resource and Documentation Guide Speaker Recognition (SR) Models NeMo Speaker Recognition Configuration Files Datasets Checkpoints NeMo Speaker Recognition API Resource and Documentation Guide Speaker Diarization Models Datasets Checkpoints End-to-End Speaker Diarization Configuration Files NeMo Speaker Diarization API Resource and Documentation Guide Speech Self-Supervised Learning Models Datasets Checkpoints NeMo SSL Configuration Files NeMo SSL collection API Resources and Documentation Speech Intent Classification and Slot Filling Models Datasets Checkpoints NeMo Speech Intent Classification and Slot Filling Configuration Files NeMo Speech Intent Classification and Slot Filling collection API Resources and Documentation SpeechLM2 Models Datasets Configuration Files Training and Scaling Text-to-Speech (TTS) Models Data Preprocessing Checkpoints NeMo TTS Configuration Files Grapheme-to-Phoneme Models Speech and Audio Processing Models Datasets Checkpoints NeMo Audio Configuration Files NeMo Audio API Speech AI Tools NeMo Forced Aligner (NFA) Dataset Creation Tool Based on CTC-Segmentation Speech Data Explorer Comparison tool for ASR Models ASR Evaluator Speech Data Processor (Inverse) Text Normalization WFST-based (Inverse) Text Normalization Neural Models for (Inverse) Text Normalization NeMo AutoModel NeMo Curator NeMo Eval NeMo Export and Deploy NeMo RL NeMo Run Guides Why should I use NemoRun? Configure NeMo-Run Execute NeMo Run Manage NeMo-Run Ray Clusters &amp; Jobs NeMo Run CLI Guide Frequently Asked Questions Releases Software Component Versions Changelog Known Issues Overview Overview # NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and developers working on Large Language Models , Multimodal, and Speech AI (e.g. Automatic Speech Recognition and Text-to-Speech ). It enables users to efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints. Setup Instructions : Install NeMo Framework Large Language Models and Multimodal Models # NeMo Framework provides end-to-end support for developing Large Language Models (LLMs) and Multimodal Models (MMs). It provides the flexibility to be used on-premises, in a data-center, or with your preferred cloud provider. It also supports execution on SLURM or Kubernetes enabled environments. Data Curation # NeMo Curator [ 1 ] is a Python library that includes a suite of modules for data-mining and synthetic data generation. They are scalable and optimized for GPUs, making them ideal for curating natural language data to train or fine-tune LLMs. With NeMo Curator, you can efficiently extract high-quality text from extensive raw web data sources. Getting Started Tutorials Data Curation features and usage API Documentation Training and Customization # NeMo Framework provides tools for efficient training and customization of LLMs and Multimodal models. It includes default configurations for compute cluster setup, data downloading, and model hyperparameters, which can be adjusted to train on new datasets and models. In addition to pre-training, NeMo supports both Supervised Fine-Tuning (SFT) and Parameter Efficient Fine-Tuning (PEFT) techniques like LoRA, Ptuning, and more. Two options are available to launch training in NeMo - using the NeMo 2.0 API interface or with NeMo Run . With NeMo Run (Recommended): NeMo Run provides an interface to streamline configuration, execution and management of experiments across various compute environments. This includes launching jobs on your workstation locally or on big clusters - both SLURM enabled or Kubernetes in a cloud environment. Pre-training &amp; PEFT Quickstart with NeMo Run Using the NeMo 2.0 API: This method works well with a simple setup involving small models, or if you are interested in writing your own custom dataloader, training loops, or change model layers. It gives you more flexibility and control over configurations, and makes it easy to extend and customize configurations programmatically. Training Quickstart with NeMo 2.0 API Migrating from NeMo 1.0 to NeMo 2.0 API RL # NeMo RL [ 1 ] is a scalable and efficient post-training library designed for models ranging from 1 GPU to thousands, and from tiny to over 100 billion parameters. What you can expect: Seamless integration with Hugging Face for ease of use, allowing users to leverage a wide range of pre-trained models and tools. High-performance implementation with Megatron Core , supporting various parallelism techniques for large models (&gt;100B) and large context lengths. Efficient resource management using Ray , enabling scalable and flexible deployment across different hardware configurations. Flexibility with a modular design that allows easy integration and customization. Comprehensive documentation that is both detailed and user-friendly, with practical examples. Check out the NeMo RL Documentation for more information. Multimodal Models # NeMo Framework provides optimized software to train and deploy state-of-the-art multimodal models across several categories: Multimodal Language Models, Vision-Language Foundations, Text-to-Image models, and Beyond 2D Generation using Neural Radiance Fields (NeRF). Each category is designed to cater to specific needs and advancements in the field, leveraging cutting-edge models to handle a wide range of data types, including text, images, and 3D models. Note We are migrating support for multimodal models from NeMo 1.0 to NeMo 2.0. If you want to explore this domain in the meantime, please refer to the documentation for the NeMo 24.07 (previous) release. Deployment and Inference # NeMo Framework provides various paths for LLM inference, catering to different deployment scenarios and performance needs. Deploy with NVIDIA NIM # NeMo Framework seamlessly integrates with enterprise-level model deployment tools through NVIDIA NIM . This integration is powered by NVIDIA TensorRT-LLM ensuring optimized and scalable inference. For more information on NIM, visit the NVIDIA website . Deploy with TensorRT-LLM or vLLM # NeMo Framework offers scripts and APIs to export models to two inference optimized libraries, TensorRT-LLM and vLLM, and to deploy the exported model with the NVIDIA Triton Inference Server. For scenarios requiring optimized performance, NeMo models can leverage TensorRT-LLM, a specialized library for accelerating and optimizing LLM inference on NVIDIA GPUs. This process involves converting NeMo models into a format compatible with TensorRT-LLM using the nemo.export module. LLM Deployment Overview Deploy NeMo Large Language Models with NIM Deploy NeMo Large Language Models with TensorRT-LLM Deploy NeMo Large Language Models with vLLM Supported Models # Large Language Models # Large Language Models # Large Language Models Pretraining &amp; SFT PEFT Alignment FP8 Training Convergence TRT/TRTLLM Convert To &amp; From Hugging Face Evaluation Llama3 8B/70B, Llama3.1 405B Yes Yes x Yes (partially verified) Yes Both Yes Mixtral 8x7B/8x22B Yes Yes x Yes (unverified) Yes Both Yes Nemotron 3 8B Yes x x Yes (unverified) x Both Yes Nemotron 4 340B Yes x x Yes (unverified) x Both Yes Baichuan2 7B Yes Yes x Yes (unverified) x Both Yes ChatGLM3 6B Yes Yes x Yes (unverified) x Both Yes DeepSeek V2/V3 Yes Yes x Yes (unverified) x Both Yes Gemma 2B/7B Yes Yes x Yes (unverified) Yes Both Yes Gemma2 2B/9B/27B Yes Yes x Yes (unverified) x Both Yes GPT-OSS 20B/120B/ Yes Yes x Yes (unverified) x Both Yes Mamba2 130M/370M/780M/1.3B/2.7B/8B/ Hybrid-8B Yes Yes x Yes (unverified) x x Yes Phi3 mini 4k x Yes x Yes (unverified) x x x Qwen2 0.5B/1.5B/7B/72B Yes Yes x Yes (unverified) Yes Both Yes Qwen3 0.6B/1.7B/4B/8B/14B/32B/30B_A3B/235B_A22B Yes Yes x Yes (unverified) Yes Both Yes StarCoder 15B Yes Yes x Yes (unverified) Yes Both Yes StarCoder2 3B/7B/15B Yes Yes x Yes (unverified) Yes Both Yes BERT 110M/340M Yes Yes x Yes (unverified) x Both x T5 220M/3B/11B Yes Yes x x x x x Vision Language Models # Vision Language Models # Vision Language Models Pretraining &amp; SFT PEFT Alignment FP8 Training Convergence TRT/TRTLLM Convert To &amp; From Hugging Face Evaluation NeVA (LLaVA 1.5) Yes Yes x Yes (unverified) x From x Llama 3.2 Vision 11B/90B Yes Yes x Yes (unverified) x From x LLaVA Next (LLaVA 1.6) Yes Yes x Yes (unverified) x From x Llama Nemotron Nano VL 8B Yes Yes x Yes (unverified) x From x Embedding Models # Embedding Models # Embedding Language Models Pretraining &amp; SFT PEFT Alignment FP8 Training Convergence TRT/TRTLLM Convert To &amp; From Hugging Face Evaluation SBERT 340M Yes x x Yes (unverified) x Both x Llama 3.2 Embedding 1B Yes x x Yes (unverified) x Both x World Foundation Models # World Foundation Models # World Foundation Models Post-Training Accelerated Inference Cosmos-1.0-Diffusion-Text2World-7B Yes Yes Cosmos-1.0-Diffusion-Text2World-14B Yes Yes Cosmos-1.0-Diffusion-Video2World-7B Coming Soon Coming Soon Cosmos-1.0-Diffusion-Video2World-14B Coming Soon Coming Soon Cosmos-1.0-Autoregressive-4B Yes Yes Cosmos-1.0-Autoregressive-Video2World-5B Coming Soon Coming Soon Cosmos-1.0-Autoregressive-12B Yes Yes Cosmos-1.0-Autoregressive-Video2World-13B Coming Soon Coming Soon Note NeMo also supports pretraining for both diffusion and autoregressive architectures text2world foundation models. Speech AI # Developing conversational AI models is a complex process that involves defining, constructing, and training models within particular domains. This process typically requires several iterations to reach a high level of accuracy. It often involves multiple iterations to achieve high accuracy, fine-tuning on various tasks and domain-specific data, ensuring training performance, and preparing models for inference deployment. NeMo Framework provides support for the training and customization of Speech AI models. This includes tasks like Automatic Speech Recognition (ASR) and Text-To-Speech (TTS) synthesis. It offers a smooth transition to enterprise-level production deployment with NVIDIA Riva . To assist developers and researchers, NeMo Framework includes state-of-the-art pre-trained checkpoints, tools for reproducible speech data processing, and features for interactive exploration and analysis of speech datasets. The components of the NeMo Framework for Speech AI are as follows: Training and Customization NeMo Framework contains everything needed to train and customize speech models ( ASR , Speech Classification , Speaker Recognition , Speaker Diarization , and TTS ) in a reproducible manner. SOTA Pre-trained Models NeMo Framework provides state-of-the-art recipes and pre-trained checkpoints of several ASR and TTS models, as well as instructions on how to load them. Speech Tools NeMo Framework provides a set of tools useful for developing ASR and TTS models, including: NeMo Forced Aligner (NFA) for generating token-, word- and segment-level timestamps of speech in audio using NeMoâs CTC-based Automatic Speech Recognition models. Speech Data Processor (SDP) , a toolkit for simplifying speech data processing. It allows you to represent data processing operations in a config file, minimizing boilerplate code, and allowing reproducibility and shareability. Speech Data Explorer (SDE) , a Dash-based web application for interactive exploration and analysis of speech datasets. Dataset creation tool which provides functionality to align long audio files with the corresponding transcripts and split them into shorter fragments that are suitable for Automatic Speech Recognition (ASR) model training. Comparison Tool for ASR Models to compare predictions of different ASR models at word accuracy and utterance level. ASR Evaluator for evaluating the performance of ASR models and other features such as Voice Activity Detection. Text Normalization Tool for converting text from the written form to the spoken form and vice versa (e.g. â31stâ vs âthirty firstâ). Path to Deployment NeMo models that have been trained or customized using the NeMo Framework can be optimized and deployed with NVIDIA Riva . Riva provides containers and Helm charts specifically designed to automate the steps for push-button deployment. Getting Started with Speech AI Quickstart Guide Tutorial Notebooks Other Resources # GitHub Repos # NeMo : The main repository for the NeMo Framework NeMo-Run : A tool to configure, launch and manage your machine learning experiments. NeMo-RL : A Scalable and Efficient Post-Training Library NeMo-Curator : Scalable data pre-processing and curation toolkit for LLMs Getting Help # Engage with the NeMo community, ask questions, get support, or report bugs. NeMo Discussions NeMo Issues Programming Languages and Frameworks # Python: The main interface to use NeMo Framework Pytorch: NeMo Framework is built on top of PyTorch Licenses # NeMo Github repo is licensed under the Apache 2.0 license NeMo Framework is licensed under the NVIDIA AI PRODUCT AGREEMENT . By pulling and using the container, you accept the terms and conditions of this license. The NeMo Framework container contains Llama materials governed by the Meta Llama3 Community License Agreement . Footnotes [ 1 ] ( 1 , 2 ) Currently, NeMo Curator and NeMo RL support for Multimodal models is a work in progress and will be available very soon. previous NVIDIA NeMo Framework User Guide next Install NeMo Framework On this page Large Language Models and Multimodal Models Data Curation Training and Customization RL Multimodal Models Deployment and Inference Deploy with NVIDIA NIM Deploy with TensorRT-LLM or vLLM Supported Models Large Language Models Vision Language Models Embedding Models World Foundation Models Speech AI Other Resources GitHub Repos Getting Help Programming Languages and Frameworks Licenses so the DOM is not blocked --> Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright Â© 2023-2025, NVIDIA Corporation. Last updated on Sep 03, 2025. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html",
      "full_text": "Title: Page Not Found\n\nURL Source: https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\nPage Not Found\n\n===============\n\nSubmit Search\n*   [NVIDIA Developer](https://developer.nvidia.com/)  \n*   [Blog](https://developer.nvidia.com/blog/)  \n*   [Forums](https://forums.developer.nvidia.com/)  \n*   [Join](https://docs.nvidia.com/login?sourceUrl=https%3A%2F%2Fdocs.nvidia.com%2Fpage-not-found)  \n\n[![Image 1: NVIDIA Documentation Home](https://docscontent.nvidia.com/bf/6f/f2d5da4743aebb3dff0e6a6129ec/nvidia-docshub-logo-2.svg)](https://docs.nvidia.com/)Submit Search\n\n*   [NVIDIA Developer](https://developer.nvidia.com/)  \n*   [Blog](https://developer.nvidia.com/blog/)  \n*   [Forums](https://forums.developer.nvidia.com/)  \n*   [Join](https://docs.nvidia.com/login?sourceUrl=https%3A%2F%2Fdocs.nvidia.com%2Fpage-not-found)  \n\nMenu\n\nPage Not Found\n\nThis page no longer exists. It is a non-supported format.\n\nIf you have any questions, please [contact us](https://forums.developer.nvidia.com/).\n\n*   \nCorporate Info \n    *   [NVIDIA.com Home](https://www.nvidia.com/en-us/)\n    *   [About NVIDIA](https://www.nvidia.com/en-us/about-nvidia/)\n\n*   \n‎NVIDIA Developer \n    *   [Developer Home](https://developer.nvidia.com/)\n    *   [Blog](https://blogs.nvidia.com/)\n\n*   \nResources \n    *   [Contact Us](https://www.nvidia.com/en-us/contact/)\n    *   [Developer Program](https://developer.nvidia.com/developer-program)\n\nCopyright © 2023 NVIDIA Corporation\n\n[Legal Information](https://www.nvidia.com/en-us/about-nvidia/legal-info/) | [Privacy](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/)| [Cookie Policy](https://www.nvidia.com/en-us/about-nvidia/cookie-policy/) | [Contact](https://www.nvidia.com/en-us/contact/)\n\n![Image 2](https://cdn.bizible.com/ipv?_biz_r=&_biz_h=-417244810&_biz_u=9a988ed427eb451eab0bc34811ca634d&_biz_l=https%3A%2F%2Fdocs.nvidia.com%2Fnemo-framework%2Fuser-guide%2Flatest%2Fperformance%2Fperformance_summary.html&_biz_t=1757777090110&_biz_i=Page%20Not%20Found&_biz_n=0&a=nvidia.com&rnd=605933&cdn_o=a&_biz_z=1757777090112)![Image 3](https://cdn.bizibly.com/u?_biz_u=9a988ed427eb451eab0bc34811ca634d&_biz_l=https%3A%2F%2Fdocs.nvidia.com%2Fnemo-framework%2Fuser-guide%2Flatest%2Fperformance%2Fperformance_summary.html&_biz_t=1757777090115&_biz_i=Page%20Not%20Found&a=nvidia.com&rnd=122479&cdn_o=a&_biz_z=1757777090115)![Image 4](https://cdn.bizible.com/u?mapType=ecid&mapValue=F207D74D549850760A4C98C6%40AdobeOrg_36498077156721552183472212933009331853&_biz_u=9a988ed427eb451eab0bc34811ca634d&_biz_l=https%3A%2F%2Fdocs.nvidia.com%2Fnemo-framework%2Fuser-guide%2Flatest%2Fperformance%2Fperformance_summary.html&_biz_t=1757777090116&_biz_i=Page%20Not%20Found&_biz_n=1&a=nvidia.com&rnd=382245&cdn_o=a&_biz_z=1757777090417)\n\nFeedback\n\nHow would you rate your overall website experience?\n---------------------------------------------------\n\n1 2 3 4 5 6 7 \n\nVery dissatisfied Very satisfied\n\nNext\n\n![Image 5](https://survey-images.hotjar.com/surveys/logo/a82491d08da94cbdb13d07b60bbcd60e)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html#performance-tuning-guide",
      "full_text": " Performance Tuning Guide &#8212; NVIDIA NeMo Framework User Guide Skip to main content Back to top Ctrl + K You are viewing the NeMo 2.0 documentation. This release introduces significant changes to the API and a new library, NeMo Run . We are currently porting all features from NeMo 1.0 to 2.0. For documentation on previous versions or features not yet available in 2.0, please refer to the NeMo 24.07 documentation . NVIDIA NeMo Framework User Guide NVIDIA NeMo Framework User Guide Table of Contents NeMo Framework Overview Install NeMo Framework Performance Why NeMo Framework? Getting Started Quickstart with NeMo-Run Quickstart with NeMo 2.0 API Tutorials Developer Guides Migration Guide Pre-Training SFT Training and Inference PEFT Training and Inference Trainer Configuration Precision Configuration Parallelisms Experiment Manager Checkpointing Configurations Optimizer Configuration Data Configuration Nsys Profiling Tokenizers Feature Guide The Bridge Between Lightning and Megatron Core Logging and Checkpointing Serialization Parameter Efficient Fine-Tuning (PEFT) Hugging Face Integration Profiling Best Practices Performance Tuning Guide Training and Customization Long Context Training Context Parallelism Optimal Configuration with Auto Configurator Parameter-Efficient Fine-tuning (PEFT) Supported PEFT Methods A Comparison of Performant and Canonical LoRA Variants Sequence Packing Resiliency Continual Training Custom Datasets Pre-Training Data Module Fine-Tuning Data Module Model Optimization Quantization Pruning Distillation Speculative Decoding Models Large Language Models Baichuan 2 ChatGLM 3 DeepSeek V2 DeepSeek V3 Gemma Gemma 2 GPT-OSS Hyena Llama 3 Llama Nemotron Mamba 2 Mixtral Nemotron Phi 3 Qwen2/2.5 Qwen3 Starcoder Starcoder 2 T5 BERT Vision Language Models NeVA (LLaVA) LLaVA-Next Llama 3.2 Vision Models Llama 4 Models Qwen2-VL Gemma 3 Models Data Preparation to Use Megatron-Energon Dataloader CLIP Llama Nemotron Nano VL 8B Audio-Vision Language Model Speech AI Models Diffusion Models Flux Diffusion Training Framework Embedding Models SBERT Llama Embedding Exporting Llama Embedding To ONNX and TensorRT Reranker Models Llama Reranker Library Documentation Overview NeMo Introduction NeMo Fundamentals Tutorials Mixed Precision Training Parallelisms Mixture of Experts Optimizations Attention Optimizations Activation Recomputation Communication Overlap CPU Offloading Checkpoints NeMo Distributed Checkpoint User Guide Converting from Megatron-LM Evaluate NeMo 2.0 Checkpoints Evaluation Adapters NeMo APIs NeMo Models Neural Modules Experiment Manager Neural Types Adapters Adapter Components Adapters API NeMo Core APIs NeMo Common Collection API Callbacks Losses Metrics Tokenizers Data S3 Checkpointing NeMo ASR API NeMo TTS API NeMo Collections Large Language Models GPT Model Training Batching Positional embeddings Megatron Core Customization Reset Learning Rate Ramp Up Batch Size Machine Translation Models Automatic Speech Recognition (ASR) Models Datasets ASR Language Modeling and Customization Checkpoints Scores NeMo ASR Configuration Files NeMo ASR API All Checkpoints Example With MCV Speech Classification Models Datasets Checkpoints NeMo Speech Classification Configuration Files Resource and Documentation Guide Speaker Recognition (SR) Models NeMo Speaker Recognition Configuration Files Datasets Checkpoints NeMo Speaker Recognition API Resource and Documentation Guide Speaker Diarization Models Datasets Checkpoints End-to-End Speaker Diarization Configuration Files NeMo Speaker Diarization API Resource and Documentation Guide Speech Self-Supervised Learning Models Datasets Checkpoints NeMo SSL Configuration Files NeMo SSL collection API Resources and Documentation Speech Intent Classification and Slot Filling Models Datasets Checkpoints NeMo Speech Intent Classification and Slot Filling Configuration Files NeMo Speech Intent Classification and Slot Filling collection API Resources and Documentation SpeechLM2 Models Datasets Configuration Files Training and Scaling Text-to-Speech (TTS) Models Data Preprocessing Checkpoints NeMo TTS Configuration Files Grapheme-to-Phoneme Models Speech and Audio Processing Models Datasets Checkpoints NeMo Audio Configuration Files NeMo Audio API Speech AI Tools NeMo Forced Aligner (NFA) Dataset Creation Tool Based on CTC-Segmentation Speech Data Explorer Comparison tool for ASR Models ASR Evaluator Speech Data Processor (Inverse) Text Normalization WFST-based (Inverse) Text Normalization Neural Models for (Inverse) Text Normalization NeMo AutoModel NeMo Curator NeMo Eval NeMo Export and Deploy NeMo RL NeMo Run Guides Why should I use NemoRun? Configure NeMo-Run Execute NeMo Run Manage NeMo-Run Ray Clusters &amp; Jobs NeMo Run CLI Guide Frequently Asked Questions Releases Software Component Versions Changelog Known Issues Performance... Performance Tuning Guide # NeMo Framework provides a wide range of features for performant and memory-efficient LLM training on GPUs, and comes pre-configured with optimal settings. However, factors such as model architecture, hyperparameters, GPU count, and GPU type can affect the available options, and additional tuning may be necessary to achieve optimal performance. This document explores the factors that affect training performance, highlights common issues, and outlines techniques for performance tuning that lead to higher MFU (Model FLOPS Utilization) and TCO. NeMo Framework(âNeMoâ) provides a wide range of features for performant and memory-efficient LLM training on GPUs, and comes pre-configured with optimal settings. However, factors such as model architecture, hyperparameters, GPU count, and GPU type can affect the available options, and additional tuning may be necessary to achieve optimal performance. This document explores the factors that affect training performance, highlights common issues, and outlines techniques for performance tuning that lead to higher MFU (Model FLOPS Utilization) and TCO. Low Precision Training # Expected speedup of FP8 training compared to BF16 training The default low-precision LLM training recipe applies FP8 computation exclusively to the linear layers within the Transformer block, typically achieving a speedup of 1.2â1.5X. However, the actual speedup depends on the proportion of training time spent on these linear layers. For instance, smaller LLMs with a limited hidden size exhibit lower FP8 speedup, as linear layers scale with O(sequence_length Ã hidden_sizeÂ²) complexity, whereas the other element-wise computation layers (e.g., layer norms, dropouts, RoPE, and simple math functions) scale with O(sequence_length Ã hidden_size), and dot-product attention scales with O(sequence_lengthÂ² Ã hidden_size). Consequently, the contribution of linear layers to the overall training time is smaller in such models. Different FP8 recipes use varying quantization block sizes, affecting performance. Smaller quantization blocks generally incur higher overhead in both quantization and GEMM execution. For example, MXFP8 with a 1Ã32 quantization block performs less efficiently than full tensor-wise FP8 scaling. Common issues of low FP8 training speedup Host performance boundness when LLM uses small GPU kernels (check `6. Lowering Host Overhead and Jitters`_ ). A low proportion of linear layers in training step time that use FP8 computation. Parallel Mapping Strategies # Data Parallelism using Distributed Optimizer You should begin with data-parallel (DP) mapping. As long as the model and activation memory fit within the GPUs, data parallelism generally offers optimal performance, minimizes communication overhead, and maximizes per-GPU tensor sizes (compared to per-tensor sharding). NeMo uses the distributed optimizer as the default method for data-parallel training. It shards master parameters and optimizer states across data-parallel ranks, reducing model state memory usage without increasing communication overhead compared to traditional data-parallel training. recipe.trainer.use_distributed_optimizer=true Per-tensor Sharding (Tensor-parallel or Context-parallel mappings) Tensor parallelism (TP) is the primary recommendation when a model exceeds GPU memory capacity under data-parallel mapping. However, since it involves higher communication overhead, the tensor-parallel size should ideally be confined to the high-bandwidth intra-node network (NVLink domain). recipe.trainer.strategy.tensor_model_parallel_size=&lt;int&gt; When the sequence length in a training run is significantly larger than the hidden size, activation memory can overflow. In such cases, context parallelism (CP) helps by sharding tensors along the sequence dimension, allowing the workload to fit within limited GPU memory and improving performance. Like tensor parallelism (TP), CP requires inter-GPU communication of activations. However, for the same tensor sizes, CP generally results in lower communication volume. That said, CPâs effectiveness depends on the relative sizes of the sequence length and hidden size. When the sequence length is smaller than the hidden size, CP produces narrow (or âskinnyâ) tensor shards on each GPU. This reduces data reuse and can degrade performance. Additionally, because CP shards activations, it also partitions optimizer states in distributed training. As a result, optimizer state partitioning spans both the data parallel (DP) and context parallel (CP) dimensions. recipe.trainer.strategy.context_parallel_size=&lt;int&gt; Performance tips: A large tensor-parallel or context-parallel size is not recommended unless the hidden size or sequence length is large enough to maintain sufficient per-GPU parallelism and avoid excessive communication overhead. For example, using a tensor-parallel size of 8 for LLAMA 3 70B could lead to low GPU utilization and make training host-performance bound. You can combine TP and CP to optimize performance by balancing communication overhead. For example, using TP=2 along with CP=2 can give better performance than TP=4 when the sequence size is larger than the hidden size. Additional tips can be found in Section `9. Long Sequence Training`_ . Pipeline Parallelism Pipeline parallelism (PP) is necessary when a model cannot fit within GPU memory using tensor parallelism. Also, virtual pipeline parallelism (VPP) should be used in conjunction with pipeline parallelism to reduce the overhead caused by pipeline warm-up and flush bubbles. recipe.trainer.strategy.pipeline_model_parallel_size=&lt;int&gt; recipe.trainer.strategy.virtual_pipeline_model_parallel_size=&lt;int&gt; Performance tips in PP and VPP sizing: PP can also be combined with per-tensor sharding methods to mitigate the impact of sharding inefficiencies and pipeline bubbles. For instance, TP4 + PP2 may outperform TP8 when both mappings fit into memory because using a large TP reduces per-GPU tensor sizes but increases the communication cost, increasing the exposed communication. VPP increases inter-stage communication overhead. When a global batch contains many micro-batches, using a smaller VPP size can improve performance, as the exposed communication cost outweighs the reduction in pipeline bubbles. Asymmetric Transformer layer allocation across pipeline stages An LLM with a large vocabulary size has computationally heavy embedding lookup and projection operations, leading to load imbalance across pipeline stages. To address this, NeMo provides an option to allocate one fewer Transformer layer in the first and last pipeline stages, which handle embedding lookup and projection, to better balance workloads. recipe.trainer.strategy.account_for_embedding_in_pipeline_split=true recipe.trainer.strategy.account_for_loss_in_pipeline_split=true Expert Parallelism Expert Parallelism (EP) is designed specifically for Mixture-of-Experts (MoE) models to efficiently distribute sparse MLP weights across multiple chips. It can be used in combination with other parallelism strategies such as Tensor Parallelism (TP), Context Parallelism (CP), Pipeline Parallelism (PP), Data Parallelism (DP), and Fully Sharded Data Parallel (FSDP). In the current design, the dense attention part and the sparse MLP part are fully decoupled in terms of their TP, CP, and DP parallelism configurations. Expert Tensor Parallelism (ETP) is introduced to specifically control the tensor parallelism for the sparse MLP part. ETP uses TP for dense layers for the ranks allocated for EP in sparse layers. On the other hand, the baseline is DEP, which folds DP in dense layers for EP in sparse layers. recipe.trainer.strategy.expert_model_parallel_size=&lt;int&gt; recipe.trainer.strategy.expert_tensor_parallel_size=&lt;int&gt; Performance tips in hybrid folding options and EP sizing: Typically, EP is kept within the high-bandwidth intra-node network (NVLink domain) to minimize the communication overhead it can introduce. However, using communication overlap techniquesâsuch as pipeline overlap or 1F1B overlapâalong with PP (e.g., DualPipe) might make it possible to expand EP into the inter-node networks. Within the sparse MLP block, DP replaces CP because it has no impact on the computation pattern based on the dispatched tokens in each EP rank. Usually, ETP is set to 1 to avoid significant communication overhead that comes with applying TP to MLP GEMMs. When multiple experts are placed on a single chip after applying Expert Parallelism, enabling grouped GEMM can significantly improve computation efficiency. recipe.model.config.moe_grouped_gemm=True Fully Sharded Data Parallelism NeMo supports two Fully Sharded Data Parallelism (FSDP) implementations: PyTorch-native FSDP and a custom Megatron FSDP built within Megatron Core. While both follow the same sharding principles, the custom implementation is further optimized for performance. The performance gain of the custom FSDP comes primarily from minimizing the data movement to the communication tensors and reusing communication buffers. Both FSDP methods can be used in combination with per-tensor sharding methods. To use PyTorch FSDP2: recipe.trainer.strategy.fsdp=âpytorchâ To use Custom Megatron FSDP: recipe.trainer.strategy.fsdp=âmegatronâ recipe.trainer.strategy.ddp.data_parallel_sharding_strategy=âoptim_grads_paramsâ FSDP can be preferred over TP+PP+DP mappings in the following scenarios: Small models with a large sequence, thus the parameter AllGather and gradient ReduceScatter can effectively be hidden under computation and the short communication overlap causes minor interference to the computation under overlap. In FSDP training, activation storage remains as the main memory bottleneck because FSDP only shards model state memory, and a large per-GPU activation is needed to hide the costly FSDP communication. On GB200 GPUs, NeMo offers an option to offload activations to the host memory via a high-speed chip-to-chip interconnect. Baseline training is host performance-bound, but FSDP allows for larger per-GPU tensor sizes by eliminating TP or enabling a larger micro-batch size. Heterogeneous Encoder Parallelism Encoder Pipeline Parallel Use recipe.trainer.strategy.encoder_pipeline_model_parallel_size . In an Encoder-Decoder architecture like Multimodal models (VLMs like NeVA etc.), Encoder Pipeline Parallel can be used to add pipeline parallelism to the encoder. Pipeline parallelism controls the amount of pipelining in the decoder part. Encoder Pipeline Parallel is limited to 1 at the moment, i.e., the encoder can occupy a maximum of 1 PP stage. By default, Encoder Pipeline Parallel is 0 and Decoder Pipeline Parallel is 1. When the Encoder Pipeline Parallel size is 0, it shares the first PP stage of the Decoder. Encoder Tensor Parallel Use recipe.trainer.strategy.encoder_tensor_model_parallel_size . Since encoders tend to be much smaller than decoders, we also provide the ability to set a different amount of tensor parallelism to the encoder than the decoder. By default, encoder tensor parallel is set to 0, i.e., the amount of tensor parallelism in the encoder is equal to tensor parallelism in the decoder. To use this option, Encoder Pipeline Parallel must be greater than 0 as we need the encoder to be on its own pipeline stage. Encoder Tensor Parallel size is limited to be less than or equal to Tensor parallel size. Total number of GPUs required when these features are used is: Data Parallel size * Context Parallel size * ((Encoder TP * Encoder PP) + (Decoder TP * Decoder PP)) These features are experimental and may still have bugs. There are critical bug fixes that will be made in a future release. Parallel mapping strategies with NVL72 Training with only data parallelism or FSDP makes it straightforward to fully utilize the bandwidth of an NVL72 system. However, when combining multiple parallelism strategies, itâs important to ensure that high-volume communicators remain confined within each NVL72 domain. For example, with TP=4, DP=16, and PP=4, the GPUs in the first TP group of DP1/PP1 spans both NVLink and network domains, causing communication performance to be bottlenecked by the slower network link. To avoid this, you may choose TP and DP sizes such that the product of TP Ã DP divides evenly into the NVL72 configuration. If the model-parallel size does not align naturally, padding may be required to support non-divisible group sizes. To avoid this partitioning complexity, you can just use 64 GPUs out of the 72 GPUs. Communication Overlaps and Tuning # Data-parallel communication of Distributed Optimizer Distributed optimizer overlaps parameter AllGathers with the forward computation of the first micro-batch and gradient ReduceScatters with the backward computation of the last micro-batch. recipe.trainer.strategy.ddp.overlap_param_gather=true recipe.trainer.strategy.ddp.overlap_grad_reduce=true When using the distributed optimizer with pipeline parallelism (PP) + virtual pipeline parallelism (VPP), DP communications overlap with multiple micro-batches, increasing the opportunity for effective overlap. Also, NeMo aligns the execution timing of DP communications across pipeline-parallel ranks to synchronize the computing kernel slowdown from the overlap. recipe.trainer.strategy.ddp.align_param_gather=true Slow DP communication at large scaling training: Distributing optimizer states across a partial DP domain reduces communication costs over high-latency Ethernet networks. Model states remain replicated outside the distributed domain. During the final micro-batch backpropagation, gradient ReduceScatters occur within the distributed domain, followed by AllReduce in the non-distributed domain. Parameter AllGathers are performed only within the distributed domain. recipe.trainer.strategy.ddp.num_distributed_optimizer_instances=&lt;int&gt; A large message size for DP communication is recommended to maximize network bandwidth utilization. You can achieve this by increasing the communication bucket size. recipe.trainer.strategy.ddp.bucket_size=&lt;number_of_elements: int&gt; A common reason for DP communication overlap failure: Persistent Layer Normalization (LN) kernels from Transformer Engine use spin-waiting for all SMs in the GPU, causing the LN kernel and subsequent computation kernels to be scheduled only after DP communication. To prevent this, an appropriate SM margin should be configured using the following environment variables. NVTE_FWD_LAYERNORM_SM_MARGIN=&lt;#SM for DP collectives = 16&gt; NVTE_BWD_LAYERNORM_SM_MARGIN=&lt;#SM for DP collectives = 16&gt; Custom Megatron FSDP Unless you specify the communication bucket size, MCORE FSDP uses fixed communication overlap that overlaps the parameter AllGather and gradient ReduceScatter of each Transformer layer with its associated forward and backward computations. Tensor-parallel (TP) communication (with sequence parallelism) NeMo currently uses the userbuffer backend in Transformer Engine for TP communication overlaps. This offers the pipelined overlap of the TP communication with dependent computation. callback.tp_comm_overlap The overlap method, resource, and precision of the TP communication overlaps are configurable, and the most performance configurations are set in the NeMo training recipes by default. Also, you can set a custom TP communication overlap configuration via the below interface following the structure of TransformerLayerTPOverlapCfg class. callback.tp_comm_overlap_cfg=&lt;TransformerLayerTPOverlapCfg&gt; TP communication overlap setting tips Balancing the number of SMs between communication and GEMM For AllGather/ReduceScatter bulk and ReduceScatter pipelined overlap, you can adjust the number of SMs to balance communication and GEMM execution. Allocating too many SMs to communication may degrade GEMM performance, while too few may expose communication overhead. The default SM allocation for communication is 16, but you can fine-tune it based on profiling results. TransformerLayerTPOverlapCfg.num_sm=&lt;int&gt; CGA sizing to improve SM utilization The CGA size can be set between 1 and 4, but it should not exceed the number of SMs allocated for communication. We recommend using CGA â¤ 2 to prevent potential SM rasterization that could impact GEMM performance. TransformerLayerTPOverlapCfg.cga_size=&lt;intâ¤4&gt; Use 4Ã splits for ReduceScatter and GEMM overlap to optimize the balance between GEMM efficiency and communication exposure. In GEMM-then-ReduceScatter pipeline overlap, a 1Ã ReduceScatter chunk remains exposed. A small split size increases communication exposure, while a large split size may degrade performance due to aggregated GEMM wave quantization. We find that num_splits = 4 generally provides the best performance. TransformerLayerTPOverlapCfg.num_split=&lt;int&gt; Common reason for TP comm overlap failure at Hopper At H100 GPU, an environment variable CUDA_DEVICE_MAX_CONNECTIONS=1 should be set. Otherwise, TP communication kernels can be scheduled at the end of GEMM to overlap with. Pipelined TP communication overlap is used by a static userbuffer registered upon model initialization. Therefore, it doesnât support activation tensors dynamically changing between steps or between Transformer layers. Context-parallel (CP) communication CP communication is configurable via âcp_comm_typeâ, which can be âp2pâ, âall_gatherâ, âa2aâ, or âa2a+p2pâ. Communications of âp2pâ are implemented as ring-exchange send/receive operations, and they are hard-coded to overlap with the attention compute of sequence chunks. See Section `9. Long Sequence Training`_ for more details. Expert-parallel communication To hide the A2A/AG communication introduced by EP, pipeline split overlap or 1F1B overlap alongside Pipeline Parallelism could be possible. It will be added to NeMo in future releases. Pipeline-parallel (PP) send/receive communication PP send/recv in steady 1F1B states are set to be overlapped with computes by default. The PP send/recv in warmup and flush are exposed by default. Communication Data Types # FP8 data-parallel parameter AllGather in Distributed Optimizer and FSDP NeMo supports FP8 parameter AllGather for per-tensor FP8 scaling recipes. This operation is lossless, enhancing performance while reducing memory usage. MegatronMixedPrecision.fp8_params=true BF16 (instead of FP32) data-parallel reduction in Distributed Optimizer and FSDP We have validated that BF16 reduction is numerically safe across numerous model training runs. However, BF16 reduction with a large data-parallel size (e.g., DP â¥ 128), especially the Ring reduction algorithmâwhich accumulates copies sequentiallyâmay impact numerical stability. When using SHARP with NVIDIA InfiniBand, BF16 reduction is more robust, as it performs binary additions with higher precision for intermediate partial reductions. recipe.trainer.strategy.ddp.grad_reduce_in_fp32=false FP8 tensor-parallel ReduceScatter When communication latency exceeds GEMM execution time, using FP8 input ReduceScatter can better hide communication overhead. This approach has low numerical impact, as the GEMM output must be cast to FP8 and then converted back to high precision during reduction. TransformerLayerTPOverlapCfg.fp8_buf=true FP8 A2A Dispatch for expert parallel communication NeMo is working on supporting FP8 A2A dispatch (before expert FC1), but still keeps BF16 A2A combine (after expert FC2). Performance at Scale # Scaling a training job is typically achieved by increasing the size of the data-parallel domain. In large-scale training, this often results in a small number of micro-batches per global batchâor even a single micro-batchâcausing most computations to overlap with data-parallel communication. To maintain high performance in such scenarios, you should focus on minimizing the overhead of data-parallel communication and reducing host-driven inter-GPU jitter. You can lower the overhead of data-parallel communication by (1) reducing the communication precision e.g., BF16 for gradient reduction and FP8 parameter gathering, (2) improving the efficiency of communication by increasing the data-parallel communication message size or using the hierarchical data-parallel reduction, or (3) using multi-cast and switch reduction with SHARP in case of InfiniBand network. Using BF16 gradient reduction and FP8 parameter gather are described in Section `4. Communication Data Types`_ For non-pipeline-parallel training, the data-parallel communication bucket size can be adjusted using the knobs below. In pipeline-parallel training, however, the bucket size is fixed and determined by the number of parameters assigned to each virtual pipeline rank. recipe.trainer.strategy.ddp.bucket_size=&lt;int: bytes&gt; Setting the knob below splits the data-parallel domain of the distributed optimizer into a sharding domain and a replication domain. Gradient reduction then occurs in two stagesâone within each domainâavoiding the use of a single large flat ring for collective operations that have high latency. recipe.trainer.strategy.num_distributed_optimizer_instances=&lt;int: â¤dp_size&gt; Ideas to reduce the host-driven inter-GPU jitters are discussed in Section `6. Lowering Host Overhead and Jitters`_ . Lowering Host Overhead and Jitters # Common observation associated with host overhead Significantly low GPU FLOPS. Small performance gain of low-precision (FP8) training. Small LLMs with small hidden size or sequence length or fine-tuning without sequence packing High multi-GPU communication variation. Increasing micro-batch size and reduce per-tensor sharding The most common way to increase per-GPU tensor size is by increasing the micro-batch size or minimizing unnecessary per-tensor sharding (e.g., TP or CP) when GPU memory permits. Manual garbage collection to align the host interruption across GPUs NeMo manually aligns the timing of garbage collection across GPUs that significantly mitigate the host overhead compared to the baseline automatic garbage collection. GarbageCollectionCallback.gc_interval_train=&lt;int&gt; GarbageCollectionCallback.gc_interval_val=&lt;int&gt; CUDA graph to eliminate repeated static host code execution NeMo supports graph capture, significantly reducing host overhead. CUDA Graph is applicable only to LLMs with a static tensor shape across training steps. For example, it supports fixed-size packed sequences but does not handle sequences with varying lengths at each step. Also, MoE models with token-dropless propagation have limited CUDA graph support, restricted to the dense modules only. CUDA graph requires additional memory for static buffer management, typically adding a few gigabytes for static buffers, while models with PP size &gt; 1 may consume over 10GB. We are actively working to reduce this memory overhead. recipe.model.config.enable_cuda_graph=true Bind CPU memory for GPU processes Binding CPU cores to GPU processes helps mitigate long latency issues and ensures minimal variation in GPU queuing latency across GPUs. This optimization significantly impacts, particularly when the communication domain size is large. Example command line for a X86-based GPU system: numactl âcpunodebind=$((SLURM_LOCALID/4)) âmembind=$((SLURM_LOCALID/4)) &lt;run script&gt; Example command line for a Grace-based GPU system: numactl âcpunodebind=$((SLURM_LOCALID/2)) âmembind=$((SLURM_LOCALID/2)) &lt;run script&gt; Techniques for Reducing Memory to Avoid Memory Overflow and Enhance Training Efficiency # Activation recomputation NeMo LLMs default to dot-product attention-only recomputation using Flash Attention, efficiently regenerating large intermediate activations from the attention operation with minimal computational overhead. NeMo also supports recomputing the full intermediate activations of a Transformer block, significantly reducing activation memory usage at the cost of approximately 30% additional computation. The number of Transformer blocks to recompute can be adjusted using a configurable setting. recipe.model.config.recompute_granuality=full recipe.model.config.recompute_method=block recipe.model.config.recompute_num_layers=&lt;int:â¤num_layers_in_the_model&gt; Activation offloading to host memory NeMo supports offloading activation memory to host memory, essential for training tasks constrained by activation memory. This is particularly useful for scenarios like (1) FSDP, where model state memory is minimized through sharding but activation memory remains high, (2) LoRA, which has frozen parameters but significant activation memory demands, and (3) the training with a large sequence length. The efficiency of activation offloading depends on both the interconnect bandwidth between the GPU and host and the host memory bandwidth. From this perspective, Grace-based systems like the GB200 enhance offloading performance by optimizing these bandwidths. The following knobs should be configured to enable offloading and specify the number of Transformer layers to offload to host memory. The maximum number of layers that can be offloaded depends on host memory capacity, which may be lower when the CPU is shared among multiple GPUs. recipe.model.config.cpu_offloading=True recipe.model.config.cpu_offloading_weights=False recipe.model.config.cpu_offloading_num_layers=&lt;int:â¤activation_offload_layers&gt; Environment variable settings to avoid resource conflict between CPU memory offloading and network communication NCCL_NET_GDR_LEVEL=PHB # NCCL &lt;=2.25 NCCL_NET_GDR_C2C=1 # NCCL &gt;=2.26 Optimization tips Given the ratio between activation volume and computational operations, offloading all layer activations naively can become a performance bottleneck. Optimizing performance requires tuning the number of layers to offload while balancing it with recomputation. Weight memory-optimized BF16 training In BF16 training, NeMo optimizes memory usage by storing only the BF16 remainder of the master weight copies for the next optimizer update. This is possible because BF16 data can be represented using a subset of FP32 bits, allowing NeMo to avoid redundant storage of the FP32 portion used for BF16 representation. This is default enabled when using precision-aware optimizer in Megatron Core. recipe.model.config.use_precision_aware_optimizer=True Common memory usage hikes from environment variable setting NeMo run scripts set the below environment variables that (1) do not preserve the buffers for NCCL communication and (2) disable NVLSharp when not used. Both these options lower the GPU memory usage. TORCH_NCCL_AVOID_RECORD_STREAMS=1 NCCL_NVLS_ENABLE=0 While not enabled by default, you can further reduce memory usage caused by segmentation penalties by setting the env var shown below. PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True Keep parameters in FP8 at FP8 training In FP8 training, after optimizer step execution, we can keep the parameters in FP8. Compared to the baseline that keeps the intermediate weight values in BF16, FP8 parameters lower memory usage and improve communication performance. The below knob enables keeping the parameters in FP8. recipe.model.config.fp8_param_gather=True Operator Fusion # All operator fusions are enabled by default in NeMo run scripts. You can control specific fusion behaviors using the following configuration knobs: recipe.model.config.masked_softmax_fusion=true recipe.model.config.cross_entropy_loss_fusion=true recipe.model.config.gradient_accumulation_fusion=true recipe.model.config.bias_activation_fusion=true recipe.model.config.bias_dropout_fusion=true recipe.model.config.apply_rope_fusion=true NeMo offers different Flash Attention options, which can be chosen by environment FlashAttention2 (default): NVTE_FLASH_ATT=1 cuDNN fused attention: NVTE_FLASH_ATT=0, NVTE_FUSED_ATT=1 Long Sequence Training # Problem of long sequence training Training with long sequence length can lead to memory overflow due to the huge memory cost of activations. The problem could be solved by recomputing activations in backward, but it can impose up to ~30% overheads in each training step. Context parallelism is a better solution which splits the sequence dimension across multiple GPUs, so that each GPU only computes and saves activations of a sequence chunk. In this way, memory overflow is addressed without introducing any redundant compute. CP to shard activation (knob) recipe.trainer.strategy.context_parallel_size=&lt;int&gt; Both TP and CP can reduce activation memory overheads. Itâs not wise to be biased to either of them. Communications of TP and CP are overlapped by GEMM and Attention respectively. Blindly enlarging their sizes can make some communications hard to overlap. Itâs recommended to sweep a combination of TP+CP configs. The optimal config is expected to make full use of all related compute and do best overlapping, thereby achieving best end-to-end performance. recipe.model.config.cp_comm_type=&lt;str&gt; or &lt;list of str&gt; Megatron-Core provides multiple implementation variants of CP and allows you to make choices based on your specific use cases by configuring âcp_comm_typeâ. The configuration value can be p2p , all_gather , a2a , or a2a+p2p . These communication types are compatible with each other, so they can be flexibly interleaved between transformer layers. You only need to provide a list, where each element corresponds to a layer. p2p : exchanges KV sequence chunks in ring-topology. The P2P communications can be fully overlapped. all_gather : inserts an all-gather before attention to get a full sequence of KV. The all-gather is exposed, but it should not impose big overheads if GQA/MQA are used, as they have very few KV heads. a2a : is an implementation of DeepSpeed Ulysses. A2A communications are added before and after the attention module to gather full sequence length and further scatter heads in CP domain. A2A cannot be overlapped. a2a+p2p : is a middle ground between a2a and p2p . This is useful for cases of big CP sizes, where each sequence chunk is too short to overlap P2P communications. It first does A2A in partial CP groups to gather relatively longer sequence chunks, then applies P2P implementation to the gathered chunks. It also can be helpful for hierarchical CP communications, for example A2A and P2P happen in NVLink and IBLink domains respectively. With small and medium CP size, p2p is the recommended configuration because communications can be fully overlapped; âall_gatherâ also should work fine with GQA/MQA. As for strongly-scaling a sequence length with big CP sizes, the short chunk length can barely overlap the p2p communications, so a2a+p2p ought to be the preferred choice. a2a could be adopted in some cases for its simplicity. However, CP size can be restricted with âa2aâ because it requires the number of attention heads to be divisible by CP size. Restricted CP size will finally limit the sequence length that can be run. Activation recomputation (in Section `7. Techniques for Reducing Memory to Avoid Memory Overflow and Enhance Training Efficiency`_ ) Activation offloading to host memory (in Section `7. Techniques for Reducing Memory to Avoid Memory Overflow and Enhance Training Efficiency`_ ) Sequence Packing for Performant Fine-Tuning # Dataset preparation Fine-tuning datasets with shorter sequences of variable length can be packed into longer sequences, up to a set maximum length, for best efficiency. To use this feature, the microbatch size must be set to 1. In place of increasing the micro batch size, the maximum sequence length can be increased, which will effectively increase the number of individual sequences per packed sequence. Enabled with: recipe.data.packed_sequence_specs.packed_sequence_size=&lt;max sequence length&gt; recipe.data.micro_batch_size=1 Performance benefits also include: Inconsistent lengths between sequences in the fine-tuning dataset would reduce the computation efficiency. With a micro-batch size over 1, all sequences must be padded with empty tokens to the length of the longest one in the micro-batch. Similarly, some optimizations like CUDA graphs require uniform sequence lengths between micro-batches. Packed sequences are arranged so that the total number of tokens per packed sequence is as close to the maximum length as possible, making most processed tokens useful. Likewise, when using data parallel, variance in time needed to process different batches can result in all batches needing to wait for the longest to finishâ and this variance is reduced with packed sequence. GPU Core Clock Optimization # Increase the clock ratio of GPU core over off-chip memory system NVIDIA GPUs support a CPU core clock boost mode, which increases the core clock rate by reducing the off-chip memory clock rate. This is particularly beneficial for LLMs, which are typically compute throughput-bound. NeMo run scripts enable this core clock boost mode by default. sudo nvidia-smi boost-slider âvboost 1 &lt;run commandline&gt; Profiling Options for Analysis-based Performance Tuning # Nsight system profile NeMo provides an interface to enable the NVIDIA Nsight Systems profiler, which displays the GPU execution trace of all CUDA streams. You can check whether communication kernels overlap with computation kernels and adjust resource allocation to balance communication and computation. The Nsight Systems profile can be enabled using NsysPlugin, as shown below. NsysPlugin(start_step=&lt;int&gt;, end_step=&lt;int&gt;, ranks=&lt;[0,...]&gt;, nsys_trace=&lt;[&quot;nvtx&quot;, &quot;cuda&quot;,...]&gt;) Memory snapshot NeMo provides an interface to extract the memory snapshot that shows the memory allocation bytes, the allocation lifespan, and the function call stack. Extracting the memory snapshot can be enabled by MemoryProfilePlugin as shown below. MemoryProfilePlugin(dir=&lt;/path/to/store/the/output/file, ranks=&lt;[0,...]&gt;) Index - List of Tuning Knobs # callback.tp_comm_overlap callback.tp_comm_overlap_cfg CUDA_DEVICE_MAX_CONNECTIONS garbageCollectionCallback.gc_interval_train garbageCollectionCallback.gc_interval_val megatronMixedPrecision.fp8_params MemoryProfilePlugin NCCL_NET_GDR_C2C NCCL_NET_GDR_LEVEL NCCL_NVLS_ENABLE NsysPlugin NVTE_BWD_LAYERNORM_SM_MARGIN=&lt;#SM for DP collectives NVTE_FLASH_ATT NVTE_FUSED_ATT NVTE_FWD_LAYERNORM_SM_MARGIN=&lt;#SM for DP collectives PYTORCH_CUDA_ALLOC_CONF recipe.data.micro_batch_size recipe.data.packed_sequence_specs.packed_sequence_size recipe.model.config.apply_rope_fusion recipe.model.config.bias_activation_fusion recipe.model.config.bias_dropout_fusion recipe.model.config.cp_comm_type recipe.model.config.cpu_offloading recipe.model.config.cpu_offloading_num_layers recipe.model.config.cpu_offloading_weights recipe.model.config.cross_entropy_loss_fusion recipe.model.config.enable_cuda_graph recipe.model.config.fp8_param_gather recipe.model.config.gradient_accumulation_fusion recipe.model.config.masked_softmax_fusion recipe.model.config.recompute_granuality recipe.model.config.recompute_method recipe.model.config.recompute_num_layers recipe.model.config.use_precision_aware_optimizer recipe.trainer.strategy.account_for_embedding_in_pipeline_split recipe.trainer.strategy.account_for_loss_in_pipeline_split recipe.trainer.strategy.context_parallel_size recipe.trainer.strategy.context_parallel_size recipe.trainer.strategy.ddp.align_param_gather recipe.trainer.strategy.ddp.bucket_size recipe.trainer.strategy.ddp.bucket_size recipe.trainer.strategy.ddp.data_parallel_sharding_strategy recipe.trainer.strategy.ddp.grad_reduce_in_fp32 recipe.trainer.strategy.ddp.num_distributed_optimizer_instances recipe.trainer.strategy.ddp.overlap_grad_reduce recipe.trainer.strategy.ddp.overlap_param_gather recipe.trainer.strategy.encoder_pipeline_model_parallel_size recipe.trainer.strategy.encoder_tensor_model_parallel_size recipe.trainer.strategy.expert_model_parallel_size=&lt;int&gt; recipe.trainer.strategy.expert_tensor_parallel_size=&lt;int&gt; recipe.trainer.strategy.fsdp recipe.trainer.strategy.num_distributed_optimizer_instances recipe.trainer.strategy.pipeline_model_parallel_size recipe.trainer.strategy.tensor_model_parallel_size recipe.trainer.strategy.virtual_pipeline_model_parallel_size recipe.trainer.use_distributed_optimizer TORCH_NCCL_AVOID_RECORD_STREAMS transformerLayerTPOverlapCfg.cga_size transformerLayerTPOverlapCfg.fp8_buf transformerLayerTPOverlapCfg.num_sm transformerLayerTPOverlapCfg.num_split previous Best Practices for NeMo Developers next Long Context Recipes On this page Low Precision Training Parallel Mapping Strategies Communication Overlaps and Tuning Communication Data Types Performance at Scale Lowering Host Overhead and Jitters Techniques for Reducing Memory to Avoid Memory Overflow and Enhance Training Efficiency Operator Fusion Long Sequence Training Sequence Packing for Performant Fine-Tuning GPU Core Clock Optimization Profiling Options for Analysis-based Performance Tuning Index - List of Tuning Knobs so the DOM is not blocked --> Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright Â© 2023-2025, NVIDIA Corporation. Last updated on Sep 03, 2025. ",
      "fetch_method": "direct-html"
    }
  ]
}