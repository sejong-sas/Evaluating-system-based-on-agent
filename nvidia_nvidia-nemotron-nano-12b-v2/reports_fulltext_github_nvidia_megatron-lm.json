{
  "repo": "NVIDIA/Megatron-LM",
  "full_texts": [
    {
      "arxiv_id": "https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat",
      "full_text": " docs: latest docs docs latest latest "
    },
    {
      "arxiv_id": "https://img.shields.io/badge/release-0.12.0-green",
      "full_text": " release: 0.12.0 release release 0.12.0 0.12.0 "
    },
    {
      "arxiv_id": "https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/",
      "full_text": " Turbocharge LLM Training Across Long&#x2d;Haul Data Center Networks with NVIDIA Nemo Framework | NVIDIA Technical Blog DEVELOPER Home Blog Forums Docs Downloads Training Join Technical Blog Subscribe Related Resources Data Center / Cloud English 中文 Turbocharge LLM Training Across Long-Haul Data Center Networks with NVIDIA Nemo Framework May 08, 2025 By Kyle Aubrey , Hao Wu , Dheevatsa Mudigere , Selvaraj Anandaraj and Wenwen Gao Like Discuss (0) L T F R E AI-Generated Summary Like Dislike The latest NVIDIA NeMo Framework 25.02 and NVIDIA Megatron-Core 0.11.0 releases enable multi-data center large language model training by introducing key innovations such as adaptive resource orchestration, Hierarchical AllReduce, distributed optimizer architecture, and chunked inter-data center communications. These advancements allow for high-efficiency training across geographically separated data centers, achieving 96% scaling efficiency when training a 340B parameter model across two data centers approximately 1,000 kilometers apart using thousands of NVIDIA GPUs. By leveraging NVIDIA GPU data center platforms and AI software stack, multi-data center training is poised to enable the creation of supercomputers that can harness over 500,000 GPUs across multiple facilities, enhancing reliability, flexibility, and energy efficiency. AI-generated content may summarize information incompletely. Verify important information. Learn more Multi-data center training is becoming essential for AI factories as pretraining scaling fuels the creation of even larger models, leading the demand for computing performance to outpace the capabilities of a single facility. By distributing workloads across multiple data centers, organizations can overcome limitations in power, cooling, and space, enabling the training of even larger, more accurate models with better efficiency.&nbsp; The latest release of NVIDIA NeMo Framework 25.02 and NVIDIA Megatron-Core 0.11.0 brings new capabilities for multi-data center large language model (LLM) training. This update enables users to scale training beyond the physical and operational limits of a single data center, unlocking unprecedented efficiency and performance by harnessing the combined power of multiple sites.&nbsp; In this post, we&#8217;ll cover how NeMo Framework and Megatron-Core are revolutionizing multi-data center training with these key advances: High efficiency across sites: Achieving 96% scaling efficiency by effectively distributing training across thousands of NVIDIA GPUs in geographically separated data centers. Advanced communication strategies: Overcoming inter-data center latency using hierarchical orchestration and gradient synchronization. Real-world success: Validating these innovations through the efficient training of a LLM with 340B parameters, paving the way for next-generation AI supercomputing. Why multi-data center training is hard Training trillion-parameter models isn’t just about adding more GPUs—it’s about overcoming key infrastructure challenges that influence cost and performance. When managing compute across multiple data centers, developers must contend with high inter-region latency (often 20 milliseconds or more) that can introduce performance bottlenecks during gradient updates and model synchronization during large-scale LLM training. Addressing these issues enables distributed LLM training architectures that boost performance, maximize hardware and energy efficiency, reduce infrastructure strain, and enable AI projects to scale across geographic regions without centralized resource bottlenecks. Key challenges include: High-latency and bandwidth limitations: Communication between data centers can be slow and constrained, reducing training efficiency.&nbsp;&nbsp; Synchronization: Keeping distributed data centers aligned demands sophisticated protocols and techniques. Traffic management: Minimizing data flow over long-haul networks is essential to maintaining low-latency and high-throughput. Enabling high-efficiency multi-data center training To overcome the challenges of multi-data center LLM training, NeMo Framework 25.02 and Megatron-Core 0.11.0 introduce four key innovations: Adaptive resource orchestration Hierarchical AllReduce (HAR) Distributed optimizer architecture Chunked inter-data center communications These capabilities optimize communication, orchestration, and compute efficiency across geographically separated sites, ensuring scalable, high-performance training of the world’s largest AI models. Adaptive resource orchestration Adaptive resource orchestration is a distributed training strategy that leverages the hierarchy of latency and bandwidth between various GPUs within and across clusters. It selects and prioritizes parallelism methods, which are resilient to communication delays and bandwidth constraints, making it ideal for cross-data center development deployments. In these setups, model-parallel techniques —such as tensor, context, and expert parallelism—demand frequent, high-bandwidth synchronization that doesn&#8217;t suit the high-latency environment between data centers. Instead, data parallelism and pipeline parallelism techniques are favored due to: Latency tolerance: Data parallelism’s batched gradient aggregation accommodates the larger delays inherent in inter-data center communications. Bandwidth efficiency: Hierarchical reduction patterns in data parallelism consolidate cross-data center traffic, significantly lowering bandwidth requirements. Hardware agnosticism: Both methods abstract hardware differences across sites through standardized sharding. By aligning the choice of parallelism technique with the network&#8217;s constraints, adaptive resource orchestration reduces the inter-data center bandwidth requirement per GPU to roughly 1/N of the intra-data center demand, yielding substantial efficiency gains over traditional flat approaches. Hierarchical all-reduce HAR involves synchronizing gradients in three steps:&nbsp; ReduceScatter within each group or data center,&nbsp; AllReduce across data centers. AllGather within each data center.&nbsp; This method minimizes traffic over long-haul networks by first optimizing inter-data center communication, ensuring high throughput and low latency.&nbsp; Figure 1 explains how HAR works. Figure 1. HAR explained Distributed optimizer architecture The partial-data parallel distributed optimizer enhances efficiency through localized weight updates and gradient reductions within each data center, followed by a single synchronized gradient reduction across sites, eliminating redundant optimizer state duplication while minimizing cross-data center communication. By sharding optimizer states within data centers (rather than globally) and replicating optimizer instances across sites, the architecture preserves memory efficiency at scale while reducing inter-data center traffic. Chunked inter-data center communications By splitting communications into chunks and overlapping those chunks with computation, inter-data center communication can be hidden behind intra-data center operations. This technique ensures that the training process remains efficient even at large scales, enabling high tolerance to latency between sites.&nbsp; Multi-data center training of NVIDIA Nemotron-4 340B&nbsp; Recently, we had an opportunity to run a large-scale training of Nemotron-4 340B .&nbsp;To set the baseline, the LLM was trained using a single data center with 3,072 NVIDIA GPUs.&nbsp;&nbsp; Next, the model was trained across two data centers, located approximately 1,000 kilometers apart, to demonstrate the effectiveness of these new features. As shown in Table 1 , the setup achieved over 96% of baseline throughput at a 3,072 GPU scale (1,500 GPUs in each data center), with independent inter- and intra-data center communications overlapping to maximize efficiency. By leveraging the capabilities of NeMo Framework and Megatron-Core, the training process achieved remarkable efficiency and scalability, setting a new standard for LLM development. Metric Single Data Center (ORD) Multi-Data Center (ORD + IAD) Total GPUs 3,072 GPUs 3,072 GPUs (1,536 in ORD, 1,536 in IAD) GPU Nodes 375 nodes (8 GPUs per node) 375 nodes (8 GPUs per node) Data Center Locations Oracle Cloud Infrastructure (OCI) – Chicago, IL (ORD) OCI – Chicago, IL (ORD) and Ashburn, VA (IAD) Distance Between Data Centers N/A Approximately 1,000 km Measured Round-Trip Latency N/A 21 milliseconds Scaling Efficiency Baseline (100%) Over 96% compared to single-site baseline Model FLOPS Utilization (MFU) 51% 49% Training Model Nemotron-4 340B Nemotron-4 340B Table 1. A comparison of the baseline compared to multi-data center training of Nemotron-4 340B Unleashing supercomputing across multiple facilities Multi-data center training is emerging as a transformative approach in AI factories, laying the groundwork for distributed systems that span several buildings and even regions. By integrating advanced networking and synchronization technologies, this approach coordinates vast arrays of GPUs across distinct facilities, ensuring that complex training tasks can run concurrently and seamlessly.&nbsp; NVIDIA GPU data center platforms , including low-latency networking solutions , and AI software stack, enable unprecedented parallelism. This full-stack platform paves the way for supercomputers that could eventually harness more than 500,000 GPUs across multiple data centers. This architecture not only scales computational power but also enhances reliability and flexibility by dynamically balancing workloads across multiple sites, reducing bottlenecks and optimizing energy efficiency.&nbsp; Get started today Support for training LLMs across multiple data centers is built into Megatron-Core , which is deeply integrated into NVIDIA NeMo Framework, an end-to-end platform for developing custom generative AI, including large language models (LLMs), vision language models (VLMs), retrieval models, video models, and speech AI—anywhere. It incorporates Megatron-Core for large-scale LLM training and provides a more extensive set of tools for building state-of-the-art custom generative AI, multimodal, and speech AI agentic systems. To learn more, see the NVIDIA NeMo Framework documentation and the GitHub examples repository . Discuss (0) Like Tags Data Center / Cloud | Models / Libraries / Frameworks | Cloud Services | NeMo | Nemotron | Intermediate Technical | featured | LLMs About the Authors About Kyle Aubrey Kyle Aubrey is the director of Technical Marketing at NVIDIA, where he leads initiatives in AI inference and training across NVIDIA accelerated computing platforms, including Hopper, Blackwell, Rubin, and beyond. With a passion for demystifying complex technologies, he empowers diverse audiences to harness the full potential of NVIDIA's cutting-edge solutions. Kyle holds a bachelor’s degree in Electrical Engineering from Rose-Hulman Institute of Technology and an MBA from Pepperdine University. View all posts by Kyle Aubrey About Hao Wu Hao Wu is a distinguished GPU architect at NVIDIA. He joined the NVIDIA Compute Architecture group in 2011 after finishing his Ph.D. at the Chinese Academy of Science. Hao’s technical focus has been low precision training, inference, and recently extreme scale LLM training. View all posts by Hao Wu About Dheevatsa Mudigere Dheevatsa Mudigere is a Distinguished Engineer in the NVIDIA Compute Architecture group, focusing on the application-driven co-design of large-scale AI systems. He and his team work on understanding current and future AI applications and developing HW/SW technology to enable more capable and efficient AI systems. Before NVIDIA, he worked on designing, building, and deploying production hyperscale AI systems. View all posts by Dheevatsa Mudigere About Selvaraj Anandaraj Selvaraj Anandaraj is a Deep Learning Performance Engineer working on accelerating Deep Learning workloads using NVIDIA hardware and software stacks. His recent work is focused on having a highly performant software stack to train and infer large language models at scale. He earned a Master’s degree from the University of Wisconsin-Madison with a specialization in Machine Learning systems. View all posts by Selvaraj Anandaraj About Wenwen Gao Wenwen Gao is a senior product manager for NeMo at NVIDIA, focusing on LLM training framework and microservices. Her past experience include LLM inference (NIM) and recommender systems (Merlin). She holds a B.S. in computer science from the University of Toronto and an M.B.A. from the MIT Sloan School of Management. View all posts by Wenwen Gao Comments Related posts NCCL Deep Dive: Cross Data Center Communication and Network Topology Awareness NCCL Deep Dive: Cross Data Center Communication and Network Topology Awareness Ensuring Reliable Model Training on NVIDIA DGX Cloud Ensuring Reliable Model Training on NVIDIA DGX Cloud New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility Networking for Data Centers and the Era of AI Networking for Data Centers and the Era of AI New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More Related posts Modeling Attacks on AI-Powered Apps with the AI Kill Chain Framework Modeling Attacks on AI-Powered Apps with the AI Kill Chain Framework How Quantization Aware Training Enables Low-Precision Accuracy Recovery How Quantization Aware Training Enables Low-Precision Accuracy Recovery Deploy Scalable AI Inference with NVIDIA NIM Operator 3.0.0 Deploy Scalable AI Inference with NVIDIA NIM Operator 3.0.0 How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M+ Token Context Workloads NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M+ Token Context Workloads L T F R E "
    },
    {
      "arxiv_id": "https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/",
      "full_text": " Train Generative AI Models More Efficiently with New NVIDIA Megatron&#x2d;Core Functionalities | NVIDIA Technical Blog DEVELOPER Home Blog Forums Docs Downloads Training Join Technical Blog Subscribe Related Resources Conversational AI English 中文 Train Generative AI Models More Efficiently with New NVIDIA Megatron-Core Functionalities Jul 12, 2024 By Erin Ho , Deepak Narayanan , Seonmyeong Bak , Zijie Yan , Shiqing Fan and Mikolaj Blaz Like Discuss (0) L T F R E AI-Generated Summary Like Dislike NVIDIA Megatron-Core is an open-source PyTorch-based library that provides GPU-optimized techniques and modular APIs for training large language models at scale, and has been used by companies like Reka AI and Codeium to train models efficiently. The latest version, Megatron-Core v0.7, supports multimodal training with the Large Language and Vision Assistant (LLaVA) pipeline, allowing model developers to blend multimodal datasets with determinism and reproducibility. Megatron-Core v0.7 also introduces improvements such as fast distributed checkpointing, training throughput optimization for mixture of experts models, and enhanced scalability features like fine-grained overlapping of data parallelism gradient all-reduce with the backward pass. AI-generated content may summarize information incompletely. Verify important information. Learn more First introduced in 2019 , NVIDIA Megatron-LM sparked a wave of innovation in the AI community, enabling researchers and developers to use the underpinnings of this open-source library to further large language model (LLM) advancements. Today, many of the most popular LLM developer frameworks have been inspired by and built using the Megatron-LM library, spurring a wave of foundation models and AI startups. Some of the most popular LLM frameworks built on top of Megatron-LM include Colossal-AI , Hugging Face Accelerate , and NVIDIA NeMo . To facilitate easy migration and enable researchers and model developers to access the latest research in distributed training, NVIDIA has recently revamped Megatron-LM . This resulted in NVIDIA Megatron-Core , an open-source PyTorch-based library with a collection of GPU-optimized techniques, cutting-edge system-level innovations, and modular APIs for training models at large scale. Megatron-Core continues to advance large-scale distributed training. This post highlights some of the recent advancements, including the new Large Language and Vision Assistant (LLaVA) pipeline for multimodal training. NVIDIA Megatron-Core Megatron-Core contains GPU-optimized techniques with cutting-edge system-level innovations. It abstracts these techniques into composable and modular APIs, providing full flexibility for framework developers and researchers to train custom transformers at scale on NVIDIA accelerated computing infrastructure. The Megatron-Core library offers the core building blocks for transformer models, such as attention mechanisms, transformer blocks and layers, normalization layers, and embedding techniques. Additional functionality, including activation recomputation and distributed checkpointing, is also natively built into this library.&nbsp; Popular LLM architectures such as GPT, BERT, T5, and RETRO can be efficiently built at large compute scales using Megatron-Core. Furthermore, Megatron-Core is compatible with all NVIDIA Tensor Core GPUs and can take advantage of the FP8 data format supported by the NVIDIA Hopper architecture to further boost compute throughput and reduce memory footprint. Megatron-Core has enabled customers like Reka AI and Codeium to train models at large scale. &#8220;Megatron-Core’s modular, composable design seamlessly integrates into our multimodal LLM architecture,” said Deyu Fu, a technical staff member at Reka AI. “With optimized GPU kernels and parallelism techniques, it helps us handle very large models and extensive contexts with ease, all while enabling dense and sparse training to scale efficiently at cluster levels.” “By using Megatron-Core, we&#8217;re able to stay on the frontier of techniques for training large language models by simply turning on a flag in the library,” said Devin Chotzen-Hartzell, a machine learning engineer at Codeium. “This enables us to focus on our differentiators in data and alignment.&#8221; Multimodal training is now supported in Megatron-Core&nbsp;&nbsp; With the introduction of visual instruction tuning , large multimodal models have garnered widespread interest from both researchers and industries. These models leverage various types of data to generate comprehensive and context-aware responses, using multiple sensory inputs to understand and interact with their environment. This advancement brings generative AI models closer to how humans process the world.&nbsp; We&#8217;re excited to announce that Megatron-Core v0.7 now supports multimodality. For a complete multimodality reference pipeline with LLaVA, visit NVIDIA/Megatron-LM on GitHub. Model developers can easily blend multimodal datasets with determinism and reproducibility using the open-source multimodal data loader under Megatron. This also works across checkpoint saves and loads. The LLaVA pipeline example walks you through how to:&nbsp; Prepare pretraining and supervised fine-tuning (SFT) datasets for Megatron webdataset-based formats. Leverage Megatron Core parallelism and memory-saving techniques to train a LLaVA architecture model initialized from Mistral and CLIP. Evaluate with different tasks like COCO captioning and VQAv2.&nbsp; The Megatron-Core v0.7 release focuses on functional aspects of the LLaVA pipeline, with a Massive Multidiscipline Multimodal Understanding (MMMU) score of 38, which is in the expected range for a 7B-parameter LLM-based LLaVA architecture. Additionally, with the Megatron-Core (Mcore) spec system , researchers can easily customize submodules in the PyTorch model definition. Within the next few releases, Megatron-Core will enable the use of heterogeneous parallelism strategies for different models. This approach is particularly beneficial because vision models, which are often smaller, typically require less complex sharding techniques compared to large language models in multimodal training. All Megatron-Core multimodal training capabilities, including the multimodal data loader , will soon be integrated into NVIDIA NeMo , enhancing the current multimodal features in NeMo for models like NeVa .&nbsp; Training throughput optimization for mixture of experts&nbsp; In the rapidly evolving landscape of generative AI, mixture of experts (MoE) models have become an attractive option, as they can be pretrained to achieve better accuracy without increasing the number of floating-point operations. In MoEs, the dense FFN layer is replaced with an MoE layer where each token is routed to a few experts, chosen by a router. Megatron-Core v0.7 expands MoE functionality and adds various training speed and memory optimizations, making Megatron-Core the most comprehensive solution for training MoEs at large scale. Specifically, Megatron-Core now supports MoE training with token dropping as used in GShard , and has training speed optimizations such as an enhanced GroupedGEMM with multi-CUDA stream computation and gradient accumulation fusion. Table 1 shows that Megatron-Core achieves throughput of over 400 TFLOP/s per-GPU throughput when training in BF16 precision, with an all-to-all dispatcher and sequence length = 4096. Each token is routed to (&#8211;moe-router-topk) two experts. We continue to optimize our FP8 recipes for MoE and will make them available in upcoming Megatron-Core releases. Megatron-Core also supports expert parallelism for MoEs, which can be combined with other parallelism techniques such as tensor, data, sequence, and pipeline parallelism already supported by Megatron-Core. For more details, see the User Guide . Model Precision # of GPUs MBS GBS TP EP PP Gradient accumulation Per-GPU throughput (TFLOP/s/GPU) Mistral 7B (Dense model baseline) BF16 128 4 256 2 N/A 1 1 492 Mixtral 8x7B BF16 128 1 256 1 8 4 8 402 Table 1. Per-GPU throughput for Mixtral 8x7B on NVIDIA H100 GPUs with dropless-token implementation in Megatron-Core v0.7 Fast distributed checkpointing for better training resiliency Distributed checkpointing is crucial for maintaining resiliency in large-scale training. The PyTorch native solution torch.save often lacks efficiency and scalability, leading to the development of more efficient solutions. For example, Azure Nebula and AWS Gemini offer asynchronous checkpointing, and the PyTorch Distributed Checkpoint (DCP) saves checkpoints per rank using threads. While these methods are faster than the vanilla `torch.save` by leveraging parallelism and asynchrony, challenges remain in achieving efficient asynchronous checkpointing.&nbsp; Specifically, these solutions perform asynchronous checkpointing either without parallelism within a multi-GPU server or by using Python threads (which can be inefficient due to the Python Global Interpreter Lock), leading to increased checkpointing times and lower training speed.&nbsp; These solutions also force users to load a checkpoint with the same parallelism configuration (PP and TP size, for example) used to store the checkpoint, preventing easy dynamic parallelism reconfiguration during a long training run. Megatron-Core v0.7 addresses these issues by introducing fully parallel and asynchronous saving capabilities. With fully parallel saving (FPS), data-parallel replicas perform parallel writes, enabling better utilization of the available file system bandwidth. Asynchronous parallel saving further speeds up distributed checkpointing by copying model parameters to the CPU (or local storage in the future) first before persisting the checkpoint to stable storage in the background, with minimal interruption to the main training process. Figure 1. Fully parallel saving in Megatron-Core uses the data-parallel replicas for parallel writing across nodes Most importantly, Megatron-Core enables users to resume training from a checkpoint saved with different tensor and pipeline parallelism degrees, providing the flexibility to change training configurations as needed during training.&nbsp; The save and load APIs in Megatron-Core are designed to be highly similar to PyTorch native APIs, making it easy to adopt Megatron-Core distributed checkpointing. When not using a distributed optimizer, this improvement reduces checkpointing overhead by 26x for Nemotron-4 340B compared to the native PyTorch solution, and by 50x for Nemotron-4 15B . With the distributed optimizer, users can achieve a 42x reduction in checkpoint overhead for Nemotron-4 340B (Figure 2).&nbsp; Figure 2. Checkpoint overhead comparison for a Nemotron-4 340B model across PyTorch native and the recent Megatron-Core releases&nbsp; Improved scalability Since the v0.5 release, Megatron-Core supports fine-grained overlapping of the data parallelism gradient all-reduce with the backward pass. This happens by grouping parameters into buckets and initiating asynchronous communication collectives for a bucket when all gradients for the bucket’s parameters are ready. This improves Megatron-Core throughput by reducing the amount of exposed data-parallel communication, and is especially useful when running configurations with a small batch size per GPU (and less gradient accumulation).&nbsp; Figure 3 shows per-GPU throughput for the Nemotron-4 15B model in a weak scaling experiment, where the batch size is increased as the data-parallel size is also increased (global batch size is 3*data_parallel_size), and a tensor-parallel size of 8. We observe that this optimization improves throughput by 34% with data-parallel size 32 and a batch size of 96. The --overlap-grad-reduce flag can enable this overlapping technique when using data parallelism. For more details, see the Megatron-Core documentation . Figure 3. Effect of the --overlap-grad-reduce optimization for Nemotron-4 15B using NVIDIA H100 GPUs and BF16 precision In the Megatron-Core v0.6 release, we introduced distributed optimizer support where the optimizer state is split over data-parallel replicas, reducing peak memory footprint. The distributed optimizer also breaks up the gradient all-reduce that was previously required into a gradient reduce-scatter (RS) and parameter all-gather (AG). Megatron-Core overlaps the reduce-scatter with the backward pass computation and the all-gather with the forward pass computation. These optimizations facilitate near-linear scaling for Nemotron-4 15B. Figure 4 shows per-GPU throughput for the 15B model with these optimizations enabled using a similar experiment setup to Figure 3. Figure 4. Effect of overlapping the reduce-scatter (RS) collective with the backward pass and all-gather (AG) collective with the forward pass for the Nemotron-4 15B model using NVIDIA H100 GPUs In addition, the v0.7 release further improves the speed of Megatron-Core at large data-parallel sizes by providing better in-built heuristics on how to size buckets. This enables communication to remain bandwidth-bound and not latency-bound even at DP sizes of &gt;300 (GPU count of &gt;3,000). Figure 5 shows per-GPU throughput for the 15B model up to a data-parallel size of 384 with the same weak scaling experiment setup and all optimizations enabled (distributed optimizer with both reduce-scatter and all-gather overlapping). Figure 5. Comparison of Megatron-Core 0.6 and 0.7 releases on Nemotron-4 15B using NVIDIA H100 GPUs up to a data-parallel size of 384 Megatron-Core optimizations also work out of the box for larger models that require other parallelism dimensions, including pipeline parallelism. The Nemotron-4 340B model uses these optimizations in Megatron-Core to achieve high training throughput at large GPU scales with BF16. Table 2 shows per-GPU throughput of Megatron-Core on the Nemotron-4 340B base model with different batch sizes. TP size is 8, PP size is 12, the number of virtual pipeline stages is 8, and sequence length is 4096. For more details, see the Nemotron-4 340B Technical Report . Note that other parallelism configurations can result in slightly higher throughputs. Precision # of GPUs (H100) Data-parallel size Batch size Per-GPU throughput (TFLOP/s/GPU) BF16 1536 16 768 419.3 BF16 3072 32 1536 418.3 BF16 6144 64 2304 405.0 Table 2. Per-GPU throughput of Megatron-Core on the Nemotron-4 340B base model with different batch sizes&nbsp; Get started Megatron-Core is available as open source in the NVIDIA/Megatron-LM repository on GitHub and can be used with Megatron-LM or NVIDIA NeMo . Megatron-LM, a lightweight training framework, offers a customizable native PyTorch training loop, ideal for users preferring fewer abstraction layers. It serves as a straightforward entry point for exploring Megatron-Core. For more details, see the Megatron-Core documentation . Megatron-Core is deeply integrated into NVIDIA NeMo , an enterprise-grade AI software platform with security, stability, manageability, and support. It incorporates Megatron-Core for LLM capabilities and provides a more extensive set of tools for multimodal and speech AI. To learn more, see the NVIDIA NeMo framework documentation . Discuss (0) Like Tags Conversational AI | Generative AI | HPC / Scientific Computing | NeMo | Intermediate Technical | Deep dive | featured | LLMs About the Authors About Erin Ho Erin Ho is the product manager for TensorRT quantization and Megatron-Core at NVIDIA, where her experience spans both training and inference. Her current focus is shaping the direction of NVIDIA's AI software to better serve the community. She holds an M.S. in computer science from National Tsing Hua University, complemented by a business degree from Carnegie Mellon University. View all posts by Erin Ho About Deepak Narayanan Deepak Narayanan is a senior applied deep learning research scientist in the ADLR group at NVIDIA, where he looks at making the training and inference of LLMs faster and more reliable. He holds a PhD in Computer Science from Stanford University. View all posts by Deepak Narayanan About Seonmyeong Bak Seonmyeong Bak is a senior system software engineer at NVIDIA, specializing in optimizing large-scale deep learning (DL) training performance on supercomputers. His expertise lies in analyzing and improving the communication and storage performance of DL training workloads. Before joining NVIDIA, Seonmyeong researched task-level parallel programming models for supercomputing. He holds a PhD in Computer Science from Georgia Institute of Technology and a bachelor’s degree in Computer Science from Sungkyunkwan University. View all posts by Seonmyeong Bak About Zijie Yan Zijie Yan is a senior DevTech engineer at NVIDIA, having joined the DevTech team in 2021. He specializes in improving the efficiency and scalability of large language model (LLM) training systems. Currently, Zijie drives the engineering initiatives for MoE support in Megatron-Core, where he collaborates closely with the team on the engineering development and performance enhancement of the MoE training system. Before joining NVIDIA, Zijie conducted research on communication optimization for distributed deep learning during his master's studies at Sun Yat-sen University. View all posts by Zijie Yan About Shiqing Fan Shiqing Fan is a senior architect in Compute Architecture at NVIDIA, where he works on improving the end-to-end performance of neural network training both at single-node scale and supercomputer scale. He received his M.S. and B.S. from Nanjing University. View all posts by Shiqing Fan About Mikolaj Blaz Mikolaj Blaz is a senior software engineer working on the efficiency of DL workloads at NVIDIA. He specializes in checkpointing, improving resiliency for large-scale LLM training. He received his M.Sc. in computer science from the University of Warsaw. View all posts by Mikolaj Blaz Comments Related posts NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility New NVIDIA NeMo Framework Features and NVIDIA H200 Supercharge LLM Training Performance and Versatility Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Framework Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Framework New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More New on NGC: SDKs for Large Language Models, Digital Twins, Digital Biology, and More NVIDIA AI Platform Delivers Big Gains for Large Language Models NVIDIA AI Platform Delivers Big Gains for Large Language Models Related posts How to Scale Your LangGraph Agents in Production From A Single User to 1,000 Coworkers How to Scale Your LangGraph Agents in Production From A Single User to 1,000 Coworkers Upcoming Livestream: Building Cross-Framework Agent Ecosystems Upcoming Livestream: Building Cross-Framework Agent Ecosystems Upcoming Livestream: Techniques for Building High-Performance RAG Applications Upcoming Livestream: Techniques for Building High-Performance RAG Applications New Video: Build Self-Improving AI Agents with the NVIDIA Data Flywheel Blueprint New Video: Build Self-Improving AI Agents with the NVIDIA Data Flywheel Blueprint Best-in-Class Multimodal RAG: How the Llama 3.2 NeMo Retriever Embedding Model Boosts Pipeline Accuracy Best-in-Class Multimodal RAG: How the Llama 3.2 NeMo Retriever Embedding Model Boosts Pipeline Accuracy L T F R E "
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2406.07887",
      "full_text": "An Empirical Study of Mamba-based Language Models\nRoger Waleffe1,2∗Wonmin Byeon1 Duncan Riach1 Brandon Norick1†\nVijay Korthikanti1 Tri Dao3,4 Albert Gu5,6 Ali Hatamizadeh1 Sudhakar Singh1\nDeepak Narayanan1 Garvit Kulshreshtha1 Vartika Singh1 Jared Casper1\nJan Kautz1 Mohammad Shoeybi1 Bryan Catanzaro1\n1NVIDIA\n2University of Wisconsin-Madison\n3Princeton University\n4Together AI\n5Carnegie Mellon University\n6Cartesia AI\nAbstract\nSelective state-space models (SSMs) like Mamba (Gu and Dao 2023) overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with sequence length\nand large inference-time memory requirements from the key-value cache. Moreover, recent studies\nhave shown that SSMs can match or exceed the language modeling capabilities of Transformers,\nmaking them an attractive alternative. In a controlled setting (e.g., same training data), however,\nstudies so far have only presented small scale experiments (training with <3B parameters and <1T\ntokens) comparing SSMs to equivalent Transformers. To understand the strengths and weaknesses\nof these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba,\nMamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to an 8B-parameter hybrid architecture consisting of 43% Mamba-2, 7%\nself-attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of natural language\ntasks, we answer the important question of whether Mamba models can match their Transformer\ncounterparts at larger training budgets. Our results show that while pure SSM-based models match\nor exceed Transformers on many tasks, both Mamba and Mamba-2 models lag behind Transformer\nmodels on tasks which require strong copying or in-context learning abilities (e.g., five-shot MMLU,\nPhonebook Lookup) or long-context reasoning. In contrast, we find that the 8B-parameter Mamba-\n2-Hybrid exceeds the 8B-parameter Transformer on all 12 standard tasks we evaluated (+2.65\npoints on average) and is predicted to be up to 8× faster when generating tokens at inference\ntime. To validate long-context capabilities, we provide additional experiments evaluating variants\nof the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequence\nlengths. On an additional 23 long-context tasks, the hybrid model continues to closely match\nor exceed the Transformer on average. To enable further study, we release the checkpoints as\nwell as the code used to train our SSM-based models as part of NVIDIA’s Megatron-LM project\n(https://github.com/NVIDIA/Megatron-LM)1.\n1\nIntroduction\nTransformer-based large language models (LLMs) (Vaswani et al. 2017) have become the dominant\nneural network architecture for natural language processing and have achieved impressive results across\na wide array of tasks (Achiam et al. 2023; Touvron et al. 2023). Much of the success of these models can\nbe attributed to their self-attention layers (Bahdanau, Cho, and Bengio 2014), which enable all-to-all\ninformation routing between tokens in a sequence, and their ability to improve with scaling model\nand dataset sizes. However, self-attention layers suffer from some drawbacks that make training and\ndeploying these models on long sequences challenging. At training time, the computation required for\nself-attention layers scales quadratically with the sequence length. At inference time, generating one\ntoken requires a memory capacity that scales linearly with the number of preceding tokens, necessitating\na large key-value cache to store the required state. Many recent works have attempted to address the\n∗Work done as an intern at NVIDIA\n†Correspondence to bnorick@nvidia.com\n1A fixed snapshot of the code used in this technical report is available at\nhttps://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba.\n1\narXiv:2406.07887v1  [cs.LG]  12 Jun 2024\n\nefficiency issues with self-attention layers (Tay et al. 2022); these works however have yet to match\nself-attention’s language modeling capabilities.\nStructured state space models (Gu, Goel, and Re 2021), in particular Mamba (Gu and Dao 2023)\nand more recently Mamba-2 (Dao and Gu 2024), have been proposed as a promising alternative to\nself-attention layers and Transformers. These models use constant computation and memory to generate\na single token at inference time (after initializing the SSM states based on the context) and can be\ncomputed efficiently using hardware-aware algorithms during training. They have been shown to match\nor exceed the downstream accuracy of Transformers on standard language modeling tasks for models up\nto 2.8B parameters (Dao and Gu 2024; Gu and Dao 2023). Follow up work has sought to further probe\nthe in-context learning abilities of these models at small scale (Park et al. 2024), and some recent work\nhas investigated combining Mamba layers with attention layers to form hybrid models (Glorioso et al.\n2024; Lieber et al. 2024). These works scale Mamba-based hybrid models beyond 7B parameters and\nshow that doing so can result in high quality models. However, in these studies the larger models were\nnot compared with equivalent Transformers in a controlled setting (i.e., same training data, parameter\ncount). Such controlled comparisons have been limited to small-scale experiments and larger-scale\nstudies of Mamba-2 models are still lacking.\nIn this technical report, we present a direct comparison between Mamba-based and Transformer-based\nLLMs trained on large datasets. In particular, our primary goal is to provide a rigorous apples-to-apples\ncomparison between Mamba, Mamba-2, Mamba-2-Hybrid (containing Mamba-2, attention, and MLP\nlayers), and Transformers for 8B-parameter models trained on up to 3.5T tokens, with the same\nhyperparameters. Using a diverse set of natural language tasks, we answer the important question\nof whether Mamba models can match their Transformer counterparts at larger training budgets. We\nevaluate these models on 35 popular downstream language modeling tasks and use the exact same\nevaluation setup for Mamba-based and Transformer models. To ensure our evaluations are standard\nand reproducible, we provide details about the specific open-source benchmark suites and versions used\nin our experiments in Section 2. Overall, our experiments eliminate the common difficulty of comparing\nLLMs, where it is often the case that both the model architecture but also the training data, tokenizer,\nand evaluation pipeline have changed.\nOur experiments show that while Mamba and Mamba-2 models are good at modeling language (e.g.,\nthey match or exceed Transformers on many downstream tasks), they lag behind Transformer models\nwhen it comes to in-context learning and recalling information from the context. This confirms recent\nfindings at smaller scales (Park et al. 2024). In particular, we highlight the difficulty pure SSM models\nface with the standard five-shot MMLU (Hendrycks, Burns, et al. 2020) and two-shot Phonebook tasks.\nFor the former, after training for 1.1T tokens, both Mamba and Mamba-2 models produce nearly 15\npoints lower accuracy when compared to a Transformer model on this task. While the MMLU accuracy\ngap is partially addressed by training with more tokens (e.g., 3.5T tokens), SSM models still lag behind\nTransformer models for this common benchmark. We find that Phonebook and standard long-context\nbenchmark tasks remain challenging for SSM models regardless of the number of training tokens.\nBased on the above findings, we study in detail the potential for hybrid SSM-Transformer models to\novercome the challenges faced by pure SSM architectures while retaining (some of) their inference-\ntime benefits. Similar to Lieber et al. 2024, we focus on LLMs consisting of a mixture of Mamba-2,\nself-attention, and MLP layers. Our ablation experiments aiming to identify the best hybrid model\narchitecture lead us to design an 8B-parameter Mamba-2-Hybrid with 24 Mamba-2 layers, 4 self-attention\nlayers, and 28 MLP layers. The self-attention and MLP layers are evenly distributed throughout the\nmodel. Extensive evaluations of this architecture show that it matches or exceeds Transformers on\ncommon natural language evaluations. When training for 3.5T tokens, a Mamba-2-Hybrid model\nexceeds a corresponding Transformer on all 12 short-context benchmarks we evaluated. On MMLU,\nthe hybrid model reaches a five-shot accuracy 3.5 points higher than the Transformer.\nWe also study long-context extensions of Mamba-2-Hybrid and the corresponding Transformer to\nsupport 16K and 32K context lengths. On 23 long-context evaluations, the 16K and 32K models closely\nmatch or exceed the Transformer baselines on average. Our results show that the hybrid models are\nparticularly good at retrieving, tracking, and aggregating information over long contexts. We highlight\nthree multi-document question answering tasks, however, which challenged the long-context hybrid\nmodels. We discuss potential reasons for these results and highlight areas of future work related to\n2\n\nX\nY\nSSM\ngroup norm\n×\nall reduce\ngroup norm\n×\nX\nY\nall reduce\nB, C\nconv1d\nprojection\nall reduce\nA\nB, C\nA\nconv1d\nSSM\nA\nX, B, C\nσ\nσ\nconv1d\nσ\nσ\nσ\nσ\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nprojection\nSSM×\n×\nSSM\nprojection\nconv1d\nprojection\nprojection\nMamba\nMamba-2\nA\nB, C\nA\nB, C\nA\nX, B, C\nX\nX\nσ\nσ\nFigure 1: Mamba and Mamba-2 blocks with tensor model parallel size two. Mamba requires two\nall-reduces per layer while Mamba-2 requires only one. More details can be found in Dao and Gu 2024.\nextending hybrid SSM-Transformer models to long sequence lengths.\nFinally we highlight that, due to our use of global attention without any explicit position encoding\nin these models, long-context Mamba-2-Hybrid models can generalize beyond their trained sequence\nlength. This is in contrast with recent hybrid models that use windowed attention and exhibit accuracy\ndegradation on contexts larger than the window size but less than the pretraining sequence length (De\net al. 2024). We find that a Mamba-2-Hybrid extended to support 128K contexts can perform the\nPhonebook lookup task perfectly even when the phone book contains more than 150K tokens.\nWe present our findings above to highlight the promise for larger-scale SSM-based models to provide\nfaster, more efficient language model inference without compromising training efficiency or model\naccuracy compared to Transformers. We hope that by releasing these results, the community is further\nexcited by the potential of Mamba-based LLMs. To help enable further adoption, we release the\ncode used to train our Mamba, Mamba-2, and Mamba-2-Hybrid hybrid models as part of NVIDIA’s\nMegatron-LM library (https://github.com/NVIDIA/Megatron-LM). We also release the model weights\nfor our Mamba-2 8B and Mamba-2-Hybrid 8B on Hugging Face.\n2\nPreliminaries\nIn this section, we discuss briefly our implementation of SSM layers in Megatron-LM and discuss the\ntraining data and evaluations used throughout this report.\n2.1\nModel Implementation\nTo support efficient large-scale training, we implement Mamba and Mamba-2 layers with support for\ntensor (Shoeybi et al. 2019), sequence (Korthikanti et al. 2022), and pipeline parallelism (Narayanan et al.\n2021) (only for Mamba-2). As described in Dao and Gu 2024, tensor-parallel support for Mamba layers\nrequires two all-reduces per block compared to just one all-reduce for Transformer layers (Figure 1),\nleading to increased communication overheads for training larger-scale Mamba models. Mamba-2 tensor\nparallel support, on the other hand, requires only one all-reduce per layer, but requires the use of\nGroupNorm rather than LayerNorm for the internal block normalization (see Figure 1).\nWe found that using GroupNorm lead to no difference in validation loss when compared to using full\nLayerNorm as long as the group size (the model hidden dimension divided by the number of groups)\nis sufficiently large to allow for accurate calculations of the per-group normalization statistics (in our\n3\n\nexperience this meant a group size greater than 256). To implement SSM-Transformer hybrid models,\nwe combine our Mamba or Mamba-2 layers with the existing self-attention and MLP layers supported\nin Megatron-LM. These layers support all the previously mentioned parallelization strategies enabling\nus to immediately train hybrid models with tensor, sequence, and pipeline parallelism.\n2.2\nTraining Data\nWe train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are\npredecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English,\nand 15% code. For additional details, refer to the discussion included in the Nemotron-4 technical\nreport (Parmar et al. 2024). We use a vocabulary of 256K tokens trained with SentencePiece (Kudo\nand Richardson 2018).\n2.3\nEvaluation Tasks and Setup\nWe now discuss the evaluations used throughout the paper. Wherever possible, we use open-source\nLLM benchmark suites to ensure our evaluations are standard and reproducible. We report results\nusing a large number of common tasks:\n• Standard Short-Context Tasks: We use the open-source LM Evaluation Harness library\n(commit 94cc1850) (L. Gao et al. 2023) to evaluate the following 12 tasks (metric used for\nevaluation reported in parentheses): WinoGrande (accuracy) (Sakaguchi et al. 2021), PIQA\n(accuracy) (Bisk et al. 2020), HellaSwag (normalized accuracy) (Zellers et al. 2019), ARC-\nEasy and ARC-Challenge (accuracy and normalized accuracy) (Clark et al. 2018), MMLU\n(accuracy) (Hendrycks, Burns, et al. 2020), OpenBookQA (normalized accuracy) (Mihaylov et al.\n2018), TruthFulQA (accuracy) (Lin, Hilton, and Evans 2021), PubMedQA (accuracy) (Jin et al.\n2019), and RACE (accuracy) (Lai et al. 2017). Each of the proceeding tasks are evaluated by\nmeasuring the probability returned by the model for each possible answer choice. We also use the\ngeneration-based tasks Natural Questions (NQ) (exact match) (Lee, Chang, and Toutanova 2019)\nand SquadV2 (F1) (Rajpurkar, Jia, and Liang 2018).\n• Natural Long-Context Tasks: To evaluate long-context models, as above, we use three tasks\nfrom LM Evaluation Harness: NarrativeQA (F1) (Kočisk`y et al. 2018), Qasper (F1) (Dasigi\net al. 2021), and QuALITY (normalized accuracy) (Shaham et al. 2022). The first two tasks are\ngeneration-based, while the latter uses continuation probabilities returned by the model for each\nanswer. Each of these three tasks requires the model to answer a given question based on a long\ninput document.\nWe also use six tasks from the LongBench (Bai et al. 2023) long-context evaluation benchmark\n(commit 48798083): MultiFieldQA-English (F1), HotpotQA (F1) (Yang et al. 2018), 2WikiMQA\n(F1) (Ho et al. 2020), Musique (F1) (Trivedi et al. 2022), TREC (accuracy) (Li and Roth 2002),\nand TriviaQA (F1) (Joshi et al. 2017). Each of these six tasks requires the model to generate the\nanswer. MultiFieldQA tests a model’s ability to perform single-document question answering\nwhile HotpotQA, 2WikiMQA, and Musique measure multi-document question answer capabilities.\nTREC and TriviaQA are used to measure a model’s ability to perform in-context learning over\nlong inputs.\n• Synthetic Long-Context Tasks: Finally, we also evaluate our models using synthetic tasks\nthat aim to measure a model’s ability to retrieve, track, and aggregate information across long\ninput texts. For these evaluations, we use the Phonebook task introduced in Jelassi et al. 2024\n(illustrated in Figure 2b) and 13 open-source, generation-based tasks in the RULER benchmark,\ndescribed explicitly in Appendix B of Hsieh et al. 2024. The RULER tasks consist of eight Needle\nIn A Haystack (NIAH) variants, one multi-hop tracing task called Variable Tracking (VT), two\nlong-context aggregation tasks (Common Words Extraction (CWE) and Keywords Extraction\n(KWE)), one single-document question answer task (SquadQA), and one multi-document question\nanswer task (HotpotQA). For all tasks, we report the accuracy on 400 synthetic samples generated\nby RULER.\n4\n\nTable 1: 8B-parameter Mamba, Mamba-2, and Transformer architectures used in the experiments.\nModel\nParams (B)\n# Layers\nModel Dim\nAttn. Heads\nState Dim.\n# Groups\nPos. Emb.\nSeq. Len\nTransformer\n8.53\n32\n4096\n32\n-\n-\nRope\n4096\nMamba\n8.15\n56\n4096\n-\n128\n-\nNone\n4096\nMamba-2\n8.24\n56\n4096\n-\n128\n8\nNone\n4096\n3\nMamba and Mamba-2 Compared to Transformers\nIn this section we discuss our observations and experimental results training 8 billion (8B) parameter\nMamba and Mamba-2 models and compare with 8B-parameter Transformer models. We find that\nMamba and Mamba-2 can match or exceed Transformers on standard zero-shot tasks (Section 3.3.1) but\nlag behind on MMLU and copying tasks, which we discuss in details in Sections 3.3.2 and 3.3.3.\n3.1\nModel Architectures\nWe train Mamba, Mamba-2, and Transformer models with the architectures summarized in Table 1.\nWe discuss the architectures in more detail next. Additional details can be found in the released model\ncheckpoints and open-sourced code in Megatron-LM.\nTransformer.\nOur 8B Transformer model follows the style of GPT3 (Brown et al. 2020) and\nconsists of 32 layers (each Multi-Head Attention + MLP) with a hidden dimension of 4096. We use 32\nattention heads, 128 KV-channels, a 4× expansion for the MLPs, SwiGLU activation (Shazeer 2020),\nLayerNorm (Ba, Kiros, and Hinton 2016), and RoPE (J. Su et al. 2024) for position embeddings. We\ndo not use bias weights for linear layers or Dropout. Additionally, we use seperate parameters for model\nembeddings and output layer weights (which we refer to as untied embeddings).\nMamba.\nWe train an 8B-parameter Mamba model with hidden dimension 4096 and 56 layers\n(typically 2 Mamba layers have around the same parameters as one block of attention + MLP). The state\ndimension for each Mamba layer is set to 128 and we use GELU (Hendrycks and Gimpel 2016) activation.\nFollowing (Gu and Dao 2023), we do not use any explicit position encoding and for normalization we\nuse RMSNorm (B. Zhang and Sennrich 2019). As for the Transformer, we do not use bias weights for\nlinear layers or Dropout and we use untied embeddings.\nMamba-2.\nFor Mamba-2 models, we use the same architecture as above for Mamba except replace\neach layer with the updated Mamba-2 block (Dao and Gu 2024). We set the internal Mamba-2 state\ndimension to 128 and use eight groups. We retain the default values from Dao and Gu 2024 and use a\nhead dimension of 64, expansion factor of two, and window size of four for convolution.\n3.2\nTraining Hyperparameters\nWe train the above models on 1.1T and 3.5T token datasets (see details in Section 2) using the following\nhyperparameters: On the smaller dataset, we use a batch size of 256, peak learning rate of 1e-4 and\nminimum learning rate of 1e-5. On the larger dataset we increase the batch size to 1024 and use higher\nlearning rates: a peak of 3e-4 and minimum of 3e-5. On both datasets we use learning rate warm up\nover 122K samples, a cosine learning rate schedule, weight decay of 0.1, 0.9 and 0.95 for Adam β1 and\nβ2 parameters respectively, and train using BF16. We performed some studies at smaller scale and\nfound that Mamba network hyperparameters are similar to that of Transformers and as a result we use\nthe same hyperparameters across models to make a rigorous direct comparison.\n3.3\nEmpirical Evaluation of Mamba and Mamba-2\n3.3.1\nDownstream Language Modeling Tasks\nIn Table 2 and 3 we report the results of training our 8B-parameter Mamba, Mamba-2, and Transformer\nmodels on 1.1T and 3.5T tokens respectively, using six standard tasks for measuring natural language\nunderstanding. On the 3.5T dataset, we train only a pure Mamba-2 model (and not a Mamba model)\n5\n\nTable 2: Evaluation results for 8B-parameter models trained on 1.1T tokens. Pure SSM models (Mamba\nand Mamba-2) match or exceed Transformers on many natural language tasks, but fall short on others\n(e.g., MMLU) (see Section 3.3).\nModel\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nAvg. w/o MMLU\nAvg\n0-Shot\n5-Shot\nTransformer\n69.22\n78.29\n75.6\n73.15\n43.09\n38.32\n46.28\n67.87\n60.56\nMamba\n68.27\n78.89\n75.63\n75.42\n42.15\n28.63\n28.00\n68.07\n56.71\nMamba-2\n70.8\n78.35\n75.54\n75.13\n43.00\n28.94\n29.19\n68.56\n57.28\nfor efficiency reasons—the pure Mamba model on the 1.1T dataset was almost 3× slower than the pure\nMamba-2 model due to the large state dimension.\nOur results confirm those of prior works (Dao and Gu 2024); both Mamba and Mamba-2 models can\nmatch or exceed Transformer models on common tasks. On both datasets, pure SSM models achieve\nhigher accuracy than Transformers when averaged over the WinoGrande, PIQA, HellaSwag, ARC-Easy,\nand ARC-Challenge evaluation tasks. The results on the 1.1T dataset also highlight that pure Mamba-2\nmodels are equal or better than Mamba models on average. The most interesting observation from these\nresults, however, is that the accuracy on MMLU is significantly worse for pure SSM models compared\nto the Transformer when training for the shorter token horizon (1.1T tokens). For example Mamba-2\nfive-shot accuracy is 17 points lower than that of the Transformer in this setting. Table 3 shows that\ntraining for more tokens helps the Mamba-2 model improve on MMLU (visualized in Figure 6), closing\nthe gap to the Transformer to just 1.37 points. We discuss the MMLU result in more detail in the\nfollowing section.\nTable 3: Evaluation results for 8B-parameter models trained on 3.5T tokens. We train only pure\nMamba-2 and Transformer models in this setting due to efficiency issues with training larger-scale\nMamba models. Training for 3.5T tokens instead of 1.1T (Table 2) allows Mamba-2 to approach\nTransformer-level accuracy on MMLU and produces a pure SSM model that exceeds the Transfomer in\nterms of average task accuracy.\nModel\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nAvg. w/o MMLU\nAvg\n0-Shot\n5-Shot\nTransformer\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n68.14\n62.35\nMamba-2\n71.59\n79.82\n77.69\n75.93\n48.12\n47.25\n48.7\n70.63\n64.16\n3.3.2\nA Closer Look at MMLU\nWe investigate the gap in MMLU accuracy between pure SSM models and Transformers by evaluating\nour 1.1T Mamba, Mamba-2, and Transformer models (where the gap is largest) on different instances\nof this task. Generally, MMLU accuracy is calculated by prompting the model with a question that\nincludes four answer choices labeled with the letters A, B, C, and D. The model is then shown each of\nthe four letters A, B, C, and D and the letter most likely to follow the prompt (measured by probabilities\noutput from the model) is taken as the model’s answer (Figure 2a). MMLU accuracy, however, can also\nbe measured by calculating the probability of the full answer choice following the prompt (which we\ncall a choice-text-in-targets variant) or using a cloze format. In the latter case, the model is prompted\nwith only the question (no answer choices are provided) and the text of each answer is used to calculate\nprobabilities.\nWe show the results of evaluating our 8B-parameter pure SSM and Transformer models trained on 1.1T\ntokens on the three formulations of MMLU described above in Table 4. While the pure SSM models\nstruggle with the standard and choice-text-in-targets formulations, they actually exceed the accuracy of\nthe Transformer in the cloze setting. This experiment, together with the MMLU results for Mamba-2\ntrained on 3.5T tokens (Table 3, Figure 6), highlight that the pure SSM models contain the\nsame knowledge as the Transformer, but that they require substantially more training to\nunderstand the format of the multiple choice questions in the first two settings. We hypothesize\n6\n\nQuestion\nWhich is a valid expression in Python 3.5?\nStandard\nChoices\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nTargets\nA\nB\nC\nD\nChoice text in targets\nChoices\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nTargets\nA. sort('ab')\nB. sorted('ab')\nC. \"ab\".sort()\nD. 1/0\nCloze\nChoices\nTargets\nsort('ab')\nsorted('ab')\n\"ab\".sort()\n1/0\n(a) Three different formats for MMLU multiple choice\nquestions. In all cases the model is first prompted with\na question. In the standard and ‘choice text in targets’\nvariants (but not ‘cloze’ variant), the prompt includes\nfour multiple choice answers following the question.\nThe correct answer is then calculated by measuring\nwhich of the four answers, represented in the target\nformat, is predicted to have the highest probability of\nfollowing the given prompt.\nStandard\nPhonebook\nRamona Thornton: 333-848-7744\nLillie Acevedo: 254-992-9261\nJake Barron: 130-894-2531\nConnor Savage: 783-474-346\nEmilio Burton: 258-542-2658\nFew-shot examples\nWhat is the phone number for Lillie Acevedo?\nAnswer: 254-992-9261\nWhat is the phone number for Emilio Burton?\nAnswer: 258-542-2658\nSuffix\nWhat is the phone number for Jake Barron?\nAnswer:\nAnswer\n130-894-2531\nReversed\nInstructions\nGiven a phone book with entries of the form:\nLillie Acevedo: 254-992-9261\nRamona Thornton: 333-848-7744\n... and so on.\nPlease find the phone number for Jake Barron.\nI will ask you for this phone number later.\nMemorize it so you can respond correctly.\nRemember Jake Barron.\nHere is the phonebook:\nPhonebook\nRamona Thornton: 333-848-7744\nLillie Acevedo: 254-992-9261\nJake Barron: 130-894-2531\nConnor Savage: 783-474-346\nEmilio Burton: 258-542-2658\nSuffix\nOkay, that is the end of the phonebook.\nRemember when I asked you to memorize the\nphone number for Jake Barron?\nWhat is the phone number for Jake Barron?\nAnswer\n130-894-2531\n(b) Two different versions of the synthetic Phonebook\ntask. In the standard configuration (left) the model is\nfirst prompted with a phone book. It is then shown two\nin-context example questions before the final question\nwhich asks it to recall the phone number for a specific\nperson.\nIn the ‘reversed’ setting (right) the model\nis told which phone number to look for before being\nshown the phone book and then later asked to recall\nthis number.\nFigure 2: Illustrations of the MMLU and Phonebook tasks used in the experiments.\nTable 4: MMLU accuracy for pure SSM and Transformer models according to the three task variants\ndescribed in Figure 2a. Results are for 8B-parameter models trained on 1.1T tokens. While pure SSM\nmodels struggle with the standard and choice-text-in-targets formulations on this token horizon, they\nmatch or exceed the Transformer model in the cloze setting. Together with the results for Mamba-2\ntrained on 3.5T tokens (Table 3, Figure 6), this suggests that SSM models contain the same knowledge\nas Transformers, but require more training to understand the MMLU task formatting.\nModel\nMMLU-Standard\nMMLU-W/Choice\nMMLU-Cloze\n0-Shot\n5-Shot\n0-Shot\n5-Shot\n0-Shot\n5-Shot\nTransformer\n38.32\n46.28\n33.54\n46.64\n37.26\n39.24\nMamba\n28.63\n28.00\n27.42\n29.17\n38.26\n39.28\nMamba-2\n28.94\n29.19\n28.54\n30.68\n37.68\n38.17\nthat the reason for this confusion, especially in the standard MMLU setting, is that pure SSM models\nare unable to directly route the knowledge of each answer into a single answer token. In contrast, the\nself-attention layers in the Transformer are particularly good at that task, especially when they are\nshown several in-context examples that teach them to do such routing (e.g., 5-Shot MMLU in the\nstandard formulation). Finally, we note that while the Mamba-2 hybrid model trained for 3.5T tokens\ncloses the MMLU gap to the Transformer, it sees an accuracy improvement on standard MMLU of\nonly 1.45 points when moving from 0- to 5- shot, compared with 4.38 for the Transformer, providing\nadditional evidence that Transformers may have superior in-context learning capabilities.\n3.3.3\nCopying Tasks\nBeyond downstream language modeling tasks, we also evaluate pure SSM-based models and compare\nto Transformers on the synthetic Phonebook task (Jelassi et al. 2024) that aims to measure a model’s\nability to perform in-context learning (through few shot examples) and copying from earlier in the\ncontext. We illustrate an example Phonebook prompt in Figure 2b. The model is first prompted with a\nlist of (name, phone number) pairs, and then asked ‘What is the phone number for {name}?’ with two\n7\n\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba\nMamba-2\nTransformer\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0\n2\n4\n6\n8\n10\n12\nAvg. # of Correct Tokens\nMamba\nMamba-2\n(b)\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba\nMamba-2\nTransformer\n(c)\nFigure 3: Evaluation results for pure SSM and Transformer models (trained for 1.1T tokens) on the\nPhonebook task illustrated in Figure 2b. (a) On the standard Phonebook task, Transformers are\ncapable of in-context learning and answering questions that require copying from the input, but SSM\nmodels struggle with this task. (b) In the standard Phonebook setting (i.e., (a)), SSM models exhibit\nfuzzy memory—while they are unable to correctly predict the phone number, they predict phone\nnumbers that share multiple digits (in the right locations) with the correct answer (see Section 3.3.3).\n(c) On the Reversed Phonebook formulation, even when notified at the beginning of the context which\nphone number they will be asked to recall, SSM models still lag behind Transformer models.\nexample question answer pairs before the actual question used for testing. For each trial, we randomly\ngenerate names and phone numbers to create the phone book and randomly select which names are\nused for the two examples and the final query. Accuracy on this task is then measured by whether the\nmodel generates the correct phone number or not.\nWe vary the length of the phone book (the number of (name, phone number) pairs) and plot the\naccuracy for each phone book length averaged over 20 different random initializations in Figure 3a. The\n8B Transformer model can respond correctly with near 100% accuracy for phone book lengths up to its\npretraining context length (4096). In contrast, both Mamba and Mamba-2 models begin to respond\nincorrectly for input sequence lengths beyond approximately 500 tokens. In contrast to MMLU, this\nbehavior persists for Mamba-2 even when training for 3.5T tokens (Figure 7a).\nA closer look at the SSM model predictions shows that while they cannot perfectly recall the correct\nphone number, these models have compressed information about each phone book entry into their\nrunning states—we show in Figure 3b the average number of correct tokens predicted by Mamba and\nMamba-2 on Phonebook by comparing the predicted answer to the true answer. Figure 3b shows that\npure SSM-based models have fuzzy memory. That is, while they cannot predict the phone number\nexactly, they do generally respond with phone numbers that are similar to the correct answer.\nFinally, we evaluate whether changing the Phonebook prompt allows for SSM models to achieve better\nresults. In particular, we prompt the model with the name of the person whose phone number it will\nbe asked to recall before showing it the phone book (the Reversed formulation in Figure 2b). Figure 3c\nshows the results of the 8B Mamba, Mamab-2, and Transformer models in this modified Phonebook\nsetting. Interestingly, while the SSM models achieve better accuracy as a function of phone book\nlength using this prompt, the accuracy still degrades for phone books with lengths shorter than 4096\n(the sequence length used for pretraining). Even with the modified Phonebook prompt, it remains\nchallenging for the SSM to decide which information to store exactly and which information to forget\non this task. We hypothesize that finetuning Mamba and Mamba-2 on the Phonebook task would lead\nto improved accuracy.\n3.3.4\nTakeaway\nOur experiments training 8B-parameter Mamba and Mamba-2 models showed that while these models\nachieve comparable or better accuracy than Transformers on many standard natural language modeling\ntasks, they achieve lower accuracy on others. In particular, we identified MMLU (with smaller token\nhorizons) and Phonebook as challenging tasks for pure SSM-based models and hypothesize that this is\nbecause these tasks require in-context learning, information routing between tokens, and copying from\nthe context.\n8\n\n4\nHybrid Mamba-Transformer Models\nMotivated by the difficulties pure SSM models face with retrieving information from the context and\nin-context learning, we now study the hypothesis that adding a few Transformer layers (made of\nself-attention and MLP layers) back into the architecture enables the model to overcome these issues.\nIn this section we consider hybrid models containing a combination of Mamba/Mamba-2, self-attention,\nand MLP layers.\n4.1\nDesigning a Hybrid SSM-Transformer Model\nWe begin by discussing the ablation studies that led us to design our final hybrid model architecture.\nFor the experiments reported in this section, all model variants have the same number of parameters\nper layer. This ensures that model quality changes are not due to an increase or decrease in the overall\nnumber of parameters, and also that we can control the ratio of parameters by controlling the ratio of\nlayers. To do so, we adjust both the number of attention heads (while keeping the head size constant)\nand the MLP expansion factor such that self-attention and MLP layers have approximately the same\nnumber of parameters as Mamba layers.\n0\n20\n40\n60\n80\n100\nPercentage of Attention Layers\n2.56\n2.58\n2.60\n2.62\nValidation Loss\nFigure 4:\nValidation loss versus per-\ncentage of attention layers for 130M-\nparameter hybrid Mamba-Transformer\nmodels (24 total layers).\nNumber of Attention and MLP Layers.\nWe first\nstudy how the number of self-attention and MLP layers in a\nhybrid model impacts model quality. For these experiments,\nwe train 130M parameter hybrid models with 24 layers, and\nvary the percentage of the those layers that are attention and\nthat are MLP. As we increase the percentage of these layer\ntypes, we evenly distribute them throughout the model, as\ndescribed in Appendix A. We report the validation loss as a\nfunction of the attention layer ratio in Figure 4. From these\nexperiments, we discover that validation loss is minimized\nwhen roughly 8% of the layers are self-attention layers. Ex-\nperiments with 840M parameter models confirm that these\nfindings scale across model sizes. These results are also\nconsistent with those reported by Dao and Gu 2024. After\nfixing the percentage of attention layers to 8, we vary the percentage of MLP layers between 5 and\n50. We conclude that 30%-50% of the layers can be MLPs without increasing model loss. In general,\nwe prefer larger MLP layer ratios from an efficiency perspective—with 50% of the layers set as MLPs,\ntraining is 20% faster than when MLP layers make up only 5% of the model.\nPosition Embeddings.\nWe next evaluate whether or not to add Rotary Position Embeddings\n(RoPE) (J. Su et al. 2024) to every self-attention layer in a hybrid model. For these experiments, we\ntrain an 840M-parameter Mamba-Hybrid model on the 1.1T token dataset with and without RoPE,\neach with an attention layer ratio of 8%. We use a 4096 sequence length. We then extend these base\nmodels to a context length of 16384 through continued pretraining on the longer sequence length\nfor an additional 16B tokens. We experiment with and without adjusting the RoPE base frequency\nduring continued pretraining (continued pretraining with an increase base frequency was introduced\nby Xiong et al. (2023)). Results are reported in Table 5. The base 840M model trained with RoPE\nprovides a similar accuracy to the model without RoPE, but achieves a lower average accuracy after\nlong context extension (regardless of whether the RoPE base frequency is modified or not). Based on\nthese experiments, as in recent work (Lieber et al. 2024), we opt to ignore RoPE position embeddings\nfor larger-scale hybrid models.\nAdditional Ablation Experiments.\nWe also evaluated how the ordering of Mamba/Mamba-2,\nself-attention, and MLP layers affects the resulting model’s natural language abilities (measured with\nvalidation loss). When testing, following Park et al. (2024), we made certain that a Mamba layer\nappears at the beginning of the architecture—this ensures that the hybrid model can operate without\nposition embeddings, as the first Mamba layer naturally learns to encode the positional information.\nOur experiments found no significantly better configuration than to evenly distribute self-attention\nand MLP layers throughout the model, as described in Appendix A. We did not find it necessary to\n9\n\nTable 5: Comparison of 840M parameter hybrid Mamba-Transformer models with and without RoPE\nposition embeddings when using an attention layer ratio of 8% and training on 1.1T tokens. We train a\nbase model with a sequence length of 4096. We extend this base model to a sequence length of 16384\nwith continued pretraining for an additional 16B tokens. We find that hybrid models do not need\nposition embeddings, and may perform better without them on long contexts.\nSeq.\nRoPE\nRoPE Freq. Base\nWinoGrande\nPIQA\nHellaSwag\nARC-E\nARC-C\nAvg\n4K\nno\n-\n56.2\n70.84\n52.13\n55.01\n26.79\n52.19\nyes\n10K\n55.8\n71.71\n52.3\n55.43\n26.71\n52.39\n16K\nno\n-\n56.83\n72.31\n53.86\n56.52\n27.65\n53.43\nyes\n10K\n54.62\n71.49\n53.17\n56.23\n27.56\n52.61\nyes\n500K\n54.7\n71.38\n50.42\n54.29\n26.79\n51.52\nTable 6: Summary of the 8B-parameter Mamba-2-Hybrid architecture used throughout the experi-\nments. We design the layer pattern to spread attention and MLP layers evenly throughout the model\n(Appendix A).\nModel\nParams (B)\n# Layers\nModel Dim\nAttn. Heads\nState Dim.\n# Groups\nPos. Emb.\nSeq. Len\nMamba-2-Hybrid\n8.66\n56\n4096\n32\n128\n8\nNone\n4096\nHybrid Layer Pattern (M=Mamba-2, *=Self-Attention, +=MLP)\nM+M+M++M+M*+M+M+M+M++M*+M+M+M+M+M*++M+M+M+M+M*+M++M+M+M+\nconstruct hybrid model architectures using a repeated block pattern. We also found that hybrid models\ncan use self-attention layers with Group-Query Attention (GQA) (Ainslie et al. 2023) rather than\nMulti-Head Attention (MHA) with little degradation in model quality (validation perplexity increases\n≈0.04%). Given the decrease in the amount of computation and memory required for inference with\nGQA compared to MHA, we thus opt to use GQA when training larger-scale hybrid models.\n4.2\nMamba-2-Hybrid 8B\nModel Architecture and Hyperparameters.\nBased on the study described in Section 4.1, we\ntrain an 8B-parameter hybrid SSM-Transformer model with the architecture summarized in Table 6.\nOut of 56 total layers, the hybrid model has 4 (7.1%) self-attention layers, 24 (42.9%) Mamba-2 layers,\nand 28 (50%) MLP layers. Rather than using a single repeated hybrid block structure, to construct our\nmodel we allocate the layers such that 1) a Mamba-2 layer comes first and 2) the attention and MLP\nlayers are evenly distributed throughout the model, as described in Appendix A. We use Mamba-2 for\nthe SSM-layers rather than Mamba, as the SSM scan used by Mamba-2 is up to 8× faster than that\nof Mamba (Dao and Gu 2024). Moreover, our experiments in Section 3.3, showed that 8B-parameter\nMamba-2 models match or exceed 8B-parameter Mamba models on common downstream natural\nlanguage tasks. For the Mamba-2 layers, we use the same parameters as for our pure Mamba-2 model\n(Section 3.1). That is, we use an internal state dimension of 128, eight groups, a head dimension of\n64, expansion factor two, and window size of four for convolution. For the attention layers, we use\nGroup Query Attention with eight groups, 32 attention heads, and 128 KV-Channels. For MLP layers,\nwe use a 4× expansion ratio. Throughout the model, we use a hidden dimension of 4096 and GELU\nactivation. We opt to use no explicit position embeddings. For each layer, we include a residual skip\nconnection and RMSNorm before the Mamba-2, self-attention, or MLP block. As for the pure SSM and\nTransformer models, we do not use Dropout, biases for linear layers, and we use separate parameters for\nmodel embeddings and output layer weights (i.e., untied embeddings). We train our Mamba-2-Hybrid\n8B on the 1.1T token and 3.5T token datasets using the hyperparameters described in Section 3.1 (i.e.,\nthe exact same ones as for the Transformer models and pure SSM models).\nTraining Efficiency.\nWe highlight that our Mamba-2-Hybrid model implemented in Megatron-LM\ncan be trained efficiently on thousands of GPUs. To do so, we compare our measured Model Flop\nUtilization (MFU) with that of Transformers. As in prior work (Korthikanti et al. 2022), we define the\nMFU as follows: First we define the model FLOPs per second to be the number of FLOPs required to\n10\n\nTable 7: Detailed evaluation results on 12 common natural language tasks comparing an 8B-parameter\nhybrid model (Mamba-2-Hybrid) with the pure Mamba-2 SSM and Transformer models from Section 3.3\nwhen training for 3.5T tokens. The Mamba-2-Hybrid model achieves the highest overall accuracy and\nis 2.65 points better than the Transformer on average.\nModel\nWG\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nOpenBook\nTruthFul\nPubMed\nRACE\nNQ\nSquadV2\nAvg\n0-Shot\n5-Shot\nTransformer\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n42.00\n35.48\n69.20\n39.52\n15.15\n53.4\n53.17\nMamba-2\n71.59\n79.82\n77.69\n75.93\n48.12\n47.25\n48.7\n44.2\n35.66\n75.2\n37.7\n17.17\n51.9\n54.69\nMamba-2-Hybrid\n71.27\n79.65\n77.68\n77.23\n47.7\n51.46\n53.60\n42.80\n38.72\n69.80\n39.71\n17.34\n58.67\n55.82\nperform a model forward and backward pass divided by the iteration time. We can then define the\nMFU to be the model FLOPs per second divided by the peak theoretical FLOPs per second of the\nGPUs used for training. When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel\nsize of four and data-parallel size of 256 (1024 total GPUs) (micro batch size 4, global batch size\n1024), our Mamba-2-Hybrid achieves an MFU of 29.9%. This can be compared to the 30.7% MFU of a\ncorresponding 8B parameter Transformer implemented in Megatron-LM and trained with the same\nparallelization configuration.\n0\n20\n40\n60\n80\n100\n120\nInput Context Length (Thousand Tokens)\n1\n2\n3\n4\n5\n6\n7\nInference Speedup (×)\nFigure 5: Predicted speedup to generate\none token for an 8B-parameter Mamba-2-\nHybrid model compared to a Transformer.\nInference Speed.\nWe also highlight that the hybrid\nmodel benefits from the inference-time speedups expected of\na pure SSM model compared to a pure Transformer model.\nIn Figure 5, we plot the predicted time to generate one\ntoken for the 8B Transformer model over the time for the 8B\nMamba-2-Hybrid model using a batch size of 32. For short\ninput context lengths, both models can generate the next\ntoken in roughly equivalent time. For long context lengths,\nhowever, the hybrid model benefits from its many SSM\nlayers and generates the content nearly 8× faster than the\nTransformer. We expect additional inference-time benefits\nfor the hybrid model due to a reduced key-value cache size\nthat should enable Mamba-2-Hybrid to use larger batch\nsizes than possible with the Transformer model.\n4.3\nEmpirical Evaluation of Mamba-2-Hybrid\n4.3.1\nDownstream Language Modeling Tasks\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTraining Completion (tokens)\n1e12\n30\n35\n40\n45\n50\n55\nMMLU Five Shot Avg Accuracy\nMamba-2-Hybrid\nMamba-2\nTransformer\nFigure 6:\nFive-shot MMLU accuracy\n(standard formulation) for 8B-parameter\nmodels trained on 3.5T tokens as a func-\ntion of training completion.\nWe evaluate the Mamba-2-Hybrid 8B model trained on the\n3.5T token dataset using downstream natural language tasks\nin Table 7. We include comparisons with the pure SSM and\nTransformer models discussed in Section 3.3. Remarkably,\nMamba-2-Hybrid achieves higher accuracy than the corre-\nsponding Transformer on all 12 common natural language\ntasks we evaluated (Table 7). The average improvement\non these tasks compared to the Transformer model is 2.65\npoints. Figure 6 shows the behavior of Mamba-2-Hybrid,\nMamba-2, and the corresponding Transformer for MMLU\naccuracy as training progresses. While Mamba-2-Hybrid\nalways leads pure Mamba-2, early in training the hybrid is\ninferior in this regard to the Transformer. However, once the\nhybrid matches the Transformer’s accuracy at 1.5T tokens it\nquickly gains and maintains a strong advantage. We believe\nthis motivates further study into the data efficiency and\nsaturation behavior of Mamba-based models.\nTable 7 and Figure 6 show that when training on sufficiently long token horizons (in our case 3.5T\ntokens), a hybrid model containing Mamba-2, self-attention, and MLP layers can exceed the accuracy\n11\n\nTable 8: Standard task evaluation results of 16K and 32K sequence length variants of the 8B-parameter\nMamba-2-Hybrid and Transformer model trained on 3.5T tokens. These tasks do not require long-\ncontext abilities but they show that the accuracy on these tasks does not degrade for the long-context\nmodels when compared to the base 4K models.\nModel\nWG\nPIQA\nHellaSwag\nARC-E\nARC-C\nMMLU\nOpenBook\nTruthFul\nPubMed\nRACE\nNQ\nSquadV2\nAvg\n0-Shot\n5-Shot\nTransformer-4K\n69.14\n78.62\n75.89\n73.27\n43.77\n45.69\n50.07\n42.00\n35.48\n69.20\n39.52\n15.15\n53.4\n53.17\nMamba-2-Hybrid-4K\n71.27\n79.65\n77.68\n77.23\n47.7\n51.46\n53.60\n42.80\n38.72\n69.80\n39.71\n17.34\n58.67\n55.82\nTransformer-16K\n70.4\n78.67\n76.4\n74.45\n44.37\n47.48\n51.22\n42.2\n36.33\n69.8\n38.95\n14.29\n55.38\n53.84\nMamba-2-Hybrid-16k\n71.67\n79.92\n78.24\n77.95\n48.12\n52.01\n54.92\n44.60\n37.23\n70.00\n39.33\n18.50\n58.99\n56.27\nTransformer-32K\n69.22\n78.51\n76.01\n73.74\n43.09\n47.80\n50.42\n41.60\n36.28\n69.40\n38.66\n15.79\n54.93\n53.50\nMamba-2-Hybrid-32K\n71.43\n79.54\n78.08\n78.07\n47.70\n52.41\n55.09\n45.40\n37.86\n71.00\n40.10\n18.64\n57.93\n56.4\nof a pure Mamba-2 and a pure Transformer model when averaged over a wide variety of downstream\nnatural language tasks. These results provide exciting evidence for the capability of hybrid models to\nprovide faster LLM inference and greater model quality when compared to Transformers.\n4.3.2\nLong-Context Evaluation\nIn this section, we evaluate the long-context ability of hybrid SSM-Transformer models by training\ntwo Mamba-2-Hybrid 8B extensions—a 16386 (16K) and 32768 (32K) variant—and compare to\ncorresponding extensions of the 8B Transformer. We extend the base models (pretrained using sequence\nlengths of 4096) to 16K and 32K versions through continued pretraining on the respective larger context\nlengths. We use full global attention in the four self-attention layers. In this initial study, we use\nthe same underlying data as in our 3.5T dataset. That is, we do not explicitly select a data subset\nconsisting of long documents, but rather use packed sequences to generate 16K and 32K inputs for\nthe model. All long context models are trained for an additional 50B tokens with a learning rate that\nincreases linearly over the first 1.7B tokens and then decays according to cosine annealing thereafter.\nWe use a max learning rate of 3e-5 and minimum learning rate of 3e-6. For the Transformer extensions,\nwe automatically adapt the RoPE base frequency to the longer context lengths using the dynamic NTK\nscaling described in bloc97 (2023).\nResults on Standard Short-Context Tasks.\nWe first evaluate the 16K and 32K Mamba-2-Hybrid\nand Transformer models on the 12 standard natural language tasks used above. While these tasks\ndo not require long-context abilities, we aim to check whether model accuracy degrades on common\ntasks as a result of extending our models to long-context variants. Results are reported in Table 8. On\naverage, we find no accuracy decrease on these tasks for the long-context variants. In fact, the 16K and\n32K models slightly improve compared to the base models which is due to the 16K and 32K models\nseeing 1.4% more data. As for the original 4K evaluations, the 16K and 32K Mamba-2-Hybrid is more\nthan 2 points better than the corresponding Transformer models on average.\nResults on Natural Long-Context Tasks.\nWe now focus on evaluating the 16K and 32K model\nextensions on tasks which require natural language reasoning across long contexts. Results when\nevaluating these models on nine common long-context tasks are shown in Table 9. In this setting,\nthe base (4K) Mamba-2-Hybrid and Transformer models achieve similar accuracy on most tasks and\nare more than 6 points better than the pure Mamba-2 model. For both architectures, the 16K and\n32K variants improve over the base models by an average of roughly 4 points. This is particularly\ndue to a large accuracy increase on tasks with many long inputs (e.g., NarrativeQA). Comparing the\n16K and 32K Mamba-2-Hybrid to the corresponding 16K and 32K Transformer, we observe that the\nTransformer models separate from the hybrid models on some tasks, particularly Multi-Document\nQuestion Answering tasks (e.g., HotpotQA). This leads the 16K and 32K Transformer models to reach\napproximately one point higher average accuracy than the 16K and 32K Mamba-2-Hybrid models\nrespectively.\nWe hypothesize that the hybrid model reaches lower accuracy than the Transformer on these tasks\nbecause the SSM layer states are sometimes confused by documents irrelevant to the question (which is\nunknown until the end of the sequence)—The Muti-Document Question Answering tasks in Table 9 are\ntaken from the LongBench evaluation suite which generates long-context inputs by concatenating the\n12\n\nTable 9: Natural long-context evaluation results for 8B-parameter base and long-context extensions of\nMamba-2-Hybrid and Transformer models trained on 3.5T tokens. While the 4K models are comparable\non these tasks, the 16K and 32K Transformer is roughly 1 point better on average when compared to\nthe 16K and 32K Mamba-2-Hybrid. These improvements come largely on Multi-Document Question\nAnswering tasks from LongBench (Bai et al. 2023).\nModel\nSingle Doc. QA\nMulti-Doc. QA\nFew-Shot Learning\nNarrativeQA\nQasper\nMultiFieldQA\nQuALITY\nHotpotQA\n2WikiMQA\nMusique\nTREC\nTriviaQA\nAvg\n(avg, max) Ctx. Len:\n(86K, 506K)\n(4.9K, 22K)\n(7.2K, 17K)\n(6.5K, 9.8K)\n(13.3K, 19K)\n(7.5K, 17K)\n(16.3K, 18K)\n(7.1K, 11.9K)\n(12.3K, 25K)\nMamba-2\n22.53\n25.74\n29.26\n33.84\n29.99\n24.6\n11.1\n54\n77.77\n34.31\nTransformer-4K\n23.44\n28.51\n38.39\n36.48\n36.28\n33.48\n17.68\n68\n83.65\n40.66\nMamba-2-Hybrid-4k\n24.53\n28.75\n39.01\n35.6\n36.24\n32.91\n15.24\n68.5\n86.7\n40.83\nTransformer-16K\n27.51\n29.71\n41.13\n39.02\n48.61\n34.87\n21.42\n78.5\n86.39\n45.24\nMamba-2-Hybrid-16k\n29.76\n30.93\n40.9\n38.93\n42.17\n31.34\n23.32\n74\n90.05\n44.6\nTransformer-32K\n30.06\n29.09\n40.61\n39.17\n48.2\n35.52\n24.55\n76.5\n86.43\n45.57\nMamba-2-Hybrid-32K\n31.56\n30.55\n40.69\n38.88\n41.9\n29.06\n21.33\n74.5\n89\n44.16\nfew documents from each task needed to answer the question (e.g., HotpotQA questions contain two\nparagraphs and then a question which requires knowledge from both) with many random paragraphs\nsampled from Wikipedia. This confusion could be due to our continued pretraining recipe which simply\npacks unrelated sequences together to make 16K and 32K inputs, potentially leading the SSM layers\nto believe separate documents are related when they are not. While this recipe is widely used for\nTransformers, it may not directly apply to hybrid models.\nBased on our experience evaluating these tasks, we also note that hybrid models may be more sensitive\nto prompt formatting than Transformer models. As evidence to support this hypothesis, we found\nthat minor prompt modifications could change the results for both models, but more so for the hybrid\nmodel. For example, on Musique, prompt modifications led the accuracy for the Mamba-2-Hybrid-4K\nmodel to fall in the range [10.63, 16.16]. In contrast, the accuracy for the Transformer was relatively\nsteady, remaining in the range [15.25, 17.68]. We highlight, however, that the prompt format for the\nmajority of the tasks in Table 9 (e.g., the Multi-Document QA tasks, see Section 2) are taken from the\nLongBench evaluation suite (Bai et al. 2023) and have been optimized for Transformer models. As a\nresult of these observations, we believe interesting areas of future work involve further study on the\nprompt robustness of hybrid models and comparing aligned and instruction-tuned hybrid models to\ntheir Transformer counterparts.\nResults on Synthetic Long-Context Tasks.\nBeyond the natural long-context tasks discussed\nabove, we also evaluate the 16K and 32K hybrid and Transformer extensions on the synthetic tasks\nin the RULER (Hsieh et al. 2024) benchmark suite. These tasks expand upon the basic Needle In\nA Haystack (NIAH) problem where the model is asked to recall information (the needle) from long\ninputs of otherwise irrelevant text. RULER also includes tasks which require tracing and aggregating\ninformation across the context. For these evaluations, the task context lengths are set to 4K for the\nbase models, 16K for the 16K extensions, and 32K for the 32K models.\nResults on the 13 RULER tasks are shown in Table 10. Overall, the Mamba-2-Hybrid models show\nsignificantly improved NIAH abilities compared to the Transformer models and pure Mamba-2 model.\nFor example, The 16K hybrid model achieves 13 points higher average accuracy on these tasks compared\nto the 16K Transformer. The long-context Transformer models are particularly challenged by the\nVariable Tracking (VT) task. This task includes a one shot demonstration of the task in the context\nand a closer inspection of the model predictions shows that the Transformer tends to directly copy\nthe answer for the in-context example instead of predicting the output of the actual question. This\nbehavior is consistent with prior observations for LWM-7B and Yi-34B models on these tasks (Hsieh\net al. 2024). Interestingly, while the hybrid model is generally better on most tasks, the Transformer\nconsistently reaches higher accuracy on Keywords Extraction (KWE).\nWe also observe that the hybrid model reaches higher accuracy than the Transformer on the HotpotQA\ntask, which contrasts the behavior in Table 9 when running HotpotQA using the LongBench evaluation\nsuite. As described above, while the latter benchmark constructs long context HotpotQA questions by\nadding random Wikipedia passages to the relevant information, RULER extends the context length\nof HotpotQA by adding paragraphs randomly sampled from HotpotQA itself. This slight difference\n13\n\nTable 10: Evaluation results of 8B-parameter Mamba-2-Hybrid and Transformer models trained for\n3.5T tokens, plus their long-context extensions, on the synthetic RULER long-context benchmark\nsuite (Hsieh et al. 2024). Mamba-2-Hybrid models achieve higher accuracy than Transformers on these\ntasks, highlighting their improved ability to recall, trace, and aggregate information across long inputs.\nModel\nSynthetic Tasks\nNeedle-In-A-Haystack Tasks (NIAH)\nHotpotQA\nSquadQA\nCWE\nVT\nKWE\nNIAH-1\nNIAH-2\nNIAH-3\nMK-NIAH-1\nMK-NIAH-2\nMK-NIAH-3\nMV-NIAH\nMQ-NIAH\nAvg\nMamba-2\n31.75\n35.5\n32.87\n76.45\n76.58\n100\n98\n83\n40.25\n13.75\n5.5\n35\n49.12\n52.14\nTransformer-4K\n42.25\n56.5\n36.42\n79.2\n76.33\n100\n100\n75.25\n99.5\n94\n58\n96.56\n95.06\n77.62\nMamba-2-Hybrid-4k\n48.75\n56.5\n32.2\n90.55\n65.58\n100\n100\n95.75\n89.5\n95.5\n96\n97.94\n97.62\n81.99\nTransformer-16K\n37.25\n45\n3\n7.25\n58.33\n100\n99.5\n75.75\n93.75\n56.5\n57.5\n88.5\n87.94\n62.33\nMamba-2-Hybrid-16k\n44\n48.75\n12.88\n83.2\n46.83\n100\n100\n81.5\n92\n92.25\n83\n89.81\n90.19\n74.19\nTransformer-32K\n35\n41.5\n4.42\n0.35\n53.5\n100\n99\n76.5\n70.75\n57.75\n41.25\n69.69\n84.69\n56.49\nMamba-2-Hybrid-32K\n38.5\n41.75\n8.4\n79.9\n36.5\n100\n100\n96.75\n84\n76.5\n81.5\n84.31\n80.94\n69.93\n0\n1\n2\n3\n4\n5\n6\n7\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nMamba-2\nTransformer\n(a) 4K base models\n0\n5\n10\n15\n20\n25\n30\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nTransformer\n(b) 16K models\n0\n10\n20\n30\n40\n50\n60\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba-2-Hybrid\nTransformer\n(c) 32K models\nFigure 7: Phonebook evaluation results for 8B-parameter models trained on 3.5T tokens and their\nlong-context extensions. We use the standard Phonebook formulation (Figure 2b). Mamba-2-Hybrid\nmodels can generalize beyond their pretraining sequence length and perform the Phonebook task on\ncontexts longer than pure SSM or Transformer models. (a) Base 4K Mamba-2, Mamba-2-Hybrid, and\nTransformer model Phonebook evaluations. (b) 16K long-context extensions of the Mamba-2-Hybrid\nand Transformer model evaluated on Phonebook. (c) Phonebook evaluations for the Mamba-2-Hybrid\nand Transformer models extended to support 32K sequence lengths.\nin the distribution used for context creation seems to confuse the hybrid model in one case (i.e.,\nLongBench HotpotQA (Table 9), but not in the other (i.e., RULER HotpotQA (Table 10) and provides\nan interesting area of future study.\nResults on Copying Tasks: Phonebook.\nFinally, we evaluate the long-context hybrid and\nTransformer models on the synthetic Phonebook task (Section 3.3.3, Figure 2b). We use the standard\nformulation which tests a model’s ability to perform in-context learning and copying from the context.\nResults for the base 4K models trained on 3.5T tokens are shown in Figure 7a. In this Figure, we also\ninclude the results for the pure Mamba-2 model trained on 3.5T tokens. As highlighted for pure Mamba\nmodels trained on 1.1T tokens (Section 3.3.3), the Mamba-2 model is unable to accurately predict the\nrequired phone numbers for sequences >1000 tokens. In contrast, the Transformer and Mamba-2-Hybrid\ncan do the Phonebook task with near perfect accuracy up to the pretraining context length (4K).\nIn fact, the hybrid model can generalize slightly beyond this sequence length, achieving 100 percent\naccuracy on Phonebook up to 5.5K tokens. Similar results hold for the long-context models (Figure 7b\nand Figure 7c). Both the 16K and 32K Mamba-2-Hybrid extensions can perform the Phonebook task\nperfectly beyond their trained context length. The long-context Transformer models, however, start to\nmake mistakes as the phone book length approaches their trained context lengths. In Griffin (De et al.\n2024), the authors make similar observations, finding that their Transformer baseline slowly degrades\nas it approaches the training context length and that hybrid architectures show near-perfect accuracy\nup to their attention window size. As with the RULER evaluations above, these experiments highlight\nagain the strong ability for hybrid models to perform in-context learning and to retrieve information\nfrom a long context.\nA 128K Mamba-2-Hybrid Model.\nWhile we focused in this section on evaluating 16K and 32K\nMamba-2-Hybrid long-context extensions and comparing them to corresponding Transformer models,\n14\n\nwe now show that the hybrid architecture can extend to context lengths well beyond 32K. We extend\nthe base 4K Mamba-2-Hybrid model to a sequence length of 128K through continued pretraining as\ndescribed above, using full global attention for the four self-attention layers. This training required only\ntensor and pipeline parallelism in Megatron-LM to prevent out-of-memory issues. We report results for\nthis model on the Phonebook task in Figure 8. As for the 4K, 16K, and 32K Mamba-2-Hybrid models,\nthe 128K model is able to do this task perfectly up to and beyond the sequence length it was trained\non. This experiment highlights the promising potential for extending hybrid models to long context\nlengths.\n0\n50\n100\n150\n200\nPhonebook Length (Thousand Tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nMamba2-Hybrid\nFigure 8:\nPhonebook accuracy (stan-\ndard setting) for an 8B Mamba-2-Hybrid\ntrained on 3.5T tokens and extended to\n128K sequence length through continued\npretraining for 50B tokens.\nTakeaway.\nWe have presented a detailed evaluation of\nlong-context 8B-parameter Mamba-2-Hybrid models and\ncompared them with their Transformer counterparts. Over-\nall, the hybrid models match or exceed the long-context\ncapabilities of the Transformers in most tasks. This is par-\nticularly true for tasks like Phonebook and the Needle In A\nHaystack (NIAH) present in the synthetic RULER bench-\nmark. We have identified, however, a few tasks where the\nhybrid models failed to reach Transformer-level accuracy\n(e.g., Multi-Document Question Answering in the Long-\nBench evaluation suite). We encourage further research\ninto these settings and into long-context versions of hybrid\nSSM-Transformer architectures.\n5\nRelated Work\nRecent work has also introduced Mamba-Attention hybrid models to improve accuracy and efficiency\ncompared to pure Mamba and Transformers. Park et al. (2024) show the limitations of Mamba on\nin-context learning (ICL) tasks and propose a hybrid model to improve the ICL accuracy. Their\nexperiments, however, are isolated to ICL tasks, and the model size is small (up to 77M parameters).\nJamba (Lieber et al. 2024) and Zamba (Glorioso et al. 2024) train Mamba-Attention hybrid models\nat 7B scale. Both show that their hybrid models significantly improve inference speed and GPU\nmemory compared to other models including Llama (Touvron et al. 2023) and Mistral-7B (Jiang\net al. 2023). Jamba improves the model accuracy and efficiency by adding Mixture-of-Experts (MoE),\nwhich increases the total model capacity (52B total parameters) but not its active parameters. They\ncompare their hybrid architecture with pure Mamba and Transformer models on four standard and three\nlong-context tasks, but only using 1.3B parameter models trained for 250B tokens, or 7B parameter\nmodels trained for 50B tokens. Zamba introduces a shared attention module and uses an annealing\nphase during training with high-quality datasets, which boosts the quality of their hybrid model. We\nfocus on combining Mamba, attention, and MLP layers into hybrid models for direct comparison with\nTransformer baselines at larger scales (>7B parameters and >1T tokens).\nOther recent work introduces hybrid models that mix either linear RNNs or convolutions with attention.\nDe et al. (2024) introduce a hybrid model that blends gated linear recurrences with local (sliding\nwindow) attention and show that the hybrid model can improve next token prediction latency with\nincreasing context length. They train 1B parameter models on 8K sequence lengths for long-context\nmodeling. Arora et al. (2023) report that a simple convolution-attention hybrid model outperforms pure\nattention in multi-query associative recall problems while reducing total FLOPs. Several additional\nworks add SSM layers to the Transformer architecture to increase accuracy: Saon, Gupta, and Cui\n(2023) use SSM layers together with Transformers to improve speech recognition quality. Pilault et al.\n(2024) combine an SSM and a block-wise Transformer at every layer. They show improved perplexity\nand generalization capabilities for longer sequences (up to 65K). Their model is scaled up to 1.3B\nparameters. We note that all the hybrid models mentioned above have manually designed architectures\nand place an MLP layer (if they use MLP) after each attention layer, similar to Transformers. Our\nstudy, however, finds that a specific hybrid architecture design or pattern is not required. Instead, the\nrelative proportions of each type of hybrid component appears to be the key factor that determines the\nquality of the model.\n15\n\n6\nConclusion\nTo address the question of whether SSM models can match the accuracy of Transformers at larger training\nbudgets, in this report we presented a direct experimental comparison between 8B-parameter Mamba,\nMamba-2, Mamba-2-Hybrid, and Transformer models trained on up to 3.5T tokens. Our experiments\nshowed that pure SSM models match or exceed the capabilities of their Transformer counterparts on\nmost downstream tasks but are challenged by tasks that require context-based information retrieval\n(e.g., copying) and in-context learning. We also showed that hybrid SSM-Transformer models (Mamba-\n2-Hybrid) reach higher accuracy than Transformers on all common benchmarks we evaluated. Further,\nthese hybrid models continue to show strong capabilities compared to Transformers when extended\nto 16K and 32K contexts. Based on these results, we are encouraged by the potential for SSM-based\nmodels to deliver inference-time speedups without accuracy degradation compared to Transformer\nmodels. We look forward to future work focusing on how hybrid models can make use of the large\necosystem of frameworks, methods, and libraries currently tailored to the large-scale training and\ninference of Transformers.\nReferences\n[1]\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. “GPT-4\nTechnical Report”. In: arXiv preprint arXiv:2303.08774 (2023).\n[2]\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and\nSumit Sanghai. “GQA: Training Generalized Multi-Query Transformer Models from Multi-head\nCheckpoints”. In: arXiv preprint arXiv:2305.13245 (2023).\n[3]\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri\nRudra, and Christopher Ré. “Zoology: Measuring and Improving Recall in Efficient Language\nModels”. In: arXiv preprint arXiv:2312.04927 (2023).\n[4]\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. “Layer Normalization”. In: arXiv\npreprint arXiv:1607.06450 (2016).\n[5]\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly\nLearning to Align and Translate”. In: arXiv preprint arXiv:1409.0473 (2014).\n[6]\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. “LongBench: A Bilingual,\nMultitask Benchmark for Long Context Understanding”. In: arXiv preprint arXiv:2308.14508\n(2023).\n[7]\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. “PIQA: Reasoning about Physical\nCommonsense in Natural Language”. In: Proceedings of the AAAI Conference on Artificial\nIntelligence. Vol. 34. 05. 2020, pp. 7432–7439.\n[8]\nbloc97. “NTK-aware Scaled RoPE allows LLaMA models to have Extended (8k+) Context\nSize Without any Fine-tuning and Minimal Perplexity Degradation”. In: (2023). url: https:\n//www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_\nllama_models_to_have.\n[9]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are\nFew-shot Learners”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 1877–\n1901.\n[10]\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge”. In: arXiv preprint arXiv:1803.05457 (2018).\n[11]\nTri Dao and Albert Gu. “Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality”. In: International Conference on Machine Learning\n(ICML). 2024.\n[12]\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. “A Dataset\nof Information-Seeking Questions and Answers Anchored in Research Papers”. In: Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. 2021, pp. 4599–4610.\n16\n\n[13]\nSoham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,\nAlbert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. “Griffin:\nMixing Gated Linear Recurrences with Local Attention for Efficient Language Models”. In: arXiv\npreprint arXiv:2402.19427 (2024).\n[14]\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas\nMuennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A Framework\nfor Few-shot Language Model Evaluation. Version v0.4.0. Dec. 2023. url: https://zenodo.org/\nrecords/10256836.\n[15]\nPaolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam\nIbrahim, and Beren Millidge. “Zamba: A Compact 7B SSM Hybrid Model”. In: arXiv preprint\narXiv:2405.16712 (2024).\n[16]\nAlbert Gu and Tri Dao. “Mamba: Linear-time Sequence Modeling with Selective State Spaces”.\nIn: arXiv preprint arXiv:2312.00752 (2023).\n[17]\nAlbert Gu, Karan Goel, and Christopher Re. “Efficiently Modeling Long Sequences with Structured\nState Spaces”. In: International Conference on Learning Representations. 2021.\n[18]\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. “Measuring Massive Multitask Language Understanding”. In: International\nConference on Learning Representations. 2020.\n[19]\nDan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: arXiv preprint\narXiv:1606.08415 (2016).\n[20]\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. “Constructing A Multi-\nhop QA Dataset for Comprehensive Evaluation of Reasoning Steps”. In: Proceedings of the 28th\nInternational Conference on Computational Linguistics. 2020, pp. 6609–6625.\n[21]\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and\nBoris Ginsburg. “RULER: What’s the Real Context Size of Your Long-Context Language Models?”\nIn: arXiv preprint arXiv:2404.06654 (2024).\n[22]\nSamy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. “Repeat After Me:\nTransformers are Better than State Space Models at Copying”. In: arXiv preprint arXiv:2402.01032\n(2024).\n[23]\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\n“Mistral 7B”. In: arXiv preprint arXiv:2310.06825 (2023).\n[24]\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. “PubMedQA:\nA Dataset for Biomedical Research Question Answering”. In: arXiv preprint arXiv:1909.06146\n(2019).\n[25]\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. “TriviaQA: A Large Scale\nDistantly Supervised Challenge Dataset for Reading Comprehension”. In: Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017,\npp. 1601–1611.\n[26]\nTomáš Kočisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor\nMelis, and Edward Grefenstette. “The NarrativeQA Reading Comprehension Challenge”. In:\nTransactions of the Association for Computational Linguistics 6 (2018), pp. 317–328.\n[27]\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. “Reducing Activation Recomputation in Large Transformer\nModels”. In: arXiv preprint arXiv:2205.05198 (2022).\n[28]\nTaku Kudo and John Richardson. “Sentencepiece: A Simple and Language Independent Subword\nTokenizer and Detokenizer for Neural Text Processing”. In: arXiv preprint arXiv:1808.06226\n(2018).\n[29]\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. “RACE: Large-scale\nReAding Comprehension Dataset From Examinations”. In: arXiv preprint arXiv:1704.04683\n(2017).\n[30]\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. “Latent Retrieval for Weakly Supervised\nOpen Domain Question Answering”. In: arXiv preprint arXiv:1906.00300 (2019).\n17\n\n[31]\nXin Li and Dan Roth. “Learning Question Classifiers”. In: COLING 2002: The 19th International\nConference on Computational Linguistics. 2002.\n[32]\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. “Jamba: A Hybrid Transformer-\nmamba Language Model”. In: arXiv preprint arXiv:2403.19887 (2024).\n[33]\nStephanie Lin, Jacob Hilton, and Owain Evans. “TruthfulQA: Measuring How Models Mimic\nHuman Falsehoods”. In: arXiv preprint arXiv:2109.07958 (2021).\n[34]\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. “Can a Suit of Armor\nConduct Electricity? A New Dataset for Open Book Question Answering”. In: arXiv preprint\narXiv:1809.02789 (2018).\n[35]\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,\nVijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro,\net al. “Efficient Large-scale Language Model Training on GPU Clusters using Megatron-LM”.\nIn: Proceedings of the International Conference for High Performance Computing, Networking,\nStorage and Analysis. 2021.\n[36]\nNVIDIA. NVIDIA H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/\nh100/. 2023.\n[37]\nJongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,\nKangwook Lee, and Dimitris Papailiopoulos. “Can Mamba Learn How to Learn? A Comparative\nStudy on In-Context Learning Tasks”. In: arXiv preprint arXiv:2402.04248 (2024).\n[38]\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subrama-\nnian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al.\n“Nemotron-4 15B Technical Report”. In: arXiv preprint arXiv:2402.16819 (2024).\n[39]\nJonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin.\n“Block-state Transformers”. In: Advances in Neural Information Processing Systems 36 (2024).\n[40]\nPranav Rajpurkar, Robin Jia, and Percy Liang. “Know what you don’t Know: Unanswerable\nQuestions for SQuAD”. In: arXiv preprint arXiv:1806.03822 (2018).\n[41]\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. “WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale”. In: Communications of the ACM 64.9 (2021),\npp. 99–106.\n[42]\nGeorge Saon, Ankit Gupta, and Xiaodong Cui. “Diagonal State Space Augmented Transformers\nfor Speech Recognition”. In: ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE. 2023, pp. 1–5.\n[43]\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan\nXiong, Mor Geva, Jonathan Berant, et al. “Scrolls: Standardized Comparison over Long Language\nSequences”. In: arXiv preprint arXiv:2201.03533 (2022).\n[44]\nNoam Shazeer. “GLU Variants Improve Transformer”. In: arXiv preprint arXiv:2002.05202 (2020).\n[45]\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. “Megatron-LM: Training Multi-billion Parameter Language Models using Model\nParallelism”. In: arXiv preprint arXiv:1909.08053 (2019).\n[46]\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. “Roformer: En-\nhanced Transformer with Rotary Position Embedding”. In: Neurocomputing 568 (2024), p. 127063.\n[47]\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efficient Transformers: A Survey”.\nIn: ACM Computing Surveys 55.6 (2022), pp. 1–28.\n[48]\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. “Llama 2: Open\nFoundation and Fine-tuned Chat Models”. In: arXiv preprint arXiv:2307.09288 (2023).\n[49]\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. “MuSiQue:\nMultihop Questions via Single-hop Question Composition”. In: Transactions of the Association\nfor Computational Linguistics 10 (2022), pp. 539–554.\n[50]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. “Attention is All You Need”. In: Advances in Neural Infor-\nmation Processing Systems 30 (2017).\n[51]\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. “Effective Long-context\nScaling of Foundation Models”. In: arXiv preprint arXiv:2309.16039 (2023).\n18\n\n[52]\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. “HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\nAnswering”. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. 2018, pp. 2369–2380.\n[53]\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a\nMachine Really Finish your Sentence?” In: arXiv preprint arXiv:1905.07830 (2019).\n[54]\nBiao Zhang and Rico Sennrich. “Root Mean Square Layer Normalization”. In: Advances in Neural\nInformation Processing Systems 32 (2019).\nA\nHybrid Layer Allocation Algorithm\nAlthough we are able to specify, and experiment with, an arbitrary sequence of Mamba, self-attention,\nand MLP layers in our hybrid models, by default we use the allocation algorithm described in\nAlgorithm 1. This algorithm first attempts to place any self-attention layers such that the intervening\nruns of contiguous Mamba layers are as equal in length as possible, while also beginning and ending the\nlayer sequence with a run of Mamba layers. Then, any MLP layers are evenly distributed throughout\nthe sequence while not replacing any self-attention layers. The MLP layers are biased away from the\nstart of the sequence so that the layer sequence begins with a Mamba layer (if there are any Mamba\nlayers) and ends with an MLP layer (if there are any MLP layers).\nAlgorithm 1 Hybrid Layer Allocation\n1: Input: total_layers_count, target_attention_ratio, target_mlp_ratio\n2: Output: layer_type_list\n3: attention_layers_count ←round(total_layers_count * target_attention_ratio)\n4: mamba_layers_count ←total_layers_count - attention_layers_count\n5: mamba_sections_count ←attention_layers_count + 1\n6: mamba_section_length ←mamba_layers_count / mamba_sections_count\n7: layer_type_list ←array of Symbols.MAMBA of size total_layers_count\n8: x ←mamba_section_length\n9: for l in 0 to total_layers_count - 1 do\n10:\nif x < 0.5 then\n11:\nlayer_type_list[l] ←Symbols.ATTENTION\n12:\nx ←x + mamba_section_length\n13:\nelse\n14:\nx ←x - 1\n15: mlp_layers_count ←round(total_layers_count * target_mlp_ratio)\n16: if mlp_layers_count > 0 then\n17:\nmamba_layers_count ←mamba_layers_count - mlp_layers_count\n18:\nmamba_to_mlp_ratio ←mamba_layers_count / mlp_layers_count\n19:\nx ←mamba_to_mlp_ratio\n20:\nfor l in 0 to total_layers_count - 1 do\n21:\nif layer_type_list[l] == Symbols.MAMBA then\n22:\nif x < 0.5 then\n23:\nlayer_type_list[l] ←Symbols.MLP\n24:\nx ←x + mamba_to_mlp_ratio\n25:\nelse\n26:\nx ←x - 1\nTable 11 provides examples of some layer patterns generated by Algorithm 1.\n19\n\nTLC\nATT\nMLP\nPattern\n24\n0.00\n0.00\nMMMMMMMMMMMMMMMMMMMMMMMM\n24\n0.08\n0.00\nMMMMMMM*MMMMMMMM*MMMMMMM\n24\n0.17\n0.00\nMMMM*MMMM*MMMM*MMMM*MMMM\n24\n0.08\n0.30\nMM+MM+M*M+MMM+MM*+MM+MM+\n24\n0.08\n0.50\nM+M+M++*M+M+M+M+*M++M+M+\n24\n0.50\n0.50\n+*+*+*+*+*+**+*+*+*+*+*+\n48\n0.08\n0.50\nM+M+M++M+*M+M+M+M++*M+M+M+M+*M++M+M+M+*M+M++M+M+\n56\n0.08\n0.50\nM+M+M++M+M*+M+M+M+M++M*+M+M+M+M+M*++M+M+M+M+M*+M++M+M+M+\nTable 11: Some examples of hybrid layer patterns generated by Algorithm 1. TLC=total_layer_count,\nATT=target_attention_ratio, MLP=target_mlp_ratio. In the pattern, M=Mamba, *=Self-Attention,\nand +=MLP.\n20\n"
    }
  ]
}