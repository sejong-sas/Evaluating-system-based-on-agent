{
  "repo": "EleutherAI/gpt-neox",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2305.18290",
      "full_text": " [2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2305.18290 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2305.18290 (cs) [Submitted on 29 May 2023 ( v1 ), last revised 29 Jul 2024 (this version, v3)] Title: Direct Preference Optimization: Your Language Model is Secretly a Reward Model Authors: Rafael Rafailov , Archit Sharma , Eric Mitchell , Stefano Ermon , Christopher D. Manning , Chelsea Finn View a PDF of the paper titled Direct Preference Optimization: Your Language Model is Secretly a Reward Model, by Rafael Rafailov and 5 other authors View PDF HTML (experimental) Abstract: While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2305.18290 [cs.LG] &nbsp; (or arXiv:2305.18290v3 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2305.18290 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Archit Sharma [ view email ] [v1] Mon, 29 May 2023 17:57:46 UTC (982 KB) [v2] Wed, 13 Dec 2023 18:48:48 UTC (983 KB) [v3] Mon, 29 Jul 2024 22:26:36 UTC (999 KB) Full-text links: Access Paper: View a PDF of the paper titled Direct Preference Optimization: Your Language Model is Secretly a Reward Model, by Rafael Rafailov and 5 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-05 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar 2 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.01306",
      "full_text": " [2402.01306] KTO: Model Alignment as Prospect Theoretic Optimization Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.01306 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2402.01306 (cs) [Submitted on 2 Feb 2024 ( v1 ), last revised 19 Nov 2024 (this version, v4)] Title: KTO: Model Alignment as Prospect Theoretic Optimization Authors: Kawin Ethayarajh , Winnie Xu , Niklas Muennighoff , Dan Jurafsky , Douwe Kiela View a PDF of the paper titled KTO: Model Alignment as Prospect Theoretic Optimization, by Kawin Ethayarajh and 4 other authors View PDF HTML (experimental) Abstract: Kahneman &amp; Tversky&#39;s $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration. Comments: ICML 2024 Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2402.01306 [cs.LG] &nbsp; (or arXiv:2402.01306v4 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.01306 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kawin Ethayarajh [ view email ] [v1] Fri, 2 Feb 2024 10:53:36 UTC (857 KB) [v2] Mon, 3 Jun 2024 02:36:09 UTC (975 KB) [v3] Tue, 3 Sep 2024 07:41:51 UTC (975 KB) [v4] Tue, 19 Nov 2024 18:12:45 UTC (977 KB) Full-text links: Access Paper: View a PDF of the paper titled KTO: Model Alignment as Prospect Theoretic Optimization, by Kawin Ethayarajh and 4 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/gpt-neox/releases/tag/v2.0",
      "full_text": " Release GPT-NeoX 2.0 Â· EleutherAI/gpt-neox Â· GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / gpt-neox Public Notifications You must be signed in to change notification settings Fork 1.1k Star 7.3k Code Issues 61 Pull requests 24 Actions Projects 0 Wiki Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Wiki Security Insights Releases v2.0 GPT-NeoX 2.0 Latest Latest Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags Quentin-Anthony released this 10 Mar 00:26 &middot; 343 commits to main since this release v2.0 9610391 This commit was created on GitHub.com and signed with GitHubâ€™s verified signature . The key has expired. GPG key ID: 4AEE18F83AFDEB23 Expired Verified Learn about vigilant mode . With GPT-NeoX 2.0, we now support upstream DeepSpeed. This enables the use of new DeepSpeed features such as Curriculum Learning , Communication Logging , and Autotuning . For any changes in upstream DeepSpeed that are fundamentally incompatible with GPT-NeoX 2.0, we do the following: Attempt to create a PR to upstream DeepSpeed Stage the PR on DeeperSpeed 2.x , so that there's always a DeepSpeed version that's guaranteed to work with GPT-Neox 2.x. Therefore, we recommend using DeeperSpeed 2.x unless your use-case relies on a specific upstream DeepSpeed feature that we haven't merged into DeeperSpeed 2.x yet. What's Changed Mup Support in #704 Bring deepspeed_main up-to-date in #746 Latest DeepSpeed Support in #663 Curriculum Learning Support in #695 Autotuning Support in #739 Full Changelog : v1.0...v2.0 Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> ðŸŽ‰ 9 ncoop57, weiyx16, ElleLeonne, Pvpasall, pterodactyl-soup, MDSADABWASIM, ipshergill, xyloflake, and WarnerCharon reacted with hooray emoji All reactions ðŸŽ‰ 9 reactions 9 people reacted 0 Join discussion Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You canâ€™t perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/DeeperSpeed/releases/tag/v2.0",
      "full_text": " Release 2.0 Â· EleutherAI/DeeperSpeed Â· GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / DeeperSpeed Public forked from deepspeedai/DeepSpeed Notifications You must be signed in to change notification settings Fork 48 Star 168 Code Pull requests 1 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Pull requests Actions Projects Security Insights Releases v2.0 2.0 Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags Quentin-Anthony released this 09 Mar 23:11 &middot; 608 commits to main since this release v2.0 457850d This commit was created on GitHub.com and signed with GitHubâ€™s verified signature . The key has expired. GPG key ID: 4AEE18F83AFDEB23 Expired Verified Learn about vigilant mode . DeeperSpeed 2.0 is based off upstream DeepSpeed 0.8.3 and is intended to closely match the upstream along with: Any additional fixes we find that are yet to be accepted into DeepSpeed Branches for optimal performance on EleutherAI's compute providers (e.g. stability, coreweave) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You canâ€™t perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/gpt-neox/releases/tag/v1.0",
      "full_text": " Release GPT-NeoX 1.0 Â· EleutherAI/gpt-neox Â· GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / gpt-neox Public Notifications You must be signed in to change notification settings Fork 1.1k Star 7.3k Code Issues 61 Pull requests 24 Actions Projects 0 Wiki Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Wiki Security Insights Releases v1.0 GPT-NeoX 1.0 Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags Quentin-Anthony released this 09 Mar 17:11 v1.0 71df4d5 This commit was created on GitHub.com and signed with GitHubâ€™s verified signature . The key has expired. GPG key ID: 4AEE18F83AFDEB23 Expired Verified Learn about vigilant mode . This is the legacy GPT-NeoX relying on old DeeperSpeed (0.3.15). We only recommend using this release under circumstance that you're loading a model based on old DeeperSpeed (e.g. GPT-J, GPT-NeoX20B, the Pythia suite, etc). The primary difference between this release and v2.x is the DeepSpeed version supported. If you're using 2.x, we're assuming that you're using either the latest release of DeepSpeed or DeeperSpeed 2.x. Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions 0 Join discussion Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You canâ€™t perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/DeeperSpeed/releases/tag/v1.0",
      "full_text": " Release 1.0 Â· EleutherAI/DeeperSpeed Â· GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / DeeperSpeed Public forked from deepspeedai/DeepSpeed Notifications You must be signed in to change notification settings Fork 48 Star 168 Code Pull requests 1 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Pull requests Actions Projects Security Insights Releases v1.0 1.0 Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags Quentin-Anthony released this 09 Mar 22:58 v1.0 7069d10 This commit was created on GitHub.com and signed with GitHubâ€™s verified signature . The key has expired. GPG key ID: 4AEE18F83AFDEB23 Expired Verified Learn about vigilant mode . DeeperSpeed 1.0 is the legacy version based on DeepSpeed 0.3.15. This version is meant to be combined with GPT-NeoX v1.0. Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You canâ€™t perform that action at this time. "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2204.06745",
      "full_text": " [2204.06745] GPT-NeoX-20B: An Open-Source Autoregressive Language Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2204.06745 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2204.06745 (cs) [Submitted on 14 Apr 2022] Title: GPT-NeoX-20B: An Open-Source Autoregressive Language Model Authors: Sid Black , Stella Biderman , Eric Hallahan , Quentin Anthony , Leo Gao , Laurence Golding , Horace He , Connor Leahy , Kyle McDonell , Jason Phang , Michael Pieler , USVSN Sai Prashanth , Shivanshu Purohit , Laria Reynolds , Jonathan Tow , Ben Wang , Samuel Weinbach View a PDF of the paper titled GPT-NeoX-20B: An Open-Source Autoregressive Language Model, by Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach View PDF Abstract: We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}&#39;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at this https URL . Comments: To appear in the Proceedings of the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models Subjects: Computation and Language (cs.CL) Cite as: arXiv:2204.06745 [cs.CL] &nbsp; (or arXiv:2204.06745v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2204.06745 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Stella Biderman [ view email ] [v1] Thu, 14 Apr 2022 04:00:27 UTC (6,828 KB) Full-text links: Access Paper: View a PDF of the paper titled GPT-NeoX-20B: An Open-Source Autoregressive Language Model, by Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://slurm.schedmd.com/documentation.html",
      "full_text": " Slurm Workload Manager - Documentation Slurm Workload Manager SchedMD Navigation Slurm Workload Manager Version 25.05 About Overview Release Notes Using Documentation FAQ Publications Installing Download Related Software Installation Guide Getting Help Mailing Lists Support and Training Troubleshooting Documentation NOTE: This documentation is for Slurm version 25.05. Documentation for older versions of Slurm are distributed with the source, or may be found in the archive . Slurm Users Quick Start User Guide Command/option Summary (two pages) Man Pages Rosetta Stone of Workload Managers Job Array Support Heterogeneous Job Support CPU Management User and Administrator Guide MPI and UPC Users Guide Support for Multi-core/Multi-threaded Architectures Multi-Cluster Operation Profiling Using HDF5 User Guide Job Reason Codes Job State Codes Job Exit Codes Resource Binding Specific Systems Intel Knights Landing (KNL) User and Administrator Guide Slurm Administrators Quick Start Administrator Guide Upgrade Guide Accounting Advanced Resource Reservation Guide Authentication Plugins Burst Buffer Guide Cgroups Guide \"Configless\" Slurm Operation Configuration Tool (Full version) Configuration Tool (Simplified version) Containers CPU Management User and Administrator Guide Dynamic Nodes Elasticsearch Guide Job Completion Kafka plugin Guide job_container/tmpfs - Job Specific Temporary File Management JSON Web Tokens Authentication Federated Scheduling Guide Job Containment (SSH Session Control) with pam_slurm_adopt Kubernetes Guide Large Cluster Administration Guide License Management Hierarchical Resources (HRES) (in Beta) Multi-Category Security (MCS) Guide Name Service Caching Through NSS Slurm Network Configuration Guide OpenAPI Plugin Release Notes Power Saving Guide (power down idle nodes) Prolog and Epilog Guide Slurm REST API Quick Start Guide API Details API Methods and Models Client Guide Slurm SELinux Context Management Troubleshooting Guide User Permissions WCKey Management Workload Prioritization Multifactor Job Priority Classic Fairshare Algorithm Depth-Oblivious Fair-share Factor Fair Tree Fairshare Algorithm Slurm Scheduling Scheduling Configuration Guide Consumable Resources Guide Core Specialization Gang Scheduling Generic Resource (GRES) Scheduling High Throughput Computing Guide Preemption Quality of Service (QOS) Resource Limits Resource Reservation Guide Sharing Consumable Resources Topology Trackable Resources (TRES) Cloud Cloud Scheduling Guide Slurm on Google Cloud Platform Deploying Slurm on AWS Parallel Computing Service Slurm on Microsoft Azure and CycleCloud Slurm Developers Contributor Guide Programmer Guide Application Programmer Interface (API) Guide Adding Files or Plugins to Slurm Design Information Generic Resource (GRES) Design Guide Job Launch Design Guide Select Plugin Design Guide Plugin Programmer Guide Plugin Interface Details Command Line Filter Plugin Programmer Guide Job Submission Plugin Programmer Guide PrEp Plugin Programmer Guide Site Factor (Priority) Plugin Programmer Guide Last modified 27 April 2025 "
    },
    {
      "arxiv_id": "https://apptainer.org/docs/user/1.0/index.html",
      "full_text": " Apptainer User Guide &mdash; Apptainer User Guide 1.0 documentation Apptainer User Guide 1.0 Introduction to Apptainer Quick Start Security in Apptainer Build a container The Definition File Build Environment Fakeroot feature Sign and Verify Key management commands Encrypted Containers Remote Endpoints Library API Registries Bind Paths and Mounts Persistent Overlays Running Services Environment and Metadata Plugins Security Options Network Options Cgroups Support Application Checkpointing Singularity Compatibility Support for Docker / OCI Containers OCI Runtime Support Apptainer and MPI applications GPU Support Contributing Appendix Command Line Reference License Apptainer User Guide Warning You're browsing the documentation for an old version of Apptainer. Consider upgrading to the latest version of Apptainer. Documentation for the latest version can always be found at: https://apptainer.org/docs/user/latest/ Apptainer User Guide Edit on GitHub Apptainer User Guide ïƒ Welcome to the Apptainer User Guide! This guide aims to give an introduction to Apptainer, brief installation instructions, and cover topics relevant to users building and running containers. For a detailed guide to installation and configuration, please see the separate Admin Guide for this version of Apptainer. Getting Started &amp; Background Information ïƒ Introduction to Apptainer Why use Apptainer? Why use containers? Use Cases Quick Start Quick Installation Steps Overview of the Apptainer Interface Downloading images Interact with images Working with Files Build images from scratch Security in Apptainer Security Policy Background Setuid &amp; User Namespaces Runtime &amp; User Privilege Model Singularity Image Format (SIF) Configuration &amp; Runtime Options Building Containers ïƒ Learn how to write a definition file that can be used to build a container. Understand the environment within a build, how to perform remote builds, and how to use the --fakeroot feature to build as a non-root user. Build a container The Definition File Build Environment Fakeroot feature Container Signing &amp; Encryption ïƒ Apptainer allows containers to be signed using a PGP key. The signature travels with the container image, allowing you to verify that the image is unmodified at any time. Encryption of containers using LUKS2 is also supported. Encrypted containers can be run without decrypting them to disk first. Sign and Verify Key management commands Encrypted Containers Sharing &amp; Online Services ïƒ Remote Endpoints Library API Registries Advanced Usage ïƒ Once youâ€™ve understood the basics, explore all the options which Apptainer provides for accessing data, running persistent services in containers, manipulating the container environment, and applying networking and security configuration. Bind Paths and Mounts Persistent Overlays Running Services Environment and Metadata Plugins Security Options Network Options Cgroups Support Application Checkpointing Compatibility ïƒ Apptainer has unique benefits and supports easy access to GPUs and other hardware. It also strives for compatibility with Docker/OCI container formats. Understand the differences between Apptainer and Docker, as well as how to use containerized MPI and GPU applications. Singularity Compatibility Support for Docker / OCI Containers OCI Runtime Support Apptainer and MPI applications GPU Support Get Involved ïƒ Weâ€™d love you to get involved in the Apptainer community! Whether through contributing feature and fixes, helping to answer questions from other users, or simply testing new releases. Contributing Reference ïƒ Appendix Apptainerâ€™s environment variables Build Modules Command Line Reference License Next Built with Sphinx using a theme provided by Read the Docs . &copy; Contributors to the Apptainer project, established as Apptainer a Series of LF Projects LLC &copy; 2017-2022, Sylabs Inc "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2101.00027",
      "full_text": " [2101.00027] The Pile: An 800GB Dataset of Diverse Text for Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2101.00027 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2101.00027 (cs) [Submitted on 31 Dec 2020] Title: The Pile: An 800GB Dataset of Diverse Text for Language Modeling Authors: Leo Gao , Stella Biderman , Sid Black , Laurence Golding , Travis Hoppe , Charles Foster , Jason Phang , Horace He , Anish Thite , Noa Nabeshima , Shawn Presser , Connor Leahy View a PDF of the paper titled The Pile: An 800GB Dataset of Diverse Text for Language Modeling, by Leo Gao and 11 other authors View PDF Abstract: Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2101.00027 [cs.CL] &nbsp; (or arXiv:2101.00027v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2101.00027 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Leo Gao [ view email ] [v1] Thu, 31 Dec 2020 19:00:10 UTC (2,152 KB) Full-text links: Access Paper: View a PDF of the paper titled The Pile: An 800GB Dataset of Diverse Text for Language Modeling, by Leo Gao and 11 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-01 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 4 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Stella Biderman Jason Phang Horace He a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.01373",
      "full_text": " [2304.01373] Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.01373 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.01373 (cs) [Submitted on 3 Apr 2023 ( v1 ), last revised 31 May 2023 (this version, v2)] Title: Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling Authors: Stella Biderman , Hailey Schoelkopf , Quentin Anthony , Herbie Bradley , Kyle O&#39;Brien , Eric Hallahan , Mohammad Aflah Khan , Shivanshu Purohit , USVSN Sai Prashanth , Edward Raff , Aviya Skowron , Lintang Sutawika , Oskar van der Wal View a PDF of the paper titled Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, by Stella Biderman and 12 other authors View PDF Abstract: How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{ this https URL }. Comments: Code at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.01373 [cs.CL] &nbsp; (or arXiv:2304.01373v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.01373 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hailey Schoelkopf [ view email ] [v1] Mon, 3 Apr 2023 20:58:15 UTC (2,683 KB) [v2] Wed, 31 May 2023 17:54:07 UTC (2,371 KB) Full-text links: Access Paper: View a PDF of the paper titled Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, by Stella Biderman and 12 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1910.03771",
      "full_text": " [1910.03771] HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1910.03771 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1910.03771 (cs) [Submitted on 9 Oct 2019 ( v1 ), last revised 14 Jul 2020 (this version, v5)] Title: HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing Authors: Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , RÃ©mi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , Alexander M. Rush View a PDF of the paper titled HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing, by Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\\&#39;emi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush View PDF Abstract: Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{ this https URL }. Comments: 8 pages, 4 figures, more details at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:1910.03771 [cs.CL] &nbsp; (or arXiv:1910.03771v5 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1910.03771 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Victor Sanh [ view email ] [v1] Wed, 9 Oct 2019 03:23:22 UTC (530 KB) [v2] Mon, 14 Oct 2019 15:33:45 UTC (530 KB) [v3] Wed, 16 Oct 2019 15:36:45 UTC (530 KB) [v4] Tue, 11 Feb 2020 14:42:10 UTC (530 KB) [v5] Tue, 14 Jul 2020 03:42:34 UTC (642 KB) Full-text links: Access Paper: View a PDF of the paper titled HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing, by Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\\&#39;emi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Thomas Wolf Victor Sanh Julien Chaumond Clement Delangue RÃ©mi Louf export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2302.12433",
      "full_text": " [2302.12433] ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2302.12433 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2302.12433 (cs) [Submitted on 24 Feb 2023] Title: ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics Authors: Zhangir Azerbayev , Bartosz Piotrowski , Hailey Schoelkopf , Edward W. Ayers , Dragomir Radev , Jeremy Avigad View a PDF of the paper titled ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics, by Zhangir Azerbayev and 5 other authors View PDF Abstract: We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) Cite as: arXiv:2302.12433 [cs.CL] &nbsp; (or arXiv:2302.12433v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2302.12433 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhangir Azerbayev Mr [ view email ] [v1] Fri, 24 Feb 2023 03:28:46 UTC (119 KB) Full-text links: Access Paper: View a PDF of the paper titled ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics, by Zhangir Azerbayev and 5 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-02 Change to browse by: cs cs.AI cs.LO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.11158",
      "full_text": " [2304.11158] Emergent and Predictable Memorization in Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.11158 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.11158 (cs) [Submitted on 21 Apr 2023 ( v1 ), last revised 31 May 2023 (this version, v2)] Title: Emergent and Predictable Memorization in Large Language Models Authors: Stella Biderman , USVSN Sai Prashanth , Lintang Sutawika , Hailey Schoelkopf , Quentin Anthony , Shivanshu Purohit , Edward Raff View a PDF of the paper titled Emergent and Predictable Memorization in Large Language Models, by Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff View PDF Abstract: Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model&#39;s memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model&#39;s full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.11158 [cs.CL] &nbsp; (or arXiv:2304.11158v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.11158 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hailey Schoelkopf [ view email ] [v1] Fri, 21 Apr 2023 17:58:31 UTC (6,352 KB) [v2] Wed, 31 May 2023 19:09:45 UTC (13,432 KB) Full-text links: Access Paper: View a PDF of the paper titled Emergent and Predictable Memorization in Large Language Models, by Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2306.02254",
      "full_text": " [2306.02254] A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2306.02254 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2306.02254 (cs) [Submitted on 4 Jun 2023 ( v1 ), last revised 6 Jun 2023 (this version, v2)] Title: A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models Authors: Hyunwoong Ko , Kichang Yang , Minho Ryu , Taekyoon Choi , Seungmu Yang , Jiwung Hyun , Sungho Park , Kyubyong Park View a PDF of the paper titled A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models, by Hyunwoong Ko and 7 other authors View PDF Abstract: Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2306.02254 [cs.CL] &nbsp; (or arXiv:2306.02254v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2306.02254 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kichang Yang [ view email ] [v1] Sun, 4 Jun 2023 04:04:04 UTC (8,210 KB) [v2] Tue, 6 Jun 2023 03:27:33 UTC (8,210 KB) Full-text links: Access Paper: View a PDF of the paper titled A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models, by Hyunwoong Ko and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-06 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2308.04014",
      "full_text": " [2308.04014] Continual Pre-Training of Large Language Models: How to (re)warm your model? Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2308.04014 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2308.04014 (cs) [Submitted on 8 Aug 2023 ( v1 ), last revised 6 Sep 2023 (this version, v2)] Title: Continual Pre-Training of Large Language Models: How to (re)warm your model? Authors: Kshitij Gupta , Benjamin ThÃ©rien , Adam Ibrahim , Mats L. Richter , Quentin Anthony , Eugene Belilovsky , Irina Rish , TimothÃ©e Lesort View a PDF of the paper titled Continual Pre-Training of Large Language Models: How to (re)warm your model?, by Kshitij Gupta and 7 other authors View PDF Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\\unicode{x2013}$even for a large downstream dataset. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2308.04014 [cs.CL] &nbsp; (or arXiv:2308.04014v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2308.04014 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Benjamin ThÃ©rien [ view email ] [v1] Tue, 8 Aug 2023 03:18:18 UTC (7,070 KB) [v2] Wed, 6 Sep 2023 23:13:07 UTC (7,070 KB) Full-text links: Access Paper: View a PDF of the paper titled Continual Pre-Training of Large Language Models: How to (re)warm your model?, by Kshitij Gupta and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-08 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.10631",
      "full_text": " [2310.10631] Llemma: An Open Language Model For Mathematics Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.10631 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2310.10631 (cs) [Submitted on 16 Oct 2023 ( v1 ), last revised 15 Mar 2024 (this version, v3)] Title: Llemma: An Open Language Model For Mathematics Authors: Zhangir Azerbayev , Hailey Schoelkopf , Keiran Paster , Marco Dos Santos , Stephen McAleer , Albert Q. Jiang , Jia Deng , Stella Biderman , Sean Welleck View a PDF of the paper titled Llemma: An Open Language Model For Mathematics, by Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck View PDF HTML (experimental) Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments. Comments: Updated references; corrected description of COPRA search budget Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) Cite as: arXiv:2310.10631 [cs.CL] &nbsp; (or arXiv:2310.10631v3 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.10631 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhangir Azerbayev Mr [ view email ] [v1] Mon, 16 Oct 2023 17:54:07 UTC (386 KB) [v2] Fri, 1 Dec 2023 03:51:33 UTC (116 KB) [v3] Fri, 15 Mar 2024 19:14:39 UTC (116 KB) Full-text links: Access Paper: View a PDF of the paper titled Llemma: An Open Language Model For Mathematics, by Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI cs.LO References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://aclanthology.org/2023.emnlp-main.530/",
      "full_text": " trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback - ACL Anthology ACL Anthology News (current) FAQ (current) Corrections (current) Submissions (current) Github trl X : A Framework for Large Scale Reinforcement Learning from Human Feedback Alexander Havrilla , Maksym Zhuravinskyi , Duy Phung , Aman Tiwari , Jonathan Tow , Stella Biderman , Quentin Anthony , Louis Castricato Correct Metadata for &#215; Important : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as &lt;fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as &lt;tex-math>. Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .) Authors concatenated from the text boxes above: ALL author names match the snapshot aboveâ€”including middle initials, hyphens, and accents. Submit Abstract Reinforcement learning from human feedback ( RLHF ) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization ( PPO ), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning ( ILQL ) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works. Anthology ID: 2023.emnlp-main.530 Volume: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing Month: December Year: 2023 Address: Singapore Editors: Houda Bouamor , Juan Pino , Kalika Bali Venue: EMNLP SIG: Publisher: Association for Computational Linguistics Note: Pages: 8578â€“8595 Language: URL: https://aclanthology.org/2023.emnlp-main.530/ DOI: 10.18653/v1/2023.emnlp-main.530 Bibkey: havrilla-etal-2023-trlx Cite (ACL): Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, and Louis Castricato. 2023. trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback . In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 8578â€“8595, Singapore. Association for Computational Linguistics. Cite (Informal): trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback (Havrilla et al., EMNLP 2023) Copy Citation: BibTeX Markdown MODS XML Endnote More optionsâ€¦ PDF: https://aclanthology.org/2023.emnlp-main.530.pdf Video: &nbsp;https://aclanthology.org/2023.emnlp-main.530.mp4 PDF Cite Search Video Fix data Export citation &#215; BibTeX MODS XML Endnote Preformatted @inproceedings{havrilla-etal-2023-trlx, title = &#34;trl{X}: A Framework for Large Scale Reinforcement Learning from Human Feedback&#34;, author = &#34;Havrilla, Alexander and Zhuravinskyi, Maksym and Phung, Duy and Tiwari, Aman and Tow, Jonathan and Biderman, Stella and Anthony, Quentin and Castricato, Louis&#34;, editor = &#34;Bouamor, Houda and Pino, Juan and Bali, Kalika&#34;, booktitle = &#34;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&#34;, month = dec, year = &#34;2023&#34;, address = &#34;Singapore&#34;, publisher = &#34;Association for Computational Linguistics&#34;, url = &#34;https://aclanthology.org/2023.emnlp-main.530/&#34;, doi = &#34;10.18653/v1/2023.emnlp-main.530&#34;, pages = &#34;8578--8595&#34;, abstract = &#34;Reinforcement learning from human feedback (\\textbf{RLHF}) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (\\textbf{PPO}), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the \\textbf{AutoRLHF} library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (\\textbf{ILQL}) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works.&#34; } Download as File Copy to Clipboard &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt; &lt;mods ID=&#34;havrilla-etal-2023-trlx&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Alexander&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Havrilla&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Maksym&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Zhuravinskyi&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Duy&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Phung&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Aman&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Tiwari&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Jonathan&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Tow&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Stella&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Biderman&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Quentin&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Anthony&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Louis&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Castricato&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;dateIssued&gt;2023-12&lt;/dateIssued&gt; &lt;/originInfo&gt; &lt;typeOfResource&gt;text&lt;/typeOfResource&gt; &lt;relatedItem type=&#34;host&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Houda&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Bouamor&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Juan&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Pino&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Kalika&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Bali&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt; &lt;place&gt; &lt;placeTerm type=&#34;text&#34;&gt;Singapore&lt;/placeTerm&gt; &lt;/place&gt; &lt;/originInfo&gt; &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt; &lt;/relatedItem&gt; &lt;abstract&gt;Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (ILQL) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works.&lt;/abstract&gt; &lt;identifier type=&#34;citekey&#34;&gt;havrilla-etal-2023-trlx&lt;/identifier&gt; &lt;identifier type=&#34;doi&#34;&gt;10.18653/v1/2023.emnlp-main.530&lt;/identifier&gt; &lt;location&gt; &lt;url&gt;https://aclanthology.org/2023.emnlp-main.530/&lt;/url&gt; &lt;/location&gt; &lt;part&gt; &lt;date&gt;2023-12&lt;/date&gt; &lt;extent unit=&#34;page&#34;&gt; &lt;start&gt;8578&lt;/start&gt; &lt;end&gt;8595&lt;/end&gt; &lt;/extent&gt; &lt;/part&gt; &lt;/mods&gt; &lt;/modsCollection&gt; Download as File Copy to Clipboard %0 Conference Proceedings %T trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback %A Havrilla, Alexander %A Zhuravinskyi, Maksym %A Phung, Duy %A Tiwari, Aman %A Tow, Jonathan %A Biderman, Stella %A Anthony, Quentin %A Castricato, Louis %Y Bouamor, Houda %Y Pino, Juan %Y Bali, Kalika %S Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing %D 2023 %8 December %I Association for Computational Linguistics %C Singapore %F havrilla-etal-2023-trlx %X Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (ILQL) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works. %R 10.18653/v1/2023.emnlp-main.530 %U https://aclanthology.org/2023.emnlp-main.530/ %U https://doi.org/10.18653/v1/2023.emnlp-main.530 %P 8578-8595 Download as File Copy to Clipboard Markdown (Informal) [trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://aclanthology.org/2023.emnlp-main.530/) (Havrilla et al., EMNLP 2023) trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback (Havrilla et al., EMNLP 2023) ACL Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, and Louis Castricato. 2023. trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback . In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 8578â€“8595, Singapore. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright Â©&nbsp;1963&ndash;2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License . Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License . The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 15 September 2025 at 14:04 UTC with commit aba5819 . "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2401.14489",
      "full_text": " [2401.14489] The Case for Co-Designing Model Architectures with Hardware Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2401.14489 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2401.14489 (cs) [Submitted on 25 Jan 2024 ( v1 ), last revised 30 Jan 2024 (this version, v2)] Title: The Case for Co-Designing Model Architectures with Hardware Authors: Quentin Anthony , Jacob Hatef , Deepak Narayanan , Stella Biderman , Stas Bekman , Junqi Yin , Aamir Shafi , Hari Subramoni , Dhabaleswar Panda View a PDF of the paper titled The Case for Co-Designing Model Architectures with Hardware, by Quentin Anthony and 8 other authors View PDF HTML (experimental) Abstract: While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes. Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2401.14489 [cs.DC] &nbsp; (or arXiv:2401.14489v2 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2401.14489 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Quentin Anthony [ view email ] [v1] Thu, 25 Jan 2024 19:50:31 UTC (26,868 KB) [v2] Tue, 30 Jan 2024 21:26:09 UTC (27,008 KB) Full-text links: Access Paper: View a PDF of the paper titled The Case for Co-Designing Model Architectures with Hardware, by Quentin Anthony and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-01 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2403.08763",
      "full_text": " [2403.08763] Simple and Scalable Strategies to Continually Pre-train Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2403.08763 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2403.08763 (cs) [Submitted on 13 Mar 2024 ( v1 ), last revised 4 Sep 2024 (this version, v4)] Title: Simple and Scalable Strategies to Continually Pre-train Large Language Models Authors: Adam Ibrahim , Benjamin ThÃ©rien , Kshitij Gupta , Mats L. Richter , Quentin Anthony , TimothÃ©e Lesort , Eugene Belilovsky , Irina Rish View a PDF of the paper titled Simple and Scalable Strategies to Continually Pre-train Large Language Models, by Adam Ibrahim and 7 other authors View PDF Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2403.08763 [cs.LG] &nbsp; (or arXiv:2403.08763v4 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2403.08763 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Benjamin ThÃ©rien [ view email ] [v1] Wed, 13 Mar 2024 17:58:57 UTC (10,694 KB) [v2] Fri, 22 Mar 2024 17:56:38 UTC (10,732 KB) [v3] Tue, 26 Mar 2024 17:58:48 UTC (10,732 KB) [v4] Wed, 4 Sep 2024 16:13:18 UTC (10,770 KB) Full-text links: Access Paper: View a PDF of the paper titled Simple and Scalable Strategies to Continually Pre-train Large Language Models, by Adam Ibrahim and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-03 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.00691",
      "full_text": " [2402.00691] Comparative Study of Large Language Model Architectures on Frontier Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.00691 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2402.00691 (cs) [Submitted on 1 Feb 2024] Title: Comparative Study of Large Language Model Architectures on Frontier Authors: Junqi Yin , Avishek Bose , Guojing Cong , Isaac Lyngaas , Quentin Anthony View a PDF of the paper titled Comparative Study of Large Language Model Architectures on Frontier, by Junqi Yin and 4 other authors View PDF Abstract: Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world&#39;s first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms. Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) Cite as: arXiv:2402.00691 [cs.DC] &nbsp; (or arXiv:2402.00691v1 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.00691 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Junqi Yin [ view email ] [v1] Thu, 1 Feb 2024 15:50:37 UTC (13,693 KB) Full-text links: Access Paper: View a PDF of the paper titled Comparative Study of Large Language Model Architectures on Frontier, by Junqi Yin and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2205.09921",
      "full_text": " [2205.09921] KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2205.09921 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2205.09921 (cs) [Submitted on 20 May 2022 ( v1 ), last revised 13 Oct 2022 (this version, v2)] Title: KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation Authors: Ta-Chung Chi , Ting-Han Fan , Peter J. Ramadge , Alexander I. Rudnicky View a PDF of the paper titled KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation, by Ta-Chung Chi and 3 other authors View PDF Abstract: Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at this https URL . Comments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). The first two authors contributed equally to this work Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2205.09921 [cs.CL] &nbsp; (or arXiv:2205.09921v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2205.09921 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Ting-Han Fan [ view email ] [v1] Fri, 20 May 2022 01:25:57 UTC (92 KB) [v2] Thu, 13 Oct 2022 15:16:58 UTC (288 KB) Full-text links: Access Paper: View a PDF of the paper titled KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation, by Ta-Chung Chi and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-05 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://aclanthology.org/2022.bigscience-1.12/",
      "full_text": " Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned - ACL Anthology ACL Anthology News (current) FAQ (current) Corrections (current) Submissions (current) Github Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned Sameera Horawalavithana , Ellyn Ayton , Shivam Sharma , Scott Howland , Megha Subramanian , Scott Vasquez , Robin Cosbey , Maria Glenski , Svitlana Volkova Correct Metadata for &#215; Important : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as &lt;fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as &lt;tex-math>. Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .) Authors concatenated from the text boxes above: ALL author names match the snapshot aboveâ€”including middle initials, hyphens, and accents. Submit Abstract Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AIâ€™s GPT-2. Anthology ID: 2022.bigscience-1.12 Original: 2022.bigscience-1.12v1 Version 2: 2022.bigscience-1.12v2 Volume: Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models Month: May Year: 2022 Address: virtual+Dublin Editors: Angela Fan , Suzana Ilic , Thomas Wolf , Matthias GallÃ© Venue: BigScience SIG: Publisher: Association for Computational Linguistics Note: Pages: 160â€“172 Language: URL: https://aclanthology.org/2022.bigscience-1.12/ DOI: 10.18653/v1/2022.bigscience-1.12 Bibkey: horawalavithana-etal-2022-foundation Cite (ACL): Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova. 2022. Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned . In Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models , pages 160â€“172, virtual+Dublin. Association for Computational Linguistics. Cite (Informal): Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned (Horawalavithana et al., BigScience 2022) Copy Citation: BibTeX Markdown MODS XML Endnote More optionsâ€¦ PDF: https://aclanthology.org/2022.bigscience-1.12.pdf Video: &nbsp;https://aclanthology.org/2022.bigscience-1.12.mp4 PDF&nbsp; (v2) PDF&nbsp; (v1) Cite Search Video Fix data Export citation &#215; BibTeX MODS XML Endnote Preformatted @inproceedings{horawalavithana-etal-2022-foundation, title = &#34;Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned&#34;, author = &#34;Horawalavithana, Sameera and Ayton, Ellyn and Sharma, Shivam and Howland, Scott and Subramanian, Megha and Vasquez, Scott and Cosbey, Robin and Glenski, Maria and Volkova, Svitlana&#34;, editor = &#34;Fan, Angela and Ilic, Suzana and Wolf, Thomas and Gall{\\&#39;e}, Matthias&#34;, booktitle = &#34;Proceedings of BigScience Episode {\\#}5 -- Workshop on Challenges {\\&amp;} Perspectives in Creating Large Language Models&#34;, month = may, year = &#34;2022&#34;, address = &#34;virtual&#43;Dublin&#34;, publisher = &#34;Association for Computational Linguistics&#34;, url = &#34;https://aclanthology.org/2022.bigscience-1.12/&#34;, doi = &#34;10.18653/v1/2022.bigscience-1.12&#34;, pages = &#34;160--172&#34;, abstract = &#34;Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AI{&#39;}s GPT-2.&#34; } Download as File Copy to Clipboard &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt; &lt;mods ID=&#34;horawalavithana-etal-2022-foundation&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Sameera&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Horawalavithana&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Ellyn&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Ayton&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Shivam&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Sharma&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Scott&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Howland&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Megha&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Subramanian&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Scott&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Vasquez&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Robin&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Cosbey&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Maria&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Glenski&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Svitlana&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Volkova&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;dateIssued&gt;2022-05&lt;/dateIssued&gt; &lt;/originInfo&gt; &lt;typeOfResource&gt;text&lt;/typeOfResource&gt; &lt;relatedItem type=&#34;host&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Proceedings of BigScience Episode #5 â€“ Workshop on Challenges &amp;amp; Perspectives in Creating Large Language Models&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Angela&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Fan&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Suzana&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Ilic&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Thomas&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Wolf&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Matthias&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;GallÃ©&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt; &lt;place&gt; &lt;placeTerm type=&#34;text&#34;&gt;virtual&#43;Dublin&lt;/placeTerm&gt; &lt;/place&gt; &lt;/originInfo&gt; &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt; &lt;/relatedItem&gt; &lt;abstract&gt;Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AIâ€™s GPT-2.&lt;/abstract&gt; &lt;identifier type=&#34;citekey&#34;&gt;horawalavithana-etal-2022-foundation&lt;/identifier&gt; &lt;identifier type=&#34;doi&#34;&gt;10.18653/v1/2022.bigscience-1.12&lt;/identifier&gt; &lt;location&gt; &lt;url&gt;https://aclanthology.org/2022.bigscience-1.12/&lt;/url&gt; &lt;/location&gt; &lt;part&gt; &lt;date&gt;2022-05&lt;/date&gt; &lt;extent unit=&#34;page&#34;&gt; &lt;start&gt;160&lt;/start&gt; &lt;end&gt;172&lt;/end&gt; &lt;/extent&gt; &lt;/part&gt; &lt;/mods&gt; &lt;/modsCollection&gt; Download as File Copy to Clipboard %0 Conference Proceedings %T Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned %A Horawalavithana, Sameera %A Ayton, Ellyn %A Sharma, Shivam %A Howland, Scott %A Subramanian, Megha %A Vasquez, Scott %A Cosbey, Robin %A Glenski, Maria %A Volkova, Svitlana %Y Fan, Angela %Y Ilic, Suzana %Y Wolf, Thomas %Y GallÃ©, Matthias %S Proceedings of BigScience Episode #5 â€“ Workshop on Challenges &amp; Perspectives in Creating Large Language Models %D 2022 %8 May %I Association for Computational Linguistics %C virtual&#43;Dublin %F horawalavithana-etal-2022-foundation %X Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AIâ€™s GPT-2. %R 10.18653/v1/2022.bigscience-1.12 %U https://aclanthology.org/2022.bigscience-1.12/ %U https://doi.org/10.18653/v1/2022.bigscience-1.12 %P 160-172 Download as File Copy to Clipboard Markdown (Informal) [Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned](https://aclanthology.org/2022.bigscience-1.12/) (Horawalavithana et al., BigScience 2022) Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned (Horawalavithana et al., BigScience 2022) ACL Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova. 2022. Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned . In Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models , pages 160â€“172, virtual+Dublin. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright Â©&nbsp;1963&ndash;2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License . Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License . The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 15 September 2025 at 14:04 UTC with commit aba5819 . "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2202.13169",
      "full_text": " [2202.13169] A Systematic Evaluation of Large Language Models of Code Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2202.13169 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Programming Languages arXiv:2202.13169 (cs) [Submitted on 26 Feb 2022 ( v1 ), last revised 4 May 2022 (this version, v3)] Title: A Systematic Evaluation of Large Language Models of Code Authors: Frank F. Xu , Uri Alon , Graham Neubig , Vincent J. Hellendoorn View a PDF of the paper titled A Systematic Evaluation of Large Language Models of Code, by Frank F. Xu and 3 other authors View PDF Abstract: Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL , which enables future research and application in this area. Comments: DL4C@ICLR 2022, and MAPS@PLDI 2022 Subjects: Programming Languages (cs.PL) ; Computation and Language (cs.CL) Cite as: arXiv:2202.13169 [cs.PL] &nbsp; (or arXiv:2202.13169v3 [cs.PL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2202.13169 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Frank F. Xu [ view email ] [v1] Sat, 26 Feb 2022 15:53:55 UTC (275 KB) [v2] Tue, 1 Mar 2022 19:13:06 UTC (275 KB) [v3] Wed, 4 May 2022 16:08:31 UTC (275 KB) Full-text links: Access Paper: View a PDF of the paper titled A Systematic Evaluation of Large Language Models of Code, by Frank F. Xu and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.PL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-02 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.11389",
      "full_text": " [2304.11389] Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.11389 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.11389 (cs) [Submitted on 22 Apr 2023 ( v1 ), last revised 22 Oct 2023 (this version, v2)] Title: Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens Authors: Byung-Doh Oh , William Schuler View a PDF of the paper titled Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens, by Byung-Doh Oh and 1 other authors View PDF Abstract: Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a &#39;tipping point&#39; at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations. Comments: Findings of the Association for Computational Linguistics: EMNLP 2023 Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.11389 [cs.CL] &nbsp; (or arXiv:2304.11389v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.11389 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Byung-Doh Oh [ view email ] [v1] Sat, 22 Apr 2023 12:50:49 UTC (1,585 KB) [v2] Sun, 22 Oct 2023 20:03:54 UTC (2,199 KB) Full-text links: Access Paper: View a PDF of the paper titled Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens, by Byung-Doh Oh and 1 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://aclanthology.org/2023.acl-long.756/",
      "full_text": " Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis - ACL Anthology ACL Anthology News (current) FAQ (current) Corrections (current) Submissions (current) Github Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis Ta-Chung Chi , Ting-Han Fan , Alexander Rudnicky , Peter Ramadge Correct Metadata for &#215; Important : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as &lt;fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as &lt;tex-math>. Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .) Authors concatenated from the text boxes above: ALL author names match the snapshot aboveâ€”including middle initials, hyphens, and accents. Submit Abstract Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich , the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design. Anthology ID: 2023.acl-long.756 Volume: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Month: July Year: 2023 Address: Toronto, Canada Editors: Anna Rogers , Jordan Boyd-Graber , Naoaki Okazaki Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 13522â€“13537 Language: URL: https://aclanthology.org/2023.acl-long.756/ DOI: 10.18653/v1/2023.acl-long.756 Award: &nbsp;Outstanding Paper Award Bibkey: chi-etal-2023-dissecting Cite (ACL): Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 13522â€“13537, Toronto, Canada. Association for Computational Linguistics. Cite (Informal): Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis (Chi et al., ACL 2023) Copy Citation: BibTeX Markdown MODS XML Endnote More optionsâ€¦ PDF: https://aclanthology.org/2023.acl-long.756.pdf PDF Cite Search Fix data Export citation &#215; BibTeX MODS XML Endnote Preformatted @inproceedings{chi-etal-2023-dissecting, title = &#34;Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis&#34;, author = &#34;Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter&#34;, editor = &#34;Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki&#34;, booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;, month = jul, year = &#34;2023&#34;, address = &#34;Toronto, Canada&#34;, publisher = &#34;Association for Computational Linguistics&#34;, url = &#34;https://aclanthology.org/2023.acl-long.756/&#34;, doi = &#34;10.18653/v1/2023.acl-long.756&#34;, pages = &#34;13522--13537&#34;, abstract = &#34;Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create \\textbf{Sandwich}, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.&#34; } Download as File Copy to Clipboard &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt; &lt;mods ID=&#34;chi-etal-2023-dissecting&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Ta-Chung&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Chi&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Ting-Han&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Fan&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Alexander&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Rudnicky&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Peter&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Ramadge&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;dateIssued&gt;2023-07&lt;/dateIssued&gt; &lt;/originInfo&gt; &lt;typeOfResource&gt;text&lt;/typeOfResource&gt; &lt;relatedItem type=&#34;host&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Anna&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Rogers&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Jordan&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Boyd-Graber&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Naoaki&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Okazaki&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt; &lt;place&gt; &lt;placeTerm type=&#34;text&#34;&gt;Toronto, Canada&lt;/placeTerm&gt; &lt;/place&gt; &lt;/originInfo&gt; &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt; &lt;/relatedItem&gt; &lt;abstract&gt;Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.&lt;/abstract&gt; &lt;identifier type=&#34;citekey&#34;&gt;chi-etal-2023-dissecting&lt;/identifier&gt; &lt;identifier type=&#34;doi&#34;&gt;10.18653/v1/2023.acl-long.756&lt;/identifier&gt; &lt;location&gt; &lt;url&gt;https://aclanthology.org/2023.acl-long.756/&lt;/url&gt; &lt;/location&gt; &lt;part&gt; &lt;date&gt;2023-07&lt;/date&gt; &lt;extent unit=&#34;page&#34;&gt; &lt;start&gt;13522&lt;/start&gt; &lt;end&gt;13537&lt;/end&gt; &lt;/extent&gt; &lt;/part&gt; &lt;/mods&gt; &lt;/modsCollection&gt; Download as File Copy to Clipboard %0 Conference Proceedings %T Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis %A Chi, Ta-Chung %A Fan, Ting-Han %A Rudnicky, Alexander %A Ramadge, Peter %Y Rogers, Anna %Y Boyd-Graber, Jordan %Y Okazaki, Naoaki %S Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) %D 2023 %8 July %I Association for Computational Linguistics %C Toronto, Canada %F chi-etal-2023-dissecting %X Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design. %R 10.18653/v1/2023.acl-long.756 %U https://aclanthology.org/2023.acl-long.756/ %U https://doi.org/10.18653/v1/2023.acl-long.756 %P 13522-13537 Download as File Copy to Clipboard Markdown (Informal) [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://aclanthology.org/2023.acl-long.756/) (Chi et al., ACL 2023) Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis (Chi et al., ACL 2023) ACL Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 13522â€“13537, Toronto, Canada. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright Â©&nbsp;1963&ndash;2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License . Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License . The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 15 September 2025 at 14:04 UTC with commit aba5819 . "
    },
    {
      "arxiv_id": "https://aclanthology.org/2023.acl-short.102/",
      "full_text": " Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings - ACL Anthology ACL Anthology News (current) FAQ (current) Corrections (current) Submissions (current) Github Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Ta-Chung Chi , Ting-Han Fan , Li-Wei Chen , Alexander Rudnicky , Peter Ramadge Correct Metadata for &#215; Important : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as &lt;fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as &lt;tex-math>. Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .) Authors concatenated from the text boxes above: ALL author names match the snapshot aboveâ€”including middle initials, hyphens, and accents. Submit Abstract The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models. Anthology ID: 2023.acl-short.102 Volume: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) Month: July Year: 2023 Address: Toronto, Canada Editors: Anna Rogers , Jordan Boyd-Graber , Naoaki Okazaki Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 1183â€“1193 Language: URL: https://aclanthology.org/2023.acl-short.102/ DOI: 10.18653/v1/2023.acl-short.102 Bibkey: chi-etal-2023-latent Cite (ACL): Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. 2023. Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1183â€“1193, Toronto, Canada. Association for Computational Linguistics. Cite (Informal): Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings (Chi et al., ACL 2023) Copy Citation: BibTeX Markdown MODS XML Endnote More optionsâ€¦ PDF: https://aclanthology.org/2023.acl-short.102.pdf PDF Cite Search Fix data Export citation &#215; BibTeX MODS XML Endnote Preformatted @inproceedings{chi-etal-2023-latent, title = &#34;Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings&#34;, author = &#34;Chi, Ta-Chung and Fan, Ting-Han and Chen, Li-Wei and Rudnicky, Alexander and Ramadge, Peter&#34;, editor = &#34;Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki&#34;, booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;, month = jul, year = &#34;2023&#34;, address = &#34;Toronto, Canada&#34;, publisher = &#34;Association for Computational Linguistics&#34;, url = &#34;https://aclanthology.org/2023.acl-short.102/&#34;, doi = &#34;10.18653/v1/2023.acl-short.102&#34;, pages = &#34;1183--1193&#34;, abstract = &#34;The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.&#34; } Download as File Copy to Clipboard &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt; &lt;mods ID=&#34;chi-etal-2023-latent&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Ta-Chung&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Chi&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Ting-Han&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Fan&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Li-Wei&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Chen&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Alexander&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Rudnicky&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Peter&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Ramadge&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;dateIssued&gt;2023-07&lt;/dateIssued&gt; &lt;/originInfo&gt; &lt;typeOfResource&gt;text&lt;/typeOfResource&gt; &lt;relatedItem type=&#34;host&#34;&gt; &lt;titleInfo&gt; &lt;title&gt;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/title&gt; &lt;/titleInfo&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Anna&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Rogers&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Jordan&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Boyd-Graber&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;name type=&#34;personal&#34;&gt; &lt;namePart type=&#34;given&#34;&gt;Naoaki&lt;/namePart&gt; &lt;namePart type=&#34;family&#34;&gt;Okazaki&lt;/namePart&gt; &lt;role&gt; &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt; &lt;/role&gt; &lt;/name&gt; &lt;originInfo&gt; &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt; &lt;place&gt; &lt;placeTerm type=&#34;text&#34;&gt;Toronto, Canada&lt;/placeTerm&gt; &lt;/place&gt; &lt;/originInfo&gt; &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt; &lt;/relatedItem&gt; &lt;abstract&gt;The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.&lt;/abstract&gt; &lt;identifier type=&#34;citekey&#34;&gt;chi-etal-2023-latent&lt;/identifier&gt; &lt;identifier type=&#34;doi&#34;&gt;10.18653/v1/2023.acl-short.102&lt;/identifier&gt; &lt;location&gt; &lt;url&gt;https://aclanthology.org/2023.acl-short.102/&lt;/url&gt; &lt;/location&gt; &lt;part&gt; &lt;date&gt;2023-07&lt;/date&gt; &lt;extent unit=&#34;page&#34;&gt; &lt;start&gt;1183&lt;/start&gt; &lt;end&gt;1193&lt;/end&gt; &lt;/extent&gt; &lt;/part&gt; &lt;/mods&gt; &lt;/modsCollection&gt; Download as File Copy to Clipboard %0 Conference Proceedings %T Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings %A Chi, Ta-Chung %A Fan, Ting-Han %A Chen, Li-Wei %A Rudnicky, Alexander %A Ramadge, Peter %Y Rogers, Anna %Y Boyd-Graber, Jordan %Y Okazaki, Naoaki %S Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) %D 2023 %8 July %I Association for Computational Linguistics %C Toronto, Canada %F chi-etal-2023-latent %X The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models. %R 10.18653/v1/2023.acl-short.102 %U https://aclanthology.org/2023.acl-short.102/ %U https://doi.org/10.18653/v1/2023.acl-short.102 %P 1183-1193 Download as File Copy to Clipboard Markdown (Informal) [Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://aclanthology.org/2023.acl-short.102/) (Chi et al., ACL 2023) Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings (Chi et al., ACL 2023) ACL Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. 2023. Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1183â€“1193, Toronto, Canada. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright Â©&nbsp;1963&ndash;2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License . Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License . The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 15 September 2025 at 14:04 UTC with commit aba5819 . "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2306.09200",
      "full_text": " [2306.09200] ChessGPT: Bridging Policy Learning and Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2306.09200 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2306.09200 (cs) [Submitted on 15 Jun 2023 ( v1 ), last revised 21 Dec 2023 (this version, v2)] Title: ChessGPT: Bridging Policy Learning and Language Modeling Authors: Xidong Feng , Yicheng Luo , Ziyan Wang , Hongrui Tang , Mengyue Yang , Kun Shao , David Mguni , Yali Du , Jun Wang View a PDF of the paper titled ChessGPT: Bridging Policy Learning and Language Modeling, by Xidong Feng and 8 other authors View PDF HTML (experimental) Abstract: When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model&#39;s chess ability. Experimental results validate our model and dataset&#39;s effectiveness. We open source our code, model, and dataset at this https URL . Comments: Published as a conference article in NeurIPS 2023 Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2306.09200 [cs.LG] &nbsp; (or arXiv:2306.09200v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2306.09200 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yicheng Luo [ view email ] [v1] Thu, 15 Jun 2023 15:35:31 UTC (189 KB) [v2] Thu, 21 Dec 2023 16:59:44 UTC (1,746 KB) Full-text links: Access Paper: View a PDF of the paper titled ChessGPT: Bridging Policy Learning and Language Modeling, by Xidong Feng and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-06 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://openreview.net/pdf?id=7UudBVsIrr",
      "full_text": "Under review as a conference paper at ICLR 2023\nMOLJET:\nMULTIMODAL\nJOINT\nEMBEDDING\nTRANSFORMER FOR CONDITIONAL DE NOVO MOLEC-\nULAR DESIGN AND MULTI-PROPERTY OPTIMIZATION\nAnonymous authors\nPaper under double-blind review\nABSTRACT\nMulti-property constrained optimization of molecules using generative de novo\ndesign models is vital for the successful application of Artificial Intelligence (AI)\ntowards materials and drug discovery. Yet there remains a gap between the re-\nported performance of such models in the literature and their practical utility in\nreal world design scenarios. Furthermore, existing models are largely inaccessi-\nble to chemists without an extensive background in computer science. To address\nthese challenges, we propose a generative foundation model, the Multimodal Joint\nEmbedding Transformer (MOLJET), which performs conditional generation of\ndesired molecular distributions based on human-interpretable chemistry prompts\nin a zero-shot manner. We assess MOLJET on the standard benchmarks available\nin the GuacaMol and MIMOSA evaluation frameworks. These include structure-\nbased sampling tasks as well as a range of multi-property optimization tasks that\nprobe a models ability to design drug-like molecules given realistic property con-\nstraints. We demonstrate that with self-supervised pretraining, MOLJET outper-\nforms 80% of task-optimized models while using zero-shot inferences and beats\nall baselines after minimal supervision. Moreover, the performance of MOLJET\non text-only conditioning tasks improves with the inclusion of property modalities\nduring training, highlighting the importance of a multimodal approach to molec-\nular design. MOLJET is the first example of text-based de novo molecular design\nusing large-scale multimodal foundation models and should serve as a building\nblock towards further improvements to accessible AI for chemists.\n1\nINTRODUCTION\nEmerging crises in climate, disease and human health threaten to permanently disrupt global stabil-\nity and must be actively met with creative solutions. Many such solutions are dependent on the rapid\ndiscovery of innovative functional materials or novel drug-like molecules with optimal properties.\nFor instance, the viability of using redox-flow batteries (RFBs) for long-term and large-scale energy\nstorage is contingent on finding stable redox species with fast electrochemical kinetics, a feasible\nredox potential and high solubility (Zhang et al., 2018). Due to the immense size and complexity\nof chemical phase space (Polishchuk et al., 2013), the search for suitable materials is far from triv-\nial and traditional â€œdirectâ€ design approaches based on iterative modifications to existing chemical\nstructures are often far too slow (Kuhn & Beratan, 1996).\nTo address this issue, researchers have increasingly begun to look towards generative de novo design\nmodels to efficiently navigate the vast molecular phase space (Meyers et al., 2021). These models\nare evaluated on their ability to generate a diverse array of novel molecular structures while simul-\ntaneously biasing them towards a desired property distribution (Polykovskiy et al., 2020). Due to\nthe ubiquity of string-based molecular representations (Weininger, 1988; Krenn et al., 2020), re-\ncent innovations in natural language modeling have been successfully applied to de novo molecular\ndesign. For instance, transformer architectures have achieved state-of-the-art results on property\nprediction tasks that require quantum-level accuracy (Ross et al., 2021) and have also been shown to\nincrease the diversity of candidates sampled from machine-learned molecular distributions (Dollar\net al., 2021).\n1\n\nUnder review as a conference paper at ICLR 2023\nI. Sample From Training Set\nII.        Train Model to \nReconstruct Molecule\n<text_type>\nJoint \nEmbedding \nTransformer Layers\nIUPAC\n</text_type>\n<text> â€¦\n<mol>\n[O]\n=\n[N]\n[Branch2]\nâ€¦\n[Ring1]\nIII. Conditional Generation\nâ€œpyrazolâ€\nText Conditions\nIUPAC\n[(4-phenylpiperazinâ€¦indole]-2'-one\nProperty Conditions\nPLogP\nQED\n-2.70\n0.82\nâ€œC7H8N2O2â€\nâ€œ90th percentile LogPâ€\nFigure 1: MOLJET Framework. Prompts are (i) stochastically sampled from the available modal-\nities in the dataset and (ii) used to condition autoregressive reconstruction of SELFIES strings. Con-\nditions are then chosen during inference to (iii) shift the generated molecular distribution towards\nthe desired structural or physicochemical properties.\nAside from string-based representations of molecular structures, there are other textual modalities\nwhich could provide additional context to generative models and thus improve their performance.\nSuch modalities include IUPAC names, molecular formulas, descriptions of important chemical\nmoieties or functional groups and natural language descriptions of chemical behavior. Yet despite\nthe large overlap between architectures used for natural language modeling and molecular sequence\nmodeling, there have only been a few attempts to incorporate more than a single modality within\na model (Rothchild et al., 2021; Sun et al., 2021; Zeng et al., 2022) and none have included the\ncapacity for property-driven molecular design. Massive scaling has also been primarily limited to\nproperty prediction tasks (Honda et al., 2019; Chithrananda et al., 2020) despite growing evidence\nof the performance benefits derived from increasing model sizes, dataset sizes and compute across\nall downstream tasks (Kaplan et al., 2020; Hoffmann et al., 2022).\nIn this work we introduce MOLJET, a large-scale multimodal joint embedding transformer for con-\nditional molecular generation and multi-property optimization. Within this framework, molecular\ngeneration is conditioned by text-based prompts that control the structural and physicochemical\ncharacteristics of the desired molecular distributions as depicted in Figure 1. We demonstrate condi-\ntional generation on three modalities - textual descriptions of molecular structural features, physic-\nochemical properties and 1D atomistic molecular graphs - and provide a general framework for the\ninclusion of additional modalities during pretraining.\nTo prove the efficacy of our models in realistic design scenarios, we evaluate MOLJET on a diverse\nset of tasks including molecular rediscovery, similarity and substructure-based sampling, isomer\ngeneration and multi-property optimization (Brown et al., 2019; Fu et al., 2021). With only self-\nsupervised pretraining, MolJET outperforms all task-optimized baseline models on five out of the\neight task categories and outperforms the baselines on all eight task categories after minimal task-\nspecific supervised optimization. Furthermore, the prompts are designed to be easily interpretable\nby chemists without any prior knowledge of deep learning and thus accessible to a wider audience.\nWe provide access to our pretrained models through an online API and hope to encourage increased\nparticipation in AI-driven de novo molecular design among scientific researchers in much the same\nway that DALL-E and GPT have inspired increased interaction with deep learning models among\nthe general public (Brown et al., 2020; Ramesh et al., 2022).\n2\nRELATED WORK\nMulti-Property Optimization. Several strategies for multi-property optimization of molecular\nstructures have been explored to date. Some works propose to condition the generation of molecular\nstructures with a learnable embedding corresponding to the values of one or more desired properties\n(Lim et al., 2018; Li et al., 2018; Gebauer et al., 2022). These models jointly learn the conditional\n2\n\nUnder review as a conference paper at ICLR 2023\ndistributions during training and then allow for the selection of specific conditions during inference.\nOthers treat optimization as a translation task, in which an improved version of the input molecule\nis reconstructed during training (Jin et al., 2018a; 2020). These models learn the desired molecular\ndistribution directly, however they also require the construction of translation pairs which can be\ntime-consuming and without careful control can introduce biases into the model or result in poste-\nrior collapse (Jin et al., 2018b). Another popular strategy for optimization is by making stepwise\nmodifications to an existing molecular structure through an efficient sampling method like Markov\nChain Monte Carlo or a reinforcement-learning driven policy network (Nigam et al., 2019; Khem-\nchandani et al., 2020; Fu et al., 2021). A reward function determines the success of the model and\nguides further modifications. These models are flexible as they can modify their actions based on\nany reward, however they often shift the generated distribution too far from the original and can\nstruggle to generate realistic samples (Popova et al., 2018; Brown et al., 2019).\nFoundation Models for Chemistry. Given that the vast majority of de novo molecular design\nmodels operate on a single molecular representation, there are only a few examples of multimodal\nlearning in the field of chemistry. KV-PLM and CHEMET both combine structural representa-\ntions of molecules with natural language, the former by embedding SMILES strings directly into\na biomedical corpus and the latter by performing cross-modal attention between embeddings of a\nmolecular graph and a description of the molecule (Sun et al., 2021; Zeng et al., 2022). However,\nthese models are better suited for classification tasks than generation tasks as it is challenging to\nbuild a corpus annotated with molecular structures that is large enough to train a generative model.\nOther examples of multimodal chemistry models include GeomGCL (Li et al., 2022) which per-\nforms contrastive learning on 2D and 3D molecular graphs for property prediction and VJTNN (Jin\net al., 2018b) which combines junction tree and atomic graph representations during the encoding\nand decoding of the latent vector in a VAE.\n3\nMODEL FRAMEWORK AND PROMPT DESIGNING\nHerein, we describe the Multimodal Joint Embedding Transformer (MOLJET), a large-scale gener-\native foundation model for conditional molecular design and multi-property optimization. The aim\nof MOLJET is to efficiently navigate the molecular phase space while simultaneously reaching a\ndesired property distribution. This task is non-trivial as the molecular landscape is high dimensional\nand rugged making optimization within this space difficult (Stumpfe et al., 2020). We hypothesize\nthat jointly learning across text, molecular structure and properties will enhance the modelâ€™s ability\nto learn structure-property relationships and thus improve its performance at designing optimized\nmolecules. We introduce the multimodal fusion with our prompt design framework in Section 3.1,\nand then present the model architecture and conditional sampling scheme in Sections 3.2 and 3.3,\nrespectively.\n3.1\nMULTIMODAL FUSION WITH PROMPT DESIGNING\nOur goal is to learn inter-modal and cross-modal information with an expressive prompt design\nthat can facilitate both the self-supervised pretraining and zero-shot evaluation. We propose an\nearly-fusion strategy to jointly reason over the text, molecular structure, and property modalities\nwith a shared multifaceted representation.\nWe represent the textual description and associated\nphysicochemical properties of a molecule in the prompt sequence x = (s1, s2, .., sn) of the form\n(stext, sprop, smol),\n<text type>...</text type> <text>..</text> <property>..</property> <val>..</val> <mol>..</mol>\nWe include\n<text type> and\n<property> tags to differentiate across molecule descriptions (stext)\nand properties (sprop). The\n<text> and\n<val> tags designate the search space on the respective\ndata modalities. The\n<mol> tag designates the SELFIES string describing the molecular structure\n(smol). The proposed prompt design is flexible so that other textual representations of molecules\nor associated properties may be easily substituted. We also allow each modality to contain multiple\nsub-prompts. For example, we can represent multiple physicochemical properties separately as sub-\nprompts in sprop. We introduce a strict ordering of the prompt sequence with the corresponding\ntext, property and molecular structure representations to enable the model to conditionally generate\nmolecular distributions given the other modalities.\n3\n\nUnder review as a conference paper at ICLR 2023\n3.2\nMODEL ARCHITECTURE\nOur objective is to pretrain a large-scale foundation model with the ability to generalize to unseen\ntasks without requiring any labeled data. This is specially relevant in molecular design scenarios\nwhere we need to generate new molecules that have not been previously seen (out-of-distribution\ngeneralization). However, it is intractable to enumerate across all possibilities due to the unbounded\nmolecular search space. We present the unsupervised distribution estimation p(x) from a set of\nprompts (x1, x2, .., xn) as the product of conditional multimodal token probabilities,\np(x) =\nn\nY\ni=1\np(sn|s1, .., snâˆ’1)\n(1)\nOur model design is inspired by the recent success of applying the transformer encoder architecture\non shared mulitmodal multifaceted representations (e.g., UTF-8 bytes in Perceiver-IO (Jaegle et al.,\n2021), vision-language decoding (Aghajanyan et al., 2022)). In this work, we investigate whether\ntransformer architectures are capable of learning over multimodal molecular information and trans-\nlating it into a rich knowledge of the relationship between a moleculeâ€™s structure and its properties.\nWe seek to analyze whether transformer architectures are suitable to distill and accumulate both\ninter- and cross-modal information from the molecular descriptions, and test whether the pretrained\nmodels generalize to novel contexts during de novo molecular design.\nTo this end, we adopt the autoregressive transformer decoder model architecture similar to GPT-\n3 (Brown et al., 2020) and apply it on conditional multimodal prompt based molecule generation\ntasks. We translate the general left-to-right language modeling objective to a joint modeling objec-\ntive that predicts the next modality token. We minimize the joint loss defined as\nL(Î¸) =\n1\n|Dtrain|\nX\nxâˆˆDtrain\nâˆ’logpÎ¸(si|sâ‰¤i)\n(2)\nThe model learns the conditional multimodal token distribution jointly given the in-context refer-\nences to other modality tokens. We do not use modality-specific encoders in this setup since we\ntranslate all modalities into the discrete language space. It remains as a future work to explore how\nother modalities such as vision (continuous), graph (2D) or atomic coordinates (3D) could be used\nin our framework to further enrich the learned multimodal molecular representations.\n3.3\nCONDITIONAL MOLECULE GENERATION\nGiven the molecular structure represented as a sequence of tokens describing the atoms, their con-\nnectivity and their valence states (m1, ..mn), the conditional multimodal prompt-based molecule\ngeneration is as follows:\nË†m â‰ˆarg max log pÎ¸\nm\n(mt|stext, sprop, m<t)\n(3)\nWe use q temperature sampling to autoregressively sample the SELFIES tokens mt conditioned on\nthe multimodal prompt. The sampling takes the molecule textual description stext, physicochemical\nproperties sprop and\n<mol> âˆˆm<t âŠ‚smol as the initial inputs in the joint multimodal embedding\nspace. In addition, the molecule generation is conditional to the property values in sprop.\nmt = q(Â·|stext, sprop, m<t)\nsmol(t) = âˆªm<tâˆˆsmol(tâˆ’1){(mâ‰¤t â—¦mn\nt )|mn\nt )}N\nn=1\n(4)\nWe sample N molecule tokens until we reach a\n</mol> tag. The sampled tokens are concatenated â—¦\nwith other top scoring molecule tokens to generate the molecule structure smol(t).\n4\n\nUnder review as a conference paper at ICLR 2023\n4\nEXPERIMENTAL SETUP\n4.1\nIMPLEMENTATION AND TRAINING DETAILS\nDataset Creation.\nWe gathered over 100M unique molecular structures from the PubChem com-\npound records database (Kim et al., 2019) to use for pretraining. Each structure includes a valid\nSMILES representation, an IUPAC1 name, and a molecular formula. Functional groups are ex-\ntracted from the full IUPAC name and SMILES are encoded as SELFIES strings. In accordance\nwith the method outlined in GuacaMol (Brown et al., 2019), we calculate the ECFP4 fingerprints\n(Rogers & Hahn, 2010) for every molecule in our dataset and a holdout set of drug-like molecules\nused in the benchmarks. Any molecule in the training set with a tanimoto fingerprint similarity of\nâ‰¥0.343 to any molecule in the holdout set is removed. This ensures the model has not simply mem-\norized solutions to the benchmark tasks during pretraining. Similarly, all isomers corresponding to\nthe two isomer generation tasks were also removed from the training set.\nConditional prompts for each molecule are generated stochastically so the model may only see a\nportion of the available modalities for any given sample. This allows the user to ignore some modal-\nities during inference while still allowing the model to jointly learn over all possible modalities. The\nrules for prompt sampling are outlined in Appendix B.\nAvailable Modalities.\nWe provide three modalities on which the models are conditioned - tex-\ntual molecule descriptions, properties and 1D atomistic molecular graphs. Table 1 shows the sub-\nmodalities available for the text and property modality types. Each text type provides a different level\nof detail regarding the molecular structure and are all commonly used by chemists when describing\nmolecules. The properties are selected to cover a wide range of chemical behavior important to\ndrug design. Each property is calculated using the cheminformatics package RDKit (Landrum et al.,\n2013) aside from DRD2 which is predicted by the model published in Olivecrona et al. (2017).\nWe use SELFIES as our 1D atomistic molecular graph to guarantee the validity of all molecules\ngenerated during inference (Krenn et al., 2020).\nTable 1: Details of the multimodal inputs used in the pretraining and zero-shot evaluation.\nTextual\nMolecule\nDescriptions\nIUPAC, text that fully\nspecifies the atomic\nconnectivity of the entire\nmolecule\nFuncGroups, text that specifies only the\natomic connectivity of local environments\nwithin the molecule\nMolFormula, text that does not\nspecify any connectivity information\nbut does specify the overall atomic\nmakeup of the molecule.\nPhysicochemical\nproperties\nTopological polar surface\narea (TPSA), a measure of the\noverall surface polarity of the\nmolecule (Prasanna &\nDoerksen, 2009)\nLogP/Penalized LogP (PLogP), a\nmethod for estimating the solubility of a\nmolecule (Wildman & Crippen,\n1999).PLogP includes penalties for\nmolecules with low synthesizability\nBertzCT, a topological index meant\nto quantify the â€œcomplexityâ€ of a\nmolecule (Bertz, 1981)\nQED, a quantitative measure\nof the â€œdrug-likenessâ€ of a\nmolecule (Bickerton et al.,\n2012)\nNumber of fluorine atoms, Number of\naromatic rings, Total number of rings\nDRD2, the biological activity of a\nmolecule towards the dopamine\nreceptor D2\nTokenization\nWe develop a custom vocabulary that consists of the tokens representing the\nmolecule textual description stext, physicochemical properties sprop and molecular structure smol.\nIUPAC and FuncGroups share a vocabulary learned from a byte-pair encoding of the IUPAC names\nin the training set. The MolFormulas and SELFIES are tokenized on a per-atom basis. Property\nvalues are represented as either scalars or decile ranges labeled 1-10 with each digit tokenized sep-\narately. Finally, all tags ( <..> , <../> ) and property names are encoded as special tokens.\n4.2\nTASK DESCRIPTIONS\nWe evaluate MOLJET on 22 tasks split across 8 different categories: molecular rediscovery, similar-\nity sampling, substructure sampling, isomer generation, median molecules, multi-property optimiza-\ntion, drug-likeness and biological activity. Each task is taken from either the GuacaMol evaluation\n1IUPAC (International Union of Pure and Applied Chemistry) nomenclature provides an international stan-\ndard of naming compounds which can be used to create unambiguous structural formula.\n5\n\nUnder review as a conference paper at ICLR 2023\nframework (Brown et al., 2019) or the MIMOSA multi-property optimization framework (Fu et al.,\n2021). Table 2 provides examples of tasks from a few of the optimization categories and their cor-\nresponding prompts. Detailed descriptions of each task category are provided below.\nTable 2: Example of the downstream tasks and prompt designs used in the zero-shot evaluation. We\ncolor each prompt with the modality(s) that they are associated with. For the prompts for all 22\ntasks, please refer to Tables 6 and 7 in Appendix A.\nTask/Example\nPrompt\nMolecular Rediscovery\n<text type>IUPAC</text type>\nCelecoxib\n<text>4-[5-(4-methylphenyl)..benzenesulfonamide</text><mol>\nSimilarity Sampling\n<text type>FuncGroups</text type>\nAlbuterol\n<text>butylamino,hydroxyethyl,phenol</text><mol>\nIsomer Generation\n<text type>MolFormula</text type>\nC11H24\n<text>C11H24</text><mol>\n<text type>IUPAC</text type>\nMulti-Property Optimization\n<text>N-[2-[2-(dimethylamino)..prop-2-enamide</text>\nOsimertinib\n<property>tpsa</property><val>146.0</val>\n<property>logp</property><val>-0.5</val><mol>\nMolecular Rediscovery. The model must generate an exact match to the target. This task tests the\nmodelâ€™s ability to explore regions of molecular phase space which it has not encountered during\ntraining.\nSimilarity Sampling. The model must generate many samples that are structurally similar to the\ntarget but not an exact match. This task tests the modelâ€™s ability to make small structural modifi-\ncations to a target without diverting too far from the original molecule. This is analogous to how a\nchemist might approach the design of a new drug by modifying small chemical motifs of a starting\nstructure to improve a specific desired behavior while maintaining other drug-like qualities from the\noriginal molecule.\nSubstructure Sampling. The model must generate many samples that contain a specific structural\nmotif or set of motifs. In some tasks, the model may also be penalized for generating molecules with\nnon-desired motifs or for diverging too far from the pharmacological properties of the molecule from\nwhich the desired motif is drawn. This task tests the modelâ€™s ability to generate functional moieties\noff a scaffold or â€œfill inâ€ the scaffold given a set of functional moieties.\nIsomer Generation. The model must generate as many structural isomers as it can from a given\nmolecular formula. This task tests the modelâ€™s ability to map coarse-grained chemical information\nto a fully connected atomic graph. It also tests if the model can enumerate all possible structures\nfrom a local region of chemical phase space.\nMedian Molecules. The model must generate samples that are maximally similar to two different\ntarget molecules. This task tests the modelâ€™s ability to interpolate between two valid chemical struc-\ntures, a common goal when trying to discover a molecule that maximizes the desired properties of\ntwo separate existing molecules.\nMulti-Property Optimization (MPO). The model must simultaneously match both structural and\nproperty requirements as dictated by the task. For instance, the model might be tasked with finding a\nstructural analogue to the antihistamine fexofenadine that is â€œless greasyâ€ by reducing the LogP and\nincreasing the TPSA while maintaining a high structural similarity to the target. These tasks put the\nmodel in realistic drug design scenarios and demonstrate its ability to perform structural sampling\nwhile also constraining the generated molecules to the desired property ranges.\nTo demonstrate the versatility of the MOLJET framework, we also evaluate the model on the multi-\nproperty optimization tasks outlined in Fu et al. (2021). These require the model to maintain high\nstructural similarity to an input drug-like molecule while simultaneously maximizing PLogP and\n6\n\nUnder review as a conference paper at ICLR 2023\neither QED (Drug-Likeness) or DRD2 (Biological Activity). We report performance on these two\ntasks as success rate which is defined as the proportion of input molecules that the model is able to\nimprove beyond a pre-defined threshold for each property while maintaining high similarity. Further\ndetails on the definition of success rate are provided in Jin et al. (2018b). Each GuacaMol task is\nevaluated based on a weighted average of the top 100 scoring molecules for that task. Further details\non the definitions of each GuacaMol metric are provided in Brown et al. (2019) and Appendix E.\nConditional Language Model Pretraining.\nWe train two independent version of MOLJET,\nMOLJET-GUAC and MOLJET-BIO. MOLJET-GUAC is trained and evaluated with the three text\ntypes and TPSA, LogP, BertzCT, number of fluorine atoms and ring counts (total and aromatic).\nMOLJET-BIO is trained and evaluated with the three text types and PLogP, QED and DRD2. We\ntrain two additional model variants - one to study the difference between scalar and decile prop-\nerty value representations (MOLJET-GUACSCALAR/DECILE)) and one without property conditioning to\nstudy the cumulative effect that additional modalities have on text-only inference tasks (MOLJET-\nGUACTEXT-ONLY/TEXT+PROP). The models are pretrained from scratch on the filtered PubChem train-\ning set. Further details on the training procedure, hyperparameters, baseline models and sampling\nscheme can be found in Appendices C & D.\n5\nEXPERIMENTAL RESULTS\nThe performances of MOLJET-BIO and MOLJET-GUAC on the MIMOSA and GuacaMol evalua-\ntion frameworks are displayed in Tables 3 and 4. Both models are very competitive during zero-shot\ninference with MOLJET-GUAC outperforming âˆ¼78% of all baselines on the GuacaMol benchmarks\nand MOLJET-BIO improving the success rate on the Drug-Likeness and Biological Activity tasks\nby 18.75% and 13.5% respectively. It should be noted that the baselines are fine-tuned on each task\nin a supervised manner, whereas MOLJET has only undergone self-supervised pretraining and is\nseeing the task-specific optimization prompts for the first time during inference. Thus, the perfor-\nmance on these benchmarks demonstrates the efficacy of our multimodal framework in generalizing\nto previously unseen molecular distributions.\nMulti-Property Optimization. We first show that MOLJET is able to leverage information from\nmultiple modalities to simultaneously control the structure and properties of generated molecules\nduring zero-shot inference. By conditioning the model on the modalities that are optimal for a given\ntask, it can generate molecular distributions that outperform previously state-of-the-art baselines\non a variety of multi-property optimization benchmarks. It accomplishes this by inferring how the\ndesired structural features must be modified to satisfy the additional property constraints. We use\nthe conditional generation sampling method described in Section 3.3 to efficiently explore the local\nregion of molecular phase space dictated by the multimodal prompt.\nFor example, MOLJET-BIO outperforms the previous state-of-the-art, MIMOSA, in both absolute\nproperty improvement and success rate on the Drug-Likeness and Biological Activity MPO tasks.\nIt does so by exploring the local region of molecular phase space surrounding the target molecule\nmore efficiently by directly sampling from the conditional distribution. Because MIMOSA makes\niterative modifications to the target molecule, it does not venture as far from the original structure\nduring optimization. While this leads to a higher similarity score on both tasks, it fails to find as\nmany molecules that satisfy the property optimization constraints and thus has a lower success rate.\nTable 3: Benchmark results on the MIMOSA MPO evaluation framework. PLogP, QED and DRD2\ncolumns refer to the absolute improvement in property values from successful samples.\nDrug-Likeness\nBiological Activity\nMethod\nSimilarity PLogP\nQED\nSuccess\nSimilarity PLogP\nDRD2 Success\nVJTNN\n0.17\n0.46\n0.02\n1.0%\n0.18\n0.55\n0.27\n3.4%\nDeepGA\n0.35\n0.93\n0.09\n24.9%\n0.38\n0.68\n0.20\n29.3%\nMIMOSA\n0.42\n0.93\n0.10\n32.0%\n0.54\n0.75\n0.35\n43.7%\nMOLJET-BIO\n(Zero-shot)\n0.37\n1.19\n0.14\n38.0%\n0.35\n3.38\n0.48\n49.6%\n7\n\nUnder review as a conference paper at ICLR 2023\nTable 4: Benchmark results on GuacaMol which contains both MPO and molecular structure gen-\neration tasks. Bold values indicate the best performing model and underlined values indicate the\nsecond best performing model measured against the baselines.\nBenchmark\nCategory\nBest of Data\nSet\nSMILES\nLSTM\nSMILES\nGA\nGraph\nGA\nMOLJET-GUAC\n(Zero-shot)\nMOLJET-GUAC\n+ Graph GA\nMPOs\n0.698\n0.778\n0.717\n0.868\n0.838\n0.878\nRediscovery\n0.613\n1.000\n0.523\n0.945\n1.000\n1.000\nSimilarity\n0.546\n1.000\n0.771\n0.977\n1.000\n1.000\nSubstructure\n0.643\n0.973\n0.769\n0.985\n0.817\n0.985\nIsomers\n0.716\n0.912\n0.745\n0.954\n1.000\n1.000\nMedian\n0.371\n0.403\n0.362\n0.417\n0.409\n0.447\nTotal\n0.623\n0.850\n0.671\n0.877\n0.857\n0.900\nWe observe a similar trend from zero-shot MOLJET-GUAC on the GuacaMol MPOs. When break-\ning the tasks down individually, it outperforms all three baselines on the ranolazine, perindopril, and\namlodipine MPOs and is within 1% and 2.5% of the best performing model on the fexofenadine\nand osimertinib MPOs, respectively (Appendix E). These tasks also require the model to meet one\nor more property specifications while maintaining high similarity to a target molecule (see Fexofe-\nnadine and Perindopril MPOs, Figure 2). In total, MOLJET outperforms or is competitive with the\nleading baseline on seven out of nine MPOs across both evaluation frameworks demonstrating the\nversatility and efficacy of our multimodal framework.\nConditional Molecular Structure Generation. MOLJET-GUAC also performs well at the zero-\nshot molecular structure generation tasks, achieving a perfect score on rediscovery, similarity sam-\npling and isomer generation (Table 4). This indicates that the model is able to accurately estimate\nthe molecular structural probability manifold of the training set and navigate it based on the condi-\ntional multimodal prompts. Each of the three text modalities provide a different degree of structural\nspecificity with which the model can be conditioned. For instance, tasks with stringent similarity\nrequirements are better suited for IUPAC conditioning, whereas FuncGroup conditioning yields a\nmore diverse set of generated molecules (see Drug-Likeness vs. Fexofenadine MPO in Fig. 2).\nFuncGroup conditioning is also the most flexible as it can be used to combine the structural charac-\nteristics of multiple input molecules (see Median Molecules, Fig. 2).\nDrug-Likeness\nText Conditions\nFuncGroups\nhydroxy, piperazin, \nindazol, methanone\nProperty Conditions\nPLogP\nQED\n10th decile\n10th decile\nInput Molecule\nplogp: 0.07\nqed: 0.57\nOutput Molecule\nplogp: 0.82\nqed: 0.68\nFexofenadine MPO\nText Conditions\nIUPAC\n2-[4-[1-hydroxy â€¦ \nmethylpropanoic acid\nProperty Conditions\nLogP\nTPSA\n<= 4\n9th decile\n>= 90\n5th decile\nInput Molecule\nlogp:   5.51\ntpsa: 81.00\nOutput Molecule\nlogp:     3.37\ntpsa: 121.46\nPerindopril MPO\nText Conditions\nProperty Conditions\n# Aromatic Rings\n2\nInput Molecule\naromatic rings: 0\nOutput Molecule\naromatic rings: 2\nFuncGroups\nethoxy, oxopentan, \npropanoyl, carboxylic \nacid\nMedian Molecules\nText Conditions\nbenzodioxol, \npyrazolo, pyrimidin, \nsulfonylphenyl\nFuncGroups\n*functional groups sampled from \nboth molecules*\nInput Molecules\nOutput Molecule\nsimilarity: 0.43\nFigure 2: Prompts, inputs and high-scoring samples for four of the de novo design tasks.\nWe confirm these observations quantitatively by measuring the performance of each text modality\nindividually on the similarity sampling tasks. We choose similarity as it is the most common struc-\ntural objective for the MPOs and thus highlights important differences in sampling performance for\nrealistic drug design scenarios. The results of this experiment are shown in Figure 3. As expected,\nwe explore the largest subset of relevant phase space when conditioning on FuncGroups. How-\n8\n\nUnder review as a conference paper at ICLR 2023\never, there are some circumstances where IUPAC conditioning is just as effective, namely when the\nmolecule is complex such as the stereoisomer mestranol.\nMestranol\nAlbuterol\nAripiprazole\nFigure 3: Similarity sampling from each\ntext modality.\nTo estimate how amenable MOLJET is to further opti-\nmization, we re-run the Graph GA method but replace\nthe starting population with the top 100 molecules gen-\nerated by MOLJET. On average, the Graph GA seeded\nwith molecules generated by MOLJET improves upon\nthe zero-shot MOLJET by âˆ¼5% and the baseline Graph\nGA by âˆ¼2.6% (Table 4). This demonstrates the capac-\nity of MOLJET to be further improved by task-specific\nfine-tuning strategies and we leave further work in this\ndirection as future research.\nEvaluating Prompt Design. We also run ablations to\nstudy a) the effect of the choice of numerical property\nrepresentation on the GuacaMol tasks with property con-\nditioning and b) the impact of the inclusion of property\nmodalities during training on GuacaMol tasks with text-\nonly conditioning. On the GuacaMol tasks with property conditioning, MOLJET-GUACSCALAR per-\nforms slightly better than MOLJET-GUACDECILE (0.881 vs. 0.872). This suggests that the property\nprediction capacity of the scalar model is only slightly greater than the average distance between\ndecile bins. For most properties, this distance is fairly large so this result indicates a potential area\nin which MOLJET could be improved.\nFinally, we evaluate MOLJET-GUACTEXT-ONLY and MOLJET-GUACTEXT+PROP on the text-only infer-\nence tasks from GuacaMol (Table 5). These tasks do not require any property conditioning during\ninference and thus the performance of the two models should be expected to be comparable if cross-\nmodal learning does not occur during training. However, we find that MOLJET-GUACTEXT+PROP\nperforms better on the text-only inference tasks, supporting our hypothesis that our multimodal\nprompt design framework supports both inter- and cross-modal learning. The property information\nthat is jointly embedded during training enhances the models understanding of molecular structure\neven when that information is not provided during inference.\nTable 5: Multimodal Model Ablations\nModality\nGuacaMol\nReconstruction\nIUPAC\nFuncGroup\nText\n0.827\n62.1%\n60.2%\nText + Property\n0.843\n68.7%\n63.4%\nTo confirm this behavior, we construct two\nadditional text-only inference tasks, IUPAC\nReconstruction and FuncGroup Reconstruc-\ntion.\nIUPAC Reconstruction tests the mod-\nels ability to accurately reconstruct a SELFIES\nstring given its IUPAC from a holdout set of\nIUPAC-SELFIES pairs that were not seen dur-\ning training. FuncGroup Reconstruction tests\nthe models ability to generate molecules that\ncontain the requested functional group from a list of 102 functional groups developed by the au-\nthors to include a wide range of atom types and complexities. Additional implementation details\nfor each task are outlined in Appendices A & F. Again, we find that MOLJET-GUACTEXT+PROP out-\nperforms MOLJET-GUACTEXT-ONLY, providing additional evidence that both inter- and cross-modal\nlearning occur during training and that multimodal joint embeddings are capable of enhancing the\nperformance of de novo molecular design models\n6\nCONCLUSION\nWe introduce MOLJET, a multimodal foundational chemistry model for conditional de novo design\nof organic molecules. MOLJET demonstrates state-of-the-art performance on realistic drug design\ntasks in a zero-shot manner. Our framework is adaptable and easy to interpret, making it well-suited\nfor the inclusion of other modalities such as scientific text. We make our code, models and data\npublicly available and provide API access to our pretrained models to allow chemistry researchers\nof all backgrounds to participate in the future development of AI-driven de novo molecular design.\n9\n\nUnder review as a conference paper at ICLR 2023\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-\nmodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hal-\nlahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu\nPurohit, Tri Songz, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive lan-\nguage modeling in pytorch, 2021. URL http://github.com/eleutherai/gpt-neox.\nSteven H Bertz. The first general index of molecular complexity. Journal of the American Chemical\nSociety, 103(12):3599â€“3601, 1981.\nG Richard Bickerton, Gaia V Paolini, JÂ´erÂ´emy Besnard, Sorel Muresan, and Andrew L Hopkins.\nQuantifying the chemical beauty of drugs. Nature chemistry, 4(2):90â€“98, 2012.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autore-\ngressive language model. 2022.\nNathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking\nmodels for de novo molecular design. Journal of chemical information and modeling, 59(3):\n1096â€“1108, 2019.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.\nSeyone Chithrananda, Gabriel Grand, and Bharath Ramsundar.\nChemberta: large-scale self-\nsupervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.\nOrion Dollar, Nisarg Joshi, David AC Beck, and Jim Pfaendtner. Attention-based generative models\nfor de novo molecular design. Chemical Science, 12(24):8362â€“8372, 2021.\nTianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, and Jimeng Sun. Mimosa: Multi-constraint\nmolecule sampling for molecule optimization. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 35, pp. 125â€“133, 2021.\nAnna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne\nLight, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a large-scale\nbioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100â€“D1107, 2012.\nNiklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert MÂ¨uller, and Kristof T\nSchÂ¨utt. Inverse design of 3d molecular structures with conditional generative neural networks.\nNature communications, 13(1):1â€“11, 2022.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nShion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint\nfor low data drug discovery. arXiv preprint arXiv:1911.04738, 2019.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative attention. In International Conference on Machine\nLearning, pp. 4651â€“4664. PMLR, 2021.\nJan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for\nthe exploration of chemical space. Chemical science, 10(12):3567â€“3572, 2019.\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\nJunction tree variational autoencoder for\nmolecular graph generation. In International conference on machine learning, pp. 2323â€“2332.\nPMLR, 2018a.\n10\n\nUnder review as a conference paper at ICLR 2023\nWengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-\ngraph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018b.\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs\nusing structural motifs. In International conference on machine learning, pp. 4839â€“4848. PMLR,\n2020.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nYash Khemchandani, Stephen Oâ€™Hagan, Soumitra Samanta, Neil Swainston, Timothy J Roberts,\nDanushka Bollegala, and Douglas B Kell. Deepgraphmolgen, a multi-objective, computational\nstrategy for generating molecules with desirable properties: a graph convolution and reinforce-\nment learning approach. Journal of cheminformatics, 12(1):1â€“17, 2020.\nSunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Ben-\njamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to\nchemical data. Nucleic acids research, 47(D1):D1102â€“D1109, 2019.\nMario Krenn, Florian HÂ¨ase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-\nreferencing embedded strings (selfies): A 100% robust molecular string representation. Machine\nLearning: Science and Technology, 1(4):045024, 2020.\nChristoph Kuhn and David N Beratan. Inverse strategies for molecular design. The Journal of\nPhysical Chemistry, 100(25):10595â€“10599, 1996.\nGreg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and\npredictive modeling. Greg Landrum, 2013.\nShuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong.\nGeomgcl: geometric graph\ncontrastive learning for molecular property prediction. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pp. 4541â€“4549, 2022.\nYibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional\ngraph generative model. Journal of cheminformatics, 10(1):1â€“24, 2018.\nJaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based\non conditional variational autoencoder for de novo molecular design. Journal of cheminformatics,\n10(1):1â€“9, 2018.\nJoshua Meyers, Benedek Fabian, and Nathan Brown. De novo molecular design and generative\nmodels. Drug Discovery Today, 26(11):2707â€“2715, 2021.\nAkshatKumar Nigam, Pascal Friederich, Mario Krenn, and AlÂ´an Aspuru-Guzik. Augmenting ge-\nnetic algorithms with deep neural networks for exploring the chemical space. arXiv preprint\narXiv:1909.11655, 2019.\nMarcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo\ndesign through deep reinforcement learning. Journal of cheminformatics, 9(1):1â€“14, 2017.\nPavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-\nlike chemical space based on gdb-17 data. Journal of computer-aided molecular design, 27(8):\n675â€“679, 2013.\nDaniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai\nTatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark\nVeselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models.\nFrontiers in pharmacology, 11:565644, 2020.\nMariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo\ndrug design. Science advances, 4(7):eaap7885, 2018.\n11\n\nUnder review as a conference paper at ICLR 2023\nS Prasanna and RJ Doerksen. Topological polar surface area: a useful descriptor in 2d-qsar. Current\nmedicinal chemistry, 16(1):21â€“41, 2009.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\npp. 3505â€“3506, 2020.\nDavid Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical informa-\ntion and modeling, 50(5):742â€“754, 2010.\nJerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel\nDas. Do large scale molecular language representations capture important structural information?\narXiv preprint arXiv:2106.09553, 2021.\nDaniel Rothchild, Alex Tamkin, Julie Yu, Ujval Misra, and Joseph Gonzalez. C5t5: Controllable\ngeneration of organic molecules with transformers. arXiv preprint arXiv:2108.10307, 2021.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism. arXiv preprint arXiv:1909.08053, 2019.\nDagmar Stumpfe, Huabin Hu, and JÂ¨urgen Bajorath. Advances in exploring activity cliffs. Journal\nof Computer-Aided Molecular Design, 34(9):929â€“942, 2020.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nChenkai Sun, Weijiang Li, Jinfeng Xiao, Nikolaus Nova Parulian, ChengXiang Zhai, and Heng Ji.\nFine-grained chemical entity typing with multimodal knowledge representation. In 2021 IEEE In-\nternational Conference on Bioinformatics and Biomedicine (BIBM), pp. 1984â€“1991. IEEE, 2021.\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to method-\nology and encoding rules. Journal of chemical information and computer sciences, 28(1):31â€“36,\n1988.\nScott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic\ncontributions. Journal of chemical information and computer sciences, 39(5):868â€“873, 1999.\nNaruki Yoshikawa, Kei Terayama, Masato Sumita, Teruki Homma, Kenta Oono, and Koji Tsuda.\nPopulation-based de novo molecule generation, using grammatical evolution. Chemistry Letters,\n47(11):1431â€“1434, 2018.\nZheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule\nstructure and biomedical text with comprehension comparable to human professionals. Nature\ncommunications, 13(1):1â€“11, 2022.\nChangkun Zhang, Leyuan Zhang, Yu Ding, Sangshan Peng, Xuelin Guo, Yu Zhao, Gaohong He,\nand Guihua Yu. Progress and prospects of next-generation redox flow batteries. Energy Storage\nMaterials, 15:324â€“350, 2018.\n12\n\nUnder review as a conference paper at ICLR 2023\nA\nPROMPT DESIGN\nTable 6: Example of the multi property optimization tasks and prompt designs used in the zero-shot\nevaluation. We color each prompt with the modality(s) that they are associated with.\nExample\nPrompt\nOsimertinib\n<text type>IUPAC</text type>\n<text>N-[2-[2-(dimethylamino)..prop-2-enamide</text>\n<property>tpsa</property><val>146.0</val>\n<property>logp</property><val>-0.5</val><mol>\nFexofenadine\n<text type>IUPAC</text type>\n<text>2-[4-(1-hydroxy...methylpropanoic acid]</text>\n<property>tpsa</property><val>9</val>\n<property>logp</property><val>5</val><mol>\nRanolazine\n<text type>IUPAC</text type>\n<text>N-(2,6-dimethylphenyl...piperazin-1-yl]acetamide</text>\n<property>logp</property><val>8.5</val>\n<property>aromatic rings</property><val>0</val><mol>\n<property>f count</property><val>1</val><mol>\nPerindopril\n<text type>FuncGroups</text type>\n<text>ethoxy,oxopentan,octahydroindole,carboxylic acid</text>\n<property>aromtic rings</property><val>2</val>\nAmlodipine\n<text type>FuncGroups</text type>\n<text>aminoethoxymethyl,chlorophenyl,dihydropyridine,dicarboxylate</text>\n<property>ring count</property><val>3</val>\nSitagliptin\n<text type>FuncGroups</text type>\n<text>amino,trifluoromethyl,triazolo,pyrazin</text>\n<text type>MolFormula</text type>\n<text>C16H15F6N5O</text>\n<property>logp</property><val>3</val>\n<property>tpsa</property><val>6</val><mol>\nZaleplon\n<text type>IUPAC</text type>\n<text>N-[3-(3-cyanopyrazolo...N-ethylacetamide]</text>\n<text type>MolFormula</text type>\n<text>C19H17N3O2</text>\nPLogP/QED\n<text type>FuncGroups</text type>\n<text>oxo,phenyl,triazaspiro,indole,carboxamide</text>\n(Drug-Likeness)\n<property>plogp</property><val>10</val>\n<property>qed</property><val>10</val><mol>\nPLogP/DRD2\n<text type>FuncGroups</text type>\n<text>oxo,triazolo,methoxyethyl,benzimidazol,dimethylacetamide</text>\n(Biological Activity)\n<property>plogp</property><val>10</val>\n<property>drd2</property><val>10</val><mol>\n13\n\nUnder review as a conference paper at ICLR 2023\nTable 7: Example of the conditional molecular structure generation tasks and prompt designs used\nin the zero-shot evaluation. We color each prompt with the modality(s) that they are associated with.\nTask\nExample\nPrompt\nMolecular\nRediscovery\nCelecoxib\n<text type>IUPAC</text type>\n<text>4-[5-(4-methylphenyl)..benzenesulfonamide</text><mol>\nTroglitazone\n<text type>IUPAC</text type>\n<text>5-[[4-[(6-hydroxy...thiazolidine-2,4-dione]]]</text><mol>\nThiothixene\n<text type>IUPAC</text type>\n<text>(9Z)-N,N-dimethyl...thioxanthene-2-sulfonamide</text><mol>\nSimilarity\nSampling\nAlbuterol\n<text type>FuncGroups</text type>\n<text>butylamino,hydroxyethyl,phenol</text><mol>\nAripiprazole\n<text type>FuncGroups</text type>\n<text>dichlorophenyl,piperazin,quinolin</text><mol>\nMestranol\n<text type>FuncGroups</text type>\n<text>ethynyl,methoxy,methyl,octahydro,phenanthren</text><mol>\nIsomer\nGeneration\nC11H24\n<text type>MolFormula</text type>\n<text>C11H24</text><mol>\nC9H10N2O2PF2Cl\n<text type>MolFormula</text type>\n<text>C9H10N2O2PF2Cl</text><mol>\nMedian\nMolecules\nCamphor/Menthol\n<text type>FuncGroups</text type>\n<text>heptan,methyl,trimethylbicyclo,ylcyclohexan</text><mol>\nTadalafil/Sildenafil\n<text type>FuncGroups</text type>\n<text>pyrazolo,triazatetracyclo,pyrimidin,methylpiperazin</text><mol>\nSubstructure\nSampling\nValsartan\n<text type>IUPAC</text type>\n<text>methanoyl-methyl...phenyl]methyl]amine</text><mol>\n<property>logp</property><val>2.0</val><\n<property>tpsa</property><val>77.0</val><\n<property>bertzct</property><val>896.4</val><\nDeco Hop\n<text type>FuncGroups</text type>\n<text>amino,hydroxy,quinazoline</text><mol>\nScaffold Hop\n<text type>FuncGroups</text type>\n<text>propanol,benzothiazol</text><mol>\n14\n\nUnder review as a conference paper at ICLR 2023\nB\nPROMPT SAMPLING STRATEGY\nPrompts are stochastically generated from the available modalities by the following set of rules:\nâ€¢ The text modality is sampled uniformly from the list\n(IUPAC, FuncGroups, MolFormula, None) .\nIf\nNone is selected then no text conditioning is included for that sample. This allows the\nuser to perform property-only conditioning by leaving out the text conditioning during\ninference.\nâ€¢ If\nFuncGroups is chosen, then the number of functional groups, N, used for conditioning is\nsampled uniformly from\n[1-M] where M is the total number of functional groups for the\ngiven molecule. Then N functional groups are selected from the list and concatenated with\ncommas.\nâ€¢ Next, the number of property conditions, K, is sampled uniformly from\n[0-L] where L is\nthe total number of property modalities available for training. Then K properties are chosen\nfrom the list and their property names and values are added to the prompt after the text type\nand text. The ordering of property sub-modalities is also stochastic.\nC\nTRAINING & SAMPLING IMPLEMENTATION DETAILS\nWe use the GPT-NeoX Python library Andonian et al. (2021) developed with Megatron Shoeybi\net al. (2019) and DeepSpeed Rasley et al. (2020). We optimize the autoregressive log-likelihood\n(i.e., cross-entropy loss) averaged over a 256-token context. We set the global batch size as 2048,\nand the learning rate to 2 Ã— 10âˆ’4, and rely on the cosine decay. We use an Adam optimizer with\nÎ²1 = 0.9, Î²2 = 0.99, and Ïƒ = 10âˆ’8 and clip the gradient norm at 1.0. We use the Rotary positional\nembeddings Su et al. (2021), parallel attention and feed-forward (FF) Black et al. (2022), and all\ndense layers in comparison to the original transformer decoder model architecture Radford et al.\n(2019).\nWe use a q temperature value of 1.0 for sampling for evaluating all 22 tasks. We found that this value\ngives us the best tradeoff between the validity and diversity of the generated molecules. For each\nGuacaMol task, we generate 128K samples to use for evaluation. This is on the order of the number\nof samples that are generated and evaluated during fine-tuning of the GuacaMol baselines. For the\nDrug-Likeness and Biological Activity tasks, we evaluate on 250 molecules randomly sampled from\na subset of the ZINC dataset provided in Jin et al. (2018b) in accordance with the methods outlined\nin Fu et al. (2021). For each molecule, we generate 1K samples which is on the order of the number\nof samples that are generated and evaluated during fine-tuning of the MIMOSA baselines.\nD\nBASELINE MODELS\nWe compare MOLJET to two sets of baselines â€“ one for the GuacaMol tasks and another for the\nDrug-Likeness/Biological Activity tasks. The GuacaMol baselines include:\nâ€¢ Best of Data Set, the metrics evaluated on the top molecules from the ChEMBL dataset\n(Gaulton et al., 2012))\nâ€¢ SMILES LSTM, an LSTM model which is fine-tuned with the hill-climbing method\n(Brown et al., 2019))\nâ€¢ SMILES GA, a genetic algorithm that makes mutations to a SMILES string (Yoshikawa\net al., 2018))\nâ€¢ Graph GA, a genetic algorithm that makes mutations directly to a molecular graph (Jensen,\n2019))\nThe Drug-Likeness/Biological Activity baselines include:\nâ€¢ VJTNN, a graph-to-graph translation VAE that utilizes adversarial regularization (Jin et al.,\n2018b))\nâ€¢ DeepGA, a genetic algorithm enhanced with a discriminator neural network to improve\nmolecular diversity (Nigam et al., 2019))\n15\n\nUnder review as a conference paper at ICLR 2023\nâ€¢ MIMOSA, a Markov chain Monte Carlo sampling strategy augmented by pretrained graph\nneural networks (Fu et al., 2021))\nE\nMODEL PERFORMANCE ON INVIDIDUAL GUACAMOL TASKS\nTable 8 shows the detailed performance view on the GuacaMol benchmark. Aside from the redis-\ncovery tasks, the final score for each metric is evaluated as a weighted average of the top 100 scoring\nmolecules that were generated during sampling. The scores for individual molecules are based on\ntheir ECFP4 (Rogers & Hahn, 2010) fingerprint similarities to the targets, calculated property values\nand structural features. These values are passed through a set of modifiers and thresholds to scale\nthem between 0 and 1. The score is then calculated as the geometric mean of each scaled task-\nspecific value. For further details on the metric definition of each benchmark, please refer to Brown\net al. (2019).\nTable 8: Benchmark results on GuacaMol which contains both MPO and molecular structure gen-\neration tasks. Bold values indicate the best performing model and underlined values indicate the\nsecond best performing model\nBenchmark\nCategory\nBenchmark\nBest of Data\nSet\nSMILES\nLSTM\nSMILES\nGA\nGraph\nGA\nMOLJET-GUAC\n(Zero-shot)\nMOLJET-GUAC\n+ Graph GA\nOsimertinib\n0.781\n0.894\n0.880\n0.937\n0.914\n0.992\nFexofenadine\n0.817\n0.926\n0.904\n1.000\n0.997\n1.000\nRanolazine\n0.836\n0.833\n0.832\n0.913\n0.920\n0.920\nMPOs\nPerindopril\n0.701\n0.764\n0.644\n0.803\n0.804\n0.823\nAmlodipine\n0.696\n0.885\n0.678\n0.888\n0.895\n0.903\nSitagliptin\n0.509\n0.536\n0.526\n0.809\n0.758\n0.823\nZaleplon\n0.547\n0.610\n0.552\n0.728\n0.625\n0.688\nCelecoxib\n0.674\n1.000\n0.570\n0.836\n1.000\n1.000\nRediscovery\nTroglitazone\n0.558\n1.000\n0.523\n1.000\n1.000\n1.000\nThiothixene\n0.608\n1.000\n0.476\n1.000\n1.000\n1.000\nAlbuterol\n0.522\n1.000\n0.871\n1.000\n1.000\n1.000\nSimilarity\nAripiprazole\n0.595\n1.000\n0.747\n0.985\n0.999\n1.000\nMestranol\n0.520\n1.000\n0.695\n0.945\n1.000\n1.000\nValsartan\n0.259\n0.931\n0.628\n0.958\n0.930\n0.977\nSubstructures Deco Hop\n0.933\n0.996\n0.876\n0.995\n0.893\n0.996\nScaffold Hop\n0.738\n0.993\n0.803\n1.000\n0.632\n0.984\nIsomers\nC11H24\n0.684\n0.963\n0.734\n0.952\n1.000\n1.000\nC9H10N2O2PF2Cl\n0.747\n0.860\n0.757\n0.955\n1.000\n1.000\nMedian\nCamphor/Menthol\n0.334\n0.398\n0.348\n0.405\n0.386\n0.416\nTadalafil/Sildenafil\n0.407\n0.408\n0.377\n0.429\n0.434\n0.478\nTotal\nâ€”\n0.623\n0.850\n0.671\n0.877\n0.857\n0.900\nF\nRECONSTRUCTION TASKS\nTo validate the ablation on the Text + Property vs. the Text-Only models, we construct two addi-\ntional tasks that evaluate the modelâ€™s performance on text-only conditioning - IUPAC Reconstruction\nand FuncGroup Reconstruction. An IUPAC reconstruction is counted as successful if the generated\nSELFIES string exactly matches the canonical SMILES from the holdout set after being decoded\nback into a SMILES and canonicalized. IUPAC Reconstruction is evaluated on 10000 randomly\nsampled IUPAC/SMILES pairs from the holdout validation set. A FuncGroup reconstruction is\ncounted as successful when the SMILES string decoded from the generated SELFIES string matches\nthe substructure pattern matching the requested functional group (we use SMARTS substructures for\nmatching). We hand select 102 functional groups to test the model on its ability to recognize simple\nfunctional groups, basic nitrogen heterocycles, basic oxygen heterocycles, basic mixed heterocycles,\ndouble ring nitrogen heterocycles, double ring oxygen heterocycles, polycyclic aromatic hydrocar-\nbons, fused rings and phenyls among others. The full dataset will be made available upon request.\n16\n"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.01119",
      "full_text": " [2310.01119] Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.01119 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2310.01119 (cs) [Submitted on 2 Oct 2023 ( v1 ), last revised 8 Jan 2024 (this version, v2)] Title: Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models Authors: Jean Kaddour , Qi Liu View a PDF of the paper titled Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models, by Jean Kaddour and 1 other authors View PDF Abstract: The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model&#39;s performance, occasionally necessitating only a minor fraction of the original training dataset. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2310.01119 [cs.CL] &nbsp; (or arXiv:2310.01119v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.01119 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jean Kaddour [ view email ] [v1] Mon, 2 Oct 2023 11:49:05 UTC (57 KB) [v2] Mon, 8 Jan 2024 13:09:24 UTC (98 KB) Full-text links: Access Paper: View a PDF of the paper titled Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models, by Jean Kaddour and 1 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.02406",
      "full_text": " [2312.02406] Efficient Online Data Mixing For Language Model Pre-Training Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.02406 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2312.02406 (cs) [Submitted on 5 Dec 2023 ( v1 ), last revised 9 Dec 2023 (this version, v2)] Title: Efficient Online Data Mixing For Language Model Pre-Training Authors: Alon Albalak , Liangming Pan , Colin Raffel , William Yang Wang View a PDF of the paper titled Efficient Online Data Mixing For Language Model Pre-Training, by Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang View PDF HTML (experimental) Abstract: The data used to pretrain large language models has a decisive impact on a model&#39;s downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2312.02406 [cs.CL] &nbsp; (or arXiv:2312.02406v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.02406 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Alon Albalak [ view email ] [v1] Tue, 5 Dec 2023 00:42:35 UTC (175 KB) [v2] Sat, 9 Dec 2023 00:27:18 UTC (175 KB) Full-text links: Access Paper: View a PDF of the paper titled Efficient Online Data Mixing For Language Model Pre-Training, by Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.06266",
      "full_text": " [2310.06266] CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.06266 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Software Engineering arXiv:2310.06266 (cs) [Submitted on 10 Oct 2023 ( v1 ), last revised 10 Jan 2024 (this version, v2)] Title: CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model Authors: Peng Di , Jianguo Li , Hang Yu , Wei Jiang , Wenting Cai , Yang Cao , Chaoyu Chen , Dajun Chen , Hongwei Chen , Liang Chen , Gang Fan , Jie Gong , Zi Gong , Wen Hu , Tingting Guo , Zhichao Lei , Ting Li , Zheng Li , Ming Liang , Cong Liao , Bingchang Liu , Jiachen Liu , Zhiwei Liu , Shaojun Lu , Min Shen , Guangpei Wang , Huan Wang , Zhi Wang , Zhaogui Xu , Jiawei Yang , Qing Ye , Gehao Zhang , Yu Zhang , Zelin Zhao , Xunjin Zheng , Hailian Zhou , Lifu Zhu , Xianying Zhu View a PDF of the paper titled CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model, by Peng Di and 37 other authors View PDF HTML (experimental) Abstract: Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup&#39;s software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts. Comments: Accepted by ICSE-SEIP 2024 Subjects: Software Engineering (cs.SE) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2310.06266 [cs.SE] &nbsp; (or arXiv:2310.06266v2 [cs.SE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.06266 Focus to learn more arXiv-issued DOI via DataCite Related DOI : https://doi.org/10.1145/3639477.3639719 Focus to learn more DOI(s) linking to related resources Submission history From: Peng Di [ view email ] [v1] Tue, 10 Oct 2023 02:38:44 UTC (4,623 KB) [v2] Wed, 10 Jan 2024 19:59:03 UTC (4,623 KB) Full-text links: Access Paper: View a PDF of the paper titled CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model, by Peng Di and 37 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.SE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.01602",
      "full_text": " [2310.01602] CAT-LM: Training Language Models on Aligned Code And Tests Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.01602 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Software Engineering arXiv:2310.01602 (cs) [Submitted on 2 Oct 2023] Title: CAT-LM: Training Language Models on Aligned Code And Tests Authors: Nikitha Rao , Kush Jain , Uri Alon , Claire Le Goues , Vincent J. Hellendoorn View a PDF of the paper titled CAT-LM: Training Language Models on Aligned Code And Tests, by Nikitha Rao and 4 other authors View PDF Abstract: Testing is an integral part of the software development process. Yet, writing tests is time-consuming and therefore often neglected. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation. Subjects: Software Engineering (cs.SE) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2310.01602 [cs.SE] &nbsp; (or arXiv:2310.01602v1 [cs.SE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.01602 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Nikitha Rao [ view email ] [v1] Mon, 2 Oct 2023 19:52:22 UTC (2,232 KB) Full-text links: Access Paper: View a PDF of the paper titled CAT-LM: Training Language Models on Aligned Code And Tests, by Nikitha Rao and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.SE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2308.12908",
      "full_text": " [2308.12908] POLCA: Power Oversubscription in LLM Cloud Providers Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2308.12908 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2308.12908 (cs) [Submitted on 24 Aug 2023] Title: POLCA: Power Oversubscription in LLM Cloud Providers Authors: Pratyush Patel , Esha Choukse , Chaojie Zhang , ÃÃ±igo Goiri , Brijesh Warrier , Nithish Mahalingam , Ricardo Bianchini View a PDF of the paper titled POLCA: Power Oversubscription in LLM Cloud Providers, by Pratyush Patel and 6 other authors View PDF Abstract: Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow. We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism. We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) ; Hardware Architecture (cs.AR); Machine Learning (cs.LG) Cite as: arXiv:2308.12908 [cs.DC] &nbsp; (or arXiv:2308.12908v1 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2308.12908 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Pratyush Patel [ view email ] [v1] Thu, 24 Aug 2023 16:32:34 UTC (847 KB) Full-text links: Access Paper: View a PDF of the paper titled POLCA: Power Oversubscription in LLM Cloud Providers, by Pratyush Patel and 6 other authors View PDF TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-08 Change to browse by: cs cs.AR cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.13322",
      "full_text": " [2312.13322] MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.13322 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Programming Languages arXiv:2312.13322 (cs) [Submitted on 20 Dec 2023 ( v1 ), last revised 19 Sep 2024 (this version, v3)] Title: MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks Authors: Tal Kadosh , Niranjan Hasabnis , Vy A. Vo , Nadav Schneider , Neva Krien , Mihai Capota , Abdul Wasay , Nesreen Ahmed , Ted Willke , Guy Tamir , Yuval Pinter , Timothy Mattson , Gal Oren View a PDF of the paper titled MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks, by Tal Kadosh and 12 other authors View PDF HTML (experimental) Abstract: With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs. Subjects: Programming Languages (cs.PL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) Cite as: arXiv:2312.13322 [cs.PL] &nbsp; (or arXiv:2312.13322v3 [cs.PL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.13322 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tal Kadosh [ view email ] [v1] Wed, 20 Dec 2023 15:11:06 UTC (3,506 KB) [v2] Tue, 17 Sep 2024 16:29:03 UTC (3,506 KB) [v3] Thu, 19 Sep 2024 19:28:09 UTC (3,506 KB) Full-text links: Access Paper: View a PDF of the paper titled MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks, by Tal Kadosh and 12 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.PL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.AI cs.LG cs.SE References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.07625",
      "full_text": " [2312.07625] Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.07625 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Neural and Evolutionary Computing arXiv:2312.07625 (cs) [Submitted on 12 Dec 2023 ( v1 ), last revised 26 Dec 2023 (this version, v2)] Title: Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling Authors: Guobin Shen , Dongcheng Zhao , Yiting Dong , Yang Li , Jindong Li , Kang Sun , Yi Zeng View a PDF of the paper titled Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling, by Guobin Shen and 6 other authors View PDF HTML (experimental) Abstract: Within the complex neuroarchitecture of the brain, astrocytes play crucial roles in development, structure, and metabolism. These cells regulate neural activity through tripartite synapses, directly impacting cognitive processes such as learning and memory. Despite the growing recognition of astrocytes&#39; significance, traditional Spiking Neural Network (SNN) models remain predominantly neuron-centric, overlooking the profound influence of astrocytes on neural dynamics. Inspired by these biological insights, we have developed an Astrocyte-Modulated Spiking Unit (AM-SU), an innovative framework that integrates neuron-astrocyte interactions into the computational paradigm, demonstrating wide applicability across various hardware platforms. Our Astrocyte-Modulated Spiking Neural Network (AstroSNN) exhibits exceptional performance in tasks involving memory retention and natural language generation, particularly in handling long-term dependencies and complex linguistic structures. The design of AstroSNN not only enhances its biological authenticity but also introduces novel computational dynamics, enabling more effective processing of complex temporal dependencies. Furthermore, AstroSNN shows low latency, high throughput, and reduced memory usage in practical applications, making it highly suitable for resource-constrained environments. By successfully integrating astrocytic dynamics into intelligent neural networks, our work narrows the gap between biological plausibility and neural modeling, laying the groundwork for future biologically-inspired neural computing research that includes both neurons and astrocytes. Subjects: Neural and Evolutionary Computing (cs.NE) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2312.07625 [cs.NE] &nbsp; (or arXiv:2312.07625v2 [cs.NE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.07625 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Guobin Shen [ view email ] [v1] Tue, 12 Dec 2023 06:56:31 UTC (10,697 KB) [v2] Tue, 26 Dec 2023 02:54:29 UTC (17,322 KB) Full-text links: Access Paper: View a PDF of the paper titled Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling, by Guobin Shen and 6 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.NE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.09099",
      "full_text": " [2402.09099] Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.09099 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Artificial Intelligence arXiv:2402.09099 (cs) [Submitted on 14 Feb 2024 ( v1 ), last revised 6 Aug 2025 (this version, v7)] Title: Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models Authors: Xiongye Xiao , Heng Ping , Chenyu Zhou , Defu Cao , Yaxing Li , Yi-Zhuo Zhou , Shixuan Li , Nikos Kanakaris , Paul Bogdan View a PDF of the paper titled Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models, by Xiongye Xiao and 8 other authors View PDF HTML (experimental) Abstract: In recent years, there has been increasing attention on the capabilities of large models, particularly in handling complex tasks that small-scale models are unable to perform. Notably, large language models (LLMs) have demonstrated ``intelligent&#39;&#39; abilities such as complex reasoning and abstract language comprehension, reflecting cognitive-like behaviors. However, current research on emergent abilities in large models predominantly focuses on the relationship between model performance and size, leaving a significant gap in the systematic quantitative analysis of the internal structures and mechanisms driving these emergent abilities. Drawing inspiration from neuroscience research on brain network structure and self-organization, we propose (i) a general network representation of large models, (ii) a new analytical framework, called Neuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and (iii) a novel structure-based metric as a proxy for emergent abilities of large models. By linking structural features to the capabilities of large models, NeuroMFA provides a quantitative framework for analyzing emergent phenomena in large models. Our experiments show that the proposed method yields a comprehensive measure of network&#39;s evolving heterogeneity and organization, offering theoretical foundations and a new perspective for investigating emergent abilities in large models. Comments: Accepted at ICLR 2025. OpenReview: this https URL Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2402.09099 [cs.AI] &nbsp; (or arXiv:2402.09099v7 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.09099 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xiongye Xiao [ view email ] [v1] Wed, 14 Feb 2024 11:20:09 UTC (24,926 KB) [v2] Mon, 4 Mar 2024 11:22:38 UTC (25,126 KB) [v3] Tue, 5 Mar 2024 10:44:36 UTC (25,126 KB) [v4] Thu, 21 Mar 2024 05:33:23 UTC (26,400 KB) [v5] Fri, 24 Jan 2025 08:22:25 UTC (29,196 KB) [v6] Tue, 4 Feb 2025 07:46:33 UTC (29,196 KB) [v7] Wed, 6 Aug 2025 03:34:21 UTC (24,999 KB) Full-text links: Access Paper: View a PDF of the paper titled Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models, by Xiongye Xiao and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.12399",
      "full_text": " [2402.12399] Turn Waste into Worth: Rectifying Top-$k$ Router of MoE Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.12399 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2402.12399 (cs) [Submitted on 17 Feb 2024 ( v1 ), last revised 21 Feb 2024 (this version, v2)] Title: Turn Waste into Worth: Rectifying Top-$k$ Router of MoE Authors: Zhiyuan Zeng , Qipeng Guo , Zhaoye Fei , Zhangyue Yin , Yunhua Zhou , Linyang Li , Tianxiang Sun , Hang Yan , Dahua Lin , Xipeng Qiu View a PDF of the paper titled Turn Waste into Worth: Rectifying Top-$k$ Router of MoE, by Zhiyuan Zeng and 9 other authors View PDF HTML (experimental) Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2402.12399 [cs.LG] &nbsp; (or arXiv:2402.12399v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.12399 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhiyuan Zeng [ view email ] [v1] Sat, 17 Feb 2024 06:23:27 UTC (211 KB) [v2] Wed, 21 Feb 2024 13:33:12 UTC (211 KB) Full-text links: Access Paper: View a PDF of the paper titled Turn Waste into Worth: Rectifying Top-$k$ Router of MoE, by Zhiyuan Zeng and 9 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    },
    {
      "arxiv_id": "https://together.ai/blog/redpajama-models-v1",
      "full_text": " Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned &amp; chat models This website uses cookies to anonymously analyze website traffic using Google Analytics. Accept Decline Ã°ÂŸÂ“Âº Deep Dive on NVIDIA Blackwell with Dylan Patel (Semianalysis) and Ian Buck (NVIDIA) on 10/1 | RSVP Ã¢Â†Â’ Ã¢ÂšÂ¡ Together Instant Clusters: self-service NVIDIA GPUs, now generally available Ã¢Â†Â’ Ã°ÂŸÂÂ‹ DeepSeek-V3.1 is now available on Together AI Ã¢Â†Â’ Ã°ÂŸÂ”Â¥ Announcing DeepSWE: our SOTA software engineering agent Ã¢Â†Â’ Ã°ÂŸÂ”Â’ Together AI achieves SOC 2 Type 2 compliance Ã¢Â†Â’ Model Platform Model Platform Products Serverless Inference API for inference on open-source models Dedicated Endpoints Deploy models on custom hardware Fine-Tuning Train &amp; improve high-quality, fast models Evaluations Measure model quality Together Chat Chat app for open-source AI Code Execution Code Sandbox Build AI development environments Code Interpreter Execute LLM-generated code Tools Which LLM to Use Find the Ã¢Â€Â˜rightÃ¢Â€Â™ model for your use case Models See all modelsÃ‚ Ã¢Â†Â’ OpenAIÃ¢Â€Â¨ gpt-oss Ã‚ Ã¢Â†Â’ OpenAIÃ¢Â€Â¨gpt-oss This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ DeepSeek Ã‚ Ã¢Â†Â’ DeepSeek This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ Qwen Ã‚ Ã¢Â†Â’ Qwen3 235B This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ Llama Ã‚ Ã¢Â†Â’ Llama This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ Kimi K2 Ã‚ Ã¢Â†Â’ Kimi K2 This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ Cogito Ã‚ Ã¢Â†Â’ Cogito This is some text inside of a div block. Ã‚ Ã¢Â†Â’ try it Ã¢Â†Â’ GPU Cloud GPU Cloud Clusters of Any Size Instant Clusters Ready to use, self-service GPUs Reserved Clusters Dedicated capacity, with expert support Frontier AI Factory 1K Ã¢Â†Â’ 10K Ã¢Â†Â’ 100K+ NVIDIA GPUs Cloud Services Data Center Locations Global GPU power in 25+ cities Slurm Cluster management system GPUs NVIDIA GB200 NVL72 Ã‚ Ã¢Â†Â’ NVIDIA GB00 NVL72 try it Ã¢Â†Â’ NVIDIA HGX B200 Ã‚ Ã¢Â†Â’ NVIDIA HGX B200 try it Ã¢Â†Â’ NVIDIA H200 Ã‚ Ã¢Â†Â’ NVIDIA H200 try it Ã¢Â†Â’ NVIDIA H100 Ã‚ Ã¢Â†Â’ NVIDIA H100 try it Ã¢Â†Â’ Solutions Solutions Solutions Enterprise Secure, reliable AI infrastructure Customer Stories Testimonials from AI pioneers Why Open Source How to own your AI Industries &amp; Use-Cases Scale your business with Together AI Customer Stories How Hedra Scales Viral AI Video Generation with 60% Cost Savings When Standard Inference Frameworks Failed, Together AI Enabled 5x Performance Breakthrough Developers Developers Developers Documentation Technical docs for using Together AI Research Advancing the open-source AI frontier Model Library All our open-source models Cookbooks Practical implementation guides Example Apps Our open-source demo apps Videos DeepSeek-R1: How It Works, Simplified! Together Code Sandbox: How To Build AI Coding Agents Pricing Pricing Pricing Pricing Overview Our platform &amp; GPU pricing. Inference Per-token &amp; per-minute pricing. Fine-Tuning LoRA and full fine-tuning pricing. GPU Clusters Hourly rates &amp; custom pricing. Questions?Ã¢Â€Â¨ WeÃ¢Â€Â™re here to help! Talk to us Ã¢Â†Â’ Company Company Company About us Get to know us Values Our approach to open-source AI Team Meet our leadership Careers Join our mission Resources Blog Our latest news &amp; blog posts Research Advancing the open-source AI frontier Events Explore our events calendar Knowledge Base Find answers to your questions Featured Blog Posts Together AI Delivers Top Speeds for DeepSeek-R1-0528 Inference on NVIDIA Blackwell Powering Secure AI: Together AI Achieves SOC 2 Type 2 Compliance Sign In Contact Sales Chat Docs Blog Support Contact Sales Company Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned &amp; chat models May 5, 2023 Ã£ÂƒÂ» ByÃ‚ Together The RedPajama project aims to create a set of leading open-source models and to rigorously understand the ingredients that yield good performance. Ã‚ A few weeks ago we released the RedPajama base dataset based on the LLaMA paper, which has galvanized the open-source community. The 5 terabyte dataset has been downloaded hundreds of times and used to train models like MPT, OpenLLaMA, OpenAlpaca. Today we are excited to release RedPajama-INCITE models, including instruct-tuned and chat versions. TodayÃ¢Â€Â™s release includes our first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. In addition we are releasing fully open-source instruction-tuned and chat models. Our key takeaways: The 3B model is the strongest in its class, and the small size makes it extremely fast and accessible (it even runs on a RTX 2070 released over 5 years ago). The instruction-tuned versions of the models achieve strong performance on HELM benchmarks. As expected, on HELM the 7B model performance is higher than the base LLaMA model by 3 points. We recommend using these models for downstream applications with few-shot, entity extraction, classification, or summarization tasks. The 7B model (which is 80% done training) is already outperforming the Pythia 7B model, which is showing the importance of a bigger dataset and the value of the RedPajama base dataset. Based on our observations, we see a clear path for creating a better version of the RedPajama dataset, which we will release in the coming weeks, that will go beyond the quality of LLaMA 7B. We plan to build models at larger scale with this new dataset. We expect differences between the LLaMA 7B and our replication, which we have investigated below. The biggest takeaway is the demonstration that performant LLMs can be built quickly by the open-source community. This work builds on top of our 1.2 trillion token RedPajama dataset, EleutherAIÃ¢Â€Â™s Pythia training code, FlashAttention from Stanford and Together, the HELM benchmarks from Stanford CRFM and generous support from EleutherAI &amp; LAION for compute time on the Summit supercomputer within the INCITE program award &quot;Scalable Foundation Models for Transferable Generalist AIÃ¢Â€Â. We believe these kind of open collaborations, at larger scales, will be behind the best AI systems of the future. Ã¢Â€ÂœRedPajama 3B model is the strongest model in itÃ¢Â€Â™s class and brings a performant large language model to a wide variety of hardware.Ã¢Â€Â TodayÃ¢Â€Â™s release includes the following models, all released under the permissive Apache 2.0 license allowing for use both in research and commercial applications. RedPajama-INCITE-Base-3B-v1 Base model that outperforms other open models of similar sizes in benchmarks. RedPajama-INCITE-Chat-3B-v1 Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-3B-v1 base model. RedPajama-INCITE-Instruct-3B-v1 Model designed for few-shot prompts, fine-tuned using the same formula as GPT-JT (but eliminate all datasets that overlap with the HELM benchmark) over the RedPajama-INCITE-Base-3B-v1 base model. RedPajama-INCITE-Base-7B-v0.1 Early preview of our RedPajama 7B part way through training having completed 800 billion out of the planned 1 trillion tokens, as quality continues to improve in training. RedPajama-INCITE-Chat-7B-v0.1 Early preview of chat model trained on RedPajama-INCITE-Base-7B-preview. RedPajama-INCITE-Instruct-7B-v0.1 Early preview of model designed for few-shot prompts trained on RedPajama-INCITE-Base-7B-preview. In only a few weeks the support, suggestions, and feedback for RedPajama from the open-source community has been incredible. Based on our learnings, we are also already starting the next version of the RedPajama base dataset which will be nearly twice the size of the original v1 dataset. Thank you for your support, feedback and suggestions! View fullsize3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens During RedPajama model training we have shared regular updates, and both the 3B and 7B models have now been trained on 800 billion tokens. We are excited to see that the 3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens. Ã‚ 3B RedPajama Models RedPajama-INCITE-Base-3B-v1 is trained over the RedPajama v1 dataset, with the same architecture as the popular Pythia model suite. We chose to start with the Pythia architecture to understand the value of training with the much larger RedPajama dataset with respect to the current leading open-source dataset, the Pile . Training on Summit leveraged the DeeperSpeed codebase developed by EleutherAI . We are excited to see that at 800B tokens, RedPajama-Base-INCITE-3B has better few-shot performance (measured in HELM , as the average score over 16 core scenarios) and better zero-shot performance (measured in EleutherÃ¢Â€Â™s LM evaluation harness ) compared with open models of similar size, including the well-regarded GPT-Neo and Pythia-2.8B (trained with 420B and 300B tokens, respectively, with the Pile). On HELM, it outperforms these models by 3-5 points. On a subset of tasks from lm-evaluation-harness, outperforms these open models by 2-7 points. Additionally, we are excited to release an instruction-tuned version of this 3B model, RedPajama-INCITE-Instruct-3B-v1, trained following TogetherÃ¢Â€Â™s GPT-JT recipe and removing any data in HELM benchmarks to ensure that there is no contamination with respect to HELM. This model shows excellent performance on few-shot tasks, even approaching the quality of LLaMA 7B in a much smaller model, as shown in the results below: Few Shot Results on HELM Core Scenarios The base model also performs well on zero-shot tasks, as measured using EleutherAIÃ¢Â€Â™s language model evaluation harness : Models Type HELM (Average score over 16 core scenarios) GPT-Neo Base model 0.357 Pythia-2.8B Base model 0.377 RedPajama-INCITE-Base-3B-v1 Base model 0.406 RedPajama-INCITE-Instruct-3B-v1 Instruction-tuned 0.453 Llama-7B Base model 0.465 (Zero Shot) Results on a subset of lm-evaluation-harness , following LLM Worksheet Ã¢Â€Â™s selection of tasks &amp; metrics. We didnÃ¢Â€Â™t run coqa because of an error as in this issue . Lambada_openai (acc) Hellaswag (acc_norm) Winogrande (acc) Piqa(acc) average GPT-Neo 0.6223 0.5579 0.5769 0.7219 0.6197 Pythia-2.8B 0.6466 0.5933 0.6006 0.7399 0.6451 Pythia-2.8B-dedup 0.6524 0.5941 0.5848 0.7404 0.6429 RedPajama-INCITE-Base-3B-v1 0.6541 0.6317 0.6322 0.7470 0.6662 Results on a subset of lm-evaluation-harness, tasks selected from what used to evaluate Pythia and GPT-J. View fullsizeRedPajama 3B results on a subset of lm-evaluation-harness RedPajama-INCITE-Chat-3B-v1 is an open-source chat model constructed with RedPajama-INCITE-Base-3B-v1 and fine-tuned over the OASST1 dataset by Open Assistant and Dolly v2.0 dataset by DataBricks . We equally mix the datasets, and fine-tune for 3 epochs. Evaluating chat models is a challenging task, and we are in the process of conducting more quantitative evaluation based on human and community feedback, and are excited to share these results soon! Nevertheless, here are some examples comparing the behavior of different chat models. We see that in many examples, RedPajama-INCITE-Chat-3B-v1 has similar quality as Open Assistant as reported in the their paper . RedPajama 3B chat model responses on example queries from the Open Assistant paper. Ã‚ RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0) Pythia-2.8B + oasst1 + Dolly 2.0 Open Assistant (Ã¢Â€Âœpythia-12b-deduped fine-tuned on the OpenAssistant and Alpaca dataÃ¢Â€Â [Appendix E of https://arxiv.org/abs/2304.07327]) Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. Time travel has always been a fascinating concept, and the idea of being able to change the past, present, and future would be an incredible opportunity. However, it would also present many ethical and philosophical questions that would need to be addressed. One of the biggest concerns would be the impact that time travel would have on the laws of physics and causality. If we were able to travel back in time and change events, it could potentially create paradoxes and open up new possibilities for time travel. However, it would also mean that the future we have now is no longer guaranteed, and our actions and decisions could have long-lasting consequences. Another concern would be the potential impact on human history. If we were able to change the past, present, and future, it could potentially lead to new discoveries, inventions, and breakthroughs that would have significant impacts on our society and culture. However, it could also mean that we are no longer living in the same world that we know today, and that our understanding of history and the universe could be fundamentally altered. Another ethical concern would be the impact that time travel would have on the concept of free will. If we were able to change the past, present, and future, it could potentially lead to situations where we are no longer responsible for our actions and decisions. This could have significant implications for our sense of agency and autonomy, and could lead to a loss of personal responsibility and accountability. Finally, there would also be the question of what to do with the knowledge that we have access to time travel. If we were able to change the past, present, and future, it would mean that we have access to a vast amount of information and knowledge that we have never had before. This could lead to new discoveries and innovations, but it also could lead to a loss of privacy and a potential breakdown of the current social order. In conclusion, time travel would be a complex and multifaceted issue that would require careful consideration of its ethical, philosophical, and societal implications. It would also require a deep understanding of the laws of physics and causality to ensure that it does not create paradoxes or open up new possibilities for time travel. Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. In a world where time travel is possible, it opens up a whole new world of possibilities. One of the most significant implications would be the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. For example, if someone could travel back in time and prevent a certain event from occurring, then they could change the course of history and create a new reality. This could potentially lead to a world where different outcomes occur, and this could have significant implications for the way we understand history and the nature of reality. Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. The potential consequences of having access to time travel are vast and far-reaching. One concern is that it could lead to unintended consequences for future generations if we alter events or change history without careful consideration. Additionally, there may be legal issues regarding ownership and rights over changes made through time travel. The ethical considerations would involve weighing the risks and benefits, as well as making sure that any changes made do not cause harm to anyone involved. Finally, it will also be important to ensure that time travelers adhere to all relevant laws and regulations when using their powers. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare is still studied so heavily today because of his mastery of the English language. His use of language is so beautiful and intricate that it has inspired many modern writers. His plays are still performed today, and many of his poems and sonnets are still read and enjoyed. His plays have been adapted for film and television, and his characters continue to be studied and analyzed by scholars and students. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare's plays are so well known because they are thought to be timeless and universal. Shakespeare's characters are relatable and his stories are thought to be universal in nature. Shakespeare is also known for his complex and nuanced writing style that is thought to be hard to imitate. Shakespeare's plays are still studied and performed to this day because of their great depth and complexity. Shakespeare's plays are often thought to be about the human condition and how we deal with the pressures of life. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare is one of the most influential writers in history, and his work continues to be studied and performed around the world. His plays are known for their powerful language, memorable characters, and timeless themes, which have made him a popular choice among playwrights, actors, and audiences alike. In addition to his literary merit, ShakespeareÃ¢Â€Â™s works also helped shape the English language and influenced many other authors who followed in his footsteps. Many of his lines continue to be quoted and used as examples of excellent writing. He remains an important figure in literature and culture, and his legacy will likely endure for generations to come. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? Creating a successful YouTube channel requires a lot of hard work, dedication, and creativity. Here are some steps to help you develop a successful channel: 1. Develop a niche: Choose a topic or subject that you are passionate about and that you can consistently provide valuable and unique content on. This will help you build an engaged audience and establish yourself as an expert in your field. 2. Create high-quality content: Invest in high-quality equipment, editing software, and other resources to create high-quality videos that are engaging and informative. Make sure to use professional lighting and sound equipment to create a polished and professional look. 3. Build a community: Create a community around your channel by engaging with your audience and responding to their comments and feedback. This will help you build a loyal following and create a sense of community around your channel. 4. Monetize your channel: Consider monetizing your channel through ads, affiliate marketing, or other revenue-generating strategies. This will help you sustain your channel and create a sustainable income. 5. Promote your channel: Promote your channel through social media, email marketing, and other platforms to build your audience and drive traffic to your videos. 6. Grow your channel: Continuously produce high-quality content and engage with your audience to grow your channel and build a loyal following. 7. Monitor analytics: Use analytics tools to monitor your channel's performance and identify areas for improvement. This will help you make data-driven decisions and continuously improve your channel. 8. Continuously learn and grow: Continuously learn and grow in your field of expertise to stay relevant and provide valuable content to your audience. By following these steps, you can develop a successful YouTube channel that can generate income and build a community around your content. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? To create a successful YouTube channel, it's important to have a niche and a clear focus. This will help you develop content that is specific to your audience and helps you build a community. Once you have a strong niche and a following, you can start monetizing your channel by creating ads, merchandise, and sponsorships. This will help you earn money from your videos and support your channel. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? Creating a successful YouTube channel involves several steps. First, you need to select a niche that aligns with your target audienceÃ¢Â€Â™s interests and passions. Once you have chosen a niche, it is important to create engaging content that resonates with your viewers. This can include videos on topics such as tutorials, reviews, vlogs, and more. To build a strong community around your channel, it is essential to engage with your subscribers by responding to comments, posting regular updates, and sharing relevant information. You should also use other social media platforms like Twitter and Facebook to promote your channel and interact with potential audiences. Finally, to make money off your channel, you will need to optimize your video titles, descriptions, tags, and thumbnails for search engine visibility, set up ads, and generate revenue through sponsorships or product integrations. Additionally, you may want to consider using YouTube analytics to track your channelÃ¢Â€Â™s performance and adjust strategies accordingly. And, following are some additional examples comparing RedPajama 3B to the Pythia 2.8B model tuned on OASST1 and Dolly v2.0 datasets. RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0) Pythia-2.8B + oasst1 + Dolly 2.0 Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share. Hi everyone, I'm excited to host a dinner party on Friday, and I hope you can join me! We'll be having a casual dinner with a twist, where we'll each bring a dish to share. I've made some appetizers, salads, and desserts, but I'm also excited to try some new dishes. I've got a big backyard, so we can all sit outside and enjoy the fresh air. I've got plenty of seating, and we can even set up a few tables in the living room if we have too many people. I hope you can join us, and I look forward to cooking and eating with you! Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share. Dear {Friend}, I hope you had a great summer and are looking forward to a fun fall. I'm so excited to share some of my favorite things with you on Friday. I hope you can join us for a delicious dinner and a great conversation. I'm looking forward to seeing you there. Best, {Your Name} Create a list of things to do in San Francisco San Francisco is a beautiful city with plenty of things to do. Here are some of the top things to do in San Francisco: 1. Visit Golden Gate Park: Golden Gate Park is the largest park in San Francisco and is home to the iconic Golden Gate Bridge, Japanese Tea Garden, and the San Francisco Botanical Garden. 2. Take a ferry ride to Alcatraz: Take a ferry ride to Alcatraz, a former federal prison that is now a popular tourist destination. 3. Visit the Golden Gate Bridge: The iconic Golden Gate Bridge is one of the most famous landmarks in San Francisco. Take a walk across the bridge for incredible views of the city and the bay. 4. Visit Chinatown: San Francisco's Chinatown is a vibrant neighborhood with traditional Chinese markets, restaurants, and temples. 5. Visit the Castro District: The Castro District is the heart of the LGBTQ+ community in San Francisco and is home to many bars, restaurants, and shops. 6. Take a cable car ride: Take a ride on the iconic cable cars, which have been operating in San Francisco since the late 19th century. 7. Visit the Palace of Fine Arts: The Palace of Fine Arts is a historic building in San Francisco's Marina District and is a popular spot for wedding ceremonies and photo shoots. 8. Visit the San Francisco Museum of Modern Art: The San Francisco Museum of Modern Art is one of the largest museums of modern and contemporary art in the United States. 9. Visit the San Francisco Zoo: The San Francisco Zoo is a popular destination for families and is home to a variety of animals, including lions, tigers, and gorillas. 10. Take a stroll along the waterfront: The waterfront in San Francisco is a great place to take a stroll Create a list of things to do in San Francisco 1. Visit the Golden Gate Bridge 2. Visit the Alcatraz Island 3. Visit the Marina District 4. Visit the City Hall 5. Visit the Yerba Buena Center for the Arts 6. Visit the Stow Lake 7. Visit the Ferry Building 8. Visit the Presidio 9. Visit the Golden Gate Park 10. Visit the Chinatown Preview of RedPajama 7B The 7B model is still training (at 800B tokens) and we see the training loss still decrease consistently. As a result, we will continue to train it to 1T tokens. Nevertheless, this checkpoint is quite useful, and interesting to build on, and can help the community better understand our training process. Therefore, we are releasing three intermediate checkpoints as a Ã¢Â€ÂœpreviewÃ¢Â€Â of the final models. RedPajama-INCITE-Base-7B-v0.1 is a base model trained over 800B tokens RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained over Dolly 2.0 and Open Assistant RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications. We follow the recipe for GPT-JT but eliminate all datasets that overlap with the HELM benchmark. Each of these checkpoints are released under the Apache 2.0 license. Even at 800B tokens, we already see promising results. On HELM, the base model outperforms open models such as GPT-J and Pythia-6.9B by 0.5-2.2 points, and on EleutherAIÃ¢Â€Â™s lm-evaluation-harness, it outperforms these models by 1-3 points on average. We also see that, compared with LLaMA 7B, there is still a quality gap Ã¢Â€Â“ 4.3 points on HELM at this moment. For few-shot applications (like those in HELM), the instruction-tuned model (RedPajama-INCITE-Instruct-7B-v0.1) improved over the base model significantly. We hope that some of this gap can be closed after we train for more iterations. (Few Shot) Results on HELM Core Scenarios Model Type HELM (Average score over 16 core scenarios) GPT-J Base model 0.417 Pythia-6.9B Base model 0.400 Llama-7B Base model 0.465 RedPajama-INCITE-Base-7B-v0.1 Base model 0.422 RedPajama-INCITE-Instruct-7B-v0.1 Instruction-tuned 0.499 The base model also performs well on zero-shot tasks, as measured using EleutherAIÃ¢Â€Â™s language model evaluation harness : (Zero Shot) Results on a subset of lm-evaluation-harness , following LLM Worksheet Ã¢Â€Â™s selection of tasks &amp; metrics. We didnÃ¢Â€Â™t run coqa because of an error as in this issue . Llama numbers marked with * are taken directly from LLM Worksheet because we run into the following issue . Lambada_openai (acc) Hellaswag (acc_norm) Winogrande (acc) Piqa (acc) average GPT-J 0.6699 0.6663 0.6503 0.7565 0.6857 Pythia-6.9B 0.6712 0.6389 0.6069 0.7519 0.6672 Pythia-6.9B-dedup 0.6893 0.6588 0.6266 0.7578 0.6831 Llama-7B 0.7360* 0.7620* 0.7040 0.7810 0.7457 RedPajama-INCITE-Base-7B-v0.1 0.7061 0.6951 0.6519 0.7611 0.7035 Results on a subset of llm-evaluation-harness , tasks selected from what used to evaluate Pythia and GPT-J. Results on a subset of llm-evaluation-harness Moving Forward: RedPajama v2 with 2T Tokens We learned a lot from the community and are working on building RedPajama v2 with 2 trillion tokens, by taking a systematic approach: We measured the validation loss of different models on different slices of the Pile (for each slice, we selected the first 5K passages). We see that RedPajama lags behind on many slices of the Pile, especially for those slices that are not directly included in the RedPajama dataset. Inspired by this, we plan to mix the Pile dataset into RedPajama and form a more diverse dataset with even more tokens. And we need more code! Another immediate to-do on our plate is to mix in data from the Stack and enrich the Github slice of RedPajama, which contains only 59 billion tokens. With all these improvements together, we are shooting for a 2T token RedPajama v2 dataset. Next week we will start doing a series of runs to understand the right data mixture and start training new models over RedPajama v2. Llama-7B GPT-J RedPajama-Base-INCITE-6.9B-v0.1 ArXiv 1.727 1.511 1.990 BookCorpus2 1.904 2.226 2.213 Books3 1.664 1.979 1.909 DM Mathematics 1.411 1.158 1.910 Enron Emails 2.494 1.844 2.962 EuroParl 1.964 1.216 2.066 FreeLaw 1.425 1.121 1.889 Github 1.126 0.756 1.273 Gutenberg (PG-19) 1.837 1.718 2.079 HackerNews 2.423 2.311 2.821 NIH ExPorter 1.864 2.135 2.413 OpenSubtitles 2.184 2.136 2.510 OpenWebText2 2.027 2.264 2.321 PhilPapers 1.947 2.225 2.280 Pile-CC 2.095 2.441 2.430 PubMed Abstracts 1.694 1.937 2.220 PubMed Central 1.697 1.494 2.122 StackExchange 1.776 1.588 2.078 USPTO Backgrounds 1.740 1.841 2.142 Ubuntu IRC 2.094 1.704 2.518 Wikipedia (en) 1.597 1.629 1.758 YoutubeSubtitles 1.943 1.955 2.226 Acknowledgements The training of the first collection of RedPajama-INCITE models is performed on 3,072 V100 GPUs provided as part of the INCITE compute grant on Summit supercomputer at the Oak Ridge Leadership Computing Facility (OLCF) . This grant was awarded to AAI CERC lab at UniversitÃƒÂ© de MontrÃƒÂ©al , Ã‚ LAION and EleutherAI in fall 2022 for their collaborative project on Scalable Foundation Models for Transferrable Generalist AI . We are thankful to all the project team members helping to build the RedPajama dataset and supporting training, including Ontocord.ai , ETH DS3Lab , AAI CERC Lab at the UniversitÃƒÂ© de MontrÃƒÂ©al , Stanford Center for Research on Foundation Models (CRFM) , Stanford Hazy Research research group , LAION and EleutherAI . We are grateful to Quentin Anthony ( EleutherAI and INCITE project team ) for sharing the GPT-NeoX model architecture and training code. We are also appreciative to the work done by the growing open-source AI community that made this project possible. That includes: Meta AI Ã¢Â€Â” Their inspiring work on LLaMA shows a concrete path towards building strong language models, and it is the original source for our dataset replication. EleutherAI Ã¢Â€Â” This project is built on the backs of the great team at EleutherAI Ã¢Â€Â” including the source code they provided for training GPT-NeoX. INCITE project team Ã¢Â€Â” Their work on GPT-NeoX adaptation to Summit during early 2023 enabled distributed training that scaled efficiently to thousands of Summit GPUs, and ensured smooth training of the models. This research used resources of the Oak Ridge Leadership Computing Facility (OLCF) , which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725. We are Ã‚ grateful for the invaluable support provided to us by the OLCF leadership and by the OLCF liaison for the INCITE project. Lower Cost 20% faster training 4x network compression 117x Q: Should I use the RedPajama-V2 Dataset out of the box? RedPajama-V2 is conceptualized as a pool of data that serves as a foundation for creating high quality datasets. The dataset is thus not intended to be used out of the box and, depending on the application, data should be filtered out using the quality signals that accompany the data. With this dataset, we take the view that the optimal filtering of data is dependent on the intended use. Our goal is to provide all the signals and tooling that enables this. No items found. Links in this article RedPajama 3B RedPajama Chat 3B RedPajama Instruct 3B RedPajama 7B RedPajama Chat 7B RedPajama Instruct 7B PJs Ã°ÂŸÂ˜Â† Twitter thread Join our Discord Start building yours here Ã¢Â†Â’ Subscribe to newsletter Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Products Solutions Research Blog About Pricing Contact Support Status Trust Center Ã‚Â© 2025 San Francisco, CA 94114 Privacy policy Terms of service "
    },
    {
      "arxiv_id": "https://stability.ai/blog/stablecode-llm-generative-ai-coding",
      "full_text": " Announcing Stable Code Alpha &mdash; Stability AI 0 Skip to Content Models Image Video Audio 3D Solutions Applications Dream Studio Stable Audio Deployment Self-Hosted License Platform API Cloud Platforms Resources Learning Hub Customer Stories Company Board of Directors News Partners Research Safety Careers Contact us Open Menu Close Menu Models Image Video Audio 3D Solutions Applications Dream Studio Stable Audio Deployment Self-Hosted License Platform API Cloud Platforms Resources Learning Hub Customer Stories Company Board of Directors News Partners Research Safety Careers Contact us Open Menu Close Menu Folder: Models Back Image Video Audio 3D Solutions Folder: Applications Back Dream Studio Stable Audio Folder: Deployment Back Self-Hosted License Platform API Cloud Platforms Folder: Resources Back Learning Hub Customer Stories Folder: Company Back Board of Directors News Partners Research Safety Careers Contact us Announcing Stable Code Alpha Product Aug 8 Written By Joshua Lopez Stability AI has just announced the release of Stable Code Alpha 3B, its very first LLM generative AI product for coding. This product is designed to assist programmers with their daily work while providing a great learning tool for new developers ready to take their skills to the next level. &nbsp;â€œa colorful parrot with glasses typing away at a computer, flat icon, vectorâ€ â€“ SDXL 0.9 &nbsp; Stable Code Alpha offers a unique way for developers to become more efficient by using three different models to help in their coding. The base model was first trained on a diverse set of programming languages from the stack-dataset (v1.2) from BigCode and then trained further with popular languages like Python, Go, Java, Javascript, C, markdown, and C++.&nbsp; In total, we trained our models on 560B tokens of code on our HPC cluster.&nbsp; After the base model had been established, the instruction model was tuned for specific use cases to help solve complex programming tasks. ~120,000 code instruction/response pairs in Alpaca format were trained on the base model to achieve this result.&nbsp; Code for using StableCode Instruct to generate a response to a given instruction. Stable Code Alpha is the ideal building block for those wanting to learn more about coding, and the long-context window model is the perfect assistant to ensure single and multiple-line autocomplete suggestions are available for the user. This model is built to handle a lot more code at once (2-4X more than previously released open models with a context window of 16,000 tokens), allowing the user to review or edit the equivalent of up to five average-sized Python files at the same time, making it the ideal learning tool for a beginner who wants to rise to bigger challenges. Stable Code Alpha completing a relatively complex python file utilizing the Pytorch deep learning library (gray text shows StableCodeâ€™s prediction). And here is how we compare to other models of a similar number of parameters and number of tokens trained on. We use the standard pass@1 and pass@10 metrics using the popular HumanEval benchmark. benchmark scores of stablecode. HumanEval Benchmark Comparison with models of similar size(3B). Stability AI aims to make technology more accessible, and Stable Code Alpha is a significant step toward this goal. People of every background will soon be able to create code to solve their everyday problems and improve their lives using AI, and weâ€™d like to help make this happen. We hope that Stable Code Alpha will help the next billion software developers learn to code while providing fairer access to technology all over the world. Updated 16/01/24 Joshua Lopez Previous Previous Japanese StableLM, Marking Entry into International Language Model&nbsp; Market Next Next Announcing SDXL 1.0 Stability AI is unlocking the power of open-source generative AI to expand human creativity. We build world-class models that are accessible, adaptable, and designed to empower creators, developers, and enterprises everywhere. Company Board of Directors Partners Safety Research Careers News Brand Resources Models Image Video Audio 3D Deployment Stability AI License Community License Agreement Platform API Cloud Platforms ResourceS Learning Hub Customer Stories Contact Us press@stability.ai partners@stability.ai Submit a Support Request Legal Acceptable Use Policy Privacy Policy Terms of Service Security Your Privacy Choices Applications Dream Studio Stable Audio Join the Mailing List Â© Stability AI Ltd, 2025 end Plugin Lightbox --> "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2406.12428",
      "full_text": " [2406.12428] PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2406.12428 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2406.12428 (cs) [Submitted on 18 Jun 2024 ( v1 ), last revised 3 Oct 2024 (this version, v2)] Title: PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems Authors: Kentaro Mitsui , Koh Mitsuda , Toshiaki Wakatsuki , Yukiya Hono , Kei Sawada View a PDF of the paper titled PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems, by Kentaro Mitsui and 4 other authors View PDF HTML (experimental) Abstract: Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at this https URL . Comments: 9 pages, 6 figures, 4 tables, accepted for Findings of EMNLP 2024. Demo samples: this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) Cite as: arXiv:2406.12428 [cs.CL] &nbsp; (or arXiv:2406.12428v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2406.12428 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kentaro Mitsui [ view email ] [v1] Tue, 18 Jun 2024 09:23:54 UTC (219 KB) [v2] Thu, 3 Oct 2024 05:17:25 UTC (253 KB) Full-text links: Access Paper: View a PDF of the paper titled PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems, by Kentaro Mitsui and 4 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-06 Change to browse by: cs cs.AI cs.LG cs.SD eess eess.AS References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}