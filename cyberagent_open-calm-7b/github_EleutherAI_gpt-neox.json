{
    "repo": "EleutherAI/gpt-neox",
    "branch": "main",
    "files": [
        ".clang-format",
        ".dockerignore",
        ".github/CODEOWNERS",
        ".github/ISSUE_TEMPLATE/bug_report.md",
        ".github/ISSUE_TEMPLATE/feature_request.md",
        ".github/workflows/.cpu_ci_on_pr.yml",
        ".github/workflows/coverity_scan.yml",
        ".github/workflows/cpu_ci.yml",
        ".github/workflows/cpu_ci_dispatch.yml",
        ".github/workflows/docker_build.yml",
        ".github/workflows/pull_request.yml",
        ".gitignore",
        ".pre-commit-config.yaml",
        "CITATION.cff",
        "CONTRIBUTING.md",
        "LICENSE",
        "MANIFEST.in",
        "README-MUP.md",
        "README.md",
        "configs/1-3B-transformer-engine.yml",
        "configs/1-3B.yml",
        "configs/125M-dmoe.yml",
        "configs/125M-json.yml",
        "configs/125M.yml",
        "configs/13B.yml",
        "configs/175B.yml",
        "configs/19M.yml",
        "configs/2-7B.yml",
        "configs/20B.yml",
        "configs/350M.yml",
        "configs/49M.yml",
        "configs/6-7B.yml",
        "configs/760M.yml",
        "configs/800M.yml",
        "configs/README.md",
        "configs/autotuning_configs/small_tune.json",
        "configs/autotuning_configs/tune.json",
        "configs/autotuning_configs/tune_1-3B.json",
        "configs/autotuning_configs/tune_6-7B.json",
        "configs/bf16_125M.yml",
        "configs/bnb_125M.yml",
        "configs/cpu_mock_config.yml",
        "configs/docker/pythia-paths.yml",
        "configs/eleutherai_cluster.yml",
        "configs/finetuning_configs/6-9B.yml",
        "configs/gen_docs.py",
        "configs/gmlp_small.yml",
        "configs/llama/13B.yml",
        "configs/llama/30B.yml",
        "configs/llama/65B.yml",
        "configs/llama/7B.yml",
        "configs/llama/README.md",
        "configs/llama/train_config.yml",
        "configs/llama2/13B.yml",
        "configs/llama2/70B.yml",
        "configs/llama2/7B.yml",
        "configs/llama2/codellama_34B.yml",
        "configs/llama2/codellama_7B.yml",
        "configs/llemma/34B.yml",
        "configs/llemma/7B.yml",
        "configs/local_setup.yml",
        "configs/local_setup_comet.yml",
        "configs/local_setup_wandb.yml",
        "configs/mamba/mamba-1.4B.yml",
        "configs/mamba/mamba-130M.yml",
        "configs/mamba/mamba-2.8B.yml",
        "configs/mamba/mamba-370M.yml",
        "configs/mamba/mamba-790M.yml",
        "configs/mistral/7B.yml",
        "configs/neox_arguments.md",
        "configs/prof.yml",
        "configs/pythia/1-4B.yml",
        "configs/pythia/12B.yml",
        "configs/pythia/14M.yml",
        "configs/pythia/160M.yml",
        "configs/pythia/1B.yml",
        "configs/pythia/2-8B.yml",
        "configs/pythia/31M.yml",
        "configs/pythia/410M.yml",
        "configs/pythia/6-9B.yml",
        "configs/pythia/70M.yml",
        "configs/rwkv/170M.yml",
        "configs/slurm_125M.yml",
        "configs/slurm_local.json",
        "configs/slurm_local.yml",
        "configs/sparse.yml",
        "configs/text_generation.yml",
        "containers/apptainer/gpt-neox.def",
        "containers/docker/Dockerfile",
        "containers/docker/docker-compose-dockerhub.yml",
        "containers/docker/docker-compose.yml",
        "deepy.py",
        "eval.py",
        "eval_tasks/__init__.py",
        "eval_tasks/eval_adapter.py",
        "generate.py",
        "images/memory_profiling.png",
        "images/nsight_profiling.png",
        "images/pytorch_profiling.png",
        "megatron/__init__.py",
        "megatron/checkpointing.py",
        "megatron/data/Makefile",
        "megatron/data/__init__.py",
        "megatron/data/blendable_dataset.py",
        "megatron/data/data_utils.py",
        "megatron/data/gpt2_dataset.py",
        "megatron/data/helpers.cpp",
        "megatron/data/indexed_dataset.py",
        "megatron/data/online_dataset.py",
        "megatron/data/pairwise_dataset.py",
        "megatron/data/samplers.py",
        "megatron/devutil.py",
        "megatron/fused_kernels/__init__.py",
        "megatron/fused_kernels/compat.h",
        "megatron/fused_kernels/fused_rotary_positional_embedding.cpp",
        "megatron/fused_kernels/fused_rotary_positional_embedding.h",
        "megatron/fused_kernels/fused_rotary_positional_embedding_cuda.cu",
        "megatron/fused_kernels/scaled_masked_softmax.cpp",
        "megatron/fused_kernels/scaled_masked_softmax.h",
        "megatron/fused_kernels/scaled_masked_softmax_cuda.cu",
        "megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp",
        "megatron/fused_kernels/scaled_upper_triang_masked_softmax.h",
        "megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu",
        "megatron/fused_kernels/setup.py",
        "megatron/fused_kernels/type_shim.h",
        "megatron/gradient_noise_scale/__init__.py",
        "megatron/gradient_noise_scale/gradient_noise_scale.py",
        "megatron/initialize.py",
        "megatron/learning_rates.py",
        "megatron/logging.py",
        "megatron/model/__init__.py",
        "megatron/model/activations.py",
        "megatron/model/fused_bias_dropout.py",
        "megatron/model/fused_layer_norm.py",
        "megatron/model/fused_rope.py",
        "megatron/model/fused_softmax.py",
        "megatron/model/gmlp.py",
        "megatron/model/gpt2_model.py",
        "megatron/model/init_functions.py",
        "megatron/model/mamba/__init__.py",
        "megatron/model/mamba/mamba.py",
        "megatron/model/moe.py",
        "megatron/model/moe_mlp.py",
        "megatron/model/norms.py",
        "megatron/model/positional_embeddings.py",
        "megatron/model/router.py",
        "megatron/model/rwkv/__init__.py",
        "megatron/model/rwkv/v6/__init__.py",
        "megatron/model/rwkv/v6/cuda/wkv6_cuda.cu",
        "megatron/model/rwkv/v6/cuda/wkv6_op.cpp",
        "megatron/model/rwkv/v6/rwkv.py",
        "megatron/model/transformer.py",
        "megatron/model/transformer_engine.py",
        "megatron/model/utils.py",
        "megatron/model/weight_server.py",
        "megatron/model/word_embeddings.py",
        "megatron/mpu/__init__.py",
        "megatron/mpu/cross_entropy.py",
        "megatron/mpu/data.py",
        "megatron/mpu/initialize.py",
        "megatron/mpu/layers.py",
        "megatron/mpu/mappings.py",
        "megatron/mpu/random.py",
        "megatron/mpu/utils.py",
        "megatron/mup_substitute.py",
        "megatron/neox_arguments/__init__.py",
        "megatron/neox_arguments/arguments.py",
        "megatron/neox_arguments/deepspeed_args.py",
        "megatron/neox_arguments/neox_args.py",
        "megatron/neox_arguments/template.py",
        "megatron/optimizers.py",
        "megatron/text_generation_utils.py",
        "megatron/tokenizer/__init__.py",
        "megatron/tokenizer/tokenizer.py",
        "megatron/tokenizer/train_tokenizer.py",
        "megatron/training.py",
        "megatron/utils.py",
        "post-training/README.md",
        "post-training/configs/benchmarking/llama-13b-dpo.yml",
        "post-training/configs/benchmarking/mistral-dpo.yml",
        "post-training/configs/llama3-8b-dpo.yml",
        "post-training/configs/llama3-8b-kto.yml",
        "post-training/configs/llama3-8b-reinforce.yml",
        "post-training/configs/llama3-8b-rm.yml",
        "post-training/configs/llama3-8b-sft.yml",
        "post-training/dpo_data.py",
        "post-training/llama_data.py",
        "post-training/online_data_example_llama3.py",
        "post-training/online_example.sh",
        "post-training/online_training.md",
        "post-training/recreating_zephyr_dpo.md",
        "prepare_data.py",
        "requirements/pyproject-apex-pip.toml",
        "requirements/pyproject-comet.toml",
        "requirements/pyproject-flashattention.toml",
        "requirements/pyproject-mamba.toml",
        "requirements/pyproject-neox-dev.toml",
        "requirements/pyproject-onebitadam.toml",
        "requirements/pyproject-s3.toml",
        "requirements/pyproject-sparseattention.toml",
        "requirements/pyproject-tensorboard.toml",
        "requirements/pyproject-transformerengine.toml",
        "requirements/pyproject-wandb.toml",
        "requirements/pyproject.toml",
        "requirements/requirements-apex-pip.txt",
        "requirements/requirements-comet.txt",
        "requirements/requirements-dev.txt",
        "requirements/requirements-flashattention.txt",
        "requirements/requirements-mamba.txt",
        "requirements/requirements-moe.txt",
        "requirements/requirements-onebitadam.txt",
        "requirements/requirements-s3.txt",
        "requirements/requirements-sparseattention.txt",
        "requirements/requirements-tensorboard.txt",
        "requirements/requirements-transformerengine.txt",
        "requirements/requirements-wandb.txt",
        "requirements/requirements.txt",
        "tests/README.md",
        "tests/__init__.py",
        "tests/common.py",
        "tests/config/test_setup.yml",
        "tests/conftest.py",
        "tests/cpu_tests/action.yml",
        "tests/cpu_tests/docker-compose.yml",
        "tests/data/enwik8_first100.txt",
        "tests/data/hf_cache/tokenizer/gpt2.json",
        "tests/data/sample_prompt.txt",
        "tests/model/__init__.py",
        "tests/model/test_fused_kernels.py",
        "tests/model/test_model_checkpoint.py",
        "tests/model/test_model_generation.py",
        "tests/model/test_model_instantiation.py",
        "tests/model/test_model_train.py",
        "tests/neox_args/__init__.py",
        "tests/neox_args/test_neoxargs_commandline.py",
        "tests/neox_args/test_neoxargs_implementation.py",
        "tests/neox_args/test_neoxargs_load.py",
        "tests/neox_args/test_neoxargs_usage.py",
        "tests/pytest.ini",
        "tests/requirements/test_requirements.py",
        "tests/test_configs/test_train_base.yml",
        "tests/unit/__init__.py",
        "tests/unit/test_arguments.py",
        "tests/unit/test_dependencies.py",
        "tests/unit/test_format_conversion_scripts.py",
        "tests/unit/test_launcher_scripts.py",
        "tests/unit/test_tokenizer.py",
        "tests/unit/test_url_accessibility.py",
        "tools/README.md",
        "tools/__init__.py",
        "tools/bash/README.md",
        "tools/bash/kill.sh",
        "tools/bash/killall.sh",
        "tools/bash/sync.sh",
        "tools/bash/sync_cmd.sh",
        "tools/bash/syncdir.sh",
        "tools/ckpts/README.md",
        "tools/ckpts/convert_hf_llama_to_neox.py",
        "tools/ckpts/convert_hf_to_sequential.py",
        "tools/ckpts/convert_neox_to_hf.py",
        "tools/ckpts/convert_neox_to_mamba_ssm.py",
        "tools/ckpts/convert_raw_llama_weights_to_neox.py",
        "tools/ckpts/inspect_checkpoints.py",
        "tools/ckpts/merge20b.py",
        "tools/ckpts/upload.py",
        "tools/datasets/README.md",
        "tools/datasets/corpora.py",
        "tools/datasets/dataset_token_count.py",
        "tools/datasets/merge_datasets.py",
        "tools/datasets/multinode_prepare_data.sh",
        "tools/datasets/preprocess_data.py",
        "tools/datasets/preprocess_data_with_chat_template.py",
        "tools/datasets/preprocess_data_with_mask.py",
        "train.py"
    ],
    "license_files": {
        "LICENSE": "                                 Apache License\n                           Version 2.0, January 2024\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n--\n\nThis repository also contains code from Hugging Face Inc., Google Research,\nand Facebook (from their Fairseq project). Files from these\norganizations have notices at the top of each file. Below are licenses\nused in those files, as indicated.\n\n\n------------- LICENSE FOR NVIDIA code  --------------\n\n\n# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n------------- LICENSE FOR huggingface and Google Research code  --------------\n\n\n                                 Apache License\n                           Version 2.0, January 2024\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------- LICENSE FOR Facebook Fairseq code --------------\n\nMIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
    },
    "readme": "[![GitHub issues](https://img.shields.io/github/issues/EleutherAI/gpt-neox)](https://github.com/EleutherAI/gpt-neox/issues)\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Weights & Biases monitoring\" height=20>](https://wandb.ai/eleutherai/neox)\n\n# GPT-NeoX\n\nThis repository records [EleutherAI](https://www.eleuther.ai)'s library for training large-scale language models on GPUs. Our current framework is based on NVIDIA's [Megatron Language Model](https://github.com/NVIDIA/Megatron-LM) and has been augmented with techniques from [DeepSpeed](https://www.deepspeed.ai) as well as some novel optimizations. We aim to make this repo a centralized and accessible place to gather techniques for training large-scale autoregressive language models, and accelerate research into large-scale training. This library is in widespread use in [academic, industry, and government labs](https://github.com/EleutherAI/gpt-neox#adoption-and-publications), including by researchers at Oak Ridge National Lab, CarperAI, Stability AI, Together.ai, Korea University, Carnegie Mellon University, and the University of Tokyo among others. Uniquely among similar libraries GPT-NeoX supports a wide variety of systems and hardwares, including launching via Slurm, MPI, and the IBM Job Step Manager, and has been run at scale on [AWS](https://aws.amazon.com/), [CoreWeave](https://www.coreweave.com/), [ORNL Summit](https://www.olcf.ornl.gov/summit/), [ORNL Frontier](https://www.olcf.ornl.gov/frontier/),  [LUMI](https://www.lumi-supercomputer.eu/), and others.\n\n**If you are not looking to train models with billions of parameters from scratch, this is likely the wrong library to use. For generic inference needs, we recommend you use the Hugging Face `transformers` library instead which supports GPT-NeoX models.**\n\n## Why GPT-NeoX?\n\nGPT-NeoX leverages many of the same features and technologies as the popular Megatron-DeepSpeed library but with substantially increased usability and novel optimizations. Major features include:\n* Distributed training with ZeRO and 3D parallelism\n* A wide variety of systems and hardwares, including launching via Slurm, MPI, and the IBM Job Step Manager, and has been run at scale on [AWS](https://aws.amazon.com/), [CoreWeave](https://www.coreweave.com/), Oak Ridge's [Summit](https://www.olcf.ornl.gov/summit/) and [Frontier](https://www.olcf.ornl.gov/frontier/),  [Pacific Northwest National Laboratory](https://hpc.pnl.gov/index.shtml), Argonne's [Polaris](https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/), [LUMI](https://www.lumi-supercomputer.eu/), and more.\n* Cutting edge architectural innovations including rotary and alibi positional embeddings, parallel feedforward attention layers, and flash attention.\n* Predefined configurations for popular architectures including Pythia, PaLM, Falcon, and LLaMA 1 \\& 2\n* Curriculum Learning\n* Easy connections with the open source ecosystem, including Hugging Face's [tokenizers](https://github.com/huggingface/tokenizers) and [transformers](https://github.com/huggingface/transformers/) libraries, monitor experiments via [WandB](https://wandb.ai/site)/[Comet](https://www.comet.com/site/)/TensorBoard, and evaluation via our [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n## News\n**[10/9/2024]** We now support [Transformer Engine](https://github.com/NVIDIA/TransformerEngine) integration\n\n**[9/9/2024]** We now support preference learning via [DPO](https://arxiv.org/abs/2305.18290), [KTO](https://arxiv.org/abs/2402.01306), and reward modeling\n\n**[9/9/2024]** We now support integration with [Comet ML](https://www.comet.com/site/), a machine learning monitoring platform\n\n**[5/21/2024]** We now support [RWKV](https://www.rwkv.com/) with pipeline parallelism!. See the PRs for [RWKV](https://github.com/EleutherAI/gpt-neox/pull/1198) and [RWKV+pipeline](https://github.com/EleutherAI/gpt-neox/pull/1221)\n\n**[3/21/2024]** We now support Mixture-of-Experts (MoE)\n\n**[3/17/2024]** We now support AMD MI250X GPUs\n\n**[3/15/2024]** We now support [Mamba](https://github.com/state-spaces/mamba) with tensor parallelism! See [the PR](https://github.com/EleutherAI/gpt-neox/pull/1184)\n\n**[8/10/2023]** We now support checkpointing with AWS S3! Activate with the `s3_path` config option (for more detail, see [the PR](https://github.com/EleutherAI/gpt-neox/pull/1010))\n\n**[9/20/2023]** As of https://github.com/EleutherAI/gpt-neox/pull/1035, we have deprecated Flash Attention 0.x and 1.x, and migrated support to Flash Attention 2.x. We don't believe this will cause problems, but if you have a specific use-case that requires old flash support using the latest GPT-NeoX, please raise an issue.\n\n**[8/10/2023]** We have experimental support for LLaMA 2 and Flash Attention v2 supported in our [math-lm](https://github.com/EleutherAI/math-lm) project that will be upstreamed later this month.\n\n**[5/17/2023]** After fixing some miscellaneous bugs we now fully support bf16.\n\n**[4/11/2023]** We have upgraded our Flash Attention implementation to now support Alibi positional embeddings.\n\n**[3/9/2023]** We have released GPT-NeoX 2.0.0, an upgraded version built on the latest DeepSpeed which will be regularly synced with going forward.\n\n## Versions\n\nPrior to 3/9/2023, GPT-NeoX relied on [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed), which was based on an old version of DeepSpeed (0.3.15). In order to migrate to the latest upstream DeepSpeed version while allowing users to access the old versions of GPT-NeoX and DeeperSpeed, we have introduced two versioned releases for both libraries:\n\n- Version 2.0 of [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/releases/tag/v2.0) and [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed/releases/tag/v2.0) are the latest versions built on the latest DeepSpeed, and will be maintained going forward.\n- Version 1.0 of [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/releases/tag/v1.0) and [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed/releases/tag/v1.0) maintain snapshots of the old stable versions that [GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) and the [Pythia Suite](https://github.com/EleutherAI/pythia) were trained on.\n\n# Contents\n\n- [GPT-NeoX](#gpt-neox)\n  * [Why GPT-NeoX?](#why-gpt-neox)\n  * [News](#news)\n  * [Versions](#versions)\n- [Contents](#contents)\n- [Quick Start](#quick-start)\n  * [Environment and Dependencies](#environment-and-dependencies)\n    + [Host Setup](#host-setup)\n    + [Flash Attention](#flash-attention)\n    + [Transformer Engine](#transformer-engine)\n    + [Multi-Node Launching](#multi-node-launching)\n    + [Containerized Setup](#containerized-setup)\n  * [Usage](#usage)\n- [Configuration](#configuration)\n    * [Mixture of Experts](#mixture-of-experts)\n- [Datasets](#datasets)\n  * [Preconfigured Datasets](#preconfigured-datasets)\n  * [Using Custom Data](#using-custom-data)\n- [Training and Finetuning](#training-and-finetuning)\n  * [Pretrained Models](#pretrained-models)\n    + [GPT-NeoX-20B](#gpt-neox-20b)\n    + [Pythia](#pythia)\n    + [Polyglot](#polyglot)\n- [Inference](#inference)\n- [Evaluation](#evaluation)\n- [Exporting to Hugging Face](#exporting-to-hugging-face)\n- [Monitoring](#monitoring)\n  * [Weights and Biases](#weights-and-biases)\n  * [TensorBoard](#tensorboard)\n- [Running on multi-node](#running-on-multi-node)\n- [Profiling](#profiling)\n- [Adoption and Publications](#adoption-and-publications)\n  * [Publications](#publications)\n  * [Models](#models)\n    + [English LLMs](#english-llms)\n    + [Non-English LLMs](#non-english-llms)\n    + [Code Models](#code-models)\n    + [Other Modalities](#other-modalities)\n- [Administrative Notes](#administrative-notes)\n  * [Citing GPT-NeoX](#citing-gpt-neox)\n  * [Contributing](#contributing)\n  * [Licensing](#licensing)\n  * [Acknowledgements](#acknowledgements)\n\n# Quick Start\n\n## Environment and Dependencies\n\n### Host Setup\n\nThis codebase has primarily developed and tested for Python 3.8-3.10, and PyTorch 1.8-2.0. This is not a strict requirement, and other versions and combinations of libraries may work.\n\nTo install the remaining basic dependencies, run:\n\n```bash\npip install -r requirements/requirements.txt\npip install -r requirements/requirements-wandb.txt # optional, if logging using WandB\npip install -r requirements/requirements-tensorboard.txt # optional, if logging via tensorboard\npip install -r requirements/requirements-comet.txt # optional, if logging via Comet\n```\n\nfrom the repository root.\n\n> [!Warning]\n> Our codebase relies on [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed), our fork of the [DeepSpeed](https://github.com/microsoft/DeepSpeed) library with some added changes. We strongly recommend using Anaconda, a virtual machine, or some other form of environment isolation before continuing. Failure to do so may cause other repositories that rely on DeepSpeed to break.\n\n</aside>\n\n### Fused Kernels\nWe now support AMD GPUs (MI100, MI250X) through JIT fused-kernel compilation. Fused kernels will be built and loaded as needed. To avoid waiting during job launching, you can also do the following for manual pre-build:\n\n```python\npython\nfrom megatron.fused_kernels import load\nload()\n```\nThis will automatically adapts building process over different GPU vendors (AMD, NVIDIA) without platform specific code changes. To further test fused kernels using `pytest`, use `pytest tests/model/test_fused_kernels.py`\n\n### Flash Attention\n\nTo use [Flash-Attention](https://github.com/HazyResearch/flash-attention), install the additional dependencies in  `./requirements/requirements-flashattention.txt` or use a PyTorch NGC container with it pre-installed (note that functionality is not guaranteed using versions different from our requirements file). Then set the attention type in your configuration accordingly (see [configs](./configs/)). This can provide significant speed-ups over regular attention on certain GPU architectures, including Ampere GPUs (such as A100s); see the repository for more details.\n\n### Transformer Engine\n\nTo use [Transformer Engine (TE)](https://github.com/NVIDIA/TransformerEngine), install the additional dependencies in  `./requirements/requirements-transformer-engine.txt` or use a PyTorch NGC container with it pre-installed (note that functionality is not guaranteed using versions different from our requirements file). See [this config](https://github.com/EleutherAI/gpt-neox/blob/main/configs/1-3B-transformer-engine.yml) for an example of using TE on a 1.3B model. This can provide significant speed-ups over regular attention on certain GPU architectures, including Ampere and Hopper GPUs; see the repository for more details.\n\n\nTE provides very efficient kernels for both A100 and H100 GPUs. We've run some sample ablations on A100:\n\n\n\nand H100:\n\n\n\n\n### Multi-Node Launching\n\nNeoX and Deep(er)Speed support training on multiple different nodes and you have the option of using a variety of different launchers to orchestrate multi-node jobs.\n\nIn general there needs to be a \"hostfile\" somewhere accessible with the format:\n\n```bash\nnode1_ip slots=8\nnode2_ip slots=8\n```\n\nwhere the first column contains the IP address for each node in your setup and the number of slots is the number of GPUs that node has access to. In your config you must pass in the path to the hostfile with `\"hostfile\": \"/path/to/hostfile\"`. Alternatively the path to the hostfile can be in the environment variable `DLTS_HOSTFILE`.\n\n#### pdsh\n\n`pdsh` is the default launcher, and if you're using `pdsh` then all you must do (besides ensuring that pdsh is installed in your environment) is set `{\"launcher\": \"pdsh\"}` in your config files.\n\n#### MPI\n\nIf using MPI then you must specify the MPI library (DeepSpeed/GPT-NeoX currently supports `mvapich`, `openmpi`, `mpich`, and `impi`, though `openmpi` is the most commonly used and tested) as well as pass the `deepspeed_mpi` flag in your config file:\n\n```json\n{\n    \"launcher\": \"openmpi\",\n    \"deepspeed_mpi\": true\n}\n```\n\nWith your environment properly set up and the correct configuration files you can use `deepy.py` like a normal python script and start (for example) a training job with:\n\n`python3 deepy.py train.py /path/to/configs/my_model.yml`\n\n#### Slurm\n\nUsing Slurm can be slightly more involved. Like with MPI, you must add the following to your config:\n\n```json\n{\n    \"launcher\": \"slurm\",\n    \"deepspeed_slurm\": true\n}\n```\nIf you do not have ssh access to the compute nodes in your Slurm cluster you need to add `{\"no_ssh_check\": true}`\n\n#### (Advanced) Custom Launching\n\nThere are many cases where the above default launching options are not sufficient\n\n- Many clusters have their own unique job scheduler or specific MPI/Slurm arguments necessary for launching jobs such as [Summit JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) or [LLNL Flux](https://computing.llnl.gov/projects/flux-building-framework-resource-management)\n- While the above Slurm/MPI/pdsh default options are enough for most job runs, advanced users may want to add arguments for optimization or debugging purposes\n\nIn these cases, you will need to modify the DeepSpeed [multinode runner](https://github.com/microsoft/DeepSpeed/blob/17957728c0362bf8ae70feca308e491e55ef9feb/deepspeed/launcher/multinode_runner.py) utility to support your usecase. Broadly, these enhancements fall under two categories:\n\n##### 1. Adding a Launcher (e.g. [JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun), [Flux](https://computing.llnl.gov/projects/flux-building-framework-resource-management), etc)\n\nIn this case, you must add a new multinode runner class to `deepspeed/launcher/multinode_runner.py` and expose it as a configuration option in GPT-NeoX. Examples on how we did this for [Summit JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) are in [this DeeperSpeed commit](https://github.com/EleutherAI/DeeperSpeed/commit/9aed6c8500d7c492d85c5c88687322dbda70e370) and [this GPT-NeoX commit](https://github.com/EleutherAI/gpt-neox/commit/3782c7ae60f8624e566e3879b89bb09e8b59b869), respectively.\n\n##### 2. Modifying Run Command or Environment Variables\n\nWe have encountered many cases where we wish to modify the MPI/Slurm run command for an optimization or to debug (e.g. to modify the [Slurm srun CPU binding](https://slurm.schedmd.com/srun.html#OPT_cpu-bind) or to tag MPI logs with the rank). In this case, you must modify the multinode runner class' run command under its `get_cmd` method (e.g. [mpirun_cmd](https://github.com/microsoft/DeepSpeed/blob/17957728c0362bf8ae70feca308e491e55ef9feb/deepspeed/launcher/multinode_runner.py#L135-L147) for OpenMPI). Examples on how we did this to provide optimized and rank-tagged run commands using Slurm and OpenMPI for the Stability cluster are in [this DeeperSpeed branch](https://github.com/microsoft/DeepSpeed/compare/master...EleutherAI:DeeperSpeed:v2.0-stability)\n\n\n#### Hostfile Generation\n\nIn general you will not be able to have a single fixed hostfile, so you need to have a script to generate one dynamically when your job starts. An example script to dynamically generate a hostfile using [Slurm](https://slurm.schedmd.com/documentation.html) and 8 GPUs per node is:\n\n```bash\n#!/bin/bash\nGPUS_PER_NODE=8\nmkdir -p /sample/path/to/hostfiles\n# need to add the current slurm jobid to hostfile name so that we don't add to previous hostfile\nhostfile=/sample/path/to/hostfiles/hosts_$SLURM_JOBID\n# be extra sure we aren't appending to a previous hostfile\nrm $hostfile &> /dev/null\n# loop over the node names\nfor i in `scontrol show hostnames $SLURM_NODELIST`\ndo\n    # add a line to the hostfile\n    echo $i slots=$GPUS_PER_NODE >>$hostfile\ndone\n```\n\n`$SLURM_JOBID` and `$SLURM_NODELIST` being environment variables Slurm will create for you. See the [sbatch documentation](https://slurm.schedmd.com/sbatch.html#SECTION_OUTPUT-ENVIRONMENT-VARIABLES) for a full list of available Slurm environment variables set at job creation time.\n\n#### Job Launching\n\nThen you can create an [sbatch](https://slurm.schedmd.com/sbatch.html) script from which to kick off your GPT-NeoX job. A bare-bones sbatch script on a Slurm-based cluster with 8 GPUs per node would look like this:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=\"neox\"\n#SBATCH --partition=your-partition\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:8\n\n# Some potentially useful distributed environment variables\nexport HOSTNAMES=`scontrol show hostnames \"$SLURM_JOB_NODELIST\"`\nexport MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nexport MASTER_PORT=12802\nexport COUNT_NODE=`scontrol show hostnames \"$SLURM_JOB_NODELIST\" | wc -l`\n\n# Your hostfile creation script from above\n./write_hostfile.sh\n# Tell DeepSpeed where to find our generated hostfile via DLTS_HOSTFILE\nexport DLTS_HOSTFILE=/sample/path/to/hostfiles/hosts_$SLURM_JOBID\n\n# Launch training\npython3 deepy.py train.py /sample/path/to/your/configs/my_model.yml\n\n```\n\nYou can then kick off a training run with `sbatch my_sbatch_script.sh`\n\n\n### Containerized Setup\n\nWe provide containers for [Apptainer](#apptainer) (formerly Singularity) and [Docker](#docker) under [containers](https://github.com/EleutherAI/gpt-neox/blob/main/containers/).\n\n#### Docker\n\nIf you prefer to run NeoX in a Docker container, we provide a Dockerfile and docker-compose configuration.\n\nRequirements to run the container are to have appropriate GPU drivers, an up-to-date installation of Docker, and [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed. To test if your installation is good you can use their \"sample workload\", which is:\n\n```\ndocker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\nProvided that will run, you need to export NEOX_DATA_PATH and NEOX_CHECKPOINT_PATH in your environment to specify your data directory and directory for storing and loading checkpoints:\n\n```\nexport NEOX_DATA_PATH=/mnt/sda/data/enwiki8 #or wherever your data is stored on your system\nexport NEOX_CHECKPOINT_PATH=/mnt/sda/checkpoints\n```\n\nAnd then, from the `gpt-neox/containers/docker` directory, you can build the image and run a shell in a container with\n\n```\ndocker compose run gpt-neox bash\n```\n\nAfter the build, you should be able to do this:\n```\nmchorse@537851ed67de:~$ echo $(pwd)\n/home/mchorse\nmchorse@537851ed67de:~$ ls -al\ntotal 48\ndrwxr-xr-x  1 mchorse mchorse 4096 Jan  8 05:33 .\ndrwxr-xr-x  1 root    root    4096 Jan  8 04:09 ..\n-rw-r--r--  1 mchorse mchorse  220 Feb 25  2020 .bash_logout\n-rw-r--r--  1 mchorse mchorse 3972 Jan  8 04:09 .bashrc\ndrwxr-xr-x  4 mchorse mchorse 4096 Jan  8 05:35 .cache\ndrwx------  3 mchorse mchorse 4096 Jan  8 05:33 .nv\n-rw-r--r--  1 mchorse mchorse  807 Feb 25  2020 .profile\ndrwxr-xr-x  2 root    root    4096 Jan  8 04:09 .ssh\ndrwxrwxr-x  8 mchorse mchorse 4096 Jan  8 05:35 chk\ndrwxrwxrwx  6 root    root    4096 Jan  7 17:02 data\ndrwxr-xr-x 11 mchorse mchorse 4096 Jan  8 03:52 gpt-neox\n```\n\nFor a long-running job, you should run\n\n```\ndocker compose up -d\n```\n\nto run the container in detached mode, and then, in a separate terminal session, run\n\n```\ndocker compose exec gpt-neox bash\n```\n\nYou can then run any job you want from inside the container.\n\nConcerns when running for a long time or in detached mode include\n - You will have to terminate the container manually when you are no longer using it\n - If you want processes to continue running when your shell session ends, you will need to background them.\n - If you then want logging, you will have to make sure to pipe logs to disk, and set up wandb and/or Comet logging.\n\nIf you prefer to run the prebuilt container image from dockerhub, you can run the docker compose commands with ```-f docker-compose-dockerhub.yml``` instead, e.g.,\n\n```\ndocker compose run -f containers/docker/docker-compose-dockerhub.yml gpt-neox bash\n```\n\n#### Singularity/Apptainer\n\nWe also support Apptainer (formerly Singularity) deployments. Some users find Apptainer useeful for systems that don't provide root-access, such as shared HPC systems at national labs and universities.\n\nRequirements to run the container are to have appropriate GPU drivers and an up-to-date installation of Apptainer. You can build an image from our Apptainer file by running:\n\n```\ncd containers/apptainer/\napptainer build gpt-neox.sif gpt-neox.def\n```\n\nYou can use your new image in a few ways:\n\n1. Run a shell inside the container:\n```\napptainer shell --nv --bind /path/to/data:/data,/path/to/code:/code gpt-neox.sif\n```\n\nUse the --nv flag to enable NVIDIA GPU support.\n\n2. Execute a command directly:\n```\napptainer exec --nv gpt-neox.sif python your_script.py\n```\n\nFor this method, you'll need to make the requirements files and fused_kernels directory available during the build process, either by:\n\n- Using the `--bind` option during build\n- Adding them to the definition file using the `%files` section\n- Copying them into specific locations during build\n\nApptainer/Singularity containers run with the user's own UID/GID by default, so some of the user creation parts may be redundant.\nBy default, your home directory is automatically mounted in Apptainer/Singularity containers, which differs from Docker behavior.\nFor more info on Apptainer deployment, we suggest consulting [their userguide](https://apptainer.org/docs/user/1.0/index.html)\n\n## Usage\n\nAll functionality should be launched using `deepy.py`, a wrapper around the `deepspeed` launcher.\n\nWe currently offer three main functions:\n1. `train.py` is used for training and finetuning models.\n2. `eval.py` is used to evaluate a trained model using the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness).\n3. `generate.py` is used to sample text from a trained model.\n\nwhich can be launched with:\n\n```bash\n./deepy.py [script.py] [./path/to/config_1.yml] [./path/to/config_2.yml] ... [./path/to/config_n.yml]\n```\n\nFor example, to launch training you can run\n```bash\n./deepy.py train.py ./configs/20B.yml ./configs/local_cluster.yml\n```\n\nFor more details on each entry point, see the [Training and Finetuning](#training-and-finetuning), [Inference](#inference) and [Evaluation](#evaluation) respectively.\n\n# Configuration\n\nGPT-NeoX parameters are defined in a YAML configuration file which is passed to the deepy.py launcher. We have provided some example .yml files in [configs](./configs/), showing a diverse array of features and model sizes.\n\nThese files are generally complete, but non-optimal. For example, depending on your specific GPU configuration, you may need to change some settings such as `pipe-parallel-size`, `model-parallel-size` to increase or decrease the degree of parallelisation, `train_micro_batch_size_per_gpu` or `gradient-accumulation-steps` to modify batch size related settings, or the `zero_optimization` dict to modify how optimizer states are parallelised across workers.\n\nFor a more detailed guide to the features available and how to configure them, see [the configuration README](configs/README.md), and for documentation of every possible argument, see [configs/neox_arguments.md](configs/neox_arguments.md).\n\n## Mixture of Experts\n\nGPT-NeoX includes support for Dropless Mixture of Experts (DMoE) through the `megablocks` library. It is compatible with both existing Megatron Tensor Parallelism and DeepSpeed Pipeline Parallel setups.\n\nThis implementation leverages the existing Tensor Parallel Group to also shard the expert weights.\nIt uses Sinkhorn routing to avoid the need for a load balancing loss.\n\nFor an example of a basic complete configuration, see configs/125M-dmoe.yml.\n\nMost MoE related configuration arguments are prefixed with `moe`. The bare minimum addition to your configuration to enable MoE is as follows:\n\n```yaml\nmoe_num_experts: 1 # 1 disables MoE. 8 is a common value.\n```\n\n# Datasets\n\n## Preconfigured Datasets\n\nSeveral preconfigured datasets are available, including most components from [the Pile](https://arxiv.org/abs/2101.00027), as well as the Pile train set itself, for straightforward tokenization using the `prepare_data.py` entry point.\n\nE.G, to download and tokenize the enwik8 dataset with the GPT2 Tokenizer, saving them to `./data` you can run:\n\n```\npython prepare_data.py -d ./data\n```\n\nor a single shard of the pile (`pile_subset`) with the GPT-NeoX-20B tokenizer (assuming you have it saved at `./20B_checkpoints/20B_tokenizer.json`):\n\n```\npython prepare_data.py -d ./data -t HFTokenizer --vocab-file ./20B_checkpoints/20B_tokenizer.json pile_subset\n```\n\nThe tokenized data will be saved out to two files: `[data-dir]/[dataset-name]/[dataset-name]_text_document.bin`and `[data-dir]/[dataset-name]/[dataset-name]_text_document.idx`. You will need to add the prefix that both these files share to your training configuration file under the `data-path` field. E.G:\n\n```yaml\n  \"data-path\": \"./data/enwik8/enwik8_text_document\",\n```\n\n## Using Custom Data\n\nTo prepare your own dataset for training with custom data, format it as one large [jsonl](https://jsonlines.org/)-formatted file with each item in the list of dictionaries being a separate document. The document text should be grouped under one JSON key, i.e `\"text\"`. Any auxiliary data stored in other fields will not be used.\n\nNext make sure to download the GPT2 tokenizer vocab, and merge files from the following links:\n\n- Vocab: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n- Merge: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n\nOr use the 20B tokenizer (for which only a single Vocab file is needed):\n\n- Vocab: https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/20B_tokenizer.json\n\n(alternatively, you can provide any tokenizer file that can be loaded by Hugging Face's tokenizers library with the `Tokenizer.from_pretrained()` command)\n\nYou can now pretokenize your data using `tools/datasets/preprocess_data.py`, the arguments for which are detailed below:\n\n```\nusage: preprocess_data.py [-h] --input INPUT [--jsonl-keys JSONL_KEYS [JSONL_KEYS ...]] [--num-docs NUM_DOCS] --tokenizer-type {HFGPT2Tokenizer,HFTokenizer,GPT2BPETokenizer,CharLevelTokenizer} [--vocab-file VOCAB_FILE] [--merge-file MERGE_FILE] [--append-eod] [--ftfy] --output-prefix OUTPUT_PREFIX\n                          [--dataset-impl {lazy,cached,mmap}] [--workers WORKERS] [--log-interval LOG_INTERVAL]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ninput data:\n  --input INPUT         Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated list\n  --jsonl-keys JSONL_KEYS [JSONL_KEYS ...]\n                        space separate listed of keys to extract from jsonl. Default: text\n  --num-docs NUM_DOCS   Optional: Number of documents in the input data (if known) for an accurate progress bar.\n\ntokenizer:\n  --tokenizer-type {HFGPT2Tokenizer,HFTokenizer,GPT2BPETokenizer,CharLevelTokenizer}\n                        What type of tokenizer to use.\n  --vocab-file VOCAB_FILE\n                        Path to the vocab file\n  --merge-file MERGE_FILE\n                        Path to the BPE merge file (if necessary).\n  --append-eod          Append an <eod> token to the end of a document.\n  --ftfy                Use ftfy to clean text\n\noutput data:\n  --output-prefix OUTPUT_PREFIX\n                        Path to binary output file without suffix\n  --dataset-impl {lazy,cached,mmap}\n                        Dataset implementation to use. Default: mmap\n\nruntime:\n  --workers WORKERS     Number of worker processes to launch\n  --log-interval LOG_INTERVAL\n                        Interval between progress updates\n\n```\n\nFor example:\n\n```bash\npython tools/datasets/preprocess_data.py \\\n            --input ./data/mydataset.jsonl.zst \\\n            --output-prefix ./data/mydataset \\\n            --vocab ./data/gpt2-vocab.json \\\n            --merge-file gpt2-merges.txt \\\n            --dataset-impl mmap \\\n            --tokenizer-type GPT2BPETokenizer \\\n            --append-eod\n```\n\nYou would then run training with the following settings added to your configuration file:\n\n```yaml\n  \"data-path\": \"data/mydataset_text_document\",\n```\n\n# Training and Finetuning\n\nTraining is launched using `deepy.py`, a wrapper around DeepSpeed's launcher, which launches the same script in parallel across many GPUs / nodes.\n\nThe general usage pattern is:\n\n```bash\npython ./deepy.py train.py [path/to/config1.yml] [path/to/config2.yml] ...\n```\n\nYou can pass in an arbitrary number of configs which will all be merged at runtime.\n\nYou can also optionally pass in a config prefix, which will assume all your configs are in the same folder and append that prefix to their path.\n\nFor example:\n\n```bash\npython ./deepy.py train.py -d configs 125M.yml local_setup.yml\n```\n\nThis will deploy the `train.py` script on all nodes with one process per GPU. The worker nodes and number of GPUs are specified in the `/job/hostfile` file (see [parameter documentation](configs/README.md)), or can simply be passed in as the `num_gpus` arg if running on a single node setup.\n\nAlthough this is not strictly necessary, we find it useful to define the model parameters in one config file (e.g `configs/125M.yml`) and the data path parameters in another (e.g `configs/local_setup.yml`).\n\n\n## Pretrained Models\n\n### GPT-NeoX-20B\n\nGPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on [the Pile](https://arxiv.org/abs/2101.00027). Technical details about GPT-NeoX-20B can be found in [the associated paper](https://arxiv.org/abs/2204.06745). The configuration file for this model is both available at [`./configs/20B.yml`](./configs/20B.yml) and included in the download links below.\n\n[Slim weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/) - (No optimizer states, for inference or finetuning, 39GB)\n\nTo download from the command line to a folder named `20B_checkpoints`, use the following command:\n\n```bash\nwget --cut-dirs=5 -nH -r --no-parent --reject \"index.html*\" https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/ -P 20B_checkpoints\n```\n\n[Full weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights/) - (Including optimizer states, 268GB)\n\nTo download from the command line to a folder named `20B_checkpoints`, use the following command:\n\n```bash\nwget --cut-dirs=5 -nH -r --no-parent --reject \"index.html*\" https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights/ -P 20B_checkpoints\n```\n\nWeights can be alternatively be downloaded using a BitTorrent client. Torrent files can be downloaded here: [slim weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights.torrent), [full weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights.torrent).\n\nWe additionally have 150 checkpoints saved throughout training, one every 1,000 steps. We are working on figuring out how to best serve these at scale, but in the meanwhile people interested in working with the partially trained checkpoints can email us at contact@eleuther.ai to arrange access.\n\n### Pythia\n\nThe Pythia Scaling Suite is a suite of models ranging from 70M parameters to 12B parameters trained on [the Pile](https://pile.eleuther.ai) intended to promote research on interpretability and training dynamics of large language models. Further details about the project and links to the models can be found in the [in the paper](https://arxiv.org/abs/2304.01373) and [on the project's GitHub](https://github.com/EleutherAI/pythia).\n\n### Polyglot\n\nThe Polyglot Project is an effort to train powerful non-English pretrained language models to promote the accessibility of this technology to researchers outside the dominant powerhouses of machine learning. EleutherAI has trained and released 1.3B, 3.8B, and 5.8B parameter Korean language models, the largest of which outpreforms all other publicly available language models on Korean language tasks. Further details about the project and links to the models can be found [here](https://github.com/EleutherAI/polyglot).\n\n# Inference\n\n**For most uses we recommend deploying models trained using the GPT-NeoX library via the Hugging Face Transformers library which is better optimized for inference.**\n\nWe support three types of generation from a pretrained model:\n1. Unconditional generation\n2. Conditional generation based on an input read from a file\n3. Interactive generation, which allows for multiple rounds of back-and-forth between a user and the language model via a command line interface\n\nAll three types of text generation can be launched via `python ./deepy.py generate.py -d configs 125M.yml local_setup.yml text_generation.yml` with the appropriate values set in `configs/text_generation.yml`.\n\n# Evaluation\n\nGPT-NeoX supports evaluation on downstream tasks through the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\nTo evaluate a trained model on the evaluation harness, simply run:\n\n```bash\npython ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n```\n\nwhere `--eval_tasks` is a list of evaluation tasks followed by spaces, e.g `--eval_tasks lambada hellaswag piqa sciq`. For details of all tasks available, refer to the [lm-evaluation-harness repo](https://github.com/EleutherAI/lm-evaluation-harness).\n\n# Exporting to Hugging Face\n\nGPT-NeoX is optimized heavily for training only, and GPT-NeoX model checkpoints are not compatible out of the box with other deep learning libraries. To make models easily loadable and shareable with end users, and for further exporting to various other frameworks, GPT-NeoX supports checkpoint conversion to the [Hugging Face Transformers](https://arxiv.org/abs/1910.03771) format.\n\nThough NeoX supports a number of different architectural configurations, including AliBi positional embeddings, not all of these configurations map cleanly onto the supported configurations within Hugging Face Transformers.\n\nNeoX supports export of compatible models into the following architectures:\n- GPTNeoXForCausalLM\n- LlamaForCausalLM\n- MistralForCausalLM\n\nTraining a model which does not fit into one of these Hugging Face Transformers architectures cleanly will require writing custom modeling code for the exported model.\n\nTo convert a GPT-NeoX library checkpoint to Hugging Face-loadable format, run:\n```bash\npython ./tools/ckpts/convert_neox_to_hf.py --input_dir /path/to/model/global_stepXXX --config_file your_config.yml --output_dir hf_model/save/location --precision {auto,fp16,bf16,fp32} --architecture {neox,mistral,llama}\n```\n\nThen to upload a model to [the Hugging Face Hub](https://huggingface.co/), run:\n```bash\nhuggingface-cli login\npython ./tools/ckpts/upload.py\n```\nand input the requested information, including HF hub user token.\n\n### Importing Models Into GPT-NeoX\n\nNeoX supplies several utilities for converting a pretrained model checkpoint into a format that can be trained within the library.\n\nThe following models or model families can be loaded in GPT-NeoX:\n- Llama 1\n- Llama 2\n- CodeLlama\n- Mistral-7b-v0.1\n\nWe provide two utilities for converting from two different checkpoint formats into a format compatible with GPT-NeoX.\n\nTo convert a Llama 1 or Llama 2 checkpoint distributed by Meta AI from its original file format (downloadable [here](https://github.com/facebookresearch/llama) or [here](https://huggingface.co/meta-llama/Llama-2-7b)) into the GPT-NeoX library, run\n\n```\npython tools/ckpts/convert_raw_llama_weights_to_neox.py --input_dir /path/to/model/parent/dir/7B --model_size 7B --output_dir /path/to/save/ckpt --num_output_shards <TENSOR_PARALLEL_SIZE> (--pipeline_parallel if pipeline-parallel-size >= 1)\n```\n\n\nTo convert from a Hugging Face model into a NeoX-loadable, run `tools/ckpts/convert_hf_to_sequential.py`. See documentation within that file for further options.\n\n\n# Monitoring\n\nIn addition to storing logs locally, we provide built-in support for two popular experiment monitoring frameworks: [Weights & Biases](https://wandb.ai/site), [TensorBoard](https://www.tensorflow.org/tensorboard/), and [Comet](https://www.comet.com/site)\n\n## Weights and Biases\n\n[Weights & Biases to record our experiments](https://wandb.ai/eleutherai/neox) is a machine learning monitoring platform. To use wandb to monitor your gpt-neox experiments:\n1. Create an account at https://wandb.ai/site to generate your API key\n2. Log into Weights & Biases on your machine&mdash;you can do this by executing `wandb login`&mdash;your runs will automatically be recorded.\n3. Dependencies required for wandb monitoring can be found in and installed from `./requirements/requirements-wandb.txt`. An example config is provided in `./configs/local_setup_wandb.yml`.\n4. There are two optional fields associated with Weights & Biases: <code><var>wandb_group</var></code> allows you to name the run group and <code><var>wandb_team</var></code> allows you to assign your runs to an organization or team account. An example config is provided in `./configs/local_setup_wandb.yml`.\n\n## TensorBoard\n\nWe support using TensorBoard via the <code><var>tensorboard-dir</var></code> field. Dependencies required for TensorBoard monitoring can be found in and installed from  `./requirements/requirements-tensorboard.txt`.\n\n## Comet\n\n[Comet](https://www.comet.com/site) is a machine learning monitoring platform. To use comet to monitor your gpt-neox experiments:\n1. Create an account at https://www.comet.com/login to generate your API key.\n2. Once generated, link your API key at runtime by running `comet login` or passing `export COMET_API_KEY=<your-key-here>`\n3. Install `comet_ml` and any dependency libraries via `pip install -r requirements/requirements-comet.txt`\n4. Enable Comet with `use_comet: True`. You can also customize where data is being logged with `comet_workspace` and `comet_project`. A full example config with comet enabled is provided in `configs/local_setup_comet.yml`.\n5. Run your experiment, and monitor metrics in the Comet workspace that you passed!\n\n# Running on multi-node\n\nIf you need to supply a hostfile for use with the MPI-based DeepSpeed launcher, you can set the environment variable `DLTS_HOSTFILE` to point to the hostfile.\n\n# Profiling\n\nWe support profiling with Nsight Systems, the PyTorch Profiler, and PyTorch Memory Profiling.\n\n## Nsight Systems Profiling\n\nTo use the Nsight Systems profiling, set config options `profile`, `profile_step_start`, and `profile_step_stop` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\nTo populate nsys metrics, launch training with:\n\n```\nnsys profile -s none -t nvtx,cuda -o <path/to/profiling/output> --force-overwrite true \\\n--capture-range=cudaProfilerApi --capture-range-end=stop python $TRAIN_PATH/deepy.py \\\n$TRAIN_PATH/train.py --conf_dir configs <config files>\n```\n\nThe generated output file can then by viewed with the Nsight Systems GUI:\n\n![nsight-prof](images/nsight_profiling.png)\n\n## PyTorch Profiling\n\nTo use the built-in PyTorch profiler, set config options `profile`, `profile_step_start`, and `profile_step_stop` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\nThe PyTorch profiler will save traces to your `tensorboard` log directory.  You can view these traces within\nTensorBoard by following the steps [here](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).\n\n![torch-prof](images/pytorch_profiling.png)\n\n## PyTorch Memory Profiling\n\nTo use PyTorch Memory Profiling, set config options `memory_profiling` and `memory_profiling_path` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\n![mem-prof](images/memory_profiling.png)\n\nView the generated profile with the [memory_viz.py](https://github.com/pytorch/pytorch/blob/main/torch/cuda/_memory_viz.py) script. Run with:\n\n```\npython _memory_viz.py trace_plot <generated_profile> -o trace.html\n```\n\n# Adoption and Publications\n\nThe GPT-NeoX library was been widely adopted by academic and industry researchers and ported on to many HPC systems.\n\nIf you have found this library useful in your research, please reach out and let us know! We would love to add you to our lists.\n\n## Publications\n\nEleutherAI and our collaborators have used it in the following publications:\n - **Sid Black**, **Stella Biderman**, **Eric Hallahan**, **Quentin Anthony**, **Leo Gao**, **Laurence Golding**, **Horace He**, **Connor Leahy**, **Kyle McDonell**, **Jason Phang**, **Michael Pieler**, **Shivanshu Purohit**, **Laria Reynolds**, **Jon Tow**, **Ben Wang**, and **Samuel Weinbach**. \"[GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745).\" In *Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models*, 2022.\n - **Stella Biderman**, **Hailey Schoelkopf**, **Quentin Anthony**, **Herbie Bradley**, **Kyle O'Brien**, **Eric Hallahan**, **Mohammad Aflah Khan**, **Shivanshu Purohit**, **USVSN Sai Prashanth**, Edward Raff, **Aviya Skowron**, **Lintang Sutawika**, **Oskar van der Wal**. \"[Pythia: A suite for analyzing large language models across training and scaling](https://arxiv.org/abs/2304.01373).\" In _International Conference on Machine Learning_, pp. 2397-2430. _PMLR_, 2023.\n - Zhangir Azerbayev, Bartosz Piotrowski, **Hailey Schoelkopf**, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. \"[Proofnet: Autoformalizing and formally proving undergraduate-level mathematics](https://arxiv.org/abs/2302.12433). *arXiv preprint arXiv:2302.12433*, 2023.\n - **Stella Biderman**, **USVSN Sai Prashanth**, **Lintang Sutawika**, **Hailey Schoelkopf**, **Quentin Anthony**, **Shivanshu Purohit**, and Edward Raff. \"[Emergent and predictable memorization in large language models.](https://arxiv.org/abs/2304.11158)\" In _Neural Information Processing Systems_, 2023.\n - **Hyunwoong Ko**, **Kichang Yang**, **Minho Ryu**, **Taekyoon Choi**, **Seungmu Yang,** and Sungho Park. \"[A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models](https://arxiv.org/abs/2306.02254).\" *arXiv preprint arXiv:2306.02254*, 2023.\n - Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats Leon Richter, **Quentin Anthony**, Eugene Belilovsky, Irina Rish, and Timothée Lesort. \"[Continual Pre-Training of Large Language Models: How to re-warm your model?](https://arxiv.org/abs/2308.04014)\" In _Workshop on Efficient Systems for Foundation Models @ ICML_, 2023.\n - **Zhangir Azerbayev**, **Hailey Schoelkopf**, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, **Stella Biderman**, and Sean Welleck. \"[Llemma: An open language model for mathematics]([https://arxiv.org/abs/2308.04014](https://arxiv.org/abs/2310.10631))\" In _Math-AI Workshop @ NeurIPS_, 2023.\n - Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, **Stella Biderman**, **Quentin Anthony**, and **Louis Castricato**. \"[trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://aclanthology.org/2023.emnlp-main.530/).\" In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.\n -  **Quentin Anthony**, **Jacob Hatef**, Deepak Narayanan, **Stella Biderman**, Stas Bekman, Junqi Yin, Aamir Shafi, Hari Subramoni, and Dhabaleswar Panda. \"[The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489).\" In _arXiv preprint_, 2024.\n - Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, **Quentin Anthony**, Timothée Lesort, Eugene Belilovsky, Irina Rish. \"[Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763).\" In _arXiv preprint_, 2024.\n - Junqi Yin, Avishek Bose, Guojing Cong, Isaac Lyngaas, **Quentin Anthony**. \"[Comparative Study of Large Language Model Architectures on Frontier](https://arxiv.org/abs/2402.00691).\" In _arXiv preprint_, 2024.\n\nThe following publications by other research groups use this library:\n- Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and Alexander Rudnicky. \"[KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation](https://arxiv.org/abs/2205.09921).\" In *Advances in Neural Information Processing Systems* 35, 2022.\n- Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova. \"[Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned](https://aclanthology.org/2022.bigscience-1.12/).\" In *Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models*, 2022.\n- Sophia Kolak, Ruben Martins, Claire Le Goues, and Vincent J. Hellendoorn. \"[Patch Generation with Language Models: Feasibility and Scaling Behavior](https://par.nsf.gov/biblio/10340618)\".\" In *Proceedings of the Deep Learning for Code Workshop at ICLR*, 2022.\n- Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. \"[A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/abs/2202.13169).\" In *Proceedings of the ICLR Workshop on Deep Learning For Code*, 2022.\n- Byung-Doh Oh and William Schuler. \"[Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](https://arxiv.org/abs/2304.11389).\" In *Findings of the Association for Computational Linguistics*, 2023.\n- Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. \"[Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://aclanthology.org/2023.acl-long.756/).\" In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13522-13537, 2023.\n- Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. \"[Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://aclanthology.org/2023.acl-short.102/).\" In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 13522-13537, 2023.\n- Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. \"[ChessGPT: Bridging Policy Learning and Language Modeling.](https://arxiv.org/abs/2306.09200)\" _arXiv preprint arXiv:2306.09200_, 2023.\n- Orion Walker Dollar, Sameera Horawalavithana, Scott Vasquez, W. James Pfaendtner, and Svitlana Volkova. \"[MolJET: Multimodal Joint Embedding Transformer for Conditional de novo Molecular Design and Multi-Property Optimization.](https://openreview.net/pdf?id=7UudBVsIrr)\" _preprint under review_, 2023.\n- Jean Kaddour and Qi Liu. \"[Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large Language Models](https://arxiv.org/abs/2310.01119).\" _arXiv:2310.01119_, 2023.\n- Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. \"[Efficient Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2312.02406).\" In _NeurIPS Workshop on R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models_, 2023.\n- Eghbal A. Hosseini and Evelina Fedorenko. \"[Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language](https://www.biorxiv.org/content/10.1101/2023.11.05.564832v1).\" In _Neural Information Processing Systems_, 2023.\n- Junqi Yin, Sajal Dash, Feiyi Wang, and Mallikarjun Shankar. \"[FORGE: Pre-Training Open Foundation Models for Science](https://dl.acm.org/doi/abs/10.1145/3581784.3613215). In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, 1-13, 2023.\n- Jean Kaddour and Qi Liu. \"[Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large Language Models](https://arxiv.org/abs/2310.01119).\" In _arXiv preprint arXiv:2310.01119_, 2023.\n- Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. \"[CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model](https://arxiv.org/abs/2310.06266).\" In _arXiv preprint arXiv:2310.06266_, 2023.\n- Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, and Vincent J Hellendoorn. \"[CAT-LM Training Language Models on Aligned Code And Tests](https://arxiv.org/abs/2310.01602).\" In _38th IEEE/ACM International Conference on Automated Software Engineering (ASE)_, pp. 409-420. IEEE, 2023.\n- Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, Ricardo Bianchini. \"[POLCA: Power Oversubscription in LLM Cloud Providers](https://arxiv.org/abs/2308.12908).\" In _arXiv preprint_, 2023.\n- Junqi Yin, Sajal Dash, John Gounley, Feiyi Wang, and Georgia Tourassi. \"[Evaluation of pre-training large language models on leadership-class supercomputers](https://link.springer.com/article/10.1007/s11227-023-05479-7).\" In _the Journal of Supercomputing_ 79, no. 18, 2023.\n- Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, and Gal Oren. \"[Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks](https://arxiv.org/abs/2312.13322).\" In _arXiv preprint_, 2023.\n- Guobin Shen, Dongcheng Zhao, Yiting Dong, Yang Li, Jindong Li, Kang Sun, and Yi Zeng. \"[Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling](https://arxiv.org/abs/2312.07625).\" In _arXiv preprint_, 2023.\n- Eghbal A. Hosseini, Martin A. Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and Evelina Fedorenko. \"[Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training.](https://www.biorxiv.org/content/10.1101/2022.10.04.510681)\" In _Neurobiology of Language_, 2024.\n- Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo Zhou, Shixuan Li, and Paul Bogdan. \"[Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective](https://arxiv.org/abs/2402.09099).\" In _arXiv preprint_, 2024.\n- Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, and Xipeng Qiu. \"[Turn Waste into Worth: Rectifying Top-k Router of MoE](https://arxiv.org/abs/2402.12399).\" In _arXiv preprint_, 2024.\n\n## Models\nThe following models were trained using this library:\n\n### English LLMs\n- EleutherAI's [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) and [Pythia (70M through 13B)](https://github.com/EleutherAI/pythia)\n- CarperAI's [FIM-NeoX-1.3B](https://huggingface.co/CarperAI/FIM-NeoX-1.3B)\n- StabilityAI's [StableLM (3B and 7B)](https://github.com/Stability-AI/StableLM)\n- Together.ai's [RedPajama-INCITE (3B and 7B)](https://together.ai/blog/redpajama-models-v1)\n- Carnegie Mellon University's [proofGPT (1.3B and 6.7B)](https://huggingface.co/hoskinson-center/proofGPT-v0.1-6.7B)\n- Dampish's [StellarX (2.8B and 4B)](https://huggingface.co/Dampish/StellarX-4B-V0.2)\n- Chinese Academy of Sciences's [AstroSNN (1.5B)](https://arxiv.org/abs/2312.07625)\n\n### Non-English LLMs\n- EleutherAI's [Polyglot-Ko (1.3B through 12.8B)](https://github.com/EleutherAI/polyglot) (Korean)\n- Korea University's [KULLM-Polyglot (5.8B and 12.8B)](https://github.com/nlpai-lab/KULLM) (Korean)\n- Stability AI's [Japanese Stable LM (7B)](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) (Japanese)\n- LearnItAnyway's [LLaVA-Polyglot-Ko (1.3B)](https://huggingface.co/LearnItAnyway/llava-polyglot-ko-1.3b-hf) (Korean)\n- Rinna Co.'s [japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) (Japanese) and [bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) (English / Japanese)\n- CyberAgent's [Open-CLM (125M through 7B)](https://huggingface.co/cyberagent/open-calm-7b) (Japanese)\n- The Hungarian Research Centre for Linguistics's [PULI GPTrio (6.7B)](https://huggingface.co/NYTK/PULI-GPTrio) (Hungarian / English / Chinese)\n- The University of Tokyo's [weblab-10b](https://huggingface.co/Kojima777/weblab-10b) and [weblab-10b-instruct](https://huggingface.co/Kojima777/weblab-10b-instruction-sft) (Japanese)\n- nolando.ai's [Hi-NOLIN (9B)](https://blog.nolano.ai/Hi-NOLIN/) (English, Hindi)\n- Renmin University of China's [YuLan (12B)](https://huggingface.co/yulan-team/YuLan-Base-12b) (English, Chinese)\n- The Basque Center for Language Technology's [Latixna (70B)](https://huggingface.co/HiTZ/latxa-70b-v1.2) (Basque)\n\n### Code Models\n- Carnegie Mellon University's [PolyCoder (160M through 2.7B)](https://github.com/VHellendoorn/Code-LMs) and [CAT-LM (2.7B)](https://huggingface.co/nikitharao/catlm)\n- StabilityAI's [StableCode (1.3B)](https://stability.ai/blog/stablecode-llm-generative-ai-coding) and [StableCode-Completion-Alpha (3B)](https://stability.ai/blog/stablecode-llm-generative-ai-coding)\n- CodeFuse AI's [CodeFuse (13B)](https://huggingface.co/codefuse-ai/CodeFuse-13B)\n\n### AI for Science\n- EleutherAI's [LLeMMA (34B)](https://arxiv.org/abs/2310.10631)\n- Oak Ridge National Lab's [FORGE (26B)](https://github.com/at-aaims/forge)\n- Oak Ridge National Lab's [Unnamed Material Science Domain Models (7B)](https://arxiv.org/abs/2402.00691)\n- Pacific Northwest National Lab's [MolJet (undisclosed size)](https://openreview.net/pdf?id=7UudBVsIrr)\n\n### Other Modalities\n-  Rinna Co.'s [PSLM (7B)](https://arxiv.org/abs/2406.12428) (speech / text)\n-  University College London's [ChessGPT-3B](https://huggingface.co/Waterhorse/chessgpt-base-v1)\n-  Gretel's [Text-to-Table (3B)](https://huggingface.co/gretelai/text2table)\n\n# Administrative Notes\n\n## Citing GPT-NeoX\n\nIf you have found the GPT-NeoX library helpful in your work, you can cite this repository as\n\n```bibtex\n@software{gpt-neox-library,\n  title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},\n  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Phang, Jason and Purohit, Shivanshu and Schoelkopf, Hailey and Stander, Dashiell and Songz, Tri and Tigges, Curt and Thérien, Benjamin and Wang, Phil and Weinbach, Samuel},\n  url = {https://www.github.com/eleutherai/gpt-neox},\n  doi = {10.5281/zenodo.5879544},\n  month = {9},\n  year = {2023},\n  version = {2.0.0},\n}\n```\n\nTo cite the 20 billion parameter model named `GPT-NeoX-20B`, please use\n\n```bibtex\n@inproceedings{gpt-neox-20b,\n  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},\n  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\n  booktitle={Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models},\n  url={https://arxiv.org/abs/2204.06745},\n  year={2022}\n}\n```\n\n## Contributing\nGPT-NeoX is built by the open-source AI community, and relies on our amazing contributors! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on our CLA, code formatting, testing,\netc.\n\n## Licensing\n\nThis repository hosts code that is part of EleutherAI's GPT-NeoX project. Copyright (c) 2024, EleutherAI. Licensed under the Apache License:\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\nThis repository is based off code written by NVIDIA that is licensed under the Apache License, Version 2.0. In accordance with the Apache License, all files that are modifications of code originally written by NVIDIA maintain a NVIDIA copyright header. All files that do not contain such a header are the exclusive copyright of EleutherAI. When the NVIDIA code has been modified from its original version, that fact is noted in the copyright header. All derivative works of this repository must preserve these headers under the terms of the Apache License.\n\nThis repository also contains code written by a number of other authors. Such contributions are marked and the relevant licensing is included where appropriate.\n\nFor full terms, see the `LICENSE` file. If you have any questions, comments, or concerns about licensing please email us at contact@eleuther.ai.\n\n## Acknowledgements\n\nWe run our experiments on a Kubernetes cluster provided by [CoreWeave](https://coreweave.com/) and a Slurm cluster provided by [Stability AI](https://stability.ai). We are thankful to the DeepSpeed team for their advice and consultation.\n",
    "py_files": {
        "configs/gen_docs.py": "import sys\nimport os\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n)\nfrom megatron.neox_arguments import neox_args, deepspeed_args\nfrom inspect import getmembers, getsource\nfrom dataclasses import field, is_dataclass\nfrom itertools import tee, zip_longest\nimport pathlib\n\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip_longest(a, b)\n\n\ndef get_docs(module):\n    ARGS_CLASSES = getmembers(module, is_dataclass)\n    results = {}\n    for name, dcls in ARGS_CLASSES:\n        assert is_dataclass(dcls)\n        src = getsource(dcls)\n        d = dcls()\n        loc = 0\n        results[name] = {\"doc\": d.__doc__.strip(), \"attributes\": {}}\n        for cur, _next in pairwise(d.__dataclass_fields__.items()):\n            field_name, field_def = cur\n            field_type = field_def.type\n            if hasattr(field_type, \"__name__\"):\n                if field_type.__name__ == \"Literal\" or field_type.__name__ == \"Union\":\n                    field_type = field_type\n                else:\n                    field_type = str(field_type.__name__)\n            else:\n                field_type = str(field_type)\n\n            field_default = field_def.default\n\n            # try to find the field definition\n            loc = src.find(f\" {field_name}:\", loc + len(field_name) + 1)\n\n            if _next is not None:\n                next_field_name, _ = _next\n                # try to find the next field definition\n                next_loc = src.find(f\"{next_field_name}:\", loc + len(field_name))\n            else:\n                next_loc = len(src)\n\n            # try to get the docstring\n            _src = src[loc:next_loc].strip()\n            if '\"\"\"' in _src:\n                doc = _src.split('\"\"\"')[1].strip()\n            elif \"'''\" in _src:\n                doc = _src.split(\"'''\")[1].strip()\n            else:\n                doc = \"\"\n            results[name][\"attributes\"][field_name] = {\n                \"name\": field_name,\n                \"type\": field_type,\n                \"default\": field_default,\n                \"doc\": doc,\n            }\n    return results\n\n\ndef to_md(docs, intro_str=\"\"):\n    \"\"\"\n    Writes the docs dictionary to markdown format\n    \"\"\"\n    lines = []\n    lines.append(intro_str)\n    for name, doc in docs.items():\n        lines.append(f\"## {name}\")\n        lines.append(f\"{doc['doc']}\")\n        lines.append(\"\")\n        for field_name, field_def in doc[\"attributes\"].items():\n            # attribute name and type\n            lines.append(f\"- **{field_name}**: {field_def['type']}\")\n            # default value\n            lines.append(f\"    Default = {str(field_def['default'])}\")\n            lines.append(f\"    {field_def['doc']}\")\n            lines.append(\"\")\n    return \"\\n\\n\".join(lines)\n\n\nif __name__ == \"__main__\":\n    docs = get_docs(neox_args)\n    docs.update(get_docs(deepspeed_args))\n    intro_str = \"\"\"Arguments for gpt-neox. All of the following can be specified in your .yml config file(s):\\n\"\"\"\n    md = to_md(docs, intro_str=intro_str)\n    with open(f\"{pathlib.Path(__file__).parent.resolve()}/neox_arguments.md\", \"w\") as f:\n        f.write(md)\n",
        "deepy.py": "#!/usr/bin/env python\n# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\n\nimport deepspeed.launcher.runner\n\n\ndef main(input_args=None):\n    logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"INFO\"))\n\n    from megatron.neox_arguments import NeoXArgs\n    from megatron.utils import get_wandb_api_key\n\n    neox_args = NeoXArgs.consume_deepy_args(input_args)\n    deepspeed_main_args = neox_args.get_deepspeed_main_args()\n\n    # Extract wandb API key and inject into worker environments\n    wandb_token = get_wandb_api_key(neox_args=neox_args)\n    if wandb_token is not None:\n        deepspeed.launcher.runner.EXPORT_ENVS.append(\"WANDB_API_KEY\")\n        os.environ[\"WANDB_API_KEY\"] = wandb_token\n\n    deepspeed.launcher.runner.main(deepspeed_main_args)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "eval.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Evaluation tasks - modified from https://github.com/EleutherAI/lm-evaluation-harness\"\"\"\nimport os\nimport sys\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n)\nfrom megatron.training import forward_step\nfrom megatron.utils import setup_for_inference_or_eval, init_wandb\nfrom megatron.logging import tb_wandb_log\nfrom eval_tasks import run_eval_harness\nfrom pprint import pprint\nfrom datetime import datetime\nimport json\n\n\ndef main(input_args=None, overwrite_values=None):\n    model, neox_args = setup_for_inference_or_eval(\n        use_cache=False, input_args=input_args, overwrite_values=overwrite_values\n    )\n    results = run_eval_harness(\n        model,\n        forward_step,\n        neox_args,\n        eval_tasks=neox_args.eval_tasks,\n        bootstrap_iters=10000,\n    )\n    if neox_args.rank == 0:\n        init_wandb(neox_args=neox_args)\n        # log to wandb\n        for k, v in results[\"results\"].items():\n            if isinstance(v, dict):\n                for k2, v2 in v.items():\n                    k3 = \"_\".join([k, k2])\n                    tb_wandb_log(\n                        f\"eval/{k3}\",\n                        v2,\n                        neox_args.iteration,\n                        use_wandb=neox_args.use_wandb,\n                        comet_experiment=neox_args.comet_experiment,\n                    )\n            else:\n                tb_wandb_log(\n                    f\"eval/{k}\",\n                    v,\n                    neox_args.iteration,\n                    use_wandb=neox_args.use_wandb,\n                    comet_experiment=neox_args.comet_experiment,\n                )\n\n        pprint(results)\n        results_path = (\n            f'eval_results_{datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")}.json'\n        )\n        if neox_args.eval_results_prefix:\n            results_path = f\"{neox_args.eval_results_prefix}_{results_path}\"\n        with open(results_path, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "eval_tasks/__init__.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .eval_adapter import EvalHarnessAdapter, run_eval_harness\n",
        "eval_tasks/eval_adapter.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.utils import is_local_main, print_rank_0\n\nimport copy\nimport os\nimport sys\nimport dataclasses\nfrom functools import partial\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n)\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\n\nfrom lm_eval.models.huggingface import HFLM\nfrom lm_eval import tasks, evaluator, utils, api\nfrom megatron.text_generation_utils import generate_samples_from_prompt\nfrom megatron import mpu\n\n\nclass EvalHarnessAdapter(HFLM):\n    \"\"\"\n    An adapter to run NeoX models on LM Evaluation Harness (https://github.com/EleutherAI/lm-evaluation-harness) tasks.\n\n    Args:\n        model: A NeoX Model\n        forward_step_fn: A function that runs a forward pass through the model, returning `tuple(loss, logits)`.\n        neox_args: a NeoXArgs object containing the model configuration.\n        batch_size (optional): An argument to override the batch size, which defaults to batch size per gpu * dp world size.\n    \"\"\"\n\n    def __init__(self, model, forward_step_fn, neox_args, batch_size=None):\n        self.cache_hook = api.model.CacheHook(None)\n        self._model = model\n        self.neox_args = neox_args\n        self.tokenizer = neox_args.tokenizer\n        self._device = torch.device(f\"cuda:{neox_args.local_rank}\")\n        self._eot_token_id = neox_args.tokenizer.eod_id\n        self._max_length = neox_args.max_position_embeddings\n        self._max_gen_toks = 128\n        self._vocab_size = neox_args.padded_vocab_size\n\n        # parallelism args:\n        self.is_main = neox_args.rank == 0\n        self.is_local_main = neox_args.local_rank == 0\n        self.is_model_parallel = neox_args.model_parallel_size > 1\n        self.is_pipe_parallel = self.model.is_pipe_parallel\n        self.is_data_parallel = self.model.is_data_parallel\n        self.is_last_stage = (\n            True if not self.is_pipe_parallel else model.is_last_stage()\n        )  # only the last stage of the pipeline model will receive the logits\n        self.dp_world_size = mpu.get_data_parallel_world_size()\n        self.dp_rank = mpu.get_data_parallel_rank()\n        self.dp_group = mpu.get_data_parallel_group()\n        self.is_mp_rank_0 = mpu.get_model_parallel_rank() == 0\n\n        self._batch_size = batch_size or (\n            neox_args.batch_size * self.dp_world_size\n        )  # default batch size to bs per gpu * dp size\n        # some utility functions:\n        # we need to patch tokenizer methods, because lm_eval uses them internally:\n        self.tokenizer.encode = self.tokenizer.tokenize\n        self.tokenizer.decode = self.tokenizer.detokenize\n        self._forward_step_fn = partial(\n            forward_step_fn, neox_args=neox_args, timers=None, return_logits=True\n        )\n        self.generate = partial(\n            generate_samples_from_prompt,\n            neox_args=neox_args,\n            model=model,\n        )\n\n    @property\n    def vocab_size(self):\n        return self._vocab_size\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self._eot_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def max_gen_toks(self):\n        return self._max_gen_toks\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return 0\n\n    @property\n    def world_size(self):\n        return 1\n\n    def tok_encode(self, string: str, **kwargs):\n        return self.tokenizer.encode(string)\n\n    def tok_decode(self, tokens, **kwargs):\n        return self.tokenizer.decode(tokens)\n\n    def generate_until(self, requests):\n        \"\"\"\n        Generate until is lm_eval harness' way to say \"do greedy generation\" - necessary for some tasks.\n        the eval harness dispatches requests to the model, and the model does argmax generation, the results of which\n        are returned to the eval harness to evaluate.\n\n        TODO: batched / data parallel generation\n\n        :param requests: Dictionary of requests containing the context (prompt) and 'until' - a token or\n                         list of stop tokens.\n        \"\"\"\n        self.model.module.inference_mode(use_cache=True)  # tell model to cache kv pairs\n        res = []\n\n        # get only the args from each Instance object\n        reqs = [req.args for req in requests]\n\n        def _collate(x):\n            toks = self.tokenizer.encode(x[0])\n            return (len(toks), x[0])\n\n        reord = utils.Reorderer(reqs, _collate)\n        for context, gen_kwargs in tqdm(\n            reord.get_reordered(), \"Running greedy generation\"\n        ):\n            if isinstance(gen_kwargs, dict):\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\n                if \"until\" in kwargs.keys():\n                    until = kwargs.pop(\"until\")\n                    if isinstance(until, str):\n                        until = [kwargs]\n                    elif not isinstance(until, list):\n                        raise ValueError(\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\n                        )\n            else:\n                raise ValueError(\n                    f\"Expected `kwargs` to be of type `dict` but got {kwargs}\"\n                )\n            if not until:\n                until = [self.tok_decode(self.eot_token_id)]\n            if \"max_gen_toks\" in kwargs.keys():\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\n            else:\n                max_gen_toks = self.max_gen_toks\n\n            if \"do_sample\" in kwargs.keys():\n                kwargs.pop(\"do_sample\")\n\n            stop_tokens = [self.tokenizer.encode(i) for i in until]\n            cont = self.generate(\n                text=context,\n                stop_tokens=stop_tokens,\n                recompute=self.neox_args.recompute,\n                maximum_tokens=max_gen_toks,\n                **kwargs,\n            )\n            if cont:\n                s = cont[0][\"text\"] or \"\"\n            else:\n                s = \"\"\n\n            for term in until:\n                s = s.split(term)[0]\n\n            # partial caching\n            self.cache_hook.add_partial(\"generate_until\", (context, until), s)\n\n            res.append(s)\n\n        self.model.module.train_mode()  # set back to train mode\n        return reord.get_original(res)\n\n    def _loglikelihood_tokens(self, requests, disable_tqdm=False):\n        \"\"\"\n        In this method, the model doesn't do any generation, but just returns log likelihoods\n        for the next token, which eval harness uses to evaluate.\n\n        :param requests: Dictionary of requests containing the context and the expected continuation.\n        :param disable_tqdm: If True, disable tqdm progress bar.\n        \"\"\"\n        self.model.module.inference_mode(\n            use_cache=False\n        )  # tell model to gather parallel outputs, but not cache key-value pairs\n\n        disable_tqdm = disable_tqdm if self.is_main else True\n        res = []\n        res_len = 0  # storing the result length for later\n        with torch.no_grad():\n\n            def _collate(x):\n                toks = x[1] + x[2]\n                return (-len(toks), tuple(toks))\n\n            reord = utils.Reorderer(requests, _collate)\n            for chunk in utils.chunks(\n                tqdm(reord.get_reordered(), disable=disable_tqdm), self.batch_size\n            ):\n                inps, contlens, inplens, padding_length = [], [], [], None\n                for cache_key, context_enc, continuation_enc in chunk:\n                    # when too long to fit in context, truncate from the left\n                    inp = torch.tensor(\n                        (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\n                        dtype=torch.long,\n                    ).to(self.device)\n                    (inplen,) = inp.shape\n\n                    cont = continuation_enc\n\n                    # since in _collate we make sure length is descending, the longest is always the first one.\n                    padding_length = (\n                        padding_length if padding_length is not None else inplen\n                    )\n\n                    # pad to length\n                    inp = torch.cat(\n                        [\n                            inp,  # [seq]\n                            torch.zeros(padding_length - inplen, dtype=torch.long).to(\n                                inp.device\n                            ),  # [padding_length - seq]\n                        ],\n                        dim=0,\n                    )\n\n                    inps.append(inp.unsqueeze(0))\n                    contlens.append(cont)\n                    inplens.append(inplen)\n\n                logits = self._model_call(torch.cat(inps, dim=0))\n                res_len += len(chunk)\n\n                if logits is not None:\n                    multi_logits = F.log_softmax(logits, dim=-1)  # [batch, seq, vocab]\n                    for (cache_key, _, _), logits, inp, inplen, cont_toks in zip(\n                        chunk, multi_logits, inps, inplens, contlens\n                    ):\n                        contlen = len(cont_toks)\n                        logits = logits[inplen - contlen : inplen].unsqueeze(\n                            0\n                        )  # [1, seq, vocab]\n                        greedy_tokens = logits.argmax(dim=-1)\n                        # cont_toks :: [1, seq]\n                        cont_toks = (\n                            torch.tensor(cont_toks, dtype=torch.long)\n                            .unsqueeze(0)\n                            .to(multi_logits.device)\n                        )\n                        max_equal = (greedy_tokens == cont_toks).all()\n                        logits = torch.gather(\n                            logits, 2, cont_toks.unsqueeze(-1)\n                        ).squeeze(\n                            -1\n                        )  # [1, seq]\n                        answer = (float(logits.sum()), bool(max_equal))\n\n                        # partial caching\n                        if cache_key is not None:\n                            self.cache_hook.add_partial(\n                                \"loglikelihood\", cache_key, answer\n                            )\n\n                        res.append(answer)\n\n            # broadcast results to all ranks\n            if self.is_pipe_parallel:\n                src_rank = self.model.grid.stage_to_global(self.model.num_stages - 1)\n                if res:\n                    logits_sums, max_equals = list(zip(*res))\n                    logits_sums = torch.FloatTensor(logits_sums).cuda()\n                    max_equals = torch.LongTensor(max_equals).cuda()\n                else:\n                    logits_sums = torch.zeros(res_len, dtype=torch.float32).cuda()\n                    max_equals = torch.zeros(res_len, dtype=torch.int64).cuda()\n                torch.distributed.broadcast(\n                    tensor=logits_sums,\n                    src=src_rank,\n                    group=mpu.get_pipe_parallel_group(),\n                )\n                torch.distributed.broadcast(\n                    tensor=max_equals, src=src_rank, group=mpu.get_pipe_parallel_group()\n                )\n                max_equals = [bool(i) for i in max_equals.tolist()]\n                logits_sums = logits_sums.tolist()\n                res = list(zip(logits_sums, max_equals))\n\n        self.model.module.train_mode()  # set back to train mode\n        return reord.get_original(res)\n\n    def _dp_scatter(self, inps):\n        \"\"\"\n        Scatters the inputs to all data parallel ranks.\n        \"\"\"\n\n        batch_size = inps.shape[0]\n        padded = False\n        if batch_size % self.dp_world_size != 0:\n            # The last batch could potentially not fill the full batch size (if the dataset size is not divisible by batch size)\n            # In this case we pad the batch\n            padded_size = self.dp_world_size - (batch_size % self.dp_world_size)\n\n            print_rank_0(\n                f\"WARNING: Batch size ({batch_size}) must be divisible by dp world size ({self.dp_world_size}). Padding inputs to {padded_size}.\"\n            )\n\n            inps = torch.cat(\n                [inps] + [inps[0:1, ...] for _ in range(padded_size)], dim=0\n            )  # pad with first inp item\n            padded = True\n\n        assert (\n            inps.shape[0] % self.dp_world_size == 0\n        ), f\"batch size ({inps.shape[0]}) must be divisible by dp world size ({self.dp_world_size})\"\n\n        # get a chunk for each data parallel rank\n        chunk_size = inps.shape[0] // self.dp_world_size\n        inps = inps[self.dp_rank * chunk_size : (self.dp_rank + 1) * chunk_size]\n        # make a dummy dataloader / iterator to pass to model\n        # we need to do this because deepspeed pipe parallel only takes an iterator\n        # in this format\n        return iter([{\"text\": F.pad(inps, pad=(0, 1))}]), padded\n\n    def _dp_gather(self, logits):\n        \"\"\"\n        Gather logits from all data parallel ranks\n        \"\"\"\n        if logits is not None:\n            tensor_list = [torch.zeros_like(logits) for _ in range(self.dp_world_size)]\n            torch.distributed.all_gather(\n                tensor_list, logits, group=mpu.get_data_parallel_group()\n            )\n            logits = torch.cat(tensor_list, dim=0)\n            return logits\n\n    def _model_call(self, inps):\n        batch_size = inps.shape[0]\n\n        # scatter inputs to all dp ranks:\n        inps, padded = self._dp_scatter(inps)\n\n        if self.neox_args.is_pipe_parallel:\n            # need these flags to stop deepspeed pipe parallel from hanging\n            self.model.first_output_send = True\n            self.model.pipe_recv_buf = None\n\n        _, logits = self._forward_step_fn(model=self.model, data_iterator=inps)\n\n        # gather outputs from all dp ranks:\n        logits = self._dp_gather(logits)\n\n        # if logits have been padded (normally just last item where batch size is unequal)\n        # restore to original shape\n        if padded and logits is not None:\n            logits = logits[:batch_size, ...]\n        return logits\n\n    def _model_generate(self, context, max_length, eos_token_id):\n        # Isn't used because we override `greedy_until``.\n        raise NotImplementedError()\n\n    @torch.no_grad()\n    def run_eval(\n        self,\n        eval_tasks=None,\n        num_fewshot=0,\n        bootstrap_iters=2,\n        use_cache=True,\n        name=\"neox\",\n        limit=None,\n    ):\n        was_training = self.model.training\n        self.model.eval()\n        in_micro_batches = (\n            self.model.micro_batches\n        )  # store input microbatches - we need to set to 1 during eval, but want to return to its original value after\n        self.model.micro_batches = 1\n        if eval_tasks is None:\n            eval_tasks = [\n                \"lambada\",\n                \"piqa\",\n                \"hellaswag\",\n                \"winogrande\",\n                \"mathqa\",\n                \"pubmedqa\",\n                \"triviaqa\",\n            ]\n\n        # register all the default tasks bundled with lm-evaluation-harness repository\n        tasks.initialize_tasks()\n\n        # Returns a list containing all values of the task registry that\n        # match at least one of the patterns\n        import fnmatch\n\n        def pattern_match(patterns, source_list):\n            task_names = set()\n            for pattern in patterns:\n                for matching in fnmatch.filter(source_list, pattern):\n                    task_names.add(matching)\n            return list(task_names)\n\n        eval_tasks = pattern_match(eval_tasks, tasks.ALL_TASKS)\n        print(f\"Found tasks: {eval_tasks}\")\n\n        assert len(eval_tasks) > 0, \"Must run at least one task\"\n\n        # **HACK INCOMING**:\n        # first get task dict on local main rank\n        # the tasks are downloaded *as they are initialized*, and the downloads don't like multithreading.\n        # so we download them once on the local main rank, wait, and then initialize them on all other ranks, which *should* load from the cache.\n        if self.is_local_main:\n            task_dict = tasks.get_task_dict(eval_tasks)\n        # torch barrier\n        if torch.distributed.is_initialized():\n            torch.distributed.barrier()\n        task_dict = tasks.get_task_dict(eval_tasks)\n\n        lm = self\n\n        if use_cache:\n            use_cache = (\n                \"lm_cache/neox\"\n                + \"_dp_rank\"\n                + str(self._dp_rank)\n                + \"_dp_group\"\n                + str(self._dp_group)\n                + \".db\"\n            )\n            print(f\"Using cache at {use_cache}...\")\n            lm = lm_eval.api.model.CachingLM(\n                lm,\n                use_cache\n                # each rank receives a different cache db.\n                # necessary to avoid multiple writes to cache at once\n                # TODO: Append a subset of `neox_args` to the cache database\n                # name arg to distinguish model runs that use different configurations.\n            )\n\n        # from simple_evaluate:\n        # override fewshot values for all tasks we can\n        for task_name in task_dict.keys():\n            task_obj = task_dict[task_name]\n            if type(task_obj) == tuple:\n                group, task_obj = task_obj\n                if task_obj is None:\n                    continue\n\n            config = task_obj._config\n\n            if num_fewshot is not None:\n                if config[\"num_fewshot\"] == 0:\n                    utils.eval_logger.info(\n                        f\"num_fewshot has been set to 0 for {task_name} in its config. Manual configuration will be ignored.\"\n                    )\n                else:\n                    default_num_fewshot = config[\"num_fewshot\"]\n                    if not default_num_fewshot:\n                        utils.eval_logger.warning(\n                            f\"Overwriting default num_fewshot of {task_name} from {default_num_fewshot} to {num_fewshot}\"\n                        )\n\n                    task_obj._config[\"num_fewshot\"] = num_fewshot\n\n        results = evaluator.evaluate(\n            lm=lm,\n            task_dict=task_dict,\n            limit=10,  # limit,\n            bootstrap_iters=bootstrap_iters,\n            log_samples=False,\n        )\n\n        results[\"config\"] = {\n            \"model\": name,\n            \"model_args\": dataclasses.asdict(self.neox_args),\n            \"batch_size\": self.batch_size,\n            \"device\": str(self.device),\n            \"use_cache\": use_cache,\n            \"limit\": limit,\n            \"bootstrap_iters\": bootstrap_iters,\n        }\n        results[\"git_hash\"] = utils.get_git_commit_hash()\n\n        print(results.keys())\n        for task_name in task_dict.keys():\n            if \"alias\" in results[\"results\"][task_name]:\n                results[\"results\"][task_name].pop(\"alias\")\n\n        if was_training:\n            self.model.train()\n        self.model.micro_batches = in_micro_batches\n        return results\n\n\ndef run_eval_harness(\n    model,\n    forward_step_fn,\n    neox_args,\n    batch_size=None,\n    eval_tasks=None,\n    num_fewshot=0,\n    bootstrap_iters=2,\n):\n    print_rank_0(\"Running evaluation harness...\")\n    adapter = EvalHarnessAdapter(\n        model, forward_step_fn, neox_args, batch_size=batch_size\n    )\n    return adapter.run_eval(\n        eval_tasks=eval_tasks,\n        num_fewshot=num_fewshot,\n        bootstrap_iters=bootstrap_iters,\n        use_cache=False,\n    )\n",
        "generate.py": "#!/usr/bin/env python\n# # Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.utils import print_rank_0, setup_for_inference_or_eval\n\nfrom megatron.text_generation_utils import (\n    generate_samples_input_from_file,\n    generate_samples_from_prompt,\n    generate_samples_unconditional,\n    generate_samples_interactive,\n    precompute_logits,\n)\n\n\ndef main(input_args=None, overwrite_values=None):\n    \"\"\"\n    Generate text/sample model\n    \"\"\"\n    model, neox_args = setup_for_inference_or_eval(\n        use_cache=True, input_args=input_args, overwrite_values=overwrite_values\n    )\n    if neox_args.recompute:\n        model.module.inference_mode(\n            use_cache=False\n        )  # don't use kv cache if recomputing\n    if neox_args.text_gen_type == \"unconditional\":\n        print_rank_0(\n            f\"Generating samples unconditionally and saving results to {neox_args.sample_output_file}\"\n        )\n        generate_samples_unconditional(\n            neox_args=neox_args,\n            model=model,\n            number_of_samples=neox_args.num_samples,\n            output_file=neox_args.sample_output_file,\n            maximum_tokens=neox_args.maximum_tokens,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"input-file\":\n        print_rank_0(\n            f\"Generating samples from input file {neox_args.sample_input_file}\"\n        )\n        assert neox_args.sample_input_file is not None\n        generate_samples_input_from_file(\n            neox_args=neox_args,\n            model=model,\n            input_file=neox_args.sample_input_file,\n            output_file=neox_args.sample_output_file,\n            maximum_tokens=neox_args.maximum_tokens,\n            prompt_end=neox_args.prompt_end,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"interactive\":\n        generate_samples_interactive(\n            neox_args=neox_args,\n            model=model,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            maximum_tokens=neox_args.maximum_tokens,\n            prompt_end=neox_args.prompt_end,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"precompute\":\n        precompute_logits(neox_args=neox_args, model=model)\n    else:\n        raise ValueError(\n            f\"`text_gen_type` either not specified or not recognised: {neox_args.text_gen_type}\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "megatron/__init__.py": "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\n\n\ndef print_rank_0(*message):\n    \"\"\"If distributed is initialized print only on rank 0.\"\"\"\n    if torch.distributed.is_initialized():\n        if torch.distributed.get_rank() == 0:\n            print(*message, flush=True)\n    else:\n        print(*message, flush=True)\n\n\nfrom .neox_arguments import NeoXArgs\n",
        "megatron/checkpointing.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Input/output checkpointing.\"\"\"\n\nimport json\nimport math\nimport os\nimport re\nimport shutil\nimport time\nimport random\nimport sys\nimport numpy as np\n\ntry:\n    import boto3\nexcept ModuleNotFoundError:\n    print(\n        \"For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3\"\n    )\ntry:\n    import hf_transfer\nexcept ModuleNotFoundError:\n    print(\n        \"For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer\"\n    )\nimport torch\nfrom glob import glob\n\nfrom megatron import mpu\nfrom megatron import print_rank_0\nfrom megatron.utils import natural_sort\nfrom megatron.text_generation_utils import get_batch, forward_model\nfrom pathlib import Path\nfrom pprint import pformat\n\n\ndef check_checkpoint_args(neox_args, checkpoint_args):\n    \"\"\"Ensure fixed arguments for a model are the same for the input\n    arguments and the one retrieved from checkpoint.\"\"\"\n\n    assert isinstance(checkpoint_args, dict), \"args stored in checkpoint is a dict\"\n    for checkpoint_arg_name, checkpoint_arg_value in checkpoint_args.items():\n        args_value = getattr(neox_args, checkpoint_arg_name)\n        error_message = \"{} value from checkpoint ({}) is not equal to the currently set argument value ({}).\".format(\n            checkpoint_arg_name, checkpoint_arg_value, args_value\n        )\n        assert checkpoint_arg_value == args_value, error_message\n\n\ndef do_forward_pass(neox_args, model, inference=False):\n\n    # set to eval mode\n    model_was_in_train = model.training\n    model.eval()\n\n    # get context tokens\n    # always forward full batch size\n    context_tokens_tensor = (\n        torch.arange(neox_args.seq_length + 1)\n        .repeat((neox_args.train_micro_batch_size_per_gpu, 1))\n        .cuda()\n    )\n\n    # forward\n    if inference:\n        tokens, attention_mask, position_ids = get_batch(\n            neox_args, context_tokens_tensor[:, : neox_args.seq_length]\n        )\n        model_inputs = (\n            tokens,\n            position_ids,\n            attention_mask,\n            torch.Tensor(),\n        )\n        logits, _ = forward_model(neox_args, model, model_inputs)\n    elif neox_args.is_pipe_parallel:\n        data_iterator = iter([{\"text\": context_tokens_tensor}])\n        _, logits = model.eval_batch(data_iter=data_iterator, return_logits=True)\n    else:\n        tokens, attention_mask, position_ids = get_batch(\n            neox_args, context_tokens_tensor[:, : neox_args.seq_length]\n        )\n        logits = model((tokens, position_ids, attention_mask))\n\n    # reset to train mode, if model was in training before\n    if model_was_in_train:\n        model.train()\n\n    if logits is not None:\n        logits = logits.detach().cpu()[\n            0\n        ]  # just return first batch item (they are all equal)\n\n    return logits\n\n\ndef check_forward_pass(neox_args, model, checkpoint_logits, inference):\n    # do forward pass with loaded checkpoint\n    logits = do_forward_pass(neox_args=neox_args, model=model, inference=inference)\n\n    # check\n    if (\n        logits is not None and checkpoint_logits is not None\n    ):  # this could be the case for non-final pipeline stages\n        if not (logits == checkpoint_logits).all().item():\n            if mpu.get_data_parallel_rank() == 0:\n                print(\n                    \" > WARNING: validate_checkpoint_forward() forward after load of checkpoint does not yield exactly same result\"\n                )\n            assert (\n                torch.isclose(logits, checkpoint_logits).all().item()\n            ), \"validate_checkpoint_forward() forward after load of checkpoint does not yield a close result\"\n\n\ndef ensure_directory_exists(filename):\n    \"\"\"Build filename's path if it does not already exists.\"\"\"\n    dirname = os.path.dirname(filename)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n\ndef get_checkpoint_name(checkpoints_path, iteration, release=False, mp_rank=None):\n    \"\"\"A unified checkpoint name.\"\"\"\n    if release:\n        directory = \"release\"\n    else:\n        directory = \"iter_{:07d}\".format(iteration)\n    return os.path.join(\n        checkpoints_path,\n        directory,\n        \"mp_rank_{:02d}\".format(\n            mpu.get_model_parallel_rank() if mp_rank is None else mp_rank\n        ),\n        \"model_optim_rng.pt\",\n    )\n\n\ndef get_checkpoint_tag(iteration: int) -> str:\n    return f\"global_step{iteration}\"\n\n\ndef delete_old_checkpoints(save_dir, n_to_keep):\n    if torch.distributed.get_rank() == 0:\n        ckpt_dir_regex = r\"global_step[\\d]*\"\n        if save_dir.endswith(\"/\"):\n            save_dir = save_dir.strip(\"/\")\n        all_ckpts = natural_sort(\n            [\n                i\n                for i in glob(f\"{save_dir}/*\")\n                if os.path.isdir(i) and re.search(ckpt_dir_regex, i)\n            ]\n        )\n        n_to_delete = len(all_ckpts) - n_to_keep\n        if n_to_delete > 0:\n            to_delete = all_ckpts[:n_to_delete]\n            print(f\"WARNING: Deleting old checkpoints: \\n\\t{', '.join(to_delete)}\")\n            for ckpt in to_delete:\n                try:\n                    shutil.rmtree(ckpt)\n                except FileNotFoundError:\n                    pass\n\n\ndef save_ds_checkpoint(iteration, model, neox_args):\n    \"\"\"Save a model checkpoint.\"\"\"\n    sd = {\n        \"iteration\": iteration,\n        \"args\": {\n            \"num_layers\": neox_args.num_layers,\n            \"hidden_size\": neox_args.hidden_size,\n            \"num_attention_heads\": neox_args.num_attention_heads,\n            \"max_position_embeddings\": neox_args.max_position_embeddings,\n            \"make_vocab_size_divisible_by\": neox_args.make_vocab_size_divisible_by,\n            \"padded_vocab_size\": neox_args.padded_vocab_size,\n            \"tokenizer_type\": neox_args.tokenizer_type,\n            \"model_parallel_size\": neox_args.model_parallel_size,\n        },\n    }\n    # rng states.\n    if not neox_args.no_save_rng:\n        sd[\"random_rng_state\"] = random.getstate()\n        sd[\"np_rng_state\"] = np.random.get_state()\n        sd[\"torch_rng_state\"] = torch.get_rng_state()\n        sd[\"cuda_rng_state\"] = torch.cuda.get_rng_state()\n        sd[\"rng_tracker_states\"] = mpu.get_cuda_rng_tracker().get_states()\n\n    if neox_args.checkpoint_validation_with_forward_pass:\n        logits = do_forward_pass(neox_args=neox_args, model=model)\n        sd[\"checkpoint_validation_logits\"] = logits\n\n    # checkpoint folder name\n    tag = get_checkpoint_tag(iteration)\n\n    # save checkpoint\n    model.save_checkpoint(neox_args.save, tag=tag, client_state=sd)\n\n    # save config files\n    if torch.distributed.get_rank() == 0 and neox_args.config_files is not None:\n        configs_directory = os.path.join(neox_args.save, tag, \"configs\")\n        os.makedirs(configs_directory, exist_ok=True)\n        for config_filename, config_data in neox_args.config_files.items():\n            with open(os.path.join(configs_directory, config_filename), \"w\") as f:\n                if isinstance(config_data, str):\n                    f.write(config_data)\n                else:\n                    json.dump(config_data, f)\n\n\ndef multiprocessing_starmap(func, args, num_processes=None):\n    \"\"\"Wrapper to allow for re-usable multiprocessing pools with `spawn` context handling\n    Args:\n        func (Callable): Function to call\n        args (Iterable): Iterable of arguments to pass to `func`\n        num_processes (int, optional): Number of processes to spawn. Defaults to `multiprocessing.cpu_count() - 1`\n    \"\"\"\n    import multiprocessing\n\n    num_processes = num_processes or (multiprocessing.cpu_count() - 1)\n    with multiprocessing.get_context(\"spawn\").Pool(\n        processes=num_processes\n    ) as process_pool:\n        process_pool.starmap(func, args)\n        process_pool.terminate()\n        process_pool.join()\n        del process_pool\n\n\ndef _upload(\n    file_path: str,\n    s3_key: str,\n    chunk_size: int = 104_857_600,\n    max_files: int = 64,\n    parallel_failures: int = 63,\n    max_retries: int = 5,\n):\n    \"\"\"Upload local file to S3 using `hf_transfer` library\n    Args:\n        file_path (str): Local filename to upload\n        s3_key (str): S3 key to upload to. E.g. `s3://bucket-name/path/to/file`\n        chunk_size (int, optional): Chunk size to use for multipart upload.\n            Defaults to 100MiB = 104_857_600\n        max_files (int, optional):  Number of open file handles, which determines\n            the maximum number of parallel downloads. Defaults to 64\n        parallel_failures (int, optional): Number of maximum failures of different\n            chunks in parallel (cannot exceed max_files). Defaults to 63\n        max_retries (int, optional): Number of retries for each chunk. Defaults to 5\n    \"\"\"\n    s3 = boto3.client(\"s3\")\n    bucket = s3_key.split(\"s3://\")[1].split(\"/\")[0]\n    key = s3_key.split(bucket)[1].lstrip(\"/\")\n\n    # 1. Init multipart upload and obtain unique upload identifier\n    upload = s3.create_multipart_upload(\n        ACL=\"bucket-owner-full-control\",\n        Bucket=bucket,\n        Key=key,\n    )\n    upload_id = upload[\"UploadId\"]\n\n    # 2. Generate presigned URLs for each part\n    file_size = os.stat(file_path).st_size\n    urls = []\n    nb_parts = math.ceil(file_size / chunk_size)\n    for part_number in range(1, nb_parts + 1):\n        params = {\n            \"Bucket\": bucket,\n            \"Key\": key,\n            \"PartNumber\": part_number,\n            \"UploadId\": upload_id,\n        }\n        urls.append(\n            s3.generate_presigned_url(\n                ClientMethod=\"upload_part\", Params=params, ExpiresIn=86400\n            )\n        )\n\n    # 3. Upload parts in parallel\n    responses = hf_transfer.multipart_upload(\n        file_path=file_path,\n        parts_urls=urls,\n        chunk_size=chunk_size,\n        max_files=max_files,\n        parallel_failures=parallel_failures,\n        max_retries=max_retries,\n    )\n\n    # 4. Complete multipart upload request with ETag values\n    etag_with_parts = []\n    for part_number, header in enumerate(responses):\n        etag = header.get(\"etag\")\n        etag_with_parts.append({\"ETag\": etag, \"PartNumber\": part_number + 1})\n    parts = {\"Parts\": etag_with_parts}\n    s3.complete_multipart_upload(\n        Bucket=bucket, Key=key, MultipartUpload=parts, UploadId=upload_id\n    )\n\n\ndef upload_checkpoint(iteration, neox_args):\n    local_checkpoint_path = os.path.join(\n        os.path.abspath(neox_args.save), get_checkpoint_tag(iteration)\n    )\n    local_checkpoint_list = sorted(\n        filter(\n            lambda x: os.path.isfile(x),\n            [str(p) for p in Path(local_checkpoint_path).rglob(\"*\")],\n        )\n    )\n    remote_checkpoint_path = os.path.join(\n        neox_args.s3_path,\n        os.path.basename(neox_args.save),\n        get_checkpoint_tag(iteration),\n    )\n    remote_checkpoint_list = [\n        os.path.join(\n            remote_checkpoint_path,\n            os.path.relpath(local_checkpoint, local_checkpoint_path),\n        )\n        for local_checkpoint in local_checkpoint_list\n    ]\n    inputs = zip(\n        local_checkpoint_list,\n        remote_checkpoint_list,\n        [neox_args.s3_chunk_size] * len(local_checkpoint_list),\n    )\n\n    print_rank_0(\n        f\"[RANK {torch.distributed.get_rank()}] Uploading checkpoint `{local_checkpoint_path}` to `{remote_checkpoint_path}`...\"\n    )\n    start = time.time()\n    multiprocessing_starmap(_upload, inputs)\n    total_time = time.time() - start\n    print_rank_0(\n        f\"[RANK {torch.distributed.get_rank()}] Uploaded checkpoint `{local_checkpoint_path}` to `{remote_checkpoint_path}` in {total_time:.2f}s\"\n    )\n\n\ndef save_checkpoint(neox_args, iteration, model, optimizer, lr_scheduler):\n    \"\"\"Save a model checkpoint.\"\"\"\n\n    if neox_args.deepspeed:\n        save_ds_checkpoint(iteration, model, neox_args)\n    else:\n        raise ValueError(\"Must be using deepspeed to use neox\")\n\n    torch.distributed.barrier()\n    upload_to_s3 = torch.distributed.get_rank() == 0 and neox_args.s3_path is not None\n    if upload_to_s3:\n        upload_checkpoint(iteration, neox_args)\n\n    # Wait so everyone is done (necessary)\n    torch.distributed.barrier()\n    if neox_args.keep_last_n_checkpoints is not None:\n        delete_old_checkpoints(neox_args.save, neox_args.keep_last_n_checkpoints)\n\n    # Wait so everyone is done (not necessary)\n    torch.distributed.barrier()\n\n\ndef load_checkpoint(\n    neox_args, model, optimizer, lr_scheduler, inference=False, iteration=None\n):\n    \"\"\"Load a model checkpoint and return the iteration.\"\"\"\n    if neox_args.deepspeed:\n        load_optim_and_scheduler = (\n            not neox_args.no_load_optim\n        )  # TODO: These should be configured by separate args\n        if neox_args.finetune:\n            load_optim_and_scheduler = False\n        if iteration is not None:\n            tag = get_checkpoint_tag(iteration)\n        else:\n            tag = None\n        checkpoint_name, state_dict = model.load_checkpoint(\n            neox_args.load,\n            load_optimizer_states=load_optim_and_scheduler,\n            load_lr_scheduler_states=load_optim_and_scheduler,\n            load_module_only=not load_optim_and_scheduler,\n            tag=tag,\n            load_module_strict=neox_args.train_impl != \"rm\",\n        )\n\n        if checkpoint_name is None:\n            # if an iteration is specified, we want to raise an error here rather than\n            # continuing silently, since we are trying to load a specific checkpoint\n            if iteration is not None:\n                available_checkpoints = sorted(\n                    [\n                        int(i.name.replace(\"global_step\", \"\"))\n                        for i in Path(neox_args.load).glob(\"global_step*\")\n                    ]\n                )\n                raise ValueError(\n                    f\"Unable to load checkpoint for iteration {iteration}. \\nAvailable iterations: {pformat(available_checkpoints)}\"\n                )\n            if mpu.get_data_parallel_rank() == 0:\n                print(\"Unable to load checkpoint.\")\n\n            return 0  # iteration 0, if not checkpoint loaded\n    else:\n        raise ValueError(\"Must be using deepspeed to use neox\")\n\n    # Set iteration.\n    if neox_args.finetune:\n        iteration = 0\n    else:\n        if \"iteration\" in state_dict:\n            iteration = state_dict[\"iteration\"]\n        else:\n            iteration = state_dict.get(\n                \"total_iters\"\n            )  # total_iters backward compatible with older checkpoints\n        if iteration is None:\n            raise ValueError(\n                f\"Unable to load iteration from checkpoint {checkpoint_name} with keys {state_dict.keys()}, exiting\"\n            )\n\n    # Check arguments.\n    if \"args\" in state_dict:\n        checkpoint_args = state_dict[\"args\"]\n        check_checkpoint_args(neox_args=neox_args, checkpoint_args=checkpoint_args)\n        print_rank_0(\n            \" > validated currently set args with arguments in the checkpoint ...\"\n        )\n    else:\n        print_rank_0(\" > could not find arguments in the checkpoint for validation...\")\n\n    # Check loaded checkpoint with forward pass\n    if neox_args.checkpoint_validation_with_forward_pass:\n        if \"checkpoint_validation_logits\" in state_dict:\n            check_forward_pass(\n                neox_args=neox_args,\n                model=model,\n                checkpoint_logits=state_dict[\"checkpoint_validation_logits\"],\n                inference=inference,\n            )\n            print_rank_0(\" > validated loaded checkpoint with forward pass ...\")\n        else:\n            if mpu.get_data_parallel_rank() == 0:\n                print(\n                    \" > WARNING: checkpoint_validation_with_forward_pass is configured but no checkpoint validation data available in checkpoint {}\".format(\n                        checkpoint_name\n                    )\n                )\n\n    # rng states.\n    if not neox_args.finetune and not neox_args.no_load_rng:\n        try:\n            random.setstate(state_dict[\"random_rng_state\"])\n            np.random.set_state(state_dict[\"np_rng_state\"])\n            torch.set_rng_state(state_dict[\"torch_rng_state\"])\n            torch.cuda.set_rng_state(state_dict[\"cuda_rng_state\"])\n            mpu.get_cuda_rng_tracker().set_states(state_dict[\"rng_tracker_states\"])\n        except KeyError:\n            print_rank_0(\n                \"Unable to load optimizer from checkpoint {}. \"\n                \"Specify --no-load-rng or --finetune to prevent \"\n                \"attempting to load the optimizer state, \"\n                \"exiting ...\".format(checkpoint_name)\n            )\n            sys.exit()\n\n    torch.distributed.barrier()\n    if mpu.get_data_parallel_rank() == 0:\n        print(\"  successfully loaded {}\".format(checkpoint_name))\n\n    return iteration\n",
        "megatron/data/__init__.py": "from . import *\n",
        "megatron/data/blendable_dataset.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Blendable dataset.\"\"\"\n\nimport time\n\nimport numpy as np\nimport torch\n\nfrom megatron import print_rank_0\nfrom megatron import mpu\n\n\nclass BlendableDataset(torch.utils.data.Dataset):\n    def __init__(self, datasets, weights):\n        self.datasets = datasets\n        num_datasets = len(datasets)\n        assert num_datasets == len(weights)\n\n        self.size = 0\n        for dataset in self.datasets:\n            self.size += len(dataset)\n\n        # Normalize weights.\n        weights = np.array(weights, dtype=np.float64)\n        sum_weights = np.sum(weights)\n        assert sum_weights > 0.0\n        weights /= sum_weights\n\n        # Build indices.\n        start_time = time.time()\n        assert num_datasets < 255\n        self.dataset_index = np.zeros(self.size, dtype=np.uint8)\n        self.dataset_sample_index = np.zeros(self.size, dtype=np.int64)\n\n        from megatron.data import helpers\n\n        helpers.build_blending_indices(\n            self.dataset_index,\n            self.dataset_sample_index,\n            weights,\n            num_datasets,\n            self.size,\n            torch.distributed.get_rank() == 0,\n        )\n\n        print(\n            \"> RANK {} elapsed time for building blendable dataset indices: \"\n            \"{:.2f} (sec)\".format(\n                torch.distributed.get_rank(), time.time() - start_time\n            )\n        )\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        try:\n            dataset_idx = self.dataset_index[idx]\n            sample_idx = self.dataset_sample_index[idx]\n            return self.datasets[dataset_idx][sample_idx]\n        except IndexError:\n            new_idx = idx % len(self)\n            print(\n                f\"WARNING: Got index out of bounds error with index {idx} - taking modulo of index instead ({new_idx})\"\n            )\n            return self[new_idx]\n",
        "megatron/data/data_utils.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport torch\nimport numpy as np\nfrom typing import List, Tuple\nfrom itertools import zip_longest, cycle\nfrom functools import partial\n\nfrom megatron import mpu, print_rank_0\nfrom megatron.data.indexed_dataset import make_dataset as make_indexed_dataset\nfrom megatron.data.blendable_dataset import BlendableDataset\nfrom megatron.data.gpt2_dataset import GPT2Dataset\nfrom megatron.data.pairwise_dataset import PairwiseDataset\nfrom megatron.data.online_dataset import OnlineDataset\nfrom megatron.data.samplers import DistributedBatchSampler\n\n\ndef make_data_loader(dataset, neox_args):\n    \"\"\"Build dataloader given an input dataset.\"\"\"\n    if dataset is None:\n        return None\n    # Data parallel arguments.\n    world_size = mpu.get_data_parallel_world_size()\n    rank = mpu.get_data_parallel_rank()\n    global_batch_size = neox_args.batch_size * world_size\n    num_workers = neox_args.num_workers\n\n    # Use a simple sampler with distributed batch sampler.\n    sampler = torch.utils.data.SequentialSampler(dataset)\n    batch_sampler = DistributedBatchSampler(\n        sampler=sampler,\n        batch_size=global_batch_size,\n        drop_last=True,\n        rank=rank,\n        world_size=world_size,\n    )\n    # Torch dataloader.\n    return torch.utils.data.DataLoader(\n        dataset, batch_sampler=batch_sampler, num_workers=num_workers, pin_memory=True\n    )\n\n\ndef build_the_dataset(\n    data_prefix,\n    pos_data_prefix,\n    neg_data_prefix,\n    name,\n    data_impl,\n    pack_impl,\n    dataset_impl,\n    allow_chopped,\n    num_samples,\n    num_epochs,\n    seq_length,\n    seed,\n    skip_warmup,\n    build_index_mappings=True,\n    label_prefix=None,\n    pos_label_prefix=None,\n    neg_label_prefix=None,\n    precompute_model_name=None,\n    reward_prefix=None,\n):\n    \"\"\"Build train/valid/test datasets.\"\"\"\n    if dataset_impl == \"gpt2\":\n        indexed_dataset = make_indexed_dataset(data_prefix, data_impl, skip_warmup)\n        if label_prefix is None:\n            label_dataset = None\n        else:\n            label_dataset = make_indexed_dataset(label_prefix, data_impl, skip_warmup)\n        if precompute_model_name is not None:\n            # If we have the name, assume it exists. If it doesn't, it will just be None which is fine.\n            precompute_indexed_dataset = make_indexed_dataset(\n                data_prefix + \"_\" + precompute_model_name, data_impl, skip_warmup\n            )\n            precompute_indexed_dataset = precompute_indexed_dataset\n        else:\n            precompute_indexed_dataset = None\n        if reward_prefix is not None:\n            reward_dataset = make_indexed_dataset(reward_prefix, data_impl, skip_warmup)\n        else:\n            reward_dataset = None\n    elif dataset_impl == \"pairwise\":\n        pos_indexed_dataset = make_indexed_dataset(\n            pos_data_prefix, data_impl, skip_warmup\n        )\n        neg_indexed_dataset = make_indexed_dataset(\n            neg_data_prefix, data_impl, skip_warmup\n        )\n        if pos_label_prefix is None:\n            pos_label_dataset = None\n            # Also do neg here since they both must be the same\n            assert neg_label_prefix is None\n            neg_label_dataset = None\n        else:\n            pos_label_dataset = make_indexed_dataset(\n                pos_label_prefix, data_impl, skip_warmup\n            )\n            # Also do neg here since they both must be the same\n            assert neg_label_prefix is not None\n            neg_label_dataset = make_indexed_dataset(\n                neg_label_prefix, data_impl, skip_warmup\n            )\n        if precompute_model_name is None:\n            pos_ref_dataset = None\n            neg_ref_dataset = None\n        else:\n            pos_ref_dataset = make_indexed_dataset(\n                pos_data_prefix + \"_\" + precompute_model_name, data_impl, skip_warmup\n            )\n            neg_ref_dataset = make_indexed_dataset(\n                neg_data_prefix + \"_\" + precompute_model_name, data_impl, skip_warmup\n            )\n    else:\n        raise NotImplementedError(f\"dataset_impl={dataset_impl} not implemented\")\n\n    total_num_of_documents = (\n        indexed_dataset.sizes.shape[0]\n        if dataset_impl == \"gpt2\"\n        else pos_indexed_dataset.sizes.shape[0]\n    )\n    print_rank_0(\"    {}:\".format(name))\n    print_rank_0(\"     no. of documents:{}\".format(total_num_of_documents))\n    dataset = None\n    documents = np.arange(start=0, stop=total_num_of_documents, step=1, dtype=np.int32)\n    if dataset_impl == \"gpt2\":\n        dataset = GPT2Dataset(\n            name,\n            data_prefix,\n            documents,\n            indexed_dataset,\n            num_samples,\n            num_epochs,\n            seq_length,\n            seed,\n            pack_impl=pack_impl,\n            allow_chopped=allow_chopped,\n            build_index_mappings=build_index_mappings,\n            label_dataset=label_dataset,\n            reward_dataset=reward_dataset,\n            ref_dataset=precompute_indexed_dataset,\n        )\n    elif dataset_impl == \"pairwise\":\n        dataset = PairwiseDataset(\n            name,\n            pos_data_prefix,\n            documents,\n            pos_indexed_dataset,\n            neg_indexed_dataset,\n            num_samples,\n            seq_length,\n            seed,\n            pack_impl=pack_impl,\n            allow_chopped=allow_chopped,\n            build_index_mappings=build_index_mappings,\n            pos_label_dataset=pos_label_dataset,\n            neg_label_dataset=neg_label_dataset,\n            pos_ref_dataset=pos_ref_dataset,\n            neg_ref_dataset=neg_ref_dataset,\n        )\n    return dataset\n\n\ndef build_train_valid_test_datasets(\n    data_prefix,\n    use_shared_fs,\n    data_impl,\n    pack_impl,\n    allow_chopped,\n    splits_string,\n    train_valid_test_num_samples,\n    train_valid_test_epochs,\n    seq_length,\n    seed,\n    skip_warmup,\n):\n    \"\"\"Build train, valid, and test datasets.\"\"\"\n\n    # Indexed dataset.\n    indexed_dataset = make_indexed_dataset(data_prefix, data_impl, skip_warmup)\n\n    total_num_of_documents = indexed_dataset.sizes.shape[0]\n    splits = get_train_valid_test_split_(splits_string, total_num_of_documents)\n\n    # Print stats about the splits.\n    print_rank_0(\" > dataset split:\")\n\n    def print_split_stats(name, index):\n        print_rank_0(\"    {}:\".format(name))\n        print_rank_0(\n            \"     document indices in [{}, {}) total of {} \"\n            \"documents\".format(\n                splits[index], splits[index + 1], splits[index + 1] - splits[index]\n            )\n        )\n\n    print_split_stats(\"train\", 0)\n    print_split_stats(\"validation\", 1)\n    print_split_stats(\"test\", 2)\n\n    def build_dataset(index, name):\n        dataset = None\n        if splits[index + 1] > splits[index]:\n            documents = np.arange(\n                start=splits[index], stop=splits[index + 1], step=1, dtype=np.int32\n            )\n            dataset = GPT2Dataset(\n                name,\n                data_prefix,\n                documents,\n                indexed_dataset,\n                train_valid_test_num_samples[index],\n                train_valid_test_epochs[index],\n                seq_length,\n                seed,\n                pack_impl=pack_impl,\n                allow_chopped=allow_chopped,\n                use_shared_fs=use_shared_fs,\n            )\n        return dataset\n\n    train_dataset = build_dataset(0, \"train\")\n    valid_dataset = build_dataset(1, \"valid\")\n    test_dataset = build_dataset(2, \"test\")\n\n    return train_dataset, valid_dataset, test_dataset\n\n\ndef get_train_valid_test_split_(splits_string, size):\n    \"\"\"Get dataset splits from comma or '/' separated string list.\"\"\"\n\n    splits = []\n    if splits_string.find(\",\") != -1:\n        splits = [float(s) for s in splits_string.split(\",\")]\n    elif splits_string.find(\"/\") != -1:\n        splits = [float(s) for s in splits_string.split(\"/\")]\n    else:\n        splits = [float(splits_string)]\n    while len(splits) < 3:\n        splits.append(0.0)\n    splits = splits[:3]\n    splits_sum = sum(splits)\n    assert splits_sum > 0.0\n    splits = [split / splits_sum for split in splits]\n    splits_index = [0]\n    for index, split in enumerate(splits):\n        splits_index.append(splits_index[index] + int(round(split * float(size))))\n    diff = splits_index[-1] - size\n    for index in range(1, len(splits_index)):\n        splits_index[index] -= diff\n    assert len(splits_index) == 4\n    assert splits_index[-1] == size\n    return splits_index\n\n\ndef get_normalized_weights_and_num_samples(\n    weights: List[float], num_samples: int\n) -> Tuple[List[float], List[int]]:\n    # Normalize weights\n    weight_sum = sum(weights)\n    assert weight_sum > 0.0\n    weights = [weight / weight_sum for weight in weights]\n    if num_samples is not None:\n        # Add 0.5% (the 1.005 factor) so in case the blending dataset does\n        # not uniformly distribute the number of samples, we still have\n        # samples left to feed to the network.\n        weighted_num_samples = []\n        for weight in weights:\n            weighted_num_samples.append(int(math.ceil(num_samples * weight * 1.005)))\n    else:\n        weighted_num_samples = [None for _ in weights]\n    return weights, weighted_num_samples\n\n\ndef build_weighted_datasets(\n    neox_args,\n    train_num_samples,\n    valid_num_samples,\n    test_num_samples,\n    train_epochs,\n    valid_epochs,\n    test_epochs,\n    build_index_mappings=True,\n):\n    # build individual datasets\n    train_datasets, valid_datasets, test_datasets = [], [], []\n    for i, (\n        train_path,\n        train_label_path,\n        train_reward_path,\n        valid_path,\n        valid_label_path,\n        valid_reward_path,\n        test_path,\n        test_label_path,\n        test_reward_path,\n        pos_train_path,\n        neg_train_path,\n        pos_train_label_path,\n        neg_train_label_path,\n        pos_valid_path,\n        neg_valid_path,\n        pos_valid_label_path,\n        neg_valid_label_path,\n        pos_test_path,\n        neg_test_path,\n        pos_test_label_path,\n        neg_test_label_path,\n    ) in enumerate(\n        zip_longest(\n            neox_args.train_data_paths if neox_args.train_data_paths else [],\n            neox_args.train_label_data_paths\n            if neox_args.train_label_data_paths\n            else [],\n            neox_args.train_reward_data_paths\n            if neox_args.train_reward_data_paths\n            else [],\n            neox_args.valid_data_paths if neox_args.valid_data_paths else [],\n            neox_args.valid_label_data_paths\n            if neox_args.valid_label_data_paths\n            else [],\n            neox_args.valid_reward_data_paths\n            if neox_args.valid_reward_data_paths\n            else [],\n            neox_args.test_data_paths if neox_args.test_data_paths else [],\n            neox_args.test_label_data_paths if neox_args.test_label_data_paths else [],\n            neox_args.test_reward_data_paths\n            if neox_args.test_reward_data_paths\n            else [],\n            neox_args.pos_train_data_paths if neox_args.pos_train_data_paths else [],\n            neox_args.neg_train_data_paths if neox_args.neg_train_data_paths else [],\n            neox_args.pos_train_label_data_paths\n            if neox_args.pos_train_label_data_paths\n            else [],\n            neox_args.neg_train_label_data_paths\n            if neox_args.neg_train_label_data_paths\n            else [],\n            neox_args.pos_valid_data_paths if neox_args.pos_valid_data_paths else [],\n            neox_args.neg_valid_data_paths if neox_args.neg_valid_data_paths else [],\n            neox_args.pos_valid_label_data_paths\n            if neox_args.pos_valid_label_data_paths\n            else [],\n            neox_args.neg_valid_label_data_paths\n            if neox_args.neg_valid_label_data_paths\n            else [],\n            neox_args.pos_test_data_paths if neox_args.pos_test_data_paths else [],\n            neox_args.neg_test_data_paths if neox_args.neg_test_data_paths else [],\n            neox_args.pos_test_label_data_paths\n            if neox_args.pos_test_label_data_paths\n            else [],\n            neox_args.neg_test_label_data_paths\n            if neox_args.neg_test_label_data_paths\n            else [],\n        )\n    ):\n        if train_path or pos_train_path:\n            train_datasets.append(\n                build_the_dataset(\n                    data_prefix=train_path,\n                    name=f\"train_{i}\",\n                    data_impl=neox_args.data_impl,\n                    pack_impl=neox_args.pack_impl,\n                    allow_chopped=neox_args.allow_chopped,\n                    num_samples=train_num_samples[i],\n                    num_epochs=train_epochs,\n                    seq_length=neox_args.seq_length,\n                    seed=neox_args.seed,\n                    skip_warmup=(not neox_args.mmap_warmup),\n                    build_index_mappings=build_index_mappings,\n                    label_prefix=train_label_path,\n                    dataset_impl=neox_args.dataset_impl,\n                    pos_data_prefix=pos_train_path,\n                    neg_data_prefix=neg_train_path,\n                    pos_label_prefix=pos_train_label_path,\n                    neg_label_prefix=neg_train_label_path,\n                    precompute_model_name=neox_args.precompute_model_name,\n                    reward_prefix=train_reward_path,\n                )\n            )\n\n        if valid_path or pos_valid_path:\n            valid_datasets.append(\n                build_the_dataset(\n                    data_prefix=valid_path,\n                    name=f\"valid_{i}\",\n                    data_impl=neox_args.data_impl,\n                    pack_impl=neox_args.pack_impl,\n                    allow_chopped=neox_args.allow_chopped,\n                    num_samples=valid_num_samples[i],\n                    num_epochs=valid_epochs,\n                    seq_length=neox_args.seq_length,\n                    seed=neox_args.seed,\n                    skip_warmup=(not neox_args.mmap_warmup),\n                    build_index_mappings=build_index_mappings,\n                    label_prefix=valid_label_path,\n                    dataset_impl=neox_args.dataset_impl,\n                    pos_data_prefix=pos_valid_path,\n                    neg_data_prefix=neg_valid_path,\n                    pos_label_prefix=pos_valid_label_path,\n                    neg_label_prefix=neg_valid_label_path,\n                    precompute_model_name=neox_args.precompute_model_name,\n                    reward_prefix=valid_reward_path,\n                )\n            )\n\n        if test_path or pos_test_path:\n            test_datasets.append(\n                build_the_dataset(\n                    data_prefix=test_path,\n                    name=f\"test_{i}\",\n                    data_impl=neox_args.data_impl,\n                    pack_impl=neox_args.pack_impl,\n                    allow_chopped=neox_args.allow_chopped,\n                    num_samples=test_num_samples[i],\n                    num_epochs=test_epochs,\n                    seq_length=neox_args.seq_length,\n                    seed=neox_args.seed,\n                    skip_warmup=(not neox_args.mmap_warmup),\n                    build_index_mappings=build_index_mappings,\n                    label_prefix=test_label_path,\n                    dataset_impl=neox_args.dataset_impl,\n                    pos_data_prefix=pos_test_path,\n                    neg_data_prefix=neg_test_path,\n                    pos_label_prefix=pos_test_label_path,\n                    neg_label_prefix=neg_test_label_path,\n                    precompute_model_name=neox_args.precompute_model_name,\n                    reward_prefix=test_reward_path,\n                )\n            )\n    return train_datasets, valid_datasets, test_datasets\n\n\ndef weights_by_num_docs(l: list, alpha=0.3):\n    \"\"\"\n    Builds weights from a multinomial distribution over groups of data according to the number of\n    samples in each group.\n\n    We sample from a group according to the probability p(L) ∝ |L| ** α,\n    where p(L) is the probability of sampling from a given group,\n          |L| is the number of examples in that datapoint,\n          and α is a coefficient that acts to upsample data from underrepresented groups\n\n    Hence α (`alpha`) allows us to control how much to 'boost' the probability of training on low-resource groups.\n\n    See https://arxiv.org/abs/1911.02116 for more details\n    \"\"\"\n    if len(l) == 1:\n        return [1.0]\n\n    total_n_docs = sum(l)\n    unbiased_sample_probs = [i / total_n_docs for i in l]\n\n    probs = [i**alpha for i in unbiased_sample_probs]\n\n    # normalize\n    total = sum(probs)\n    probs = [i / total for i in probs]\n\n    # weights should be the inverse of the number of samples\n    unbiased_sample_probs_inverse = [1 - p for p in unbiased_sample_probs]\n    weights = [p * p2 for p, p2 in zip(probs, unbiased_sample_probs_inverse)]\n\n    # normalize\n    total = sum(weights)\n    weights = [i / total for i in weights]\n\n    return weights\n\n\ndef validate_train_epochs(neox_args):\n    \"\"\"Check for unsupported neox_args when using train_epochs instead of train_iters\"\"\"\n    if neox_args.train_epochs is None:\n        return\n\n    if neox_args.train_epochs and neox_args.train_iters:\n        raise ValueError(\n            \"Cannot specify both train epochs and train iters simultaneously\"\n        )\n\n    if neox_args.pack_impl != \"packed\":\n        raise ValueError(\n            \"Packing implementations other than 'packed' are currently unsupported with train_epochs\"\n        )\n\n    if neox_args.weight_by_num_documents:\n        raise ValueError(\n            \"Weighting by number of documents is currently unsupported with train_epochs\"\n        )\n\n    if neox_args.train_data_weights and (\n        not all(weight == 1.0 for weight in neox_args.train_data_weights)\n    ):\n        raise ValueError(\n            \"train_data_weights != None is currently unsupported with train_epochs\"\n        )\n\n    if neox_args.dataset_impl != \"gpt2\":\n        raise ValueError(\n            \"non gpt2 datasets are not currently unsupported with train_epochs\"\n        )\n\n\ndef build_train_valid_test_data_loaders(neox_args):\n    \"\"\"XXX\"\"\"\n\n    validate_train_epochs(neox_args)\n\n    (train_dataloader, valid_dataloader, test_dataloader) = (None, None, None)\n\n    print_rank_0(\"> building train, validation, and test datasets ...\")\n\n    # Ensure only the first/last pipeline stages have data loaders\n    if neox_args.is_pipe_parallel:\n        is_first_stage = mpu.get_pipe_parallel_rank() == 0\n        is_last_stage = (\n            mpu.get_pipe_parallel_rank() == mpu.get_pipe_parallel_world_size() - 1\n        )\n        pipe_load = is_first_stage or is_last_stage\n    else:\n        pipe_load = True\n\n    # Data loader only on rank 0 of each model parallel group.\n    if (\n        pipe_load\n        and (neox_args.dataset_impl == \"online\")\n        and (mpu.get_model_parallel_rank() == 0)\n    ):\n        # Can skip most of the work...\n        train_iters = neox_args.train_iters\n        eval_iters = (train_iters // neox_args.eval_interval + 1) * neox_args.eval_iters\n        test_iters = neox_args.eval_iters\n        # Build datasets...\n        print(\n            f\"train_iters: {train_iters}, eval_iters: {eval_iters}, test_iters: {test_iters}\"\n        )\n        train_datasets = OnlineDataset(\n            leave_one_out=neox_args.reinforce_leave_one_out,\n            data_split=\"train\",\n            num_samples=train_iters * neox_args.train_batch_size,\n            seq_length=neox_args.seq_length,\n            dataserver_ips=neox_args.online_dataserver_ips,\n            dataserver_ports=neox_args.online_dataserver_ports,\n        )\n        valid_datasets = OnlineDataset(\n            leave_one_out=neox_args.reinforce_leave_one_out,\n            data_split=\"valid\",\n            num_samples=eval_iters * neox_args.train_batch_size,\n            seq_length=neox_args.seq_length,\n            dataserver_ips=neox_args.online_dataserver_ips,\n            dataserver_ports=neox_args.online_dataserver_ports,\n        )\n        test_datasets = OnlineDataset(\n            leave_one_out=neox_args.reinforce_leave_one_out,\n            data_split=\"test\",\n            num_samples=test_iters * neox_args.train_batch_size,\n            seq_length=neox_args.seq_length,\n            dataserver_ips=neox_args.online_dataserver_ips,\n            dataserver_ports=neox_args.online_dataserver_ports,\n        )\n        # print length of datasets\n        # Build dataloders.\n        train_dataloader = make_data_loader(train_datasets, neox_args=neox_args)\n        valid_dataloader = make_data_loader(valid_datasets, neox_args=neox_args)\n        test_dataloader = make_data_loader(test_datasets, neox_args=neox_args)\n\n        # Flags to know if we need to do training/validation/testing.\n        do_train = train_dataloader is not None and neox_args.train_iters > 0\n        do_valid = valid_dataloader is not None and neox_args.eval_iters > 0\n        do_test = test_dataloader is not None and neox_args.eval_iters > 0\n        # Need to broadcast num_tokens and num_type_tokens.\n        flags = torch.cuda.LongTensor([int(do_train), int(do_valid), int(do_test)])\n    elif mpu.get_model_parallel_rank() == 0 and pipe_load:\n        # Number of train/valid/test samples.\n        if neox_args.train_iters is not None:\n            train_iters = neox_args.train_iters\n            eval_iters = (\n                train_iters // neox_args.eval_interval + 1\n            ) * neox_args.eval_iters\n            test_iters = neox_args.eval_iters\n            train_val_test_num_samples = [\n                train_iters * neox_args.train_batch_size,\n                eval_iters * neox_args.train_batch_size,\n                test_iters * neox_args.train_batch_size,\n            ]\n            train_val_test_epochs = [None, None, None]\n        elif neox_args.train_epochs is not None:\n            train_val_test_num_samples = [None, None, None]\n            train_val_test_epochs = [1, 1, 1]\n\n        if (neox_args.train_data_paths) or (neox_args.pos_train_data_paths):\n            # when individual train / valid / test data paths are provided\n            # normalize weight values and get num samples for each dataset\n            train_weights, train_num_samples = get_normalized_weights_and_num_samples(\n                neox_args.train_data_weights, train_val_test_num_samples[0]\n            )\n            valid_weights, valid_num_samples = get_normalized_weights_and_num_samples(\n                neox_args.valid_data_weights, train_val_test_num_samples[1]\n            )\n            test_weights, test_num_samples = get_normalized_weights_and_num_samples(\n                neox_args.test_data_weights, train_val_test_num_samples[2]\n            )\n\n            # build individual datasets\n            train_datasets, valid_datasets, test_datasets = build_weighted_datasets(\n                neox_args,\n                train_num_samples,\n                valid_num_samples,\n                test_num_samples,\n                train_val_test_epochs[0],\n                train_val_test_epochs[1],\n                train_val_test_epochs[2],\n                build_index_mappings=not neox_args.weight_by_num_documents,\n            )\n\n            if neox_args.weight_by_num_documents:\n                # gets the number of documents in each datapath\n                get_num_docs_list = lambda datasets: [\n                    dataset.indexed_dataset.sizes.shape[0] for dataset in datasets\n                ]\n                train_num_docs, valid_num_docs, test_num_docs = (\n                    get_num_docs_list(train_datasets),\n                    get_num_docs_list(valid_datasets),\n                    get_num_docs_list(test_datasets),\n                )\n\n                # builds weights according to alpha + the number of docs\n                fn = partial(\n                    weights_by_num_docs, alpha=neox_args.weighted_sampler_alpha\n                )\n                train_weights, valid_weights, test_weights = (\n                    fn(train_num_docs),\n                    fn(valid_num_docs),\n                    fn(test_num_docs),\n                )\n                (\n                    train_weights,\n                    train_num_samples,\n                ) = get_normalized_weights_and_num_samples(\n                    train_weights, train_val_test_num_samples[0]\n                )\n                (\n                    valid_weights,\n                    valid_num_samples,\n                ) = get_normalized_weights_and_num_samples(\n                    valid_weights, train_val_test_num_samples[1]\n                )\n                test_weights, test_num_samples = get_normalized_weights_and_num_samples(\n                    test_weights, train_val_test_num_samples[2]\n                )\n\n                # rebuild datasets weighted according to new weights\n                train_datasets, valid_datasets, test_datasets = build_weighted_datasets(\n                    neox_args,\n                    train_num_samples,\n                    valid_num_samples,\n                    test_num_samples,\n                    train_val_test_epochs[0],\n                    train_val_test_epochs[1],\n                    train_val_test_epochs[2],\n                )\n\n            if train_datasets:\n                train_ds = BlendableDataset(train_datasets, train_weights)\n            if valid_datasets:\n                valid_ds = BlendableDataset(valid_datasets, valid_weights)\n            if test_datasets:\n                test_ds = BlendableDataset(test_datasets, test_weights)\n        else:\n            # when just data_path is provided\n            # split dataset into train, valid and test from data_path\n            train_ds, valid_ds, test_ds = build_train_valid_test_datasets(\n                data_prefix=neox_args.data_path,\n                use_shared_fs=neox_args.use_shared_fs,\n                data_impl=neox_args.data_impl,\n                splits_string=neox_args.split,\n                train_valid_test_num_samples=train_val_test_num_samples,\n                train_valid_test_epochs=train_val_test_epochs,\n                seq_length=neox_args.seq_length,\n                seed=neox_args.seed,\n                skip_warmup=(not neox_args.mmap_warmup),\n                pack_impl=neox_args.pack_impl,\n                allow_chopped=neox_args.allow_chopped,\n            )\n\n        # Build dataloders.\n        train_dataloader = make_data_loader(train_ds, neox_args=neox_args)\n        valid_dataloader = make_data_loader(valid_ds, neox_args=neox_args)\n        test_dataloader = make_data_loader(test_ds, neox_args=neox_args)\n\n        # Flags to know if we need to do training/validation/testing.\n        if neox_args.train_epochs:\n            do_train = train_dataloader is not None\n            do_valid = valid_dataloader is not None\n            do_test = test_dataloader is not None\n        else:\n            do_train = train_dataloader is not None and neox_args.train_iters > 0\n            do_valid = valid_dataloader is not None and neox_args.eval_iters > 0\n            do_test = test_dataloader is not None and neox_args.eval_iters > 0\n\n        # Need to broadcast num_tokens and num_type_tokens.\n        flags = torch.cuda.LongTensor([int(do_train), int(do_valid), int(do_test)])\n    else:\n        flags = torch.cuda.LongTensor([0, 0, 0])\n\n    # Broadcast num tokens.\n    if neox_args.is_pipe_parallel:\n        # Only first/last pipeline stages have data loaders, so pipeline parallelism should\n        # broadcast globally instead of just the model parallel group.\n        torch.distributed.broadcast(flags, src=0)\n    else:\n        torch.distributed.broadcast(\n            flags,\n            mpu.get_model_parallel_src_rank(),\n            group=mpu.get_model_parallel_group(),\n        )\n    neox_args.do_train = flags[0].item()\n    neox_args.do_valid = flags[1].item()\n    neox_args.do_test = flags[2].item()\n    data_loaders = {\n        \"train\": train_dataloader,\n        \"valid\": valid_dataloader,\n        \"test\": test_dataloader,\n    }\n    return data_loaders\n\n\ndef shift_and_wrap_data_loaders(neox_args, data_loaders, loop=True):\n    \"\"\"Shift start iteration and wrap data_loaders in iterators\"\"\"\n    train_dataloader = data_loaders[\"train\"]\n    valid_dataloader = data_loaders[\"valid\"]\n    test_dataloader = data_loaders[\"test\"]\n\n    # Shift the start iterations.\n    if train_dataloader is not None:\n        train_dataloader.batch_sampler.start_iter = (\n            neox_args.iteration * neox_args.gradient_accumulation_steps\n        ) % len(train_dataloader)\n        print_rank_0(\n            \"setting training data start iteration to {}\".format(\n                train_dataloader.batch_sampler.start_iter\n            )\n        )\n    if valid_dataloader is not None:\n        start_iter_val = (\n            (neox_args.iteration * neox_args.gradient_accumulation_steps)\n            // neox_args.eval_interval\n        ) * neox_args.eval_iters\n        valid_dataloader.batch_sampler.start_iter = start_iter_val % len(\n            valid_dataloader\n        )\n        print_rank_0(\n            \"setting validation data start iteration to {}\".format(\n                valid_dataloader.batch_sampler.start_iter\n            )\n        )\n\n    def loop_iterator(data_loader):\n        while True:\n            for x in data_loader:\n                yield x\n            data_loader.start_iter = 0\n\n    # Build iterators.\n    if train_dataloader is not None:\n        if loop:\n            train_data_iterator = cycle(train_dataloader)\n        else:\n            train_data_iterator = iter(train_dataloader)\n    else:\n        train_data_iterator = None\n\n    if valid_dataloader is not None:\n        if loop:\n            valid_data_iterator = cycle(valid_dataloader)\n        else:\n            valid_data_iterator = iter(valid_dataloader)\n    else:\n        valid_data_iterator = None\n\n    if test_dataloader is not None:\n        if loop:\n            test_data_iterator = cycle(test_dataloader)\n        else:\n            test_data_iterator = iter(test_dataloader)\n    else:\n        test_data_iterator = None\n\n    return train_data_iterator, valid_data_iterator, test_data_iterator\n\n\ndef compile_helper():\n    \"\"\"Compile helper function at runtime. Make sure this\n    is invoked on a single process.\"\"\"\n    import os\n    import subprocess\n\n    path = os.path.abspath(os.path.dirname(__file__))\n    ret = subprocess.run([\"make\", \"-C\", path])\n    if ret.returncode != 0:\n        print(\"Making C++ dataset helpers module failed, exiting.\")\n        import sys\n\n        sys.exit(1)\n",
        "megatron/data/gpt2_dataset.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"GPT2 style dataset.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nfrom megatron import mpu, print_rank_0\n\n\nclass GPT2Dataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        name,\n        data_prefix,\n        documents,\n        indexed_dataset,\n        num_samples,\n        num_epochs,\n        seq_length,\n        seed,\n        pack_impl=\"packed\",\n        allow_chopped=True,\n        build_index_mappings=True,\n        use_shared_fs=True,\n        label_dataset=None,\n        reward_dataset=None,\n        ref_dataset=None,\n    ):\n\n        self.name = name\n        self.pack_impl = pack_impl\n        self.allow_chopped = allow_chopped\n        self.indexed_dataset = indexed_dataset\n        self.label_dataset = label_dataset\n        self.reward_dataset = reward_dataset\n        self.ref_dataset = ref_dataset\n        self.seq_length = seq_length\n\n        # Checks\n        assert self.reward_dataset is None or (\n            pack_impl == \"unpacked\"\n        ), \"Reward dataset only supported with unpacked data.\"\n        assert np.min(documents) >= 0\n        assert np.max(documents) < indexed_dataset.sizes.shape[0]\n\n        if build_index_mappings:\n            # Build index mappings.\n            self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(\n                self.name,\n                data_prefix,\n                documents,\n                self.indexed_dataset.sizes,\n                self.label_dataset,\n                num_samples,\n                num_epochs,\n                seq_length,\n                seed,\n                self.pack_impl,\n                use_shared_fs=use_shared_fs,\n                allow_chopped=self.allow_chopped,\n            )\n            self.shuffle_idx_len = self.shuffle_idx.shape[0] - 1\n            self.sample_idx_len = self.sample_idx.shape[0] - 1\n\n            if self.shuffle_idx_len != self.sample_idx_len - 1:\n                print(\n                    f\"WARNING: shuffle index length ({self.shuffle_idx_len}) is not equal to sample index length ({self.sample_idx_len})\"\n                )\n\n    def __len__(self):\n        return min(self.shuffle_idx_len, self.sample_idx_len)\n\n    def __getitem__(self, idx):\n        try:\n            # Get the shuffled index.\n            idx = self.shuffle_idx[idx]\n            # Start and end documents and offsets.\n            doc_index_f = self.sample_idx[idx][0]\n            doc_index_l = self.sample_idx[idx + 1][0]\n            offset_f = self.sample_idx[idx][1]\n            offset_l = self.sample_idx[idx + 1][1]\n            # Labels and texts are supposed to be fully in sync.\n            datasets = [self.indexed_dataset]\n            rw_indx = 1\n            if self.label_dataset is not None:\n                rw_indx += 1\n                datasets.append(self.label_dataset)\n            if self.reward_dataset is not None:\n                datasets.append(self.reward_dataset)\n            else:\n                rw_indx = -1\n            if self.ref_dataset is not None:\n                datasets.append(self.ref_dataset)\n            samples = []\n            sample_lengths = []\n            # If we are within the same document, just extract the chunk.\n            for n, dataset in enumerate(datasets):\n                if doc_index_f == doc_index_l:\n                    if rw_indx == n:\n                        # If we are in the reward dataset, we only need the last token.\n                        rw = dataset.get(self.doc_idx[doc_index_f])\n                        samples.append(\n                            np.array([rw[0] for _ in range(len(samples[-1]))])\n                        )\n                    else:\n                        samples.append(\n                            dataset.get(\n                                self.doc_idx[doc_index_f],\n                                offset=offset_f,\n                                length=offset_l - offset_f + 1,\n                            )\n                        )\n                else:\n                    if n != rw_indx:\n                        # reset\n                        sample_lengths = []\n                    # Otherwise, get the rest of the initial document.\n                    if n == rw_indx:\n                        rw = dataset.get(self.doc_idx[doc_index_f])\n                        sample_list = [\n                            np.array([rw[0] for _ in range(sample_lengths.pop(0))])\n                        ]\n                    else:\n                        sample_list = [\n                            dataset.get(self.doc_idx[doc_index_f], offset=offset_f)\n                        ]\n                        sample_lengths.append(len(sample_list[-1]))\n                    # Loop over all in between documents and add the entire document.\n                    for i in range(doc_index_f + 1, doc_index_l):\n                        if n == rw_indx:\n                            rw = dataset.get(self.doc_idx[i])\n                            sample_list.append(\n                                np.array([rw[0] for _ in range(sample_lengths.pop(0))])\n                            )\n                        else:\n                            sample_list.append(dataset.get(self.doc_idx[i]))\n                            sample_lengths.append(len(sample_list[-1]))\n                    # And finally add the relevant portion of last document.\n                    if n == rw_indx:\n                        rw = dataset.get(self.doc_idx[doc_index_l])\n                        sample_list.append(\n                            np.array([rw[0] for _ in range(sample_lengths.pop(0))])\n                        )\n                    else:\n                        sample_list.append(\n                            dataset.get(self.doc_idx[doc_index_l], length=offset_l + 1)\n                        )\n                        sample_lengths.append(len(sample_list[-1]))\n                    samples.append(np.concatenate(sample_list))\n            for i in range(len(samples)):\n                mask = (self.label_dataset is not None) and (i == 1)\n                if len(samples[i]) < (self.seq_length + 1):\n                    # Pad\n                    samples[i] = np.pad(\n                        samples[i],\n                        (0, (self.seq_length + 1) - len(samples[i])),\n                        mode=\"constant\",\n                        constant_values=-100 if mask else 0,\n                    )\n                elif len(samples[i]) > (self.seq_length + 1):\n                    # Truncate\n                    samples[i] = samples[i][: (self.seq_length + 1)]\n            ret = {\"text\": np.array(samples[0], dtype=np.int64)}\n            next_idx = 1\n            if self.label_dataset is not None:\n                ret[\"label\"] = np.array(samples[next_idx], dtype=np.int64)\n                next_idx += 1\n            if self.reward_dataset is not None:\n                ret[\"reward\"] = np.array(samples[next_idx], dtype=np.float32)\n                next_idx += 1\n            if self.ref_dataset is not None:\n                ret[\"ref\"] = np.array(samples[next_idx], dtype=np.float32)\n            return ret\n        except IndexError as err:\n            new_idx = idx % len(self)\n            print(\n                f\"WARNING: Got index out of bounds error with index {idx} - taking modulo of index instead ({new_idx}), error: {err}\"\n            )\n            return self[new_idx]\n\n\ndef _build_index_mappings(\n    name,\n    data_prefix,\n    documents,\n    sizes,\n    label_dataset,\n    num_samples,\n    num_epochs,\n    seq_length,\n    seed,\n    packing_impl,\n    use_shared_fs=True,\n    allow_chopped=True,\n):\n    \"\"\"Build doc-idx, sample-idx, and shuffle-idx.\n    doc-idx: is an array (ordered) of documents to be used in training.\n    sample-idx: is the start document index and document offset for each\n       training sample.\n    shuffle-idx: maps the sample index into a random index into sample-idx.\n    \"\"\"\n    # Number of tokens in each epoch and number of required epochs.\n    tokens_per_epoch = _num_tokens(documents, sizes)\n    if not num_epochs:\n        num_epochs = _num_epochs(tokens_per_epoch, seq_length, num_samples)\n    # rng state\n    np_rng = np.random.RandomState(seed=seed)\n\n    # Filename of the index mappings.\n    _filename = data_prefix\n    _filename += \"_{}_indexmap\".format(name)\n    _filename += \"_{}ns\".format(num_samples)\n    _filename += \"_{}sl\".format(seq_length)\n    _filename += \"_{}s\".format(seed)\n    _filename += \"_{}pi\".format(packing_impl)\n    if allow_chopped:\n        _filename += \"_ac\"\n    doc_idx_filename = _filename + \"_doc_idx.npy\"\n    sample_idx_filename = _filename + \"_sample_idx.npy\"\n    shuffle_idx_filename = _filename + \"_shuffle_idx.npy\"\n\n    if not use_shared_fs:\n        should_process_dataset = int(os.environ[\"LOCAL_RANK\"]) == 0\n    else:\n        should_process_dataset = torch.distributed.get_rank() == 0\n\n    # Build the indexed mapping if not exist.\n    if should_process_dataset:\n        if (\n            (not os.path.isfile(doc_idx_filename))\n            or (not os.path.isfile(sample_idx_filename))\n            or (not os.path.isfile(shuffle_idx_filename))\n        ):\n            print_rank_0(\n                \" > WARNING: could not find index map files, building \"\n                \"the indices on rank 0 ...\"\n            )\n            # doc-idx.\n            start_time = time.time()\n            if packing_impl == \"packed\":\n                doc_idx = _build_doc_idx(documents, num_epochs, np_rng)\n                np.save(doc_idx_filename, doc_idx, allow_pickle=True)\n                print_rank_0(\n                    \" > elapsed time to build and save doc-idx mapping \"\n                    \"(seconds): {:4f}\".format(time.time() - start_time)\n                )\n                # sample-idx.\n                start_time = time.time()\n                # Use C++ implementation for speed.\n                from megatron.data import helpers\n\n                assert doc_idx.dtype == np.int32\n                assert sizes.dtype == np.int32\n\n                num_samples = (num_epochs * tokens_per_epoch - 1) / seq_length\n                if 2 * (num_samples + 1) < np.iinfo(np.int32).max:\n                    sample_idx = helpers.build_sample_idx_int32(\n                        sizes, doc_idx, seq_length, num_epochs, tokens_per_epoch\n                    )\n                else:\n                    sample_idx = helpers.build_sample_idx_int64(\n                        sizes, doc_idx, seq_length, num_epochs, tokens_per_epoch\n                    )\n                np.save(sample_idx_filename, sample_idx, allow_pickle=True)\n                print_rank_0(\n                    \" > elapsed time to build and save sample-idx mapping \"\n                    \"(seconds): {:4f}\".format(time.time() - start_time)\n                )\n                # shuffle-idx.\n                start_time = time.time()\n                # -1 is due to data structure used to retrieve the index:\n                #    sample i --> [sample_idx[i], sample_idx[i+1])\n                shuffle_idx = _build_shuffle_idx(sample_idx.shape[0] - 1, np_rng)\n                np.save(shuffle_idx_filename, shuffle_idx, allow_pickle=True)\n                print_rank_0(\n                    \" > elapsed time to build and save shuffle-idx mapping\"\n                    \" (seconds): {:4f}\".format(time.time() - start_time)\n                )\n            elif packing_impl == \"pack_until_overflow\":\n                # Naively pack data until it overflows, then roll it over to a new one instead.\n                shuffle_idx = np.arange(num_samples)  # Shuffle index around epochs\n                np_rng.shuffle(shuffle_idx)\n                sample_idx = []\n                doc_idx = []\n                # Iterate over files until we have enough samples.\n                temp_shuffle_idx = np.arange(len(documents))\n                np_rng.shuffle(temp_shuffle_idx)\n                running_length = 0\n                curr_shuffle_idx = 0\n                while len(sample_idx) < num_samples:\n                    if not allow_chopped:\n                        # +1 since we shift left/right by 1\n                        if sizes[temp_shuffle_idx[curr_shuffle_idx]] > seq_length + 1:\n                            curr_shuffle_idx += 1\n                            continue\n                    # First, check if we need to skip this item...\n                    if label_dataset is not None:\n                        if np.all(\n                            label_dataset.get(temp_shuffle_idx[curr_shuffle_idx])[\n                                : seq_length + 1\n                            ]\n                            == -100\n                        ):\n                            curr_shuffle_idx += 1\n                            continue\n                    doc_length = sizes[temp_shuffle_idx[curr_shuffle_idx]]\n                    if running_length == 0:\n                        sample_idx.append(np.array([len(doc_idx), 0]))\n                        doc_idx.append(temp_shuffle_idx[curr_shuffle_idx])\n                        running_length += doc_length\n                    else:\n                        if running_length + doc_length > (seq_length + 1):\n                            running_length = doc_length\n                            sample_idx.append(np.array([len(doc_idx), 0]))\n                        else:\n                            running_length += doc_length\n                        doc_idx.append(temp_shuffle_idx[curr_shuffle_idx])\n                    curr_shuffle_idx += 1\n                    if curr_shuffle_idx == len(documents):\n                        curr_shuffle_idx = 0\n                        np_rng.shuffle(temp_shuffle_idx)\n                sample_idx.append(np.array([len(doc_idx), 0]))\n                np.save(doc_idx_filename, doc_idx, allow_pickle=True)\n                np.save(sample_idx_filename, sample_idx, allow_pickle=True)\n                np.save(shuffle_idx_filename, shuffle_idx, allow_pickle=True)\n            elif packing_impl == \"unpacked\":\n                # Unpacked data, one sample per document.\n                shuffle_idx = np.arange(num_samples)  # Shuffle index around epochs\n                np_rng.shuffle(shuffle_idx)\n                sample_idx = np.zeros((num_samples + 1, 2), dtype=np.int64)\n                sample_idx[:, 0] = np.array([i for i in range(num_samples + 1)])\n                sample_idx[:, 1] = 0\n                doc_idx = list()\n                doc_i = 0\n                while len(doc_idx) <= num_samples:\n                    if not allow_chopped:\n                        # +1 since we shift left/right by 1\n                        if sizes[doc_i] > seq_length + 1:\n                            doc_i = (doc_i + 1) % len(documents)\n                            continue\n                    # Just in case we have bad data in the loop...\n                    if np.all(label_dataset.get(doc_i)[:seq_length] == -100):\n                        doc_i = (doc_i + 1) % len(documents)\n                        continue\n                    doc_idx.append(doc_i)\n                    doc_i = (doc_i + 1) % len(documents)\n                np.save(doc_idx_filename, doc_idx, allow_pickle=True)\n                np.save(sample_idx_filename, sample_idx, allow_pickle=True)\n                np.save(shuffle_idx_filename, shuffle_idx, allow_pickle=True)\n\n    # This should be a barrier but nccl barrier assumes\n    # device_index=rank which is not the case for model\n    # parallel case\n    counts = torch.cuda.LongTensor([1])\n    torch.distributed.all_reduce(counts, group=mpu.get_io_parallel_group())\n    assert counts[0].item() == torch.distributed.get_world_size(\n        group=mpu.get_io_parallel_group()\n    )\n\n    # Load mappings.\n    start_time = time.time()\n    print_rank_0(\" > loading doc-idx mapping from {}\".format(doc_idx_filename))\n    doc_idx = np.load(doc_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\" > loading sample-idx mapping from {}\".format(sample_idx_filename))\n    sample_idx = np.load(sample_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\" > loading shuffle-idx mapping from {}\".format(shuffle_idx_filename))\n    shuffle_idx = np.load(shuffle_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\n        \"    loaded indexed file in {:3.3f} seconds\".format(time.time() - start_time)\n    )\n    print_rank_0(\"    total number of samples: {}\".format(sample_idx.shape[0]))\n    print_rank_0(\"    total number of epochs: {}\".format(num_epochs))\n\n    return doc_idx, sample_idx, shuffle_idx\n\n\ndef _num_tokens(documents, sizes):\n    \"\"\"Total number of tokens in the dataset.\"\"\"\n    return np.sum(sizes[documents])\n\n\ndef _num_epochs(tokens_per_epoch, seq_length, num_samples):\n    \"\"\"Based on number of samples and sequence length, calculate how many\n    epochs will be needed.\"\"\"\n    num_epochs = 0\n    total_tokens = 0\n    while True:\n        num_epochs += 1\n        total_tokens += tokens_per_epoch\n        # -1 is because we need to retrieve seq_length + 1 token each time\n        # but the last token will overlap with the first token of the next\n        # sample except for the last sample.\n        if ((total_tokens - 1) // seq_length) >= num_samples:\n            return num_epochs\n\n\ndef _build_doc_idx(documents, num_epochs, np_rng):\n    \"\"\"Build an array with length = number-of-epochs * number-of-documents.\n    Each index is mapped to a corresponding document.\"\"\"\n    doc_idx = np.mgrid[0:num_epochs, 0 : len(documents)][1]\n    doc_idx[:] = documents\n    doc_idx = doc_idx.reshape(-1)\n    doc_idx = doc_idx.astype(np.int32)\n    np_rng.shuffle(doc_idx)\n    return doc_idx\n\n\ndef _build_sample_idx(sizes, doc_idx, seq_length, num_epochs, tokens_per_epoch):\n    \"\"\"Sample index mapping is a 2D array with sizes\n    [number-of-samples + 1, 2] where [..., 0] contains\n    the index into `doc_idx` and [..., 1] is the\n    starting offset in that document.\"\"\"\n\n    # Total number of samples. For -1 see comments in `_num_epochs`.\n    num_samples = (num_epochs * tokens_per_epoch - 1) // seq_length\n    sample_idx = np.zeros([num_samples + 1, 2], dtype=np.int64)\n\n    # Index into sample_idx.\n    sample_index = 0\n    # Index into doc_idx.\n    doc_idx_index = 0\n    # Beginning offset for each document.\n    doc_offset = 0\n    # Start with first document and no offset.\n    sample_idx[sample_index][0] = doc_idx_index\n    sample_idx[sample_index][1] = doc_offset\n    sample_index += 1\n    while sample_index <= num_samples:\n        # Start with a fresh sequence.\n        remaining_seq_length = seq_length + 1\n        while remaining_seq_length != 0:\n            # Get the document length.\n            doc_id = doc_idx[doc_idx_index]\n            doc_length = sizes[doc_id] - doc_offset\n            # And add it to the current sequence.\n            remaining_seq_length -= doc_length\n            # If we have more than a full sequence, adjust offset and set\n            # remaining length to zero so we return from the while loop.\n            # Note that -1 here is for the same reason we have -1 in\n            # `_num_epochs` calculations.\n            if remaining_seq_length <= 0:\n                doc_offset += remaining_seq_length + doc_length - 1\n                remaining_seq_length = 0\n            else:\n                # Otherwise, start from the beginning of the next document.\n                doc_idx_index += 1\n                doc_offset = 0\n        # Record the sequence.\n        sample_idx[sample_index][0] = doc_idx_index\n        sample_idx[sample_index][1] = doc_offset\n        sample_index += 1\n\n    return sample_idx\n\n\ndef _build_shuffle_idx(size, np_rng):\n    \"\"\"Build the range [0, size) and shuffle.\"\"\"\n    dtype_ = np.uint32\n    if size >= (np.iinfo(np.uint32).max - 1):\n        dtype_ = np.int64\n    shuffle_idx = np.arange(start=0, stop=size, step=1, dtype=dtype_)\n    np_rng.shuffle(shuffle_idx)\n    return shuffle_idx\n",
        "megatron/data/indexed_dataset.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n# copied from fairseq/fairseq/data/indexed_dataset.py\n# Removed IndexedRawTextDataset since it relied on Fairseq dictionary\n# other slight modifications to remove fairseq dependencies\n# Added document index to index file and made it accessible.\n#    An empty sentence no longer separates documents.\n\nimport os\nimport shutil\nimport struct\nfrom functools import lru_cache\nfrom itertools import accumulate\n\nimport numpy as np\nimport torch\n\nfrom megatron import print_rank_0\n\n\ndef __best_fitting_dtype(vocab_size=None):\n    if vocab_size is not None and vocab_size < 65500:\n        return np.uint16\n    else:\n        return np.int32\n\n\ndef infer_dataset_impl(path):\n    if IndexedDataset.exists(path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            if magic == IndexedDataset._HDR_MAGIC:\n                return \"cached\"\n            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:\n                return \"mmap\"\n            else:\n                return None\n    else:\n        print(f\"Dataset does not exist: {path}\")\n        print(\n            \"Path should be a basename that both .idx and .bin can be appended to get full filenames.\"\n        )\n        return None\n\n\ndef make_builder(out_file, impl, vocab_size=None):\n    if impl == \"mmap\":\n        return MMapIndexedDatasetBuilder(\n            out_file, dtype=__best_fitting_dtype(vocab_size)\n        )\n    else:\n        return IndexedDatasetBuilder(out_file)\n\n\ndef make_dataset(path, impl, skip_warmup=False):\n    if not IndexedDataset.exists(path):\n        print(f\"Dataset does not exist: {path}\")\n        print(\n            \"Path should be a basename that both .idx and .bin can be appended to get full filenames.\"\n        )\n        return None\n    if impl == \"infer\":\n        impl = infer_dataset_impl(path)\n    elif impl == \"cached\" and IndexedDataset.exists(path):\n        return IndexedCachedDataset(path)\n    elif impl == \"mmap\" and MMapIndexedDataset.exists(path):\n        return MMapIndexedDataset(path, skip_warmup)\n    print(f\"Unknown dataset implementation: {impl}\")\n    return None\n\n\ndef dataset_exists(path, impl):\n    if impl == \"mmap\":\n        return MMapIndexedDataset.exists(path)\n    else:\n        return IndexedDataset.exists(path)\n\n\ndef read_longs(f, n):\n    a = np.empty(n, dtype=np.int64)\n    f.readinto(a)\n    return a\n\n\ndef write_longs(f, a):\n    f.write(np.array(a, dtype=np.int64))\n\n\ndtypes = {\n    1: np.uint8,\n    2: np.int8,\n    3: np.int16,\n    4: np.int32,\n    5: np.int64,\n    6: np.float32,\n    7: np.float64,\n    8: np.uint16,\n}\n\n\ndef code(dtype):\n    for k in dtypes.keys():\n        if dtypes[k] == dtype:\n            return k\n    raise ValueError(dtype)\n\n\ndef index_file_path(prefix_path):\n    return prefix_path + \".idx\"\n\n\ndef data_file_path(prefix_path):\n    return prefix_path + \".bin\"\n\n\ndef create_doc_idx(sizes):\n    doc_idx = [0]\n    for i, s in enumerate(sizes):\n        if s == 0:\n            doc_idx.append(i + 1)\n    return doc_idx\n\n\nclass IndexedDataset(torch.utils.data.Dataset):\n    \"\"\"Loader for IndexedDataset\"\"\"\n\n    _HDR_MAGIC = b\"TNTIDX\\x00\\x00\"\n\n    def __init__(self, path):\n        super().__init__()\n        self.path = path\n        self.data_file = None\n        self.read_index(path)\n\n    def read_index(self, path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            assert magic == self._HDR_MAGIC, (\n                \"Index file doesn't match expected format. \"\n                \"Make sure that --dataset-impl is configured properly.\"\n            )\n            version = f.read(8)\n            assert struct.unpack(\"<Q\", version) == (1,)\n            code, self.element_size = struct.unpack(\"<QQ\", f.read(16))\n            self.dtype = dtypes[code]\n            self._len, self.s = struct.unpack(\"<QQ\", f.read(16))\n            self.doc_count = struct.unpack(\"<Q\", f.read(8))\n            self.dim_offsets = read_longs(f, self._len + 1)\n            self.data_offsets = read_longs(f, self._len + 1)\n            self.sizes = read_longs(f, self.s)\n            self.doc_idx = read_longs(f, self.doc_count)\n\n    def read_data(self, path):\n        self.data_file = open(data_file_path(path), \"rb\", buffering=0)\n\n    def check_index(self, i):\n        if i < 0 or i >= self._len:\n            raise IndexError(\"index out of range\")\n\n    def __del__(self):\n        if self.data_file:\n            self.data_file.close()\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if not self.data_file:\n            self.read_data(self.path)\n        if isinstance(idx, int):\n            i = idx\n            self.check_index(i)\n            tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n            a = np.empty(tensor_size, dtype=self.dtype)\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            return a\n        elif isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            if step != 1:\n                raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n            sizes = self.sizes[self.dim_offsets[start] : self.dim_offsets[stop]]\n            size = sum(sizes)\n            a = np.empty(size, dtype=self.dtype)\n            self.data_file.seek(self.data_offsets[start] * self.element_size)\n            self.data_file.readinto(a)\n            offsets = list(accumulate(sizes))\n            sents = np.split(a, offsets[:-1])\n            return sents\n\n    def __len__(self):\n        return self._len\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(index_file_path(path)) and os.path.exists(\n            data_file_path(path)\n        )\n\n    @property\n    def supports_prefetch(self):\n        return False  # avoid prefetching to save memory\n\n\nclass IndexedCachedDataset(IndexedDataset):\n    def __init__(self, path):\n        super().__init__(path)\n        self.cache = None\n        self.cache_index = {}\n\n    @property\n    def supports_prefetch(self):\n        return True\n\n    def prefetch(self, indices):\n        if all(i in self.cache_index for i in indices):\n            return\n        if not self.data_file:\n            self.read_data(self.path)\n        indices = sorted(set(indices))\n        total_size = 0\n        for i in indices:\n            total_size += self.data_offsets[i + 1] - self.data_offsets[i]\n        self.cache = np.empty(total_size, dtype=self.dtype)\n        ptx = 0\n        self.cache_index.clear()\n        for i in indices:\n            self.cache_index[i] = ptx\n            size = self.data_offsets[i + 1] - self.data_offsets[i]\n            a = self.cache[ptx : ptx + size]\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            ptx += size\n        if self.data_file:\n            # close and delete data file after prefetch so we can pickle\n            self.data_file.close()\n            self.data_file = None\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            i = idx\n            self.check_index(i)\n            tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n            a = np.empty(tensor_size, dtype=self.dtype)\n            ptx = self.cache_index[i]\n            np.copyto(a, self.cache[ptx : ptx + a.size])\n            return a\n        elif isinstance(idx, slice):\n            # Hack just to make this work, can optimizer later if necessary\n            sents = []\n            for i in range(*idx.indices(len(self))):\n                sents.append(self[i])\n            return sents\n\n\nclass IndexedDatasetBuilder(object):\n    element_sizes = {\n        np.uint8: 1,\n        np.int8: 1,\n        np.int16: 2,\n        np.int32: 4,\n        np.int64: 8,\n        np.float32: 4,\n        np.float64: 8,\n    }\n\n    def __init__(self, out_file, dtype=np.int32):\n        self.out_file = open(out_file, \"wb\")\n        self.dtype = dtype\n        self.data_offsets = [0]\n        self.dim_offsets = [0]\n        self.sizes = []\n        self.element_size = self.element_sizes[self.dtype]\n        self.doc_idx = [0]\n\n    def add_item(self, np_array):\n        assert isinstance(np_array, np.ndarray) and np_array.dtype == self.dtype\n        bytes = self.out_file.write(np_array)\n        self.data_offsets.append(self.data_offsets[-1] + bytes / self.element_size)\n        for s in np_array.shape:\n            self.sizes.append(s)\n        self.dim_offsets.append(self.dim_offsets[-1] + len(np_array.shape))\n\n    def end_document(self):\n        self.doc_idx.append(len(self.sizes))\n\n    def merge_file_(self, another_file):\n        index = IndexedDataset(another_file)\n        assert index.dtype == self.dtype\n\n        begin = self.data_offsets[-1]\n        for offset in index.data_offsets[1:]:\n            self.data_offsets.append(begin + offset)\n        self.sizes.extend(index.sizes)\n        begin = self.dim_offsets[-1]\n        for dim_offset in index.dim_offsets[1:]:\n            self.dim_offsets.append(begin + dim_offset)\n\n        with open(data_file_path(another_file), \"rb\") as f:\n            while True:\n                data = f.read(1024)\n                if data:\n                    self.out_file.write(data)\n                else:\n                    break\n\n    def finalize(self, index_file):\n        self.out_file.close()\n        index = open(index_file, \"wb\")\n        index.write(b\"TNTIDX\\x00\\x00\")\n        index.write(struct.pack(\"<Q\", 1))\n        index.write(struct.pack(\"<QQ\", code(self.dtype), self.element_size))\n        index.write(struct.pack(\"<QQ\", len(self.data_offsets) - 1, len(self.sizes)))\n        index.write(struct.pack(\"<Q\", len(self.doc_idx)))\n        write_longs(index, self.dim_offsets)\n        write_longs(index, self.data_offsets)\n        write_longs(index, self.sizes)\n        write_longs(index, self.doc_idx)\n        index.close()\n\n\ndef _warmup_mmap_file(path):\n    with open(path, \"rb\") as stream:\n        while stream.read(100 * 1024 * 1024):\n            pass\n\n\nclass MMapIndexedDataset(torch.utils.data.Dataset):\n    class Index(object):\n        _HDR_MAGIC = b\"MMIDIDX\\x00\\x00\"\n\n        @classmethod\n        def writer(cls, path, dtype):\n            class _Writer(object):\n                def __enter__(self):\n                    self._file = open(path, \"wb\")\n\n                    # Write Magic string so we can check the file format then opening it again.\n                    self._file.write(cls._HDR_MAGIC)\n                    # Write version number\n                    # Little endian unsigned 64 Bit integer\n                    self._file.write(struct.pack(\"<Q\", 1))\n                    # Little endian unsigned 8 Bit integer\n                    self._file.write(struct.pack(\"<B\", code(dtype)))\n\n                    return self\n\n                @staticmethod\n                def _get_pointers(sizes):\n                    pointers = np.zeros(len(sizes), dtype=np.int64)\n                    sizes = np.array(sizes, dtype=np.int64)\n\n                    np.cumsum(sizes[:-1], out=pointers[1:])\n                    pointers = pointers * dtype().itemsize\n                    return pointers\n\n                def write(self, sizes, doc_idx):\n                    pointers = self._get_pointers(sizes)\n\n                    # Little endian unsigned 64 Bit integer\n                    self._file.write(struct.pack(\"<Q\", len(sizes)))\n                    # Little endian unsigned 64 Bit integer\n                    self._file.write(struct.pack(\"<Q\", len(doc_idx)))\n\n                    sizes = np.array(sizes, dtype=np.int32)\n                    self._file.write(sizes.tobytes(order=\"C\"))\n                    del sizes\n\n                    pointers = np.array(pointers, dtype=np.int64)\n                    self._file.write(pointers.tobytes(order=\"C\"))\n                    del pointers\n\n                    doc_idx = np.array(doc_idx, dtype=np.int64)\n                    self._file.write(doc_idx.tobytes(order=\"C\"))\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    self._file.close()\n\n            return _Writer()\n\n        def __init__(self, path, skip_warmup=False):\n            with open(path, \"rb\") as stream:\n                magic_test = stream.read(9)\n                assert self._HDR_MAGIC == magic_test, (\n                    \"Index file doesn't match expected format. \"\n                    \"Make sure that --dataset-impl is configured properly.\"\n                )\n                # Little endian unsigned 64 Bit integer\n                version = struct.unpack(\"<Q\", stream.read(8))\n                assert (1,) == version\n\n                # Little endian unsigned 8 Bit integer\n                (dtype_code,) = struct.unpack(\"<B\", stream.read(1))\n                self._dtype = dtypes[dtype_code]\n                self._dtype_size = self._dtype().itemsize\n\n                self._len = struct.unpack(\"<Q\", stream.read(8))[0]\n                self._doc_count = struct.unpack(\"<Q\", stream.read(8))[0]\n                offset = stream.tell()\n\n            if not skip_warmup:\n                print_rank_0(\"    warming up index mmap file...\")\n                _warmup_mmap_file(path)\n\n            self._bin_buffer_mmap = np.memmap(path, mode=\"r\", order=\"C\")\n            self._bin_buffer = memoryview(self._bin_buffer_mmap)\n            print_rank_0(\"    reading sizes...\")\n            self._sizes = np.frombuffer(\n                self._bin_buffer, dtype=np.int32, count=self._len, offset=offset\n            )\n            print_rank_0(\"    reading pointers...\")\n            self._pointers = np.frombuffer(\n                self._bin_buffer,\n                dtype=np.int64,\n                count=self._len,\n                offset=offset + self._sizes.nbytes,\n            )\n            print_rank_0(\"    reading document index...\")\n            self._doc_idx = np.frombuffer(\n                self._bin_buffer,\n                dtype=np.int64,\n                count=self._doc_count,\n                offset=offset + self._sizes.nbytes + self._pointers.nbytes,\n            )\n\n        def __del__(self):\n            self._bin_buffer_mmap._mmap.close()\n            del self._bin_buffer_mmap\n\n        @property\n        def dtype(self):\n            return self._dtype\n\n        @property\n        def sizes(self):\n            return self._sizes\n\n        @property\n        def doc_idx(self):\n            return self._doc_idx\n\n        @lru_cache(maxsize=8)\n        def __getitem__(self, i):\n            return self._pointers[i], self._sizes[i]\n\n        def __len__(self):\n            return self._len\n\n    def __init__(self, path, skip_warmup=False):\n        super().__init__()\n\n        self._path = None\n        self._index = None\n        self._bin_buffer = None\n\n        self._do_init(path, skip_warmup)\n\n    def __getstate__(self):\n        return self._path\n\n    def __setstate__(self, state):\n        self._do_init(state)\n\n    def _do_init(self, path, skip_warmup):\n        self._path = path\n        self._index = self.Index(index_file_path(self._path), skip_warmup)\n\n        if not skip_warmup:\n            print_rank_0(\"    warming up data mmap file...\")\n            _warmup_mmap_file(data_file_path(self._path))\n        print_rank_0(\"    creating numpy buffer of mmap...\")\n        self._bin_buffer_mmap = np.memmap(\n            data_file_path(self._path), mode=\"r\", order=\"C\"\n        )\n        print_rank_0(\"    creating memory view of numpy buffer...\")\n        self._bin_buffer = memoryview(self._bin_buffer_mmap)\n\n    def __del__(self):\n        self._bin_buffer_mmap._mmap.close()\n        del self._bin_buffer_mmap\n        del self._index\n\n    def __len__(self):\n        return len(self._index)\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            ptr, size = self._index[idx]\n            np_array = np.frombuffer(\n                self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr\n            )\n            return np_array\n        elif isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            if step != 1:\n                raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n            ptr = self._index._pointers[start]\n            sizes = self._index._sizes[idx]\n            offsets = list(accumulate(sizes))\n            total_size = sum(sizes)\n            np_array = np.frombuffer(\n                self._bin_buffer, dtype=self._index.dtype, count=total_size, offset=ptr\n            )\n            sents = np.split(np_array, offsets[:-1])\n            return sents\n\n    def get(self, idx, offset=0, length=None):\n        \"\"\"Retrieves a single item from the dataset with the option to only\n        return a portion of the item.\n\n        get(idx) is the same as [idx] but get() does not support slicing.\n        \"\"\"\n        ptr, size = self._index[idx]\n        if length is None:\n            length = size - offset\n        ptr += offset * np.dtype(self._index.dtype).itemsize\n        np_array = np.frombuffer(\n            self._bin_buffer, dtype=self._index.dtype, count=length, offset=ptr\n        )\n        return np_array\n\n    @property\n    def sizes(self):\n        return self._index.sizes\n\n    @property\n    def doc_idx(self):\n        return self._index.doc_idx\n\n    def get_doc_idx(self):\n        return self._index._doc_idx\n\n    def set_doc_idx(self, doc_idx_):\n        self._index._doc_idx = doc_idx_\n\n    @property\n    def supports_prefetch(self):\n        return False\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(index_file_path(path)) and os.path.exists(\n            data_file_path(path)\n        )\n\n\nclass MMapIndexedDatasetBuilder(object):\n    def __init__(self, out_file, dtype=np.int64):\n        self._data_file = open(out_file, \"wb\")\n        self._dtype = dtype\n        self._sizes = []\n        self._doc_idx = [0]\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    def add_item(self, np_array):\n        assert isinstance(np_array, np.ndarray) and np_array.dtype == self.dtype\n        self._data_file.write(np_array.tobytes(order=\"C\"))\n        self._sizes.append(np_array.size)\n\n    def end_document(self):\n        self._doc_idx.append(len(self._sizes))\n\n    def merge_file_(self, another_file):\n        # Concatenate index\n        index = MMapIndexedDataset.Index(index_file_path(another_file))\n        assert index.dtype == self._dtype\n\n        for size in index.sizes:\n            self._sizes.append(size)\n\n        # Concatenate data\n        with open(data_file_path(another_file), \"rb\") as f:\n            shutil.copyfileobj(f, self._data_file)\n\n    def finalize(self, index_file):\n        self._data_file.close()\n\n        with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:\n            index.write(self._sizes, self._doc_idx)\n",
        "megatron/data/online_dataset.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Online dataset.\"\"\"\nfrom typing import Union, List\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport socket\nimport pickle\nfrom megatron.mpu.initialize import get_data_parallel_rank\n\n\nclass OnlineDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        num_samples,\n        seq_length,\n        leave_one_out=False,\n        data_split=\"train\",\n        dataserver_ips: Union[str, List[str]] = \"localhost\",\n        dataserver_ports: Union[int, List[int]] = 10000,\n    ):\n        self.num_samples = num_samples\n        self.global_rank = get_data_parallel_rank()\n        self.leave_one_out = leave_one_out\n        self.reward_buffer = []\n        self.online_batching_data = []\n        self.data_split = data_split\n        self.seq_length = seq_length\n        self.dataserver_ips = dataserver_ips\n        self.dataserver_ports = dataserver_ports\n\n    def __len__(self):\n        # dummy value since it's decided by the Online Trainer\n        return self.num_samples\n\n    def update_online_batches(self):\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        if isinstance(self.dataserver_ips, str):\n            ipaddr = self.dataserver_ips\n        else:\n            ipaddr = self.dataserver_ips[self.global_rank]\n        if isinstance(self.dataserver_ports, int):\n            # simply add over the global rank\n            port = self.dataserver_ports\n        else:\n            # in case we want to use different ports for different ranks, e.g. per machine sampling\n            port = self.dataserver_ports[self.global_rank]\n        print(f\"Connecting to {ipaddr}:{port}\")\n        s.connect((ipaddr, port))\n        s.send(self.data_split.encode())\n        data = b\"\"\n        while True:\n            chunk = s.recv(4096)\n            if not chunk:\n                break\n            data += chunk\n        batch_data = pickle.loads(data)\n        s.close()\n        print(f\"Received {len(batch_data)} samples from the server.\")\n        for data in batch_data:\n            if self.leave_one_out:\n                rewards = list()\n                for i in range(len(data[\"rewards\"])):\n                    rewards.append(\n                        data[\"rewards\"][i]\n                        - np.mean(\n                            [\n                                data[\"rewards\"][j]\n                                for j in range(len(data[\"rewards\"]))\n                                if j != i\n                            ]\n                        )\n                    )\n                data[\"raw_rewards\"] = data[\"rewards\"]\n                data[\"rewards\"] = rewards\n            else:\n                moving_average = 0\n                if len(self.reward_buffer) > 0:\n                    moving_average = np.mean(self.reward_buffer)\n                self.reward_buffer.append(np.mean(data[\"rewards\"]))\n                if len(self.reward_buffer) > 100:\n                    self.reward_buffer.pop(0)\n                # For metrics...\n                data[\"raw_rewards\"] = data[\"rewards\"]\n                data[\"rewards\"] = [r - moving_average for r in data[\"rewards\"]]\n            for i in range(len(data[\"completions\"])):\n                self.online_batching_data.append(\n                    [\n                        data[\"prefix\"],\n                        data[\"completions\"][i],\n                        data[\"rewards\"][i],\n                        data[\"raw_rewards\"][i],\n                    ]\n                )\n\n    def __getitem__(self, idx):\n        if len(self.online_batching_data) == 0:\n            self.update_online_batches()\n        batch = self.online_batching_data.pop(0)\n        text = batch[0] + batch[1]\n        label = [-100 for _ in batch[0]] + batch[1]\n        # +1 because of causal masking\n        if len(text) <= self.seq_length:\n            text = text + [0] * ((self.seq_length + 1) - len(text))\n            label = label + [-100] * ((self.seq_length + 1) - len(label))\n        return {\n            \"text\": np.array(text, dtype=np.int64),\n            \"label\": np.array(label, dtype=np.int64),\n            \"reward\": np.array([batch[2]], dtype=np.float32),\n            \"raw_reward\": np.array([batch[3]], dtype=np.float32),\n        }\n",
        "megatron/data/pairwise_dataset.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Pairwise style dataset.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nfrom megatron import mpu, print_rank_0\n\n\nclass PairwiseDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        name,\n        pos_data_prefix,  # Don't need neg since it's assumed you have paired the data already.\n        documents,\n        pos_indexed_dataset,\n        neg_indexed_dataset,\n        num_samples,\n        seq_length,\n        seed,\n        pack_impl=\"unpacked\",\n        build_index_mappings=True,\n        use_shared_fs=True,\n        pos_label_dataset=None,\n        pos_ref_dataset=None,\n        neg_label_dataset=None,\n        neg_ref_dataset=None,\n        allow_chopped=True,\n    ):\n\n        self.name = name\n        self.pos_indexed_dataset = pos_indexed_dataset\n        self.pos_label_dataset = pos_label_dataset\n        self.pos_ref_dataset = pos_ref_dataset\n        self.neg_indexed_dataset = neg_indexed_dataset\n        self.neg_label_dataset = neg_label_dataset\n        self.neg_ref_dataset = neg_ref_dataset\n        self.pack_impl = pack_impl\n        self.seq_length = seq_length\n        # Checks\n        assert np.min(documents) >= 0\n        assert (neg_label_dataset is not None and pos_label_dataset is not None) or (\n            neg_label_dataset is None and pos_label_dataset is None\n        ), \"Label datasets must be both None or both not None\"\n        assert np.max(documents) < pos_indexed_dataset.sizes.shape[0]\n        assert pos_indexed_dataset.sizes.shape[0] == neg_indexed_dataset.sizes.shape[0]\n        assert (\n            pack_impl != \"packed\"\n        ), \"Packed implementation not supported for pairwise dataset\"\n\n        if build_index_mappings:\n            # Build index mappings.\n            self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(\n                self.name,\n                pos_data_prefix,\n                documents,\n                self.pos_indexed_dataset.sizes,\n                self.neg_indexed_dataset.sizes,\n                self.pos_label_dataset,\n                self.neg_label_dataset,\n                num_samples,\n                seq_length,\n                seed,\n                pack_impl,\n                use_shared_fs=use_shared_fs,\n                allow_chopped=allow_chopped,\n            )\n            self.shuffle_idx_len = self.shuffle_idx.shape[0] - 1\n            self.sample_idx_len = self.sample_idx.shape[0] - 1\n\n            if self.shuffle_idx_len != self.sample_idx_len - 1:\n                print(\n                    f\"WARNING: shuffle index length ({self.shuffle_idx_len}) is not equal to sample index length ({self.sample_idx_len})\"\n                )\n\n    def __len__(self):\n        return min(self.shuffle_idx_len, self.sample_idx_len)\n\n    def __getitem__(self, idx):\n        try:\n            # Get the shuffled index.\n            idx = self.shuffle_idx[idx]\n            # Start and end documents and offsets.\n            doc_index_f = self.sample_idx[idx][0]\n            doc_index_l = self.sample_idx[idx + 1][0]\n            offset_f = self.sample_idx[idx][1]\n            offset_l = self.sample_idx[idx + 1][1]\n            # Labels and texts are supposed to be fully in sync.\n            datasets = [self.pos_indexed_dataset, self.neg_indexed_dataset]\n\n            if self.pos_label_dataset is not None:\n                datasets += [\n                    self.pos_label_dataset,\n                    self.neg_label_dataset,\n                ]\n            if self.pos_ref_dataset is not None:\n                datasets += [\n                    self.pos_ref_dataset,\n                    self.neg_ref_dataset,\n                ]\n            samples = []\n            pos_ref_samples = []\n            neg_ref_samples = []\n            # If we are within the same document, just extract the chunk.\n            for n, dataset in enumerate(datasets):\n                if doc_index_f == doc_index_l:\n                    samples.append(\n                        dataset.get(\n                            self.doc_idx[doc_index_f],\n                            offset=offset_f,\n                            length=offset_l - offset_f + 1,\n                        )\n                    )\n                else:\n                    # Otherwise, get the rest of the initial document.\n                    sample_list = [\n                        dataset.get(self.doc_idx[doc_index_f], offset=offset_f)\n                    ]\n                    # Loop over all in between documents and add the entire document.\n                    for i in range(doc_index_f + 1, doc_index_l):\n                        sample_list.append(dataset.get(self.doc_idx[i]))\n                    # And finally add the relevant portion of last document.\n                    sample_list.append(\n                        dataset.get(self.doc_idx[doc_index_l], length=offset_l + 1)\n                    )\n                    samples.append(np.concatenate(sample_list))\n            for i in range(len(samples)):\n                if len(samples[i]) < (self.seq_length + 1):\n                    if ((i == 2) or (i == 3)) and self.pos_label_dataset is not None:\n                        # Labels... So pad with -100\n                        samples[i] = np.pad(\n                            samples[i],\n                            (0, (self.seq_length + 1) - len(samples[i])),\n                            mode=\"constant\",\n                            constant_values=-100,\n                        )\n                    else:\n                        # Pad with 0s, can use any number since it's masked.\n                        samples[i] = np.pad(\n                            samples[i],\n                            (0, (self.seq_length + 1) - len(samples[i])),\n                            mode=\"constant\",\n                            constant_values=0,\n                        )\n                elif len(samples[i]) > (self.seq_length + 1):\n                    # Check for overflow and truncate.\n                    samples[i] = samples[i][: (self.seq_length + 1)]\n            ret = {}\n            ret[\"pos\"] = np.array(samples[0], dtype=np.int64)\n            ret[\"neg\"] = np.array(samples[1], dtype=np.int64)\n            if self.pos_label_dataset is not None:\n                ret[\"pos_label\"] = np.array(samples[2], dtype=np.int64)\n                ret[\"neg_label\"] = np.array(samples[3], dtype=np.int64)\n                if self.pos_ref_dataset is not None:\n                    ret[\"pos_ref\"] = np.array(samples[4], dtype=np.float32)\n                    ret[\"neg_ref\"] = np.array(samples[5], dtype=np.float32)\n            elif self.pos_ref_dataset is not None:\n                # Don't have labels...\n                ret[\"pos_ref\"] = np.array(samples[2], dtype=np.float32)\n                ret[\"neg_ref\"] = np.array(samples[3], dtype=np.float32)\n            return ret\n        except IndexError:\n            new_idx = idx % len(self)\n            print(\n                f\"WARNING: Got index out of bounds error with index {idx} - taking modulo of index instead ({new_idx})\"\n            )\n            return self[new_idx]\n\n\ndef _build_index_mappings(\n    name,\n    pos_data_prefix,\n    documents,\n    pos_sizes,\n    neg_sizes,\n    pos_label_dataset,\n    neg_label_dataset,\n    num_samples,\n    seq_length,\n    seed,\n    packing_impl,\n    use_shared_fs=True,\n    allow_chopped=True,\n):\n    \"\"\"Build doc-idx, sample-idx, and shuffle-idx.\n    doc-idx: is an array (ordered) of documents to be used in training.\n    sample-idx: is the start document index and document offset for each\n       training sample.\n    shuffle-idx: maps the sample index into a random index into sample-idx.\n    \"\"\"\n    # Number of tokens in each epoch and number of required epochs.\n    tokens_per_epoch = _num_tokens(documents, pos_sizes)\n    num_epochs = _num_epochs(tokens_per_epoch, seq_length, num_samples)\n    # rng state\n    np_rng = np.random.RandomState(seed=seed)\n\n    # Filename of the index mappings.\n    _filename = pos_data_prefix\n    _filename += \"_{}_indexmap\".format(name)\n    _filename += \"_{}ns\".format(num_samples)\n    _filename += \"_{}sl\".format(seq_length)\n    _filename += \"_{}s\".format(seed)\n    _filename += \"_{}pi\".format(packing_impl)\n    doc_idx_filename = _filename + \"_doc_idx.npy\"\n    sample_idx_filename = _filename + \"_sample_idx.npy\"\n    shuffle_idx_filename = _filename + \"_shuffle_idx.npy\"\n\n    if not use_shared_fs:\n        should_process_dataset = int(os.environ[\"LOCAL_RANK\"]) == 0\n    else:\n        should_process_dataset = torch.distributed.get_rank() == 0\n\n    # Build the indexed mapping if not exist.\n    if should_process_dataset:\n        if (\n            (not os.path.isfile(doc_idx_filename))\n            or (not os.path.isfile(sample_idx_filename))\n            or (not os.path.isfile(shuffle_idx_filename))\n        ):\n            print_rank_0(\n                \" > WARNING: could not find index map files, building \"\n                \"the indices on rank 0 ...\"\n            )\n            # doc-idx.\n            start_time = time.time()\n            if packing_impl == \"pack_until_overflow\":\n                # Naively pack data until it overflows, then roll it over to a new one instead.\n                shuffle_idx = np.arange(num_samples)  # Shuffle index around epochs\n                np_rng.shuffle(shuffle_idx)\n                sample_idx = []\n                doc_idx = []\n                # Iterate over files until we have enough samples.\n                temp_shuffle_idx = np.arange(len(documents))\n                np_rng.shuffle(temp_shuffle_idx)\n                running_length = 0\n                curr_shuffle_idx = 0\n                while len(sample_idx) < num_samples:\n                    # If not allow_chopped, skip this item if it's chopped.\n                    if not allow_chopped:\n                        if (\n                            pos_sizes[temp_shuffle_idx[curr_shuffle_idx]]\n                            < seq_length + 1\n                        ):\n                            curr_shuffle_idx += 1\n                            continue\n                        if (\n                            neg_sizes[temp_shuffle_idx[curr_shuffle_idx]]\n                            < seq_length + 1\n                        ):\n                            curr_shuffle_idx += 1\n                            continue\n                    # Then, check if we need to skip this item...\n                    if pos_label_dataset is not None:\n                        if np.all(\n                            pos_label_dataset.get(temp_shuffle_idx[curr_shuffle_idx])[\n                                : seq_length + 1\n                            ]\n                            == -100\n                        ):\n                            curr_shuffle_idx += 1\n                            continue\n                        if np.all(\n                            neg_label_dataset.get(temp_shuffle_idx[curr_shuffle_idx])[\n                                : seq_length + 1\n                            ]\n                            == -100\n                        ):\n                            curr_shuffle_idx += 1\n                            continue\n                    doc_length = max(\n                        pos_sizes[temp_shuffle_idx[curr_shuffle_idx]],\n                        neg_sizes[temp_shuffle_idx[curr_shuffle_idx]],\n                    )\n                    if running_length == 0:\n                        sample_idx.append(np.array([len(doc_idx), 0]))\n                        doc_idx.append(temp_shuffle_idx[curr_shuffle_idx])\n                        running_length += doc_length\n                    else:\n                        if running_length + doc_length > (seq_length + 1):\n                            running_length = doc_length\n                            sample_idx.append(np.array([len(doc_idx), 0]))\n                        else:\n                            running_length += doc_length\n                        doc_idx.append(temp_shuffle_idx[curr_shuffle_idx])\n                    curr_shuffle_idx += 1\n                    if curr_shuffle_idx == len(documents):\n                        curr_shuffle_idx = 0\n                        np_rng.shuffle(temp_shuffle_idx)\n                sample_idx.append(np.array([len(doc_idx), 0]))\n                np.save(doc_idx_filename, doc_idx, allow_pickle=True)\n                np.save(sample_idx_filename, sample_idx, allow_pickle=True)\n                np.save(shuffle_idx_filename, shuffle_idx, allow_pickle=True)\n            elif packing_impl == \"unpacked\":\n                # Unpacked data, one sample per document.\n                shuffle_idx = np.array([i % len(documents) for i in range(num_samples)])\n                np_rng.shuffle(shuffle_idx)\n                sample_idx = np.zeros((num_samples + 1, 2), dtype=np.int64)\n                sample_idx[:, 0] = np.array([i for i in range(num_samples + 1)])\n                sample_idx[:, 1] = 0\n                doc_idx = list()\n                doc_i = 0\n                while len(doc_idx) <= num_samples:\n                    # Check if we need to skip this item...\n                    if not allow_chopped:\n                        # +1 since we shift left/right by 1\n                        if pos_sizes[doc_i] > seq_length + 1:\n                            doc_i = (doc_i + 1) % len(documents)\n                            continue\n                        if neg_sizes[doc_i] > seq_length + 1:\n                            doc_i = (doc_i + 1) % len(documents)\n                            continue\n                    # In theory if we don't allow chopped we should be able to skip it, but the warm fuzzies I get\n                    # from this are worth the extra bool check\n                    if np.all(pos_label_dataset.get(doc_i)[:seq_length] == -100):\n                        doc_i = (doc_i + 1) % len(documents)\n                        continue\n                    if np.all(neg_label_dataset.get(doc_i)[:seq_length] == -100):\n                        doc_i = (doc_i + 1) % len(documents)\n                        continue\n                    doc_idx.append(doc_i)\n                    doc_i = (doc_i + 1) % len(documents)\n                np.save(doc_idx_filename, doc_idx, allow_pickle=True)\n                np.save(sample_idx_filename, sample_idx, allow_pickle=True)\n                np.save(shuffle_idx_filename, shuffle_idx, allow_pickle=True)\n\n    # This should be a barrier but nccl barrier assumes\n    # device_index=rank which is not the case for model\n    # parallel case\n    counts = torch.cuda.LongTensor([1])\n    torch.distributed.all_reduce(counts, group=mpu.get_io_parallel_group())\n    assert counts[0].item() == torch.distributed.get_world_size(\n        group=mpu.get_io_parallel_group()\n    )\n\n    # Load mappings.\n    start_time = time.time()\n    print_rank_0(\" > loading doc-idx mapping from {}\".format(doc_idx_filename))\n    doc_idx = np.load(doc_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\" > loading sample-idx mapping from {}\".format(sample_idx_filename))\n    sample_idx = np.load(sample_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\" > loading shuffle-idx mapping from {}\".format(shuffle_idx_filename))\n    shuffle_idx = np.load(shuffle_idx_filename, allow_pickle=True, mmap_mode=\"r\")\n    print_rank_0(\n        \"    loaded indexed file in {:3.3f} seconds\".format(time.time() - start_time)\n    )\n    print_rank_0(\"    total number of samples: {}\".format(sample_idx.shape[0]))\n    print_rank_0(\"    total number of epochs: {}\".format(num_epochs))\n\n    return doc_idx, sample_idx, shuffle_idx\n\n\ndef _num_tokens(documents, sizes):\n    \"\"\"Total number of tokens in the dataset.\"\"\"\n    return np.sum(sizes[documents])\n\n\ndef _num_epochs(tokens_per_epoch, seq_length, num_samples):\n    \"\"\"Based on number of samples and sequence length, calculate how many\n    epochs will be needed.\"\"\"\n    num_epochs = 0\n    total_tokens = 0\n    while True:\n        num_epochs += 1\n        total_tokens += tokens_per_epoch\n        # -1 is because we need to retrieve seq_length + 1 token each time\n        # but the last token will overlap with the first token of the next\n        # sample except for the last sample.\n        if ((total_tokens - 1) // seq_length) >= num_samples:\n            return num_epochs\n\n\ndef _build_doc_idx(documents, num_epochs, np_rng):\n    \"\"\"Build an array with length = number-of-epochs * number-of-documents.\n    Each index is mapped to a corresponding document.\"\"\"\n    doc_idx = np.mgrid[0:num_epochs, 0 : len(documents)][1]\n    doc_idx[:] = documents\n    doc_idx = doc_idx.reshape(-1)\n    doc_idx = doc_idx.astype(np.int32)\n    np_rng.shuffle(doc_idx)\n    return doc_idx\n\n\ndef _build_sample_idx(sizes, doc_idx, seq_length, num_epochs, tokens_per_epoch):\n    \"\"\"Sample index mapping is a 2D array with sizes\n    [number-of-samples + 1, 2] where [..., 0] contains\n    the index into `doc_idx` and [..., 1] is the\n    starting offset in that document.\"\"\"\n\n    # Total number of samples. For -1 see comments in `_num_epochs`.\n    num_samples = (num_epochs * tokens_per_epoch - 1) // seq_length\n    sample_idx = np.zeros([num_samples + 1, 2], dtype=np.int64)\n\n    # Index into sample_idx.\n    sample_index = 0\n    # Index into doc_idx.\n    doc_idx_index = 0\n    # Beginning offset for each document.\n    doc_offset = 0\n    # Start with first document and no offset.\n    sample_idx[sample_index][0] = doc_idx_index\n    sample_idx[sample_index][1] = doc_offset\n    sample_index += 1\n    while sample_index <= num_samples:\n        # Start with a fresh sequence.\n        remaining_seq_length = seq_length + 1\n        while remaining_seq_length != 0:\n            # Get the document length.\n            doc_id = doc_idx[doc_idx_index]\n            doc_length = sizes[doc_id] - doc_offset\n            # And add it to the current sequence.\n            remaining_seq_length -= doc_length\n            # If we have more than a full sequence, adjust offset and set\n            # remaining length to zero so we return from the while loop.\n            # Note that -1 here is for the same reason we have -1 in\n            # `_num_epochs` calculations.\n            if remaining_seq_length <= 0:\n                doc_offset += remaining_seq_length + doc_length - 1\n                remaining_seq_length = 0\n            else:\n                # Otherwise, start from the beginning of the next document.\n                doc_idx_index += 1\n                doc_offset = 0\n        # Record the sequence.\n        sample_idx[sample_index][0] = doc_idx_index\n        sample_idx[sample_index][1] = doc_offset\n        sample_index += 1\n\n    return sample_idx\n\n\ndef _build_shuffle_idx(size, np_rng):\n    \"\"\"Build the range [0, size) and shuffle.\"\"\"\n    dtype_ = np.uint32\n    if size >= (np.iinfo(np.uint32).max - 1):\n        dtype_ = np.int64\n    shuffle_idx = np.arange(start=0, stop=size, step=1, dtype=dtype_)\n    np_rng.shuffle(shuffle_idx)\n    return shuffle_idx\n",
        "megatron/data/samplers.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Batch samplers that work with either random or sequential data samplers.\"\"\"\n\nimport torch\nfrom torch.utils import data\n\n\nclass RandomSampler(data.sampler.Sampler):\n    \"\"\"Based off of pytorch RandomSampler and DistributedSampler. Essentially\n    a RandomSampler, but this class lets the user set an epoch like\n    DistributedSampler Samples elements randomly. If without replacement, then\n    sample from a shuffled dataset. If with replacement, then user can\n    specify ``num_samples`` to draw.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n        num_samples (int): number of samples to draw, default=len(dataset)\n        replacement (bool): samples are drawn with replacement if ``True``,\n        default=False\n    \"\"\"\n\n    def __init__(self, data_source, replacement=False, num_samples=None):\n        self.data_source = data_source\n        self.replacement = replacement\n        self._num_samples = num_samples\n        self.epoch = -1\n\n        if self._num_samples is not None and replacement is False:\n            raise ValueError(\n                \"With replacement=False, num_samples should not \"\n                \"be specified, since a random permute will be \"\n                \"performed.\"\n            )\n\n        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n            raise ValueError(\n                \"num_samples should be a positive integer \"\n                \"value, but got num_samples={}\".format(self.num_samples)\n            )\n        if not isinstance(self.replacement, bool):\n            raise ValueError(\n                \"replacement should be a boolean value, but got \"\n                \"replacement={}\".format(self.replacement)\n            )\n\n    @property\n    def num_samples(self):\n        # dataset size might change at runtime\n        if self._num_samples is None:\n            return len(self.data_source)\n        return self._num_samples\n\n    def __iter__(self):\n        n = len(self.data_source)\n        g = torch.Generator()\n        if self.epoch >= 0:\n            g.manual_seed(self.epoch)\n        if self.replacement:\n            return iter(\n                torch.randint(\n                    high=n, size=(self.num_samples,), dtype=torch.int64, generator=g\n                ).tolist()\n            )\n        return iter(torch.randperm(n, generator=g).tolist())\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass DistributedBatchSampler(data.sampler.BatchSampler):\n    \"\"\"Similar to normal implementation of distributed sampler, except\n    implementation is at the batch sampler level, instead of just the\n    sampler level. This allows wrapping of arbitrary data samplers\n    (sequential, random, WeightedRandomSampler, etc.) with this batch\n    sampler.\n\n    The `interleave` argument specifies how to distribute a batch. A value\n    of True combined with the above random sampler is equivalent to pytorch's\n    torch.utils.data.distributed.DistributedSampler.\n\n    For the following batch [0,1,2,3,4,5,6,7] and data parallelism of 2\n    specifying True will result in the following samples for each gpu:\n        GPU0: [0,2,4,6] GPU1: [1,3,5,7]\n    specifying False will result in the following samples:\n        GPU0: [0,1,2,3] GPU1: [4,5,6,7]\n\n    The `infinite_loop` parameter allows the sampler to yield batches indefinitely,\n    restarting from the beginning of the dataset when all samples have been iterated over.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler,\n        batch_size,\n        drop_last,\n        rank=-1,\n        world_size=2,\n        wrap_last=False,\n        interleave=False,\n    ):\n        super(DistributedBatchSampler, self).__init__(sampler, batch_size, drop_last)\n        if rank == -1:\n            assert False, \"should not be here\"\n            rank = torch.distributed.get_rank()\n        self.rank = rank\n        self.world_size = world_size\n        self.sampler.wrap_around = 0\n        self.wrap_around = 0\n        self.wrap_last = wrap_last\n        self.start_iter = 0\n        self.interleave = interleave\n\n    def __iter__(self):\n        batch = []\n        i = 0\n        for idx in self.data_iterator(self.sampler, wrap_around=False):\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                tbatch = self._batch(batch)\n                if i >= self.start_iter:\n                    yield tbatch\n                    self.start_iter = 0\n                i += 1\n                batch = []\n        batch_len = len(batch)\n        if batch_len > 0 and not self.drop_last:\n            if self.wrap_last:\n                self.sampler.wrap_around -= self.batch_size\n                self.wrap_around += len(batch)\n                self.wrap_around %= self.batch_size\n            yield self._batch(batch)\n        if self.wrap_last:\n            self.sampler.wrap_around += self.batch_size\n\n    def data_iterator(self, _iter, wrap_around=False):\n        \"\"\"iterates through data and handles wrap around\"\"\"\n        for i, idx in enumerate(_iter):\n            if i < self.wrap_around % self.batch_size:\n                continue\n            if wrap_around:\n                self.wrap_around += 1\n                self.wrap_around %= self.batch_size\n            yield idx\n\n    def _batch(self, batch):\n        \"\"\"extracts samples only pertaining to this worker's batch\"\"\"\n        if self.interleave:\n            return batch[self.rank : self.batch_size : self.world_size]\n        start = self.rank * self.batch_size // self.world_size\n        end = (self.rank + 1) * self.batch_size // self.world_size\n        return batch[start:end]\n",
        "megatron/devutil.py": "import torch.cuda\n\n\nclass Metric:\n    \"\"\"\n    Dumb utility to collect and report average wall-time metrics.\n    \"\"\"\n\n    def __init__(self, label):\n        self.label = label\n        self.measurements = []\n\n    def collect(self, measurement):\n        self.measurements.append(measurement)\n\n    def get_measurements(self):\n        return self.measurements[:]\n\n    def report(self):\n        print(\n            self.label,\n            torch.quantile(torch.tensor(self.measurements), torch.arange(10) / 10.0),\n        )\n\n\ndef monitor_method_cuda_wall_times(metric, obj, methodname):\n    \"\"\"\n    Measure timings for a method on an object or class.\n\n    For instance:\n\n    >>> metric = Metric('!LNORM')\n    >>> monitor_method_wall_times(metric, LayerNorm, 'forward')\n    \"\"\"\n    oldmeth = getattr(obj, methodname)\n\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n\n    def newmeth(*args, **kw):\n        start_event.record()\n        try:\n            return oldmeth(*args, **kw)\n        finally:\n            end_event.record()\n            torch.cuda.synchronize()\n            elapsed = start_event.elapsed_time(end_event)\n            metric.collect(elapsed)\n            metric.report()\n\n    setattr(obj, methodname, newmeth)\n",
        "megatron/fused_kernels/__init__.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file has been modified from its original version\n#\n\nimport os\nimport pathlib\nimport subprocess\nimport torch\nfrom torch.utils import cpp_extension\n\n# Setting this param to a list has a problem of generating different\n# compilation commands (with different order of architectures) and\n# leading to recompilation of fused kernels. Set it to empty string\n# to avoid recompilation and assign arch flags explicitly in\n# extra_cuda_cflags below\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"\"\n\n\ndef load(neox_args=None):\n\n    # Check if cuda 11 is installed for compute capability 8.0\n    cc_flag = []\n    if torch.version.hip is None:\n        _, bare_metal_major, bare_metal_minor = _get_cuda_bare_metal_version(\n            cpp_extension.CUDA_HOME\n        )\n        if int(bare_metal_major) >= 11:\n            cc_flag.append(\"-gencode\")\n            cc_flag.append(\"arch=compute_80,code=sm_80\")\n            if int(bare_metal_minor) >= 1:\n                cc_flag.append(\"-gencode\")\n                cc_flag.append(\"arch=compute_86,code=sm_86\")\n            elif int(bare_metal_minor) >= 4:\n                cc_flag.append(\"-gencode\")\n                cc_flag.append(\"arch=compute_87,code=sm_87\")\n            elif int(bare_metal_minor) >= 8:\n                cc_flag.append(\"-gencode\")\n                cc_flag.append(\"arch=compute_89,code=sm_89\")\n        if int(bare_metal_major) >= 12:\n            cc_flag.append(\"-gencode\")\n            cc_flag.append(\"arch=compute_90,code=sm_90\")\n\n    # Build path\n    srcpath = pathlib.Path(__file__).parent.absolute()\n    buildpath = srcpath / \"build\"\n    _create_build_dir(buildpath)\n\n    # Determine verbosity\n    verbose = True if neox_args is None else (neox_args.rank == 0)\n\n    # Helper function to build the kernels.\n    def _cpp_extention_load_helper(\n        name, sources, extra_cuda_flags, extra_include_paths\n    ):\n        if torch.version.hip is not None:\n            extra_cuda_cflags = [\"-O3\"] + extra_cuda_flags + cc_flag\n        else:\n            extra_cuda_cflags = (\n                [\"-O3\", \"-gencode\", \"arch=compute_70,code=sm_70\", \"--use_fast_math\"]\n                + extra_cuda_flags\n                + cc_flag\n            )\n\n        return cpp_extension.load(\n            name=name,\n            sources=sources,\n            build_directory=buildpath,\n            extra_cflags=[\n                \"-O3\",\n            ],\n            extra_cuda_cflags=extra_cuda_cflags,\n            extra_include_paths=extra_include_paths,\n            verbose=verbose,\n        )\n\n    # ==============\n    # Fused softmax.\n    # ==============\n\n    if torch.version.hip is not None:\n        extra_include_paths = [os.path.abspath(srcpath)]\n    else:\n        extra_include_paths = []\n\n    if torch.version.hip is not None:\n        extra_cuda_flags = [\n            \"-D__HIP_NO_HALF_OPERATORS__=1\",\n            \"-D__HIP_NO_HALF_CONVERSIONS__=1\",\n        ]\n    else:\n        extra_cuda_flags = [\n            \"-U__CUDA_NO_HALF_OPERATORS__\",\n            \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n            \"--expt-relaxed-constexpr\",\n            \"--expt-extended-lambda\",\n        ]\n\n    # Upper triangular softmax.\n    sources = [\n        srcpath / \"scaled_upper_triang_masked_softmax.cpp\",\n        srcpath / \"scaled_upper_triang_masked_softmax_cuda.cu\",\n    ]\n    scaled_upper_triang_masked_softmax_cuda = _cpp_extention_load_helper(\n        \"scaled_upper_triang_masked_softmax_cuda\",\n        sources,\n        extra_cuda_flags,\n        extra_include_paths,\n    )\n    # Masked softmax.\n    sources = [\n        srcpath / \"scaled_masked_softmax.cpp\",\n        srcpath / \"scaled_masked_softmax_cuda.cu\",\n    ]\n    scaled_masked_softmax_cuda = _cpp_extention_load_helper(\n        \"scaled_masked_softmax_cuda\", sources, extra_cuda_flags, extra_include_paths\n    )\n    # fused rope\n    sources = [\n        srcpath / \"fused_rotary_positional_embedding.cpp\",\n        srcpath / \"fused_rotary_positional_embedding_cuda.cu\",\n    ]\n    fused_rotary_positional_embedding = _cpp_extention_load_helper(\n        \"fused_rotary_positional_embedding\",\n        sources,\n        extra_cuda_flags,\n        extra_include_paths,\n    )\n\n\ndef _get_cuda_bare_metal_version(cuda_dir):\n    raw_output = subprocess.check_output(\n        [cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True\n    )\n    output = raw_output.split()\n    release_idx = output.index(\"release\") + 1\n    release = output[release_idx].split(\".\")\n    bare_metal_major = release[0]\n    bare_metal_minor = release[1][0]\n\n    return raw_output, bare_metal_major, bare_metal_minor\n\n\ndef _create_build_dir(buildpath):\n    try:\n        os.mkdir(buildpath)\n    except OSError:\n        if not os.path.isdir(buildpath):\n            print(f\"Creation of the build directory {buildpath} failed\")\n\n\ndef load_fused_kernels():\n    try:\n        import scaled_upper_triang_masked_softmax_cuda\n        import scaled_masked_softmax_cuda\n        import fused_rotary_positional_embedding\n    except (ImportError, ModuleNotFoundError) as e:\n        print(\"\\n\")\n        print(e)\n        print(\"=\" * 100)\n        print(\n            f\"ERROR: Fused kernels configured but not properly installed. Please run `from megatron.fused_kernels import load()` then `load()` to load them correctly\"\n        )\n        print(\"=\" * 100)\n        exit()\n    return\n",
        "megatron/fused_kernels/setup.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom setuptools import setup, find_packages\nfrom torch.utils import cpp_extension\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\nfrom pathlib import Path\nimport subprocess\n\n\ndef _get_cuda_bare_metal_version(cuda_dir):\n    raw_output = subprocess.check_output(\n        [cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True\n    )\n    output = raw_output.split()\n    release_idx = output.index(\"release\") + 1\n    release = output[release_idx].split(\".\")\n    bare_metal_major = release[0]\n    bare_metal_minor = release[1][0]\n\n    return raw_output, bare_metal_major, bare_metal_minor\n\n\nsrcpath = Path(__file__).parent.absolute()\ncc_flag = []\n_, bare_metal_major, _ = _get_cuda_bare_metal_version(cpp_extension.CUDA_HOME)\nif int(bare_metal_major) >= 11:\n    cc_flag.append(\"-gencode\")\n    cc_flag.append(\"arch=compute_80,code=sm_80\")\n\nnvcc_flags = [\n    \"-O3\",\n    \"-gencode\",\n    \"arch=compute_70,code=sm_70\",\n    \"--use_fast_math\",\n    \"-U__CUDA_NO_HALF_OPERATORS__\",\n    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n    \"--expt-relaxed-constexpr\",\n    \"--expt-extended-lambda\",\n]\ncuda_ext_args = {\"cxx\": [\"-O3\"], \"nvcc\": nvcc_flags + cc_flag}\nlayernorm_cuda_args = {\n    \"cxx\": [\"-O3\"],\n    \"nvcc\": nvcc_flags + cc_flag + [\"-maxrregcount=50\"],\n}\nsetup(\n    name=\"fused_kernels\",\n    version=\"0.0.2\",\n    author=\"EleutherAI\",\n    author_email=\"contact@eleuther.ai\",\n    include_package_data=False,\n    ext_modules=[\n        CUDAExtension(\n            name=\"scaled_upper_triang_masked_softmax_cuda\",\n            sources=[\n                str(srcpath / \"scaled_upper_triang_masked_softmax.cpp\"),\n                str(srcpath / \"scaled_upper_triang_masked_softmax_cuda.cu\"),\n            ],\n            extra_compile_args=cuda_ext_args,\n        ),\n        CUDAExtension(\n            name=\"scaled_masked_softmax_cuda\",\n            sources=[\n                str(srcpath / \"scaled_masked_softmax.cpp\"),\n                str(srcpath / \"scaled_masked_softmax_cuda.cu\"),\n            ],\n            extra_compile_args=cuda_ext_args,\n        ),\n        CUDAExtension(\n            name=\"fused_rotary_positional_embedding\",\n            sources=[\n                str(srcpath / \"fused_rotary_positional_embedding.cpp\"),\n                str(srcpath / \"fused_rotary_positional_embedding_cuda.cu\"),\n            ],\n            extra_compile_args=cuda_ext_args,\n        ),\n    ],\n    cmdclass={\"build_ext\": BuildExtension},\n)\n",
        "megatron/gradient_noise_scale/__init__.py": "from .gradient_noise_scale import GradientNoiseScale\n",
        "megatron/gradient_noise_scale/gradient_noise_scale.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n\ndef ema(avg, beta, yi, i):\n    \"\"\"Exponential moving average\"\"\"\n    if avg is None:\n        avg = 0\n    avg = beta * avg + (1 - beta) * yi\n    return avg, avg / (1 - beta ** (i + 1))\n\n\nclass GradientNoiseScale:\n    \"\"\"\n    A class to measure the gradient noise scale of a model while training (cf. https://arxiv.org/abs/1812.06162).\n\n    The core thesis of the paper is that, if our batch size is small, there will be a lot of noise present in the gradients, and we might update our weights only on noise.\n    After several updates the optimizer may still push us in the right direction, but we would be better off having used a larger batch size, which is more computationally\n    efficient and directly averages out the noise in the gradients.\n\n    But there's a limit to the gains large batch sizes can give you - if, after a certain batch size, your gradient is already accurate, there's no point in increasing the\n    batch size further, as we'll just be wasting compute for little to no gain in accuracy.\n\n    This means there is some theoretically optimal batch size for a given model, which measuring the gradient noise scale can help us to estimate.\n\n    To estimate the 'simple' noise scale (Bsimple), we need to have a measure of the gradients using a large batch size (Bbig) and a small\n    batch size (Bsmall).\n\n    when we have those:\n        Bsimple ≈ (tr(Σ) / |G|^2)\n\n    tr(Σ) can be approximated by:\n        tr(Σ) ≈ (1 / ((1/Bsmall) - (1/Bbig))) * (|Gsmall|^2 - |Gbig|^2)\n\n    and |G|^2 by:\n        |G|^2 ≈ (1 / (Bbig - Bsmall)) * (Bbig*|Gbig|^2 - Bsmall*|Gsmall|^2)\n\n    - With multi-gpu training, we can do this by taking the gradients of the microbatch_size_per_gpu for Bsmall,\n    and the gradients of the entire batch for Bbig.\n    - Alternatively, we can just take Bsmall as a single batch, and Bbig as several sequential batches in a row.\n    This is the option we've opted for in this implementation because a) it's easier to implement and b) also works in\n    single-gpu environments. Unfortunately it does come with some memory overhead.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        batch_size_small,\n        n_batches=10,\n        beta=0.99,\n        cpu_offload=False,\n        neox_args=None,\n        mpu=None,\n    ):\n        self.batch_size_small = batch_size_small\n        self.batch_size_large = batch_size_small * n_batches\n        self.n_batches = n_batches\n        self.beta = beta\n        self.model = model\n        self.buffer = None\n        self.ema_scale = None\n        self.ema_noise = None\n        self.noise_scale = None\n        self.n_updates = 0\n        self.cpu_offload = cpu_offload\n        self.model.store_gradients = True\n        self.model.store_gradients_cpu = cpu_offload\n        self.neox_args = neox_args\n        self.mpu = mpu\n\n    def flatten_grads(self):\n        grads = []\n        assert hasattr(\n            self.model, \"stored_gradients\"\n        ), \"You might need to update DeeperSpeed\"\n        if self.model.stored_gradients is not None:\n            for g in self.model.stored_gradients:\n                if g is not None and not g.isnan().any() and not g.isinf().any():\n                    g = g.flatten().view(-1, 1)\n                    if self.cpu_offload:\n                        g = g.cpu()\n                    grads.append(g)\n                else:\n                    return None\n            if not grads:\n                return None\n            return torch.cat(grads)\n\n    def _sync_overflow(self, is_overflow):\n        if self.neox_args.is_pipe_parallel:\n            # Since each model parallel GPU carries only part of the model,\n            # make sure overflow flag is synced across all the pipe parallel GPUs\n            overflow_gpu = torch.cuda.ByteTensor([is_overflow])\n            torch.distributed.all_reduce(\n                overflow_gpu,\n                op=torch.distributed.ReduceOp.MAX,\n                group=self.mpu.get_pipe_parallel_group(),\n            )\n            overflow = overflow_gpu[0].item()\n        else:\n            overflow = is_overflow\n        return overflow\n\n    def _update(self):\n\n        grad = self.flatten_grads()\n        is_overflow = self._sync_overflow(grad is None)\n        if is_overflow:\n            return\n        if self.buffer is None:\n            self.buffer = grad\n        else:\n            self.buffer += grad\n        if self.n_updates % self.n_batches == self.n_batches - 1:\n            # average grads every n_batches iteration to get a simulation of Bbig\n            self.buffer /= self.n_batches\n            grads = self.buffer\n            self.buffer = None\n\n            # calculate Gbig and Gsmall\n            # this needs to be done in fp32 or it overflows\n            if self.neox_args.is_pipe_parallel:\n\n                g_big = torch.square(torch.norm(grads.to(torch.float)))\n                g_small = torch.square(torch.norm(grad.to(torch.float)))\n\n                # we need to put the tensors back on gpu to do the allreduce\n                if self.cpu_offload:\n                    g_big = g_big.to(self.model.device)\n                    g_small = g_small.to(self.model.device)\n\n                # avg g_big / g_small across pipe parallel groups\n                torch.distributed.all_reduce(\n                    g_big,\n                    op=torch.distributed.ReduceOp.SUM,\n                    group=self.mpu.get_pipe_parallel_group(),\n                )\n                torch.distributed.all_reduce(\n                    g_small,\n                    op=torch.distributed.ReduceOp.SUM,\n                    group=self.mpu.get_pipe_parallel_group(),\n                )\n                g_big /= self.mpu.get_pipe_parallel_world_size()\n                g_small /= self.mpu.get_pipe_parallel_world_size()\n\n            else:\n                g_big = torch.square(torch.norm(grads.to(torch.float)))\n                g_small = torch.square(torch.norm(grad.to(torch.float)))\n\n            # communicate any overflows\n            is_overflow = (\n                g_small.isinf().any()\n                or g_small.isnan().any()\n                or g_big.isinf().any()\n                or g_big.isnan().any()\n            )\n            is_overflow = self._sync_overflow(is_overflow)\n            if is_overflow:\n                return\n\n            # calculate noise / scale\n            noise = (\n                1\n                / (self.batch_size_large - self.batch_size_small)\n                * (self.batch_size_large * g_big - self.batch_size_small * g_small)\n            )\n            scale = (\n                1\n                / (1 / self.batch_size_small - 1 / self.batch_size_large)\n                * (g_small - g_big)\n            )\n\n            # calculate running average\n            self.ema_noise, noise = ema(\n                self.ema_noise, self.beta, noise, self.n_updates\n            )\n            self.ema_scale, scale = ema(\n                self.ema_scale, self.beta, scale, self.n_updates\n            )\n\n            # calculate noise scale\n            scale = scale.item()\n            noise = noise.item()\n            self.noise_scale = scale / noise\n\n        self.n_updates += 1\n\n    def update(self):\n        if self.neox_args.is_pipe_parallel:\n            # update on all ranks\n            self._update()\n        else:\n            # for mp / dp only, the grads will be the same across all ranks, so we can just do the process on a single rank\n            if torch.distributed.get_rank() == 0:\n                # only update on 0th rank\n                self._update()\n            torch.distributed.barrier()\n",
        "megatron/initialize.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Megatron initialization.\"\"\"\n\nimport random\nimport os\n\nimport numpy as np\nimport torch\n\nfrom megatron import fused_kernels\nfrom megatron import mpu\nfrom megatron.mpu import set_model_parallel_rank, set_model_parallel_world_size\n\nimport deepspeed\nimport inspect\n\n\ndef initialize_megatron(neox_args, allow_no_cuda=False):\n    \"\"\"Set initialize distributed and set autoresume and random seeds.\n    `allow_no_cuda` should not be set unless using megatron for cpu only\n    data processing. In general this arg should not be set unless you know\n    what you are doing.\n    Returns a function to finalize distributed env initialization\n    (optionally, only when args.lazy_mpu_init == True)\n    \"\"\"\n    if not allow_no_cuda:\n        # Make sure cuda is available.\n        assert torch.cuda.is_available(), \"Megatron requires CUDA.\"\n\n    # torch.distributed initialization\n    def finish_mpu_init():\n        # Pytorch distributed.\n        _initialize_distributed(neox_args=neox_args)\n\n        # Random seeds for reproducibility.\n        if neox_args.rank == 0:\n            print(\"> setting random seeds to {} ...\".format(neox_args.seed))\n        _set_random_seed(neox_args.seed)\n\n    # check fused kernels are installed:\n    if (\n        neox_args.scaled_upper_triang_masked_softmax_fusion\n        or neox_args.scaled_masked_softmax_fusion\n        or neox_args.rope_fusion\n    ):\n        fused_kernels.load(neox_args)\n        fused_kernels.load_fused_kernels()\n\n    if neox_args.lazy_mpu_init:\n        neox_args.use_cpu_initialization = True\n        # delayed initialization of DDP-related stuff\n        # We only set basic DDP globals\n        set_model_parallel_world_size(neox_args.model_parallel_size)\n        # and return function for external DDP manager to call when it has DDP initialized\n        set_model_parallel_rank(neox_args.rank)\n        return finish_mpu_init\n    else:\n        # Megatron's MPU is the master. Complete initialization right away.\n        finish_mpu_init()\n\n        # Compile dataset C++ code.\n        if neox_args.local_rank == 0:\n            from megatron.data.data_utils import compile_helper\n\n            compile_helper()\n\n        # Write arguments to tensorboard.\n        _write_args_to_tensorboard(neox_args=neox_args)\n        # No continuation function\n        return None\n\n\ndef setup_deepspeed_random_and_activation_checkpointing(neox_args):\n    \"\"\"Optional DeepSpeed Activation Checkpointing features.\n    Gives access to partition activations, contiguous memory optimizations\n    and cpu checkpointing.\n\n    Activation checkpoint requires keep track of the random states\n    and setting the random seed for each MP process. Megatron uses\n    mpu.get_cuda_rng_tracker and mpu.model_parallel_cuda_manual_seed\n    for keeping track of the random states and setting the random seeds.\n    Since they are used in places outside of activation checkpointing,\n    we overwrite them to maintain consistency.\n\n    This must be called before all the calls to mpu.model_parallel_cuda_manual_seed\n    \"\"\"\n    num_layers = neox_args.num_layers // neox_args.checkpoint_num_layers\n    num_layers = (\n        num_layers\n        if neox_args.num_layers % neox_args.checkpoint_num_layers == 0\n        else num_layers + 1\n    )\n\n    deepspeed.checkpointing.configure(\n        mpu,\n        partition_activations=neox_args.partition_activations,\n        contiguous_checkpointing=neox_args.contiguous_checkpointing,\n        num_checkpoints=num_layers,\n        checkpoint_in_cpu=neox_args.checkpoint_in_cpu,\n        synchronize=neox_args.synchronize_each_layer,\n        profile=neox_args.profile_backward,\n    )\n\n\ndef _initialize_distributed(neox_args):\n    \"\"\"Initialize torch.distributed and mpu.\"\"\"\n\n    device_count = torch.cuda.device_count()\n    if torch.distributed.is_initialized():\n\n        if neox_args.rank == 0:\n            print(\n                \"torch distributed is already initialized, \"\n                \"skipping initialization ...\",\n                flush=True,\n            )\n        neox_args.rank = torch.distributed.get_rank()\n        neox_args.world_size = torch.distributed.get_world_size()\n\n    else:\n\n        if neox_args.rank == 0:\n            print(\"> initializing torch distributed ...\", flush=True)\n        # Manually set the device ids.\n        if device_count > 0:\n            device = neox_args.rank % device_count\n            if neox_args.local_rank is not None:\n                assert (\n                    neox_args.local_rank == device\n                ), \"expected local-rank to be the same as rank % device-count.\"\n            else:\n                neox_args.local_rank = device\n            torch.cuda.set_device(device)\n\n        deepspeed.init_distributed(\n            dist_backend=neox_args.distributed_backend,\n            auto_mpi_discovery=True,\n            distributed_port=os.getenv(\"MASTER_PORT\", \"6000\"),\n            verbose=True,\n        )\n\n    # Setup 3D topology.\n    pp = neox_args.pipe_parallel_size if neox_args.pipe_parallel_size >= 1 else 1\n    mp = neox_args.model_parallel_size if neox_args.model_parallel_size >= 1 else 1\n    assert (\n        neox_args.world_size % (pp * mp) == 0\n    ), f\"world_size={neox_args.world_size}, pp={pp}, mp={mp}\"\n    dp = neox_args.world_size // (pp * mp)\n\n    from deepspeed.runtime.pipe.topology import PipeModelDataParallelTopology\n\n    # this does pipe on the most outside, then data, then model.\n    # PipeModelDataParallelTopology is just a wrapper over ProcessTopology that predefines this order.\n    topo = PipeModelDataParallelTopology(num_pp=pp, num_mp=mp, num_dp=dp)\n\n    # Offset base seeds for the interior pipeline stages.\n    # TODO: adjust last stage too once IO is improved.\n    stage_id = topo.get_coord(rank=torch.distributed.get_rank()).pipe\n    if 0 < stage_id < topo.get_dim(\"pipe\") - 1:\n        offset = neox_args.seed + 1138\n        neox_args.seed = offset + (stage_id * mp)\n\n    # Set the model-parallel / data-parallel communicators.\n    if device_count > 0:\n        if mpu.model_parallel_is_initialized():\n            print(\n                \"_initialize_distributed() model parallel is already initialized\",\n                flush=True,\n            )\n        else:\n            mpu.initialize_model_parallel(\n                neox_args.model_parallel_size,\n                topology=topo,\n                fp32_allreduce=neox_args.fp32_allreduce,\n            )\n\n    # Init DeepSpeed Activation Checkpointing Features\n    setup_deepspeed_random_and_activation_checkpointing(neox_args=neox_args)\n\n\ndef _init_autoresume(neox_args):\n    \"\"\"Set autoresume start time.\"\"\"\n\n    if neox_args.adlr_autoresume:\n        print_rank_0(\"> enabling autoresume ...\")\n        sys.path.append(os.environ.get(\"SUBMIT_SCRIPTS\", \".\"))\n        try:\n            from userlib.auto_resume import AutoResume\n        except BaseException:\n            print(\"> ADLR autoresume is not available, exiting ...\", flush=True)\n            sys.exit()\n        neox_args.adlr_autoresume_object = AutoResume\n\n    if neox_args.adlr_autoresume_object:\n        torch.distributed.barrier()\n        neox_args.adlr_autoresume_object.init()\n        torch.distributed.barrier()\n\n\ndef _set_random_seed(seed):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    if seed is not None and seed > 0:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.device_count() > 0:\n            mpu.model_parallel_cuda_manual_seed(seed)\n    else:\n        raise ValueError(\"Seed ({}) should be a positive integer.\".format(seed))\n\n\ndef _write_args_to_tensorboard(neox_args):\n\n    \"\"\"Write arguments to tensorboard.\"\"\"\n    if neox_args.tensorboard_writer:\n        for arg_name in vars(neox_args):\n            neox_args.tensorboard_writer.add_text(\n                arg_name, str(getattr(neox_args, arg_name))\n            )\n",
        "megatron/learning_rates.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Learning rate decay functions.\"\"\"\n\nimport math\n\nfrom megatron import print_rank_0\n\n\nclass AnnealingLR(object):\n    \"\"\"Anneals the learning rate.\"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        start_lr,\n        warmup_iter,\n        total_iters,\n        decay_style,\n        last_iter,\n        min_lr=0.0,\n        use_checkpoint_lr_scheduler=True,\n        override_lr_scheduler=False,\n        use_mup=False,\n    ):\n\n        # Class values.\n        self.optimizer = optimizer\n        self.start_lr = start_lr\n        self.min_lr = min_lr\n        self.warmup_iter = warmup_iter\n        self.num_iters = last_iter\n        self.end_iter = total_iters\n        assert self.end_iter > 0\n        self.decay_style = decay_style\n        self.override_lr_scheduler = override_lr_scheduler\n        self.use_checkpoint_lr_scheduler = use_checkpoint_lr_scheduler\n        self.use_mup = use_mup\n        if self.override_lr_scheduler:\n            assert not self.use_checkpoint_lr_scheduler, (\n                \"both override and \" \"use-checkpoint are set.\"\n            )\n        # Set the learning rate\n        self.step(self.num_iters)\n\n        print_rank_0(\"> learning rate decay style: {}\".format(self.decay_style))\n\n    def get_lr(self):\n        \"\"\"Learning rate decay functions from:\n        https://openreview.net/pdf?id=BJYwwY9ll pg. 4\"\"\"\n\n        num_iters_ = self.num_iters\n        # Warmup.\n        if self.warmup_iter > 0 and self.num_iters <= self.warmup_iter:\n            return float(self.start_lr) * num_iters_ / self.warmup_iter\n\n        num_iters_ = num_iters_ - self.warmup_iter\n        if self.decay_style == \"linear\":\n            end_iter_ = self.end_iter - self.warmup_iter\n            lr = self.start_lr * (end_iter_ - num_iters_) / end_iter_\n        elif self.decay_style == \"cosine\":\n            end_iter_ = self.end_iter - self.warmup_iter\n            lr = self.min_lr + (\n                (self.start_lr - self.min_lr)\n                / 2.0\n                * (math.cos(math.pi * num_iters_ / end_iter_) + 1)\n            )\n        elif self.decay_style == \"exponential\":\n            # exp(-0.693) = 1/2\n            end_iter = self.end_iter - self.warmup_iter\n            lr = self.start_lr * math.exp(-0.693 * num_iters_ / end_iter)\n        else:\n            lr = self.start_lr\n        return max(lr, self.min_lr)\n\n    def step(self, step_num=None):\n        \"\"\"Set lr for all parameters groups.\"\"\"\n        if step_num is None:\n            step_num = self.num_iters + 1\n        self.num_iters = step_num\n        new_lr = self.get_lr()\n        for group in self.optimizer.param_groups:\n            if self.use_mup and \"width_mult\" in group:\n                group[\"lr\"] = new_lr / group[\"width_mult\"]\n            else:\n                group[\"lr\"] = new_lr\n\n    def state_dict(self):\n        state_dict = {\n            \"start_lr\": self.start_lr,\n            \"warmup_iter\": self.warmup_iter,\n            \"num_iters\": self.num_iters,\n            \"decay_style\": self.decay_style,\n            \"end_iter\": self.end_iter,\n            \"min_lr\": self.min_lr,\n        }\n        return state_dict\n\n    def _check_and_set(self, cls_value, sd_value, name):\n        \"\"\"Auxiliary function for checking the values in the checkpoint and\n        setting them.\"\"\"\n        if self.override_lr_scheduler:\n            print_rank_0(\" > overriding {} value to {}\".format(name, cls_value))\n            return cls_value\n\n        if not self.use_checkpoint_lr_scheduler:\n            assert cls_value == sd_value, (\n                \"AnnealingLR: class input value\"\n                \"and checkpoint values for {} do not match\".format(name)\n            )\n        print_rank_0(\" > using checkpoint value {} for {}\".format(sd_value, name))\n        return sd_value\n\n    def load_state_dict(self, sd):\n\n        self.start_lr = self._check_and_set(\n            self.start_lr, sd[\"start_lr\"], \"learning rate\"\n        )\n        self.min_lr = self._check_and_set(\n            self.min_lr, sd[\"min_lr\"], \"minimum learning rate\"\n        )\n        self.warmup_iter = self._check_and_set(\n            self.warmup_iter, sd[\"warmup_iter\"], \"warmup iterations\"\n        )\n        self.end_iter = self._check_and_set(\n            self.end_iter, sd[\"end_iter\"], \"total number of iterations\"\n        )\n        self.decay_style = self._check_and_set(\n            self.decay_style, sd[\"decay_style\"], \"decay style\"\n        )\n\n        self.num_iters = sd[\"num_iters\"]\n        self.step(self.num_iters)\n",
        "megatron/logging.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\nimport torch\n\ntry:\n    import wandb\nexcept ModuleNotFoundError:\n    pass\n\nfrom megatron import mpu, print_rank_0\nfrom megatron.utils import report_memory\nimport math\n\n\nclass Tee:\n    \"\"\"Duplicate output to both stdout/err and file\"\"\"\n\n    def __init__(self, file, err: bool = False) -> None:\n        self.file = open(file, \"w\")\n        self.err = err\n        if not err:\n            self.std = sys.stdout\n            sys.stdout = self\n        else:\n            self.std = sys.stderr\n            sys.stderr = self\n\n    def __del__(self) -> None:\n        if not self.err:\n            sys.stdout = self.std\n        else:\n            sys.stderr = self.std\n        self.file.close()\n\n    def write(self, data) -> None:\n        try:\n            self.file.write(data)\n        except OSError:\n            pass\n        try:\n            self.std.write(data)\n        except OSError:\n            pass\n\n    def flush(self) -> None:\n        try:\n            self.file.flush()\n        except OSError:\n            pass\n\n\ndef human_readable_flops(num) -> str:\n    for unit in [\n        \"\",\n        \"KFLOPS\",\n        \"MFLOPS\",\n        \"GFLOPS\",\n        \"TFLOPS\",\n        \"PFLOPS\",\n        \"EFLOPS\",\n        \"ZFLOPS\",\n    ]:\n        if abs(num) < 1000.0:\n            return \"%3.1f%s\" % (num, unit)\n        num /= 1000.0\n    return \"%.1f%s\" % (num, \"Yi\")\n\n\ndef get_actual_flops(neox_args, iter_time_s) -> float:\n    \"\"\"\n    This function finds the actual FLOPs achieved accounting for implementation and hardware details. Also used for HFU.\n\n    For more detail on flop calculations, see https://github.com/EleutherAI/cookbook/tree/main/calc and https://github.com/Zyphra/zcookbook/tree/main/calc\n\n    Use FLOPS calculation from Megatron-DeepSpeed:\n    https://github.com/microsoft/Megatron-DeepSpeed/blob/cc3a94c636789f74be2bc6cfc62a3d723fd5d749/megatron/utils.py#L253\n    They get it from https://arxiv.org/pdf/2104.04473.pdf\n    \"\"\"\n    world_size = torch.distributed.get_world_size()\n    vocab_size = neox_args.padded_vocab_size\n    batch_size = neox_args.train_batch_size\n    seq_len = neox_args.seq_length\n    hidden_size = neox_args.hidden_size\n    num_layers = neox_args.num_layers\n    ckpt_activations_factor = 4 if neox_args.checkpoint_activations else 3\n    if \"rwkv\" in neox_args.attention_config:\n        num_heads = neox_args.num_attention_heads\n\n        flops_per_iteration = (\n            batch_size\n            * seq_len\n            * (\n                78 * hidden_size * hidden_size * num_layers\n                + 84 * hidden_size * num_layers\n                + 16 * hidden_size\n                + 12 * hidden_size * vocab_size\n                + 18 * hidden_size * hidden_size * num_layers / num_heads\n            )\n        )\n    elif \"mamba\" in neox_args.attention_config:\n        # from https://github.com/Zyphra/zcookbook/blob/main/calc/calc_mamba_flops.py\n        if neox_args.expansion_factor:\n            d_inner = neox_args.hidden_size * neox_args.expansion_factor\n        elif neox_args.intermediate_size:\n            d_inner = neox_args.intermediate_size\n        else:\n            d_inner = neox_args.hidden_size * 2  # default expansion factor\n        d_state = 16  # TODO make d_state an arg. Currently hardcoded in neox mamba definition and here\n        conv_dimension = 4  # TODO make conv_dimension an arg. Currently hardcoded in neox mamba definition and here\n        dt_rank = math.ceil(neox_args.hidden_size / 16)\n        ssm_flops = (\n            ckpt_activations_factor\n            * d_inner\n            * seq_len\n            * batch_size\n            * (11 * d_state + 4 * dt_rank + 1)\n        )\n        mamba_projectors_flops = (\n            ckpt_activations_factor * seq_len * batch_size * 6 * d_inner * hidden_size\n        )\n        mamba_conv_flops = (\n            ckpt_activations_factor\n            * seq_len\n            * batch_size\n            * 2\n            * d_inner\n            * conv_dimension\n        )\n        mamba_flops = ssm_flops + mamba_projectors_flops + mamba_conv_flops\n        embedding_flops = 6 * seq_len * batch_size * hidden_size * vocab_size\n        flops_per_iteration = mamba_flops * num_layers + embedding_flops\n    else:\n        flops_per_iteration = (\n            24\n            * ckpt_activations_factor\n            * batch_size\n            * seq_len\n            * num_layers\n            * (hidden_size**2)\n            * (\n                1.0\n                + (seq_len / (6.0 * hidden_size))\n                + (vocab_size / (16.0 * num_layers * hidden_size))\n            )\n        )\n    return flops_per_iteration / (iter_time_s * world_size)\n\n\ndef get_forward_backward_flops(neox_args, iter_time_s) -> float:\n    \"\"\"\n    This function finds the estimated FLOPs required by a single forward+backward pass without accounting for implementation and hardware details. Also used for MFU.\n\n    Mostly duplicated from get_actual_flops with just a change in activation checkpointing for now, but these may diverge over time as implementation details accumulate so I think 2 separate functions are appropriate.\n\n    For more detail on flop calculations, see https://github.com/EleutherAI/cookbook/tree/main/calc and https://github.com/Zyphra/zcookbook/tree/main/calc\n\n    Use FLOPS calculation from Megatron-DeepSpeed:\n    https://github.com/microsoft/Megatron-DeepSpeed/blob/cc3a94c636789f74be2bc6cfc62a3d723fd5d749/megatron/utils.py#L253\n    They get it from https://arxiv.org/pdf/2104.04473.pdf\n    \"\"\"\n    world_size = torch.distributed.get_world_size()\n    vocab_size = neox_args.padded_vocab_size\n    batch_size = neox_args.train_batch_size\n    seq_len = neox_args.seq_length\n    hidden_size = neox_args.hidden_size\n    num_layers = neox_args.num_layers\n    fwd_bwd_factor = 3  # 1 for fwd, 2 for bwd and weight update\n    if \"rwkv\" in neox_args.attention_config:\n        num_heads = neox_args.num_attention_heads\n\n        flops_per_iteration = (\n            batch_size\n            * seq_len\n            * (\n                78 * hidden_size * hidden_size * num_layers\n                + 84 * hidden_size * num_layers\n                + 16 * hidden_size\n                + 12 * hidden_size * vocab_size\n                + 18 * hidden_size * hidden_size * num_layers / num_heads\n            )\n        )\n    elif \"mamba\" in neox_args.attention_config:\n        # from https://github.com/Zyphra/zcookbook/blob/main/calc/calc_mamba_flops.py\n        if neox_args.expansion_factor:\n            d_inner = neox_args.hidden_size * neox_args.expansion_factor\n        elif neox_args.intermediate_size:\n            d_inner = neox_args.intermediate_size\n        else:\n            d_inner = neox_args.hidden_size * 2  # default expansion factor\n        d_state = 16  # TODO make d_state an arg. Currently hardcoded in neox mamba definition and here\n        conv_dimension = 4  # TODO make conv_dimension an arg. Currently hardcoded in neox mamba definition and here\n        dt_rank = math.ceil(neox_args.hidden_size / 16)\n        ssm_flops = (\n            fwd_bwd_factor\n            * d_inner\n            * seq_len\n            * batch_size\n            * (11 * d_state + 4 * dt_rank + 1)\n        )\n        mamba_projectors_flops = (\n            fwd_bwd_factor * seq_len * batch_size * 6 * d_inner * hidden_size\n        )\n        mamba_conv_flops = (\n            fwd_bwd_factor * seq_len * batch_size * 2 * d_inner * conv_dimension\n        )\n        mamba_flops = ssm_flops + mamba_projectors_flops + mamba_conv_flops\n        embedding_flops = 6 * seq_len * batch_size * hidden_size * vocab_size\n        flops_per_iteration = mamba_flops * num_layers + embedding_flops\n    else:\n        flops_per_iteration = (\n            24\n            * fwd_bwd_factor\n            * batch_size\n            * seq_len\n            * num_layers\n            * (hidden_size**2)\n            * (\n                1.0\n                + (seq_len / (6.0 * hidden_size))\n                + (vocab_size / (16.0 * num_layers * hidden_size))\n            )\n        )\n    return flops_per_iteration / (iter_time_s * world_size)\n\n\ndef training_log(\n    neox_args,\n    timers,\n    loss_dict,\n    total_loss_dict,\n    learning_rate,\n    iteration,\n    loss_scale,\n    report_memory_flag,\n    skipped_iter,\n    model,\n    optimizer,\n    noise_scale_logger,\n):\n    \"\"\"Log training information such as losses, timing, etc.\"\"\"\n\n    # Update losses.\n    skipped_iters_key = \"skipped iterations\"\n    total_loss_dict[skipped_iters_key] = (\n        total_loss_dict.get(skipped_iters_key, 0) + skipped_iter\n    )\n    got_nan_key = \"got nan\"\n\n    got_nan = False\n    for key in loss_dict:\n        if not skipped_iter:\n            total_loss_dict[key] = total_loss_dict.get(key, 0.0) + loss_dict[key]\n        else:\n            value = loss_dict[key].float().sum().item()\n            is_nan = value == float(\"inf\") or value == -float(\"inf\") or value != value\n            got_nan = got_nan or is_nan\n\n    total_loss_dict[got_nan_key] = total_loss_dict.get(got_nan_key, 0) + int(got_nan)\n\n    # Logging.\n    timers_to_log = []\n\n    def add_to_logging(name):\n        if name in timers.timers:\n            timers_to_log.append(name)\n\n    if not neox_args.is_pipe_parallel:\n        add_to_logging(\"forward\")\n        add_to_logging(\"backward\")\n        add_to_logging(\"backward-backward\")\n        add_to_logging(\"backward-allreduce\")\n        add_to_logging(\"backward-master-grad\")\n        add_to_logging(\"backward-clip-grad\")\n        add_to_logging(\"optimizer\")\n        add_to_logging(\"batch generator\")\n\n        # Log timer info to tensorboard and wandb\n        normalizer = iteration % neox_args.log_interval\n        if normalizer == 0:\n            normalizer = neox_args.log_interval\n        if torch.distributed.get_rank() == 0:\n            timers.write(\n                names=timers_to_log, iteration=iteration, normalizer=normalizer\n            )\n    else:\n        # with pipeline parallel, the megatron timers are overridden by the deepspeed ones.\n        # Try to grab timer values from model engine. Only recently added to deeperspeed, so check that the engine\n        # has that attribute first\n        if hasattr(model, \"timer_values\") and model.timer_values is not None:\n            if (\n                model.wall_clock_breakdown()\n                and model.global_steps % model.steps_per_print() == 0\n            ):\n                timer_values = model.timer_values\n                # deepspeed already logs to tensorboard / prints values, so just log to wandb\n                if neox_args.use_wandb and torch.distributed.get_rank() == 0:\n                    for key in timer_values:\n                        tb_wandb_log(\n                            f\"timers/{key}\",\n                            timer_values[key],\n                            iteration,\n                            use_wandb=neox_args.use_wandb,\n                            tensorboard_writer=neox_args.tensorboard_writer,\n                            comet_experiment=neox_args.comet_experiment,\n                        )\n\n    # write losses, lr, etc. every step\n    tb_wandb_log(\n        \"train/learning_rate\",\n        learning_rate,\n        iteration,\n        use_wandb=neox_args.use_wandb,\n        tensorboard_writer=neox_args.tensorboard_writer,\n        comet_experiment=neox_args.comet_experiment,\n    )\n    for key in loss_dict:\n        tb_wandb_log(\n            f'train/{key.replace(\" \", \"_\")}',\n            loss_dict[key],\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n    if neox_args.fp16:\n        tb_wandb_log(\n            f\"train/loss_scale\",\n            loss_scale,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n\n    # log gradient noise scale\n    if neox_args.log_gradient_noise_scale:\n        if noise_scale_logger.noise_scale is not None:\n            tb_wandb_log(\n                f\"train/noise_scale\",\n                noise_scale_logger.noise_scale,\n                iteration,\n                use_wandb=neox_args.use_wandb,\n                tensorboard_writer=neox_args.tensorboard_writer,\n                comet_experiment=neox_args.comet_experiment,\n            )\n\n    # (optional) Log optimizer states to wandb / tb every step\n    if neox_args.log_optimizer_states:\n        for k, v in optimizer.state_dict()[\"optimizer_state_dict\"][\"state\"].items():\n            for ki, vi in v.items():  # step, module\n                if ki != \"step\":\n                    opt_state_norm = torch.norm(vi) if hasattr(vi, \"dim\") else vi\n                    tb_wandb_log(\n                        f\"optimizer_state_norms/{k}_{ki}\",\n                        opt_state_norm,\n                        iteration,\n                        use_wandb=neox_args.use_wandb,\n                        tensorboard_writer=neox_args.tensorboard_writer,\n                        comet_experiment=neox_args.comet_experiment,\n                    )\n\n    # (optional) Log grad/param norms to wandb / tb every step\n    if (\n        neox_args.log_grad_pct_zeros\n        or neox_args.log_grad_norm\n        or neox_args.log_param_norm\n    ):\n        if neox_args.log_grad_pct_zeros or neox_args.log_grad_norm:\n            model.store_gradients = True  # start storing gradients\n\n        for i, (name, param) in enumerate(model.module.named_parameters()):\n            if neox_args.log_grad_pct_zeros:\n                if (\n                    hasattr(model, \"stored_gradients\")\n                    and model.stored_gradients is not None\n                ):\n                    grad = model.stored_gradients[i]\n                    if grad is not None:\n                        tb_wandb_log(\n                            f\"pct_grad_zeros/{name}\",\n                            (grad == 0).float().mean().item() * 100,\n                            iteration,\n                            use_wandb=neox_args.use_wandb,\n                            tensorboard_writer=neox_args.tensorboard_writer,\n                            comet_experiment=neox_args.comet_experiment,\n                            all_ranks=True,\n                        )\n            if neox_args.log_grad_norm:\n                if (\n                    hasattr(model, \"stored_gradients\")\n                    and model.stored_gradients is not None\n                ):\n                    grad = model.stored_gradients[i]\n                    if grad is not None:\n                        tb_wandb_log(\n                            f\"gradient_norms/{name}\",\n                            torch.norm(grad),\n                            iteration,\n                            use_wandb=neox_args.use_wandb,\n                            tensorboard_writer=neox_args.tensorboard_writer,\n                            comet_experiment=neox_args.comet_experiment,\n                            all_ranks=True,\n                        )\n            if neox_args.log_param_norm:\n                tb_wandb_log(\n                    f\"parameter_norms/{name}\",\n                    torch.norm(param),\n                    iteration,\n                    use_wandb=neox_args.use_wandb,\n                    tensorboard_writer=neox_args.tensorboard_writer,\n                    comet_experiment=neox_args.comet_experiment,\n                    all_ranks=True,\n                )\n\n    if iteration % neox_args.log_interval == 0:\n        # log other stuff every neox_args.log_interval iters\n        elapsed_time = timers(\"interval time\").elapsed()\n        iteration_time = elapsed_time / neox_args.log_interval\n        samples_per_sec = neox_args.train_batch_size / iteration_time\n        steps_per_sec = 1 / iteration_time\n        tokens_per_sec = samples_per_sec * neox_args.seq_length\n        log_string = \" samples/sec: {:.3f} |\".format(samples_per_sec)\n        tb_wandb_log(\n            \"runtime/samples_per_sec\",\n            samples_per_sec,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n        tb_wandb_log(\n            \"runtime/iteration_time\",\n            iteration_time,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n        tb_wandb_log(\n            \"runtime/steps_per_sec\",\n            steps_per_sec,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n        tb_wandb_log(\n            \"runtime/tokens_per_sec\",\n            tokens_per_sec,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n        log_string += \" iteration {:8d}/{:8d} |\".format(\n            iteration, neox_args.train_iters\n        )\n        log_string += \" elapsed time per iteration (ms): {:.1f} |\".format(\n            elapsed_time * 1000.0 / neox_args.log_interval\n        )\n        log_string += \" learning rate: {:.3E} |\".format(learning_rate)\n        num_iterations = max(\n            1, neox_args.log_interval - total_loss_dict[skipped_iters_key]\n        )\n\n        # log curriculum learning\n        if neox_args.curriculum_learning:\n            tb_wandb_log(\n                \"curriculum_seqlen\",\n                neox_args.curriculum_seqlen,\n                iteration,\n                use_wandb=neox_args.use_wandb,\n                tensorboard_writer=neox_args.tensorboard_writer,\n                comet_experiment=neox_args.comet_experiment,\n            )\n\n        # log tflop / gpu\n        flops_per_s_per_gpu = get_actual_flops(neox_args, iteration_time)\n\n        log_string += (\n            f\" approx flops per GPU: {human_readable_flops(flops_per_s_per_gpu)} |\"\n        )\n        tb_wandb_log(\n            \"runtime/flops_per_sec_per_gpu\",\n            flops_per_s_per_gpu,\n            iteration,\n            use_wandb=neox_args.use_wandb,\n            tensorboard_writer=neox_args.tensorboard_writer,\n            comet_experiment=neox_args.comet_experiment,\n        )\n\n        if neox_args.peak_theoretical_tflops:\n            # Convert peak theoretical TFLOPS to FLOPS for consistent units\n            peak_theoretical_flops = neox_args.peak_theoretical_tflops * (10**12)\n\n            # Calculate MFU and HFU as percentages\n            mfu = (\n                get_forward_backward_flops(neox_args, iteration_time)\n                / peak_theoretical_flops\n            ) * 100\n            hfu = (flops_per_s_per_gpu / peak_theoretical_flops) * 100\n\n            # Add to log string\n            log_string += f\" MFU: {mfu:.2f}% | HFU: {hfu:.2f}% |\"\n\n            # Log to tracking systems\n            tb_wandb_log(\n                \"runtime/model_flops_utilization\",\n                mfu,\n                iteration,\n                use_wandb=neox_args.use_wandb,\n                tensorboard_writer=neox_args.tensorboard_writer,\n                comet_experiment=neox_args.comet_experiment,\n            )\n\n            tb_wandb_log(\n                \"runtime/hardware_flops_utilization\",\n                hfu,\n                iteration,\n                use_wandb=neox_args.use_wandb,\n                tensorboard_writer=neox_args.tensorboard_writer,\n                comet_experiment=neox_args.comet_experiment,\n            )\n\n        for key in total_loss_dict:\n            if key not in [skipped_iters_key, got_nan_key]:\n                v = (\n                    total_loss_dict[key].item()\n                    if hasattr(total_loss_dict[key], \"item\")\n                    else total_loss_dict[key]\n                )\n                avg = v / float(num_iterations)\n                log_string += \" {}: {:.6E} |\".format(key, avg)\n                total_loss_dict[key] = 0.0\n        if neox_args.precision == \"fp16\":\n            log_string += \" loss scale: {:.1f} |\".format(loss_scale)\n        log_string += \" number of skipped iterations: {:3d} |\".format(\n            total_loss_dict[skipped_iters_key]\n        )\n        log_string += \" number of nan iterations: {:3d} |\".format(\n            total_loss_dict[got_nan_key]\n        )\n        total_loss_dict[skipped_iters_key] = 0\n        total_loss_dict[got_nan_key] = 0\n        print_rank_0(log_string)\n        if report_memory_flag:\n            report_memory(\"after {} iterations\".format(iteration))\n            report_memory_flag = False\n\n        timers.log(timers_to_log, normalizer=neox_args.log_interval)\n\n    return report_memory_flag\n\n\ndef tb_wandb_log(\n    key: str,\n    value: float,\n    iteration_no: int,\n    use_wandb: bool,\n    tensorboard_writer=None,\n    comet_experiment=None,\n    all_ranks: bool = False,\n):\n    # logs to both tb and wandb (if present) from the zeroth rank\n    do_log = torch.distributed.get_rank() == 0 or all_ranks\n    if do_log and value is not None:\n        if tensorboard_writer:\n            tensorboard_writer.add_scalar(key, value, iteration_no)\n        if use_wandb:\n            wandb.log({key: value}, step=iteration_no)\n        if comet_experiment:\n            comet_experiment.__internal_api__log_metric__(\n                key, value, framework=\"gpt-neox\", step=iteration_no\n            )\n",
        "megatron/model/__init__.py": "# Copyright (c) 2025, EleutherAI\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .gpt2_model import GPT2ModelPipe\nfrom .utils import (\n    get_params_for_weight_decay_optimization,\n    mark_norms_for_sequence_parallel_grad_sync,\n)\nfrom .word_embeddings import SoftEmbedding\n",
        "megatron/model/activations.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn.functional as F\n\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_override_can_fuse_on_cpu(True)\ntorch._C._jit_override_can_fuse_on_gpu(True)\n\n\ndef get_activation(neox_args):\n    \"\"\"retrieves the activation function specified in neox_args and whether or not the activation is gated\"\"\"\n    is_gated = False\n    if neox_args.activation == \"geglu\":\n        is_gated = True\n        activation_func = F.gelu\n    elif neox_args.activation == \"reglu\":\n        is_gated = True\n        activation_func = F.relu\n    elif neox_args.activation == \"bilinear\":\n        is_gated = True\n        activation_func = lambda x: x\n    elif neox_args.activation == \"swiglu\":\n        is_gated = True\n        activation_func = swish\n    elif neox_args.activation == \"glu\":\n        is_gated = True\n        activation_func = F.sigmoid\n    elif neox_args.activation == \"gelu\":\n        if neox_args.onnx_safe and neox_args.bias_gelu_fusion:\n            raise ValueError(\"onnx_safe + bias_gelu_fusion not compatible\")\n        if neox_args.onnx_safe:\n            activation_func = erf_gelu\n        elif neox_args.bias_gelu_fusion:\n            activation_func = bias_gelu_impl\n        else:\n            activation_func = F.gelu\n    elif neox_args.activation == \"relu\":\n        activation_func = F.relu\n    elif neox_args.activation == \"softsign\":\n        activation_func = F.softsign\n    elif neox_args.activation == \"swish\":\n        activation_func = swish\n    elif neox_args.activation == \"mish\":\n        activation_func = mish\n    elif neox_args.activation == \"silu\":\n        activation_func = F.silu\n    else:\n        raise ValueError(f\"Activation function {neox_args.activation} not recognized\")\n    return activation_func, is_gated\n\n\n###### BIAS GELU FUSION/ NO AUTOGRAD ################\n# 1/sqrt(2*pi)-> 0.3989423\n# 1/sqrt(2)   -> 0.70710678\n# sqrt(2/pi)  -> 0.79788456\n# this function is tanh approximation of gelu\n# actual gelu is:\n# x * 0.5 * (1.0 + torch.erf(x * 0.70710678))\n\n\n@torch.jit.script\ndef bias_gelu(bias, y):\n    x = bias + y\n    return x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))\n\n\n# gradient of tanh approximation of gelu\n# gradient of actual gelu is:\n# 0.5 * (1. + torch.erf(x * 0.70710678)) + 0.3989423 * x * torch.exp(-0.5 * x * x)\n@torch.jit.script\ndef bias_gelu_back(g, bias, y):\n    x = bias + y\n    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243\n    ff = 0.5 * x * (\n        (1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)\n    ) + 0.5 * (1 + tanh_out)\n    return ff * g\n\n\nclass GeLUFunction(torch.autograd.Function):\n    @staticmethod\n    # bias is an optional argument\n    def forward(ctx, input, bias):\n        ctx.save_for_backward(input, bias)\n        return bias_gelu(bias, input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, bias = ctx.saved_tensors\n        tmp = bias_gelu_back(grad_output, bias, input)\n        return tmp, tmp\n\n\nbias_gelu_impl = GeLUFunction.apply\n\n\n# This is actually Python equivalent of torch.nn.functional.gelu(), also with type hints for ONNX exporter\n@torch.jit.script\ndef erf_gelu(x):\n    return (\n        x\n        * 0.5\n        * (\n            torch.erf(x / 1.41421).to(dtype=x.dtype)\n            + torch.ones_like(x).to(dtype=x.dtype)\n        )\n    )\n\n\n@torch.jit.script\ndef swish(x, beta: float = 1.0):\n    return x * torch.sigmoid(beta * x)\n\n\n@torch.jit.script\ndef mish(x):\n    return x * torch.tanh(F.softplus(x))\n",
        "megatron/model/fused_bias_dropout.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom torch import Tensor\n\n# flags required to enable jit fusion kernels\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_override_can_fuse_on_cpu(True)\ntorch._C._jit_override_can_fuse_on_gpu(True)\n\n\ndef bias_dropout_add(\n    x: Tensor, bias: Tensor, residual: Optional[Tensor], prob: float, training: bool\n) -> Tensor:\n    out = torch.nn.functional.dropout(x + bias, p=prob, training=training)\n    if residual is not None:\n        out = residual + out\n    return out\n\n\ndef get_bias_dropout_add(training):\n    def _bias_dropout_add(x, bias, residual, prob):\n        return bias_dropout_add(x, bias, residual, prob, training)\n\n    return _bias_dropout_add\n\n\n@torch.jit.script\ndef bias_dropout_add_fused_train(\n    x: Tensor, bias: Tensor, residual: Optional[Tensor], prob: float\n) -> Tensor:\n    return bias_dropout_add(x, bias, residual, prob, True)\n\n\n@torch.jit.script\ndef bias_dropout_add_fused_inference(\n    x: Tensor, bias: Tensor, residual: Optional[Tensor], prob: float\n) -> Tensor:\n    return bias_dropout_add(x, bias, residual, prob, False)\n",
        "megatron/model/fused_layer_norm.py": "# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\n\"\"\"This code is copied from NVIDIA apex:\n      https://github.com/NVIDIA/apex\n   with some changes. \"\"\"\n\nimport numbers\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import init\nimport importlib\nfrom torch.nn import functional as F\nimport inspect\n\nfrom megatron.utils import make_viewless_tensor\n\ntry:\n    from apex.contrib.layer_norm.layer_norm import FastLayerNormFN\n\n    HAVE_PERSIST_LAYER_NORM = True\nexcept:\n    HAVE_PERSIST_LAYER_NORM = False\n\nfrom apex.normalization.fused_layer_norm import (\n    FusedLayerNormAffineFunction,\n    FusedRMSNormAffineFunction,\n)\n\n\nglobal fused_layer_norm_cuda\nfused_layer_norm_cuda = None\n\n\nclass MixedFusedLayerNorm(torch.nn.Module):\n    def __init__(\n        self,\n        normalized_shape,\n        eps=1e-5,\n        no_persist_layer_norm=True,\n        sequence_parallel=False,\n        apply_layernorm_1p=False,\n        mem_efficient_ln=True,\n    ):\n        super(MixedFusedLayerNorm, self).__init__()\n\n        self.apply_layernorm_1p = apply_layernorm_1p\n        self.mem_efficient_ln = mem_efficient_ln\n\n        global fused_layer_norm_cuda\n        fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\n\n        # List of hiddens sizes supported in the persistent layer norm kernel\n        # If the hidden size is not supported, fall back to the non-persistent\n        # kernel.\n        persist_ln_hidden_sizes = [\n            1024,\n            1536,\n            2048,\n            2304,\n            3072,\n            3840,\n            4096,\n            5120,\n            6144,\n            8192,\n            10240,\n            12288,\n            12800,\n            15360,\n            16384,\n            18432,\n            20480,\n            24576,\n            25600,\n            30720,\n            32768,\n            40960,\n            49152,\n            65536,\n        ]\n        if (\n            normalized_shape not in persist_ln_hidden_sizes\n            or not HAVE_PERSIST_LAYER_NORM\n        ):\n            no_persist_layer_norm = True\n\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.eps = eps\n        self.weight = Parameter(torch.Tensor(*normalized_shape))\n        self.bias = Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n        self.no_persist_layer_norm = no_persist_layer_norm\n        self.sequence_parallel = sequence_parallel\n\n        # set sequence parallelism flag on weight and bias parameters\n        setattr(self.weight, \"sequence_parallel\", self.sequence_parallel)\n        setattr(self.bias, \"sequence_parallel\", self.sequence_parallel)\n\n    def reset_parameters(self):\n\n        if self.apply_layernorm_1p:\n            init.zeros_(self.weight)\n            init.zeros_(self.bias)\n        else:\n            init.ones_(self.weight)\n            init.zeros_(self.bias)\n\n    def forward(self, input):\n\n        weight = self.weight + 1 if self.apply_layernorm_1p else self.weight\n        # CPU path is here for unittest sake.\n        if not input.is_cuda:\n            print(\n                \"WARNING! The input of FusedLayerNorm should be on the GPU.\"\n                \"This warning should only be triggered in the FusedLayerNorm unit tests.\"\n            )\n            return F.layer_norm(\n                input, self.normalized_shape, weight, self.bias, self.eps\n            )\n\n        if self.no_persist_layer_norm:\n            # Apex does not have versions yet (https://github.com/NVIDIA/apex/pull/1648), so we need to inspect\n            # the function manually on whether the extra arg introduced in https://github.com/NVIDIA/apex/pull/1715 exists yet\n            if (\n                \"memory_efficient\"\n                in inspect.getfullargspec(FusedLayerNormAffineFunction.forward).args\n            ):\n                return FusedLayerNormAffineFunction.apply(\n                    input,\n                    weight,\n                    self.bias,\n                    self.normalized_shape,\n                    self.eps,\n                    self.mem_efficient_ln,\n                )\n            else:\n                return FusedLayerNormAffineFunction.apply(\n                    input, weight, self.bias, self.normalized_shape, self.eps\n                )\n        else:\n            output = FastLayerNormFN.apply(input, weight, self.bias, self.eps)\n\n            # Apex's fast layer norm function outputs a 'view' tensor (i.e., has\n            # a populated '_base' field). This will result in schedule.py's\n            # deallocate_output_tensor() throwing an error, so a viewless tensor is\n            # created to prevent this.\n            output = make_viewless_tensor(\n                inp=output, requires_grad=input.requires_grad, keep_graph=True\n            )\n\n            return output\n\n\nclass MixedFusedRMSNorm(torch.nn.Module):\n    def __init__(\n        self,\n        normalized_shape,\n        eps=1e-5,\n        no_persist_layer_norm=True,\n        sequence_parallel=False,\n        apply_rmsnorm_1p=False,\n        mem_efficient_rms=True,\n    ):\n        super(MixedFusedRMSNorm, self).__init__()\n\n        self.apply_rmsnorm_1p = apply_rmsnorm_1p\n        self.mem_efficient_rms = mem_efficient_rms\n        self.norm_fn = FusedRMSNormAffineFunction\n\n        global fused_layer_norm_cuda\n        fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\n\n        # List of hiddens sizes supported in the persistent layer norm kernel\n        # If the hidden size is not supported, fall back to the non-persistent\n        # kernel.\n        persist_ln_hidden_sizes = [\n            1024,\n            1536,\n            2048,\n            2304,\n            3072,\n            3840,\n            4096,\n            5120,\n            6144,\n            8192,\n            10240,\n            12288,\n            12800,\n            15360,\n            16384,\n            18432,\n            20480,\n            24576,\n            25600,\n            30720,\n            32768,\n            40960,\n            49152,\n            65536,\n        ]\n        if (\n            normalized_shape not in persist_ln_hidden_sizes\n            or not HAVE_PERSIST_LAYER_NORM\n        ):\n            no_persist_layer_norm = True\n\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.eps = eps\n        self.scale = Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n        self.no_persist_layer_norm = no_persist_layer_norm\n        self.sequence_parallel = sequence_parallel\n\n        # set sequence parallelism flag on weight and bias parameters\n        setattr(self.scale, \"sequence_parallel\", self.sequence_parallel)\n\n    def reset_parameters(self):\n\n        if self.apply_rmsnorm_1p:\n            init.zeros_(self.scale)\n        else:\n            init.ones_(self.scale)\n\n    def forward(self, input):\n\n        weight = self.scale + 1 if self.apply_rmsnorm_1p else self.scale\n        # CPU path is here for unittest sake.\n        if not input.is_cuda:\n            print(\n                \"WARNING! The input of FusedLayerNorm should be on the GPU.\"\n                \"This warning should only be triggered in the FusedRMSNorm unit tests.\"\n            )\n            # Latest pytorch actually supports F.rms_norm but I don't want to break builds so...\n            return F.layer_norm(input, self.normalized_shape, weight, None, self.eps)\n\n        # Apex does not have versions yet (https://github.com/NVIDIA/apex/pull/1648), so we need to inspect\n        # the function manually on whether the extra arg introduced in https://github.com/NVIDIA/apex/pull/1715 exists yet\n        if \"memory_efficient\" in inspect.getfullargspec(self.norm_fn.forward).args:\n            return self.norm_fn.apply(\n                input,\n                weight,\n                self.normalized_shape,\n                self.eps,\n                self.mem_efficient_rms,\n            )\n        else:\n            return self.norm_fn.apply(input, weight, self.normalized_shape, self.eps)\n\n            # Apex's fast layer norm function outputs a 'view' tensor (i.e., has\n            # a populated '_base' field). This will result in schedule.py's\n            # deallocate_output_tensor() throwing an error, so a viewless tensor is\n            # created to prevent this.\n            output = make_viewless_tensor(\n                inp=output, requires_grad=input.requires_grad, keep_graph=True\n            )\n\n            return output\n",
        "megatron/model/fused_rope.py": "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Tuple, Union\nimport torch\n\n\nclass FusedRoPEFunc(torch.autograd.Function):\n    \"\"\"\n    Fused RoPE function\n\n    This implementation assumes the input tensor to be in `sbhd` format and the RoPE tensor to be\n    of shape (s, 1, 1, d). It accepts arbitrary memory layouts to avoid the expensive\n    `.contiguous()` calls, thus it may not achieve the best memory access pattern.\n    \"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        t: torch.Tensor,\n        freqs: torch.Tensor,\n        transpose_output_memory: bool = False,\n    ) -> torch.Tensor:\n        import fused_rotary_positional_embedding\n\n        output = fused_rotary_positional_embedding.forward(\n            t, freqs, transpose_output_memory\n        )\n        ctx.save_for_backward(freqs)\n        ctx.transpose_output_memory = transpose_output_memory\n\n        return output\n\n    @staticmethod\n    def backward(\n        ctx, grad_output: torch.Tensor\n    ) -> Tuple[Union[torch.Tensor, None], ...]:\n        import fused_rotary_positional_embedding\n\n        (freqs,) = ctx.saved_tensors\n        grad_input = fused_rotary_positional_embedding.backward(\n            grad_output, freqs, ctx.transpose_output_memory\n        )\n\n        return grad_input, None, None\n\n\ndef fused_apply_rotary_pos_emb(\n    t: torch.Tensor,\n    freqs: torch.Tensor,\n    transpose_output_memory: bool = False,\n) -> torch.Tensor:\n    \"\"\"Apply rotary positional embedding to input tensor T.\n\n    Args:\n        t (Tensor): Input tensor T is of shape [s, b, h, d]\n        freqs (Tensor): Rotary Positional embedding tensor freq is of shape [s, 1, 1, d] and\n        `float` dtype\n        transpose_output_memory (bool): Default to False. Whether to transpose the 's' and 'b'\n        dimension of the output's underlying memory format. This is very helpful when you want to\n        get a contiguous tensor after calling `output.transpose(0, 1)`.\n\n    Returns:\n        Tensor: The input tensor after applying RoPE\n    \"\"\"\n    return FusedRoPEFunc.apply(t, freqs, transpose_output_memory)\n\n\nclass FusedRoPECachedFunc(torch.autograd.Function):\n    \"\"\"\n    Fused RoPE function\n\n    This implementation assumes the input tensor to be in `sbhd` format and the RoPE tensor to be\n    of shape (s, 1, 1, d). It accepts arbitrary memory layouts to avoid the expensive\n    `.contiguous()` calls, thus it may not achieve the best memory access pattern.\n    \"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        t: torch.Tensor,\n        cos_: torch.Tensor,\n        sin_: torch.Tensor,\n        transpose_output_memory: bool = False,\n    ) -> torch.Tensor:\n        import fused_rotary_positional_embedding\n\n        output = fused_rotary_positional_embedding.forward_cached(\n            t, cos_, sin_, transpose_output_memory\n        )\n        ctx.save_for_backward(cos_, sin_)\n        ctx.transpose_output_memory = transpose_output_memory\n\n        return output\n\n    @staticmethod\n    def backward(\n        ctx, grad_output: torch.Tensor\n    ) -> Tuple[Union[torch.Tensor, None], ...]:\n        import fused_rotary_positional_embedding\n\n        cos_, sin_ = ctx.saved_tensors\n        grad_input = fused_rotary_positional_embedding.backward_cached(\n            grad_output, cos_, sin_, ctx.transpose_output_memory\n        )\n\n        return grad_input, None, None, None\n\n\ndef fused_apply_rotary_pos_emb_cached(\n    t: torch.Tensor,\n    cos_: torch.Tensor,\n    sin_: torch.Tensor,\n    transpose_output_memory: bool = False,\n) -> torch.Tensor:\n    \"\"\"Apply rotary positional embedding to input tensor T.\n\n    Args:\n        t (Tensor): Input tensor T is of shape [s, b, h, d]\n        cos_ (Tensor): Cached cosine of the rotary positional embedding tensor is of\n        shape [s, 1, 1, d] and dtype either `float` or the same as `t`.\n        sin_ (Tensor): Cached sine of the rotary positional embedding tensor is of\n        shape [s, 1, 1, d] and dtype either `float` or the same as `t`.\n        transpose_output_memory (bool): Default to False. Whether to transpose the 's' and 'b'\n        dimension of the output's underlying memory format. This is very helpful when you want to\n        get a contiguous tensor after calling `output.transpose(0, 1)`.\n\n    Returns:\n        Tensor: The input tensor after applying RoPE\n    \"\"\"\n    return FusedRoPECachedFunc.apply(t, cos_, sin_, transpose_output_memory)\n",
        "megatron/model/fused_softmax.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport enum\nfrom ..fused_kernels import load_fused_kernels\n\n\nclass ScaledUpperTriangMaskedSoftmax(torch.autograd.Function):\n    \"\"\"\n    Fused operation which performs following three operations in sequence\n    1. Scale the tensor.\n    2. Apply upper triangular mask (typically used in gpt models).\n    3. Perform softmax.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs, scale):\n        import scaled_upper_triang_masked_softmax_cuda\n\n        scale_t = torch.tensor([scale])\n\n        softmax_results = scaled_upper_triang_masked_softmax_cuda.forward(\n            inputs, scale_t[0]\n        )\n        ctx.save_for_backward(softmax_results, scale_t)\n        return softmax_results\n\n    @staticmethod\n    def backward(ctx, output_grads):\n        import scaled_upper_triang_masked_softmax_cuda\n\n        softmax_results, scale_t = ctx.saved_tensors\n\n        input_grads = scaled_upper_triang_masked_softmax_cuda.backward(\n            output_grads, softmax_results, scale_t[0]\n        )\n        return input_grads, None\n\n\nclass ScaledMaskedSoftmax(torch.autograd.Function):\n    \"\"\"\n    Fused operation which performs following three operations in sequence\n    1. Scale the tensor.\n    2. Apply the mask.\n    3. Perform softmax.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs, mask, scale):\n        import scaled_masked_softmax_cuda\n\n        scale_t = torch.tensor([scale])\n\n        softmax_results = scaled_masked_softmax_cuda.forward(inputs, mask, scale_t[0])\n        ctx.save_for_backward(softmax_results, scale_t)\n        return softmax_results\n\n    @staticmethod\n    def backward(ctx, output_grads):\n        import scaled_masked_softmax_cuda\n\n        softmax_results, scale_t = ctx.saved_tensors\n\n        input_grads = scaled_masked_softmax_cuda.backward(\n            output_grads, softmax_results, scale_t[0]\n        )\n        return input_grads, None, None\n\n\nclass SoftmaxFusionTypes(enum.Enum):\n    upper_triang = 1  # causal mask\n    general = 2  # general mask\n    none = 3  # no fusion\n\n\nclass FusedScaleMaskSoftmax(nn.Module):\n    \"\"\"\n    fused operation: scaling + mask + softmax\n    Arguments:\n        input_in_fp16: flag to indicate if input in fp16 data format.\n        input_in_bf16: flag to indicate if input in bf16 data format.\n        fusion_type: type of fusion to perform, should be either upper_triang, general or none. None will perform a regular torch softmax.\n        mask_func: mask function to be applied.\n        softmax_in_fp32: if true, softmax in performed at fp32 precision.\n        scale: scaling factor used in input tensor scaling.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_in_fp16,\n        input_in_bf16,\n        fusion_type,\n        mask_func,\n        softmax_in_fp32,\n        scale,\n    ):\n        super().__init__()\n        self.input_in_fp16 = input_in_fp16\n        self.input_in_bf16 = input_in_bf16\n        self.input_in_float16 = self.input_in_fp16 or self.input_in_bf16\n\n        assert fusion_type in [\n            SoftmaxFusionTypes.upper_triang,\n            SoftmaxFusionTypes.general,\n            SoftmaxFusionTypes.none,\n        ], f\"Invalid fusion type {fusion_type}\"\n\n        if fusion_type != SoftmaxFusionTypes.none:\n            load_fused_kernels()  # check fused kernels are installed\n\n        self.upper_triang_mask_fusion = fusion_type == SoftmaxFusionTypes.upper_triang\n        self.general_mask_fusion = fusion_type == SoftmaxFusionTypes.general\n        self.fusion = fusion_type != SoftmaxFusionTypes.none\n\n        self.mask_func = mask_func\n        self.softmax_in_fp32 = softmax_in_fp32\n        self.scale = scale\n\n        assert (\n            self.scale is None or softmax_in_fp32\n        ), \"softmax should be in fp32 when scaled\"\n\n    def forward(self, input, mask):\n        # [b, np, sq, sk]\n        assert input.dim() == 4\n        if self.is_kernel_available(mask, *input.size()):\n            return self.forward_fused_softmax(input, mask)\n        else:\n            return self.forward_torch_softmax(input, mask)\n\n    def is_kernel_available(self, mask, b, np, sq, sk):\n        attn_batches = b * np\n\n        if (\n            self.fusion  # user wants to fuse\n            and self.input_in_float16  # input must be fp16\n            and mask is not None  # mask tensor must not be None\n            and 16 < sk <= 2048  # sk must be 16 ~ 2048\n            and sq % 4 == 0  # sq must be divisor of 4\n            and attn_batches % 4 == 0  # np * b must be divisor of 4\n        ):\n            if 0 <= sk <= 2048:\n                batch_per_block = self.get_batch_per_block(sq, sk, b, np)\n\n                if self.upper_triang_mask_fusion:\n                    if attn_batches % batch_per_block == 0:\n                        return True\n                else:\n                    if sq % batch_per_block == 0:\n                        return True\n        return False\n\n    def forward_fused_softmax(self, input, mask):\n        b, np, sq, sk = input.size()\n        scale = self.scale if self.scale is not None else 1.0\n        if self.upper_triang_mask_fusion:\n            assert sq == sk, \"causal mask is only for self attention\"\n\n            # input is 3D tensor (attn_batches, sq, sk)\n            input = input.view(-1, sq, sk)\n            probs = ScaledUpperTriangMaskedSoftmax.apply(input, scale)\n            return probs.view(b, np, sq, sk)\n        else:\n            # input is 4D tensor (b, np, sq, sk)\n            return ScaledMaskedSoftmax.apply(input, mask, scale)\n\n    def forward_torch_softmax(self, input, mask):\n        if self.input_in_float16 and self.softmax_in_fp32:\n            input = input.float()\n\n        if self.scale is not None:\n            input = input * self.scale\n        mask_output = self.mask_func(input, mask) if mask is not None else input\n        probs = torch.nn.Softmax(dim=-1)(mask_output)\n\n        if self.input_in_float16 and self.softmax_in_fp32:\n            if self.input_in_fp16:\n                probs = probs.half()\n            else:\n                probs = probs.bfloat16()\n\n        return probs\n\n    @staticmethod\n    def get_batch_per_block(sq, sk, b, np):\n        import scaled_masked_softmax_cuda\n\n        return scaled_masked_softmax_cuda.get_batch_per_block(sq, sk, b, np)\n",
        "megatron/model/gmlp.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom megatron.model.fused_softmax import FusedScaleMaskSoftmax\nfrom megatron.model.activations import get_activation\nfrom megatron.model.norms import get_norm\nfrom megatron.model.utils import get_fusion_type\n\nfrom megatron import mpu\n\n\nclass TinyAttention(nn.Module):\n    def __init__(self, neox_args, d_attn, d_ff, mask_fn):\n        super().__init__()\n        self.proj_qkv = nn.Linear(d_ff * 2, 3 * d_attn)\n        self.scale = d_attn**-0.5\n        self.proj_ffn = nn.Linear(d_attn, d_ff)\n        self.softmax = FusedScaleMaskSoftmax(\n            input_in_fp16=neox_args.precision == \"fp16\",\n            input_in_bf16=neox_args.precision == \"bfloat16\",\n            fusion_type=get_fusion_type(neox_args),\n            mask_func=mask_fn,\n            softmax_in_fp32=neox_args.attention_softmax_in_fp32,\n            scale=None,\n        )\n\n    def forward(self, x, attention_mask):\n        q, k, v = torch.chunk(self.proj_qkv(x), 3, dim=-1)\n        w = torch.einsum(\"bnd,bmd->bnm\", q, k).unsqueeze(1) * self.scale\n        a = self.softmax(\n            w, mask=attention_mask[..., : w.size(-2), : w.size(-1)]\n        ).squeeze(1)\n        x = torch.einsum(\"bnm,bmd->bnd\", a, v)\n        return self.proj_ffn(x)\n\n\nclass SpatialGatingUnit(nn.Module):\n    def __init__(self, neox_args, d_ff, d_attn=None, causal=True, mask_fn=None):\n        super().__init__()\n        self.causal = causal\n        self.use_attn = d_attn is not None\n\n        norm, eps = get_norm(neox_args)\n        self.norm = norm(d_ff, eps=eps)\n        self.proj = nn.Linear(neox_args.seq_length, neox_args.seq_length)\n        if self.use_attn:\n            assert mask_fn is not None\n            self.attn = TinyAttention(\n                neox_args=neox_args, d_attn=d_attn, d_ff=d_ff, mask_fn=mask_fn\n            )\n        nn.init.zeros_(self.proj.weight)\n        nn.init.constant_(self.proj.bias, 1.0)\n\n    def forward(self, x, attention_mask):\n        device, n = x.device, x.shape[1]\n        x = x.transpose(0, 1)  # [s, b, d] -> [b, s, d]\n\n        res, gate = x.chunk(2, dim=-1)  # split along dim\n        gate = self.norm(gate)\n\n        weight, bias = self.proj.weight, self.proj.bias\n        if self.causal:\n            weight, bias = weight[:n, :n], bias[:n]\n            mask = torch.ones(weight.shape[:2], device=device).triu_(1).bool()\n            weight = weight.masked_fill(mask, 0.0)\n\n        gate = F.linear(gate.transpose(2, 1), weight, self.proj.bias).transpose(2, 1)\n\n        if self.use_attn:\n            gate = gate + self.attn(x, attention_mask)\n\n        return (gate * res).transpose(0, 1)  # [b, s, d] -> [s, b, d]\n\n\nclass GMLPBlock(nn.Module):\n    def __init__(\n        self,\n        neox_args,\n        init_method,\n        output_layer_init_method,\n        layer_number,\n        ff_mult=4,\n        mask_fn=None,\n    ):\n        super().__init__()\n        self.layer_number = layer_number\n\n        ff_dim = neox_args.hidden_size * ff_mult\n        norm, eps = get_norm(neox_args)\n        self.norm = norm(neox_args.hidden_size, eps=eps)\n        self.input_linear = mpu.ColumnParallelLinear(\n            neox_args=neox_args,\n            input_size=neox_args.hidden_size,\n            output_size=ff_dim * 2,\n            gather_output=False,\n            init_method=init_method,\n            skip_bias_add=True,\n        )\n        self.activation_func, _ = get_activation(neox_args)\n        ff_dim_parallel = mpu.divide(ff_dim, mpu.get_model_parallel_world_size())\n        if neox_args.attention_config[layer_number] == \"amlp\":\n            d_attn = neox_args.gmlp_attn_dim\n        else:\n            d_attn = None\n        self.sgu = SpatialGatingUnit(\n            neox_args, ff_dim_parallel, d_attn, causal=True, mask_fn=mask_fn\n        )\n        self.output_linear = mpu.RowParallelLinear(\n            neox_args=neox_args,\n            input_size=ff_dim,\n            output_size=neox_args.hidden_size,\n            input_is_parallel=True,\n            init_method=output_layer_init_method,\n            skip_bias_add=True,\n        )\n\n    def forward(self, args):\n        assert len(args) == 2, \"GMLPBlock expects 2 arguments\"\n        x, attention_mask = args\n        x = self.norm(x)\n        x, _ = self.input_linear(x)\n        x = self.activation_func(x)\n        x = self.sgu(x, attention_mask)\n        x, _ = self.output_linear(x)\n        return x, attention_mask\n",
        "megatron/model/gpt2_model.py": "# # Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"GPT-2 model.\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom collections import defaultdict\n\nfrom functools import partial\nfrom megatron.model.utils import Lambda, SequentialWrapper, recursive_setattr\nfrom megatron.model.norms import get_norm\nfrom megatron.model.init_functions import get_init_methods\n\nfrom megatron import mpu\nfrom megatron.mpu import ParallelRelativePositionBias\nfrom megatron.model.transformer import (\n    ParallelTransformerLayerPipe,\n    NormPipe,\n    ParallelLinearPipe,\n    parallel_lm_logits,\n    ParallelLinear,\n)\nfrom megatron.model.gmlp import GMLPBlock\nfrom megatron.model.rwkv.v6 import RWKVResidualLayerPipe\nfrom megatron.model.mamba import ParallelMambaResidualLayerPipe\nfrom megatron.model.word_embeddings import EmbeddingPipe, SoftEmbedding\n\n# Pipeline parallelism\nfrom deepspeed.pipe import PipelineModule, LayerSpec, TiedLayerSpec\nfrom typing import Union, List\n\n\ndef gpt2_attention_mask_func(attention_scores, ltor_mask):\n    mask_value = torch.finfo(attention_scores.dtype).min\n    # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n    # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n    mask_value = torch.tensor(\n        mask_value, dtype=attention_scores.dtype, device=attention_scores.device\n    )\n    attention_scores.masked_fill_(ltor_mask, mask_value)\n    return attention_scores\n\n\ndef cross_entropy(output, labels, _fp16=False):\n    \"\"\"From pretrain_gpt2:forward_step()\"\"\"\n    \"\"\"\n    if self.fp16_lm_cross_entropy:\n        assert output.dtype == torch.half\n        loss = mpu.vocab_parallel_cross_entropy(output, labels)\n    else:\n        loss = mpu.vocab_parallel_cross_entropy(output.float(), labels)\n        return loss\n    \"\"\"\n    labels, loss_mask = labels[0], labels[1]\n    if _fp16:\n        assert output.dtype == torch.half and loss_mask.dtype == torch.half\n        losses = mpu.vocab_parallel_cross_entropy(output.contiguous(), labels)\n    else:\n        losses = mpu.vocab_parallel_cross_entropy(output.float().contiguous(), labels)\n    loss_mask = loss_mask.view(-1)\n    loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n    return loss\n\n\ndef _pre_transformer_block(args):\n    # data format change for hidden_states to avoid explicit tranposes : [b s h] --> [s b h]\n    assert len(args) == 2, \"Incorrect number of arguments to _pre_transformer_block\"\n    fn = lambda _args: (_args[0].transpose(0, 1).contiguous(), *_args[1:])\n    return fn(args)\n\n\ndef _post_transformer_block(args):\n    # from (hidden_states, attention_mask)\n    # to (hidden_states.T)\n    assert len(args) == 2, \"Incorrect number of arguments to _post_transformer_block\"\n    fn = lambda _args: (_args[0].transpose(0, 1).contiguous())\n    return fn(args)\n\n\nclass GPT2ModelPipe(PipelineModule, torch.nn.Module):\n    \"\"\"GPT2Model adapted for pipeline parallelism.\n\n    The largest change is flattening the GPTModel class so we can express it as a\n    sequence of layers including embedding, transformer layers, and output.\n\n    :param neox_args: NeoX arguments object (configuration)\n    :param num_tokentypes: number of token types (TODO: deprecated, remove)\n    :param parallel_output: if true, don't gather the output logits, and calculate loss in parallel. Set to true by default in training for efficiency, but set to false for inference.\n    :param topology: deepspeed topology object specifying pipe / model parallelism topology.\n    :param use_cache: if true, cache key/value pairs for each layer in inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        num_tokentypes=0,\n        parallel_output=True,\n        topology=None,\n        use_cache=False,\n    ):\n        self.neox_args = neox_args\n\n        self.use_cache = use_cache\n        self.parallel_output = parallel_output\n        self.hidden_size = self.neox_args.hidden_size\n        self.num_tokentypes = num_tokentypes\n        self.init_method, self.output_layer_init_method = get_init_methods(\n            self.neox_args\n        )\n        self.__topology__ = topology\n\n        self.specs = []\n        self.init_specs()  # initializes the layer specs (basically a fancy nn.Sequential)\n\n        super().__init__(\n            layers=self.specs,\n            loss_fn=partial(cross_entropy, _fp16=self.neox_args.fp16_lm_cross_entropy),\n            topology=topology,\n            activation_checkpoint_interval=self.neox_args.checkpoint_num_layers\n            if self.neox_args.checkpoint_activations\n            else 0,\n            partition_method=neox_args.pipe_partition_method,\n            checkpointable_layers=[\n                \"GMLPBlock\",\n                \"ParallelTransformerLayerPipe\",\n                \"ParallelMambaResidualLayerPipe\",\n            ],\n        )\n\n    def insert_layers(\n        self, layers: Union[nn.Module, nn.ModuleList, nn.Sequential, List], idx\n    ):\n        \"\"\"\n        inserts the layers in `layers` into the pipe model at `idx`.\n        \"\"\"\n        if isinstance(layers, nn.Module):\n            self.specs.insert(idx, layers)\n        elif any(\n            [isinstance(layers, nn.ModuleList), isinstance(layers, nn.Sequential)]\n        ):\n            self.specs[idx:idx] = layers\n        elif isinstance(layers, list):\n            assert all(\n                [hasattr(l, \"__call__\") for l in layers]\n            ), \"all items in `layers` must be Callables\"\n            self.specs[idx:idx] = layers\n        else:\n            raise ValueError(\n                f\"layer passed into {self.__class__.__name__}.insert_layer() should be either an nn.Module, an nn.ModuleList, an nn.Sequential object, or a list of callables not a {type(layers)}\"\n            )\n\n        # re-initialize parent class\n        super().__init__(\n            layers=self.specs,\n            loss_fn=self.loss_fn,\n            topology=self.__topology__,\n            activation_checkpoint_interval=self.activation_checkpoint_interval,\n            partition_method=self.neox_args.pipe_partition_method,\n            checkpointable_layers=[\n                \"GMLPBlock\",\n                \"ParallelTransformerLayerPipe\",\n                \"ParallelMambaResidualLayerPipe\",\n                \"RWKVResidualLayerPipe\",\n            ],\n        )\n\n    def init_specs(self):\n\n        weight_tying = not self.neox_args.no_weight_tying\n        self.specs = []\n\n        # Embedding layer\n        # input will be (input_ids, position_ids, attention_mask)\n\n        if weight_tying:\n            self.specs.append(\n                TiedLayerSpec(\n                    \"embed\",\n                    EmbeddingPipe,\n                    self.neox_args,\n                    self.hidden_size,\n                    self.neox_args.padded_vocab_size,\n                    self.neox_args.max_position_embeddings,\n                    self.neox_args.hidden_dropout,\n                    self.init_method,\n                    self.num_tokentypes,\n                    tied_weight_attr=\"word_embeddings_weight\",\n                )\n            )\n        else:\n            self.specs.append(\n                LayerSpec(\n                    EmbeddingPipe,\n                    self.neox_args,\n                    self.hidden_size,\n                    self.neox_args.padded_vocab_size,\n                    self.neox_args.max_position_embeddings,\n                    self.neox_args.hidden_dropout,\n                    self.init_method,\n                    self.num_tokentypes,\n                )\n            )\n\n        # NB: the attention mask always needs to be the *last* item in the args when being passed from\n        # one stage to the next, because deepspeed is hacks on top of hacks.\n        #\n        # outputs are now (hidden_states,  attention_mask)\n\n        self.specs.append(_pre_transformer_block)\n\n        # T5 RPE positional embedding\n        if self.neox_args.pos_emb == \"rpe\":\n            hidden_size_per_attention_head = mpu.divide(\n                self.neox_args.hidden_size, self.neox_args.num_attention_heads\n            )\n            rpe_scale = math.sqrt(hidden_size_per_attention_head)\n            rpe_emb = ParallelRelativePositionBias(\n                neox_args=self.neox_args,\n                scale=rpe_scale,\n                causal=True,\n                num_buckets=self.neox_args.rpe_num_buckets,\n                max_distance=self.neox_args.rpe_max_distance,\n                heads=self.neox_args.num_attention_heads,\n            )\n\n        # Transformer layers\n        for i in range(self.neox_args.num_layers):\n            layer_type = self.neox_args.attention_config[i]\n            if layer_type in [\"gmlp\", \"amlp\"]:\n                self.specs.append(\n                    LayerSpec(\n                        GMLPBlock,\n                        init_method=self.init_method,\n                        layer_number=i,\n                        output_layer_init_method=self.output_layer_init_method,\n                        neox_args=self.neox_args,\n                        mask_fn=gpt2_attention_mask_func,\n                    )\n                )\n            elif layer_type == \"rwkv\":\n                self.specs.append(\n                    LayerSpec(\n                        RWKVResidualLayerPipe,\n                        neox_args=self.neox_args,\n                        layer_number=i,\n                    )\n                )\n            elif layer_type in [\"mamba\"]:\n                self.specs.append(\n                    LayerSpec(\n                        ParallelMambaResidualLayerPipe,\n                        neox_args=self.neox_args,\n                        init_method=self.init_method,\n                        output_layer_init_method=self.output_layer_init_method,\n                        layer_number=i,\n                    )\n                )\n            else:\n                self.specs.append(\n                    LayerSpec(\n                        ParallelTransformerLayerPipe,\n                        neox_args=self.neox_args,\n                        attention_mask_func=gpt2_attention_mask_func,\n                        init_method=self.init_method,\n                        output_layer_init_method=self.output_layer_init_method,\n                        layer_number=i,\n                        rpe=rpe_emb if self.neox_args.pos_emb == \"rpe\" else None,\n                        rotary=self.neox_args.pos_emb == \"rotary\",\n                        use_cache=self.use_cache,\n                    )\n                )\n\n        # used to drop attention mask + reshape hidden states\n        self.specs.append(_post_transformer_block)\n\n        # NormPipe is a (deprecated) helper class that used to be used to pass presents along the pipeline - since presents are now cached to the `TransformerLayer` class this is no longer needed\n        norm, eps = get_norm(self.neox_args)\n        self.specs.append(\n            LayerSpec(NormPipe, norm, self.neox_args.hidden_size, eps=eps)\n        )\n\n        # outputs are now a single tensor: hidden_states\n\n        def _logits_helper(embedding, lm_output):\n            \"\"\"Just a wrapper to massage inputs/outputs from pipeline.\"\"\"\n            if self.neox_args.use_mup:\n                # Since we're using pipeline parallelism, we can't directly use MuReadout. Instead, use this workaround that does the same thing as MuReadout.\n                # https://github.com/microsoft/mup/issues/6#issuecomment-1082156274\n                lm_output = (\n                    lm_output\n                    / self.tied_modules.embed.word_embeddings.weight.infshape.width_mult()\n                )\n\n            logits = parallel_lm_logits(\n                lm_output,\n                embedding.word_embeddings_weight,\n                self.parallel_output,\n                seq_parallel=self.neox_args.sequence_parallel,\n            )\n            return logits\n\n        if weight_tying:\n            self.specs.append(\n                TiedLayerSpec(\n                    \"embed\",\n                    EmbeddingPipe,\n                    self.neox_args,\n                    self.hidden_size,\n                    self.neox_args.padded_vocab_size,\n                    self.neox_args.max_position_embeddings,\n                    self.neox_args.hidden_dropout,\n                    self.init_method,\n                    self.num_tokentypes,\n                    forward_fn=_logits_helper,\n                    tied_weight_attr=\"word_embeddings_weight\",\n                )\n            )\n        else:\n            self.specs.append(\n                LayerSpec(\n                    ParallelLinearPipe,\n                    neox_args=self.neox_args,\n                    init_method=self.init_method,\n                    parallel_output=self.parallel_output,\n                    is_last_layer=True,\n                )\n            )\n\n    def _set_parallel_output(self, value):\n        # sets the parallel output value of the final layer to value\n        final_layer = list(self.forward_funcs)[-1]\n        if isinstance(final_layer, (ParallelLinearPipe, ParallelLinear)):\n            final_layer.final_linear.set_parallel_output(value)\n\n    def inference_mode(self, use_cache=True):\n        \"\"\"\n        Sets up the model for inference by turning on k/v caching (if specified) and setting `parallel output` of the final layer to false,\n        so logits are gathered across model parallel ranks.\n\n        :param cache: (bool) True if you want to use caching during inference, False otherwise\n        \"\"\"\n        # first set caching to true if specified\n        recursive_setattr(self.forward_funcs, \"use_cache\", use_cache, assert_type=bool)\n        # then set parallel output of the final layer to false so we don't have to gather the output manually\n        self._set_parallel_output(False)\n        recursive_setattr(self.forward_funcs, \"training\", False)\n\n    def train_mode(self):\n        \"\"\"\n        Sets up the model for training by turning off k/v caching and setting `parallel output` of the final layer to True,\n        so logits are not gathered across model parallel ranks, and loss is computed in parallel (more efficient).\n        \"\"\"\n        # set caching to false\n        recursive_setattr(self.forward_funcs, \"use_cache\", False)\n        # then set parallel output to true (more efficient training)\n        self._set_parallel_output(True)\n        recursive_setattr(self.forward_funcs, \"training\", True)\n\n    def clear_cache(self):\n        \"\"\"\n        Recursively clears the kv cache on all layers\n        \"\"\"\n        recursive_setattr(self.forward_funcs, \"layer_past\", None)\n\n    def to_sequential(self):\n        \"\"\"\n        Transforms the PipelineModule to a plain nn.Sequential module\n        :return:\n        \"\"\"\n        layers = []\n        tied_layers = defaultdict(list)\n        for n, spec in enumerate(self.specs):\n            if isinstance(spec, TiedLayerSpec):\n                if spec.key in tied_layers:\n                    # receiver\n                    layers.append(\n                        Lambda(lambda x: spec.forward_fn(tied_layers[spec.key][0], x))\n                    )\n                else:\n                    # owner\n                    module = spec.build(log=False)\n                    layers.append(module)\n                    tied_layers[spec.key].append(module)\n            elif isinstance(spec, LayerSpec):\n                layers.append(spec.build(log=False))\n            elif hasattr(spec, \"__call__\"):\n                # check that it's a callable function\n                layers.append(Lambda(spec))\n            else:\n                raise ValueError(f\"Layer number {n} ({spec}) Not recognized\")\n        model = SequentialWrapper(\n            layers,\n            self.activation_checkpoint_interval,\n            self.activation_checkpoint_func,\n            parent_class_name=self.__class__.__name__,\n        )\n        return model\n",
        "megatron/model/init_functions.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\n\ntry:\n    import mup\nexcept ImportError:\n    pass\n\n\ndef init_method_normal(sigma, use_mup_outer=False, mup_init_scale=1.0):\n    \"\"\"Init method based on N(0, sigma).\"\"\"\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.normal_(tensor, mean=0.0, std=sigma)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=sigma)\n\n    return init_\n\n\ndef scaled_init_method_normal(\n    sigma,\n    num_layers,\n    use_mup_outer=False,\n    mup_init_scale=1.0,\n    num_residuals_per_layer=2,\n):\n    \"\"\"Init method based on N(0, sigma/sqrt(2*num_layers).\n\n    Also allows for N(0, sigma/sqrt(x*num_layers)) where\n    x=number of residuals per layer (e.g. 1 for Mamba.)\n    \"\"\"\n    std = sigma / math.sqrt(num_residuals_per_layer * num_layers)\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.normal_(tensor, mean=0.0, std=std)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n\n    return init_\n\n\n# orthogonal init does not support fp16, so have to patch it\ndef _orthogonal(tensor, gain=1):\n\n    if tensor.ndimension() < 2:\n        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n\n    rows = tensor.size(0)\n    cols = tensor.numel() // rows\n    flattened = tensor.new(rows, cols).normal_(0, 1)\n\n    if rows < cols:\n        flattened.t_()\n\n    # Compute the qr factorization\n    dt = flattened.dtype\n    flattened = flattened.to(torch.float32)  # orthogonal init does not support fp16\n    q, r = torch.qr(flattened)\n    q, r = q.to(dtype=dt), r.to(dtype=dt)\n    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n    d = torch.diag(r, 0)\n    ph = d.sign()\n    q *= ph\n\n    if rows < cols:\n        q.t_()\n\n    with torch.no_grad():\n        tensor.view_as(q).copy_(q)\n        tensor.mul_(gain)\n    return tensor\n\n\ndef orthogonal_init_method(n_layers=1, use_mup=False, mup_init_scale=1.0):\n    \"\"\"Fills the input Tensor with a (semi) orthogonal matrix, as described in\n    Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013)\n    Optionally scaling by number of layers possible, as introduced in OBST - Nestler et. al. (2021, to be released)\"\"\"\n\n    if use_mup:\n        raise ValueError(\n            \"Orthogonal init needs to be patched to support mup. Disable mup or use a different init method to avoid this error\"\n        )\n\n    def init_(tensor):\n        return _orthogonal(tensor, math.sqrt(2 / n_layers))\n\n    return init_\n\n\ndef xavier_uniform_init_method(use_mup_outer=False, mup_init_scale=1.0):\n    \"\"\"Fills the input Tensor with values according to the method described in Understanding the difficulty of\n    training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution.\"\"\"\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.xavier_uniform_(tensor)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.xavier_uniform_(tensor)\n\n    return init_\n\n\ndef xavier_normal_init_method(use_mup_outer=False, mup_init_scale=1.0):\n    \"\"\"Fills the input Tensor with values according to the method described in Understanding the difficulty of\n    training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution.\"\"\"\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.xavier_normal_(tensor)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.xavier_normal_(tensor)\n\n    return init_\n\n\ndef small_init_init_method(dim, use_mup_outer=False, mup_init_scale=1.0):\n    \"\"\"Fills the input Tensor with values according to the method described in Transformers without Tears: Improving\n    the Normalization of Self-Attention - Nguyen, T. & Salazar, J. (2019), using a normal distribution.\"\"\"\n    std = math.sqrt(2 / (5 * dim))\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.normal_(tensor, mean=0.0, std=std)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n\n    return init_\n\n\ndef wang_init_method(n_layers, dim, use_mup_outer=False, mup_init_scale=1.0):\n    std = 2 / n_layers / math.sqrt(dim)\n\n    def init_(tensor, use_mup=use_mup_outer):\n        if use_mup:\n            mup.init.normal_(tensor, mean=0.0, std=std)\n            with torch.no_grad():\n                tensor.mul_(mup_init_scale)\n            return tensor\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n\n    return init_\n\n\ndef get_init_methods(args):\n\n    if args.use_mup:\n        try:\n            import mup\n        except ModuleNotFoundError:\n            print(\"Please install mup https://github.com/microsoft/mup\")\n            raise Exception\n\n    def _get(name):\n        if name == \"normal\":\n            return init_method_normal(\n                args.init_method_std, args.use_mup, args.mup_init_scale\n            )\n        elif name == \"scaled_normal\":\n            return scaled_init_method_normal(\n                args.init_method_std, args.num_layers, args.use_mup, args.mup_init_scale\n            )\n        elif name == \"orthogonal\":\n            return orthogonal_init_method(args.use_mup, args.mup_init_scale)\n        elif name == \"scaled_orthogonal\":\n            return orthogonal_init_method(\n                args.num_layers, args.use_mup, args.mup_init_scale\n            )\n        elif name == \"xavier_uniform\":\n            return xavier_uniform_init_method(args.use_mup, args.mup_init_scale)\n        elif name == \"xavier_normal\":\n            return xavier_normal_init_method(args.use_mup, args.mup_init_scale)\n        elif name == \"wang_init\":\n            return wang_init_method(\n                args.num_layers, args.hidden_size, args.use_mup, args.mup_init_scale\n            )\n        elif name == \"small_init\":\n            return small_init_init_method(\n                args.hidden_size, args.use_mup, args.mup_init_scale\n            )\n        elif name == \"single_residual_scaled_normal\":\n            # mamba init uses scaled_normal but no need for 2 * num_layers\n            # since only one residual per layer\n            return scaled_init_method_normal(\n                args.init_method_std,\n                args.num_layers,\n                args.use_mup,\n                args.mup_init_scale,\n                num_residuals_per_layer=1,\n            )\n        else:\n            raise NotImplementedError(f\"Unknown init method {name}\")\n\n    return _get(args.init_method), _get(args.output_layer_init_method)\n",
        "megatron/model/mamba/__init__.py": "from .mamba import (\n    ParallelMambaResidualLayer,\n    ParallelMambaResidualLayerPipe,\n)\n",
        "megatron/model/mamba/mamba.py": "import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from mamba_ssm.ops.selective_scan_interface import (\n        selective_scan_ref,\n        selective_scan_fn,\n        mamba_inner_fn,\n    )\n    from causal_conv1d import causal_conv1d_fn\n    import einops\nexcept ModuleNotFoundError:\n    print(\n        \"Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt, \\\n    or directly from https://github.com/state-spaces/mamba\"\n    )\n    pass\n\nfrom megatron.model.norms import get_norm\nfrom megatron import mpu\n\n# Mamba sublayer, with tensor parallelism\nclass ParallelMambaBlock(nn.Module):\n    def __init__(\n        self,\n        neox_args,\n        init_method,\n        output_layer_init_method,\n    ):\n        super().__init__()\n\n        self.neox_args = neox_args\n\n        dtype = {\n            \"fp16\": torch.float16,\n            \"bf16\": torch.bfloat16,\n            \"fp32\": torch.float32,\n        }[neox_args.precision]\n        self.precision = dtype\n        factory_kwargs = {\"device\": torch.cuda.current_device(), \"dtype\": dtype}\n\n        assert not (\n            neox_args.mamba_use_bias_in_linears and neox_args.mamba_inner_func_fusion\n        ), \"Mamba fused inner fn and bias in x_proj not compatible!\"\n\n        assert (\n            neox_args.intermediate_size == None or neox_args.expansion_factor == None\n        ), \"Must pass either the absolute intermediate size or the relative expansion factor for the mamba projections\"\n\n        # set variables, mostly following mamba defaults\n        self.d_model = neox_args.hidden_size\n        self.d_state = 16  # state dimensions per channel\n        self.d_conv = 4  # convolution width\n        if neox_args.intermediate_size:\n            self.d_inner = neox_args.intermediate_size\n        else:\n            self.expand = (\n                neox_args.expansion_factor if neox_args.expansion_factor else 2\n            )\n            self.d_inner = int(self.expand * self.d_model)\n        self.dt_rank = math.ceil(self.d_model / 16)  # rank of dt / Delta parameter\n        self.dt_scale = 1.0\n\n        self.dt_init = \"random\"\n        self.dt_min, self.dt_max, self.dt_init_floor = 0.001, 0.1, 1e-4\n        assert self.dt_init in [\"constant\", \"random\"]\n\n        # TP-specific setup\n        world_size = mpu.get_model_parallel_world_size()\n        self.d_inner_per_rank = mpu.divide(self.d_inner, world_size)\n\n        if neox_args.mamba_inner_func_fusion and world_size > 1:\n            # as with gpt-j residual, we must manually reduce output from final proj\n            # across TP ranks, since it is not done by fused mamba_inner_fn .\n            self.reduce = mpu.mappings.reduce_from_model_parallel_region\n\n        # up-projection.\n        self.in_proj = mpu.ColumnParallelLinear(\n            neox_args=neox_args,\n            input_size=self.d_model,\n            output_size=self.d_inner * 2,\n            gather_output=False,\n            init_method=init_method,\n            skip_bias_add=not neox_args.mamba_use_bias_in_linears,\n            bias=neox_args.mamba_use_bias_in_linears,\n        )\n\n        # convolution (parallelized across d_inner)\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner_per_rank,\n            out_channels=self.d_inner_per_rank,\n            bias=neox_args.mamba_use_bias_in_conv,\n            kernel_size=self.d_conv,\n            groups=self.d_inner_per_rank,\n            padding=self.d_conv - 1,\n            **factory_kwargs,\n        )\n        # Conv bias sometimes in 32-bit erroneously, when holding other parameters in fp32.\n        # Uncertain why\n        self.conv1d.to(self.precision)\n\n        self.act_fn = F.silu  # we do not allow for other activation fns\n\n        # x_proj corresponds to s_B(x), s_C(x), s_Delta(x)\n        # in https://arxiv.org/pdf/2312.00752.pdf Algorithm 2\n        # (computes data-dependent B, C, Delta/dt)\n        self.x_proj = mpu.RowParallelLinear(\n            neox_args=neox_args,\n            input_size=self.d_inner,\n            output_size=self.dt_rank + self.d_state * 2,\n            input_is_parallel=True,\n            init_method=init_method,\n            skip_bias_add=not neox_args.mamba_use_bias_in_linears,\n            parallel_output=True,\n            bias=neox_args.mamba_use_bias_in_linears,\n        )\n\n        # up-project dt / Delta from dt_rank to d_inner\n        # dt_proj 's bias is a special case and should be kept always turned on -- Alg. 2 in the Mamba paper (https://arxiv.org/abs/2312.00752)\n        # defines Delta as Delta = Tau_{Delta}(Parameter + s_{Delta}(x)) where s_{Delta}(x) = Broadcast_{D}(Linear_{1}(x))\n        # or as they further explain in section 3.6 can be also s_{Delta}(x) = Linear_{D}(Linear_{R}(x)) where Linear_R\n        # is the delta portion of x_proj and Linear_D is the dt_proj weight. Then, the Parameter term from Alg. 2 can\n        # be viewed as the bias term in dt_proj, with a special initialization from https://arxiv.org/abs/2206.12037\n        self.dt_proj = nn.Linear(\n            self.dt_rank, self.d_inner_per_rank, bias=True, **factory_kwargs\n        )\n\n        # special init for dt_proj\n        dt_init_std = (self.dt_rank**-0.5) * self.dt_scale\n        if self.dt_init == \"constant\":\n            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n        elif self.dt_init == \"random\":\n            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n        else:\n            raise NotImplementedError\n\n        # more dt_proj init stuff. copied from https://github.com/state-spaces/mamba/blob/009bec5ee37f586844a3fc89c040a9c1a9d8badf/mamba_ssm/modules/mamba_simple.py#L91-L101\n        dt = torch.exp(\n            torch.rand(self.d_inner_per_rank, **factory_kwargs)\n            * (math.log(self.dt_max) - math.log(self.dt_min))\n            + math.log(self.dt_min)\n        ).clamp(min=self.dt_init_floor)\n        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        with torch.no_grad():\n            self.dt_proj.bias.copy_(inv_dt)\n\n        # initialize A . uses S4D real initialization\n        A = einops.repeat(\n            torch.arange(\n                1,\n                self.d_state + 1,\n                dtype=torch.float32,\n                device=torch.cuda.current_device(),\n            ),\n            \"n -> d n\",\n            d=self.d_inner_per_rank,\n        ).contiguous()\n        A_log = torch.log(A).to(\n            torch.float32\n        )  # Keep in fp32, following https://github.com/state-spaces/mamba#precision and code comments\n        self.A_log = nn.Parameter(A_log)\n        self.A_log._no_weight_decay = (\n            True  # setting this attribute turns off weight decay for this param\n        )\n        # setting this attribute prevents deeperspeed from casting this param to fp32\n        # requires DeepersSpeed commit https://github.com/EleutherAI/DeeperSpeed/commit/6d097beccc4e3b0ac806c7d975f8c10d4689de26 or later\n        if self.neox_args.mamba_selective_fp32_params:\n            self.A_log._deepspeed_no_cast = True\n\n        # D parameter\n        self.D = nn.Parameter(\n            torch.ones(\n                self.d_inner_per_rank,\n                device=torch.cuda.current_device(),\n                dtype=torch.float32,\n            )\n        ).to(\n            torch.float32\n        )  # Keep in fp32, following https://github.com/state-spaces/mamba#precision and code comments\n        self.D._no_weight_decay = (\n            True  # setting this attribute turns off weight decay for this param\n        )\n        # setting this attribute prevents deeperspeed from casting this param to fp32\n        # requires DeeperSpeed commit https://github.com/EleutherAI/DeeperSpeed/commit/6d097beccc4e3b0ac806c7d975f8c10d4689de26 or later\n        if self.neox_args.mamba_selective_fp32_params:\n            self.D._deepspeed_no_cast = True\n\n        # out down-projection.\n        # use \"single_residual_scaled_normal\"\n        # for output_layer_init_method\n        # to perform gpt-2 style scaled init as done in Mamba paper.\n        self.out_proj = mpu.RowParallelLinear(\n            neox_args=neox_args,\n            input_size=self.d_inner,\n            output_size=self.d_model,\n            input_is_parallel=True,\n            init_method=output_layer_init_method,\n            skip_bias_add=not neox_args.mamba_use_bias_in_linears,\n            bias=neox_args.mamba_use_bias_in_linears,\n            parallel_output=False,\n        )\n\n    def selective_scan(\n        self,\n        x,\n        dt,\n        A,\n        B,\n        C,\n        D,\n        z=None,\n        delta_bias=None,\n        delta_softplus=True,\n    ):\n\n        if not self.neox_args.mamba_selective_scan_fusion:\n            y = selective_scan_ref(\n                u=x,\n                delta=dt,\n                A=A,\n                B=B,\n                C=C,\n                D=D,\n                z=z,\n                delta_bias=delta_bias,\n                delta_softplus=delta_softplus,\n                return_last_state=False,\n            )\n        else:\n            y = selective_scan_fn(\n                x,\n                dt,\n                A,\n                B,\n                C,\n                D=D,\n                z=z,\n                delta_bias=delta_bias,\n                delta_softplus=delta_softplus,\n                return_last_state=False,\n            )\n\n        return y\n\n    def forward(self, hidden_states):\n        \"\"\" \"\"\"\n        # TODO: support inference natively in neox.\n        # For now, we only handle training (parallel scan).\n        assert self.training, \"Mamba in NeoX does not support inference!\"\n\n        # hidden_states: [sq, b, h]\n        seqlen, batch, dim = hidden_states.shape\n\n        # first up: perform in_proj\n        xz, _ = self.in_proj(hidden_states)\n        xz = einops.rearrange(xz, \"l b d -> b d l\")\n\n        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n\n        if self.neox_args.mamba_inner_func_fusion:\n            # =================\n            # Fused mamba inner\n            # =================\n\n            # mamba provides a mamba_inner fn that computes the entire (post-in_proj) Mamba block.\n            # we want to use it if we can, as it saves memory and provides speedups.\n            # equivalent to use_fast_path=True in state-spaces/mamba.\n            out = mamba_inner_fn(\n                xz,\n                self.conv1d.weight,\n                # for some bizarre reason this becomes fp32 sometime after init, when A and D held in fp32.\n                # cast it manually if the bias exists\n                self.conv1d.bias.to(self.precision)\n                if self.conv1d.bias is not None\n                else self.conv1d.bias,\n                self.x_proj.weight,\n                self.dt_proj.weight,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                A,\n                None,  # B is input-dependent, will compute from x_proj\n                None,  # C is input-dependent, will compute from x_proj\n                self.D.float(),\n                delta_bias=self.dt_proj.bias.float(),\n                delta_softplus=True,\n            )\n            if getattr(self, \"reduce\", None):\n                # manually reduce after mamba_inner_fn\n                # to collect outputs from different TP ranks.\n                # handled by running self.out_proj(y) below\n                # so only needed here.\n                out = self.reduce(out)\n\n            out = einops.rearrange(out, \"b l h -> l b h\")\n\n            return out\n\n        x, z = xz.chunk(2, dim=1)\n\n        # ===========\n        # Convolution\n        # ===========\n\n        if not self.neox_args.mamba_causal_conv_fusion:\n            self.conv1d.to(self.precision)  # required if keeping fp32 A_log, D\n            x = self.act_fn(self.conv1d(x)[..., :seqlen])\n        else:\n            # Note: this requires silu as activation.\n            x = causal_conv1d_fn(\n                x=x,\n                weight=einops.rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n                bias=self.conv1d.bias.to(self.precision)\n                if self.conv1d.bias is not None\n                else self.conv1d.bias,\n                activation=\"silu\",\n            )\n\n        # ==============\n        # SSM (S6) layer\n        # ==============\n\n        # project: perform s_B, s_C, s_Delta projections\n        x_dbl, _ = self.x_proj(einops.rearrange(x, \"b d l -> (b l) d\"))\n        # split into component dt, B, C\n        dt, B, C = torch.split(\n            x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1\n        )\n\n        # up-project Delta / dt\n        dt = self.dt_proj.weight @ dt.t()\n        dt = einops.rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n\n        # rearrange B, C\n        B = einops.rearrange(B, \"(b l) d_state -> b d_state l\", l=seqlen).contiguous()\n        C = einops.rearrange(C, \"(b l) d_state -> b d_state l\", l=seqlen).contiguous()\n\n        # perform selective scan.\n        y = self.selective_scan(\n            x,\n            dt,\n            A,\n            B,\n            C,\n            self.D.float(),\n            z=z,\n            delta_bias=self.dt_proj.bias.float(),\n            delta_softplus=True,\n        )\n\n        # ===============\n        # Down-Projection\n        # ===============\n        y = einops.rearrange(y, \"b d l -> b l d\")\n\n        out, _ = self.out_proj(y)\n\n        out = einops.rearrange(out, \"b l h -> l b h\")\n\n        return out\n\n\nclass ParallelMambaResidualLayer(nn.Module):\n    \"\"\"\n    Pre-norm Mamba Block with residual connection. No parallelism yet supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        init_method,\n        output_layer_init_method,\n        layer_number,\n    ):\n        super().__init__()\n        # TODO: allow for residual in fp32 if it helps?\n        self.layer_number = layer_number\n\n        # TODO: Add support for triton RMSNorm fused kernel at https://github.com/state-spaces/mamba/blob/v1.2.0/mamba_ssm/ops/triton/layernorm.py\n        norm, eps = get_norm(neox_args)\n\n        self.norm = norm(neox_args.hidden_size, eps=eps)\n\n        self.mixer = ParallelMambaBlock(\n            neox_args=neox_args,\n            init_method=init_method,\n            output_layer_init_method=output_layer_init_method,\n        )\n\n    def forward(self, x, attention_mask=None, layer_past=None):\n\n        # pseudocode:\n        # x = x + mixer(norm(x))\n        residual = x\n\n        hidden_states = self.mixer(self.norm(x))\n\n        return hidden_states + residual\n\n\nclass ParallelMambaResidualLayerPipe(ParallelMambaResidualLayer):\n    \"\"\"Extends MambaResidualLayer to forward attention_mask through the pipeline. DeepSpeed requires this.\"\"\"\n\n    def forward(self, args):\n        assert (\n            len(args) == 2\n        ), \"MambaResidualLayerPipe expects 2 arguments - hidden_states and attention_mask\"\n        hidden_states, attention_mask = args\n        # we are returning just [hidden_states, mask]\n        return super().forward(hidden_states, attention_mask), attention_mask\n",
        "megatron/model/moe.py": "# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2023 MegaBlocks authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n\nfrom typing import Optional\n\nimport megablocks.ops\nimport numpy as np\nimport torch\n\nfrom megatron import mpu\nfrom megatron.mpu import get_expert_token_counts_for_rank\nfrom megatron.mpu import get_expert_tokens_for_rank\nfrom megatron.mpu import copy_to_expert_model_parallel_region\nfrom megatron.mpu import gather_from_expert_model_parallel_region\nfrom megatron.neox_arguments.arguments import NeoXArgs\n\nfrom .moe_mlp import ParallelGroupedLLaMAMLP, ParallelGroupedMLP\nfrom .router import TopKTokenChoiceRouter, SinkhornRouter\n\n\nclass ParallelDroplessMLP(torch.nn.Module):\n    \"\"\"\n    This class defines MoE expert computation, using tensor (model) parallel size as the expert parallel size\n\n    The implication of this parallelism decision is that the expert weights can only be sharded within a single node\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n        output_layer_init_method,\n    ):\n        \"\"\"\n\n        Bias is currently not supported\n        \"\"\"\n        super(ParallelDroplessMLP, self).__init__()\n\n        # Calculate the number of experts to allocate on this rank\n        world_size = mpu.get_model_parallel_world_size()\n        assert neox_args.moe_num_experts % world_size == 0\n        self.num_experts = neox_args.moe_num_experts\n        self.experts_per_rank = self.num_experts // world_size\n        self.top_k = neox_args.moe_top_k\n\n        # Calculate the number of bits needed to represent the expert indices\n        # so that we can pass it to radix sort\n        self.sort_end_bit = max(int(np.ceil(np.log2(self.num_experts))), 1)\n\n        # decide which parallel grouped MLP implementation to use\n        self.mlp = ParallelGroupedMLP(\n            neox_args=neox_args,\n            init_method=init_method,\n            output_layer_init_method=output_layer_init_method,\n        )\n\n    def indices_and_bins(self, top_expert: torch.Tensor):\n        # Sort the expert ids to produce the scatter/gather\n        # indices for the permutation.\n        #\n        # TODO(tgale): Is it worth doing this conversion to 32-bit\n        # prior? Could we place the `torch.max` operation to return\n        # 32-bit expert indices?\n        top_expert = top_expert.int()\n        bin_ids, indices = megablocks.ops.sort(top_expert, self.sort_end_bit)\n\n        # Histogram the expert ids to identify the number of\n        # tokens routed to each expert.\n        #\n        # TODO(tgale): Does the sorted data produce a more favorable\n        # data distribution for histogram? Or is the op parallelism\n        # worth more?\n        tokens_per_expert = megablocks.ops.histogram(top_expert, self.num_experts)\n\n        # Calculate the bin bounds for the sorted tokens.\n        bins = megablocks.ops.inclusive_cumsum(tokens_per_expert, 0)\n        bins = bins.view(1) if not len(bins.size()) else bins\n        return indices, bin_ids, bins, tokens_per_expert\n\n    def permute_and_compute(\n        self,\n        input_: torch.Tensor,\n        tokens_per_expert: torch.Tensor,\n        indices: torch.Tensor,\n        bin_ids: torch.Tensor,\n        expert_weights: torch.Tensor,\n        bins: torch.Tensor,\n        top_k: int,\n    ):\n        \"\"\"\n        grouped_permute_and_compute\n\n        torch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)\n\n        NOTE: Megablocks sets up all MLP tensors as column parallel and uses transposes on some of the grouped_gemm calls for the ops that would be row parallel. This seems to be fine and since we aren't using the underlying NeoX ColumnParallelLinear and RowParallelLinear classes, there doesn't seem to be a reason to change it...because that'd introduce a lot of additional complexity.\n\n        column parallel linear forward\n\n        ```python\n        def forward(self, input_):\n            if self.use_mup and self.mup_rescale_parameters:\n                input_ /= self.width_mult()\n            # Set up backprop all-reduce.\n            input_parallel = copy_to_model_parallel_region(input_)\n            # Matrix multiply.\n\n            bias = self.bias if not self.skip_bias_add else None\n            output_parallel = F.linear(input_parallel, self.weight, bias)\n            if self.gather_output:\n                # All-gather across the partitions.\n                output = gather_from_model_parallel_region(output_parallel)\n            else:\n                output = output_parallel\n            output_bias = self.bias if self.skip_bias_add else None\n            return output, output_bias\n        ```\n        \"\"\"\n        # Route the tokens for MoE computation.\n        ## stack (sl, bs, hs) into (sl * bs, hs)\n        input_ = input_.view(-1, input_.shape[-1])\n\n        ## repeat each token top_k times and shuffle tokens to group them by their respective experts\n        input_ = megablocks.ops.gather(input_, indices, bin_ids, bins, top_k)\n\n        # get tokens routed to this rank's experts only\n        input_parallel = copy_to_expert_model_parallel_region(input_, tokens_per_expert)\n\n        # get tokens_per_expert for this rank's experts only\n        # with torch.no_grad():\n        local_tokens_per_expert = get_expert_token_counts_for_rank(tokens_per_expert)\n        # if torch.cuda.current_device() == 0:\n        #     print(f\"{torch.cuda.current_device()}: local_tokens_per_expert {local_tokens_per_expert}, global tokens {tokens_per_expert}\")\n\n        # Perform the expert computation for this rank's experts\n        output_parallel = self.mlp(input_parallel, local_tokens_per_expert)\n\n        # all gather masked results from across Tensor parallel ranks here and cat them together\n        # this will replicate the calculation of each expert across all ranks\n        # NOTE: this combined all_gather and torch.cat operation is performed by gather_from_model_parallel_region(output_parallel)\n        # Unlike ColumnParallelLinear, it is nonsensical in the MoE world\n        # to optionally return the output_parallel result...we still have to scatter the tokens back to their original positions\n        output = gather_from_expert_model_parallel_region(\n            output_parallel,\n            tokens_per_expert,\n        )\n\n        # Un-route the data for the MoE output\n        return megablocks.ops.scatter(\n            output,\n            indices,\n            bin_ids,\n            expert_weights,\n            bins,\n            top_k,\n        )\n\n    def forward(self, x, expert_weights, expert_indices):\n        \"\"\"\n        grouped_forward_once\n\n            x: [sl, bs, hs]\n            expert_weights: [sl * bs, top-k]\n            expert_indices: [sl * bs, top-k]\n        \"\"\"\n        # save shape so we can re-shape the outputs later\n        in_shape = x.size()\n\n        # both are now (sl * bs * top_k)\n        expert_weights = expert_weights.flatten()\n        expert_indices = expert_indices.flatten()\n\n        with torch.no_grad():\n            indices, bin_ids, bins, tokens_per_expert = self.indices_and_bins(\n                expert_indices\n            )\n\n        x = self.permute_and_compute(\n            x,\n            tokens_per_expert,\n            indices,\n            bin_ids,\n            expert_weights,\n            bins,\n            self.top_k,\n        )\n\n        # restore input shape\n        x = x.view(in_shape)\n        return x\n\n\ndef cast_if_autocast_enabled(tensor: torch.Tensor):\n    if torch.is_autocast_enabled():\n        if tensor.device.type == \"cuda\":\n            dtype = torch.get_autocast_gpu_dtype()\n        elif tensor.device.type == \"cpu\":\n            dtype = torch.get_autocast_cpu_dtype()\n        else:\n            raise NotImplementedError()\n        return tensor.to(dtype=dtype)\n    return tensor\n\n\nclass ParallelDroplessMoE(torch.nn.Module):\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n        output_layer_init_method,\n    ):\n        super(ParallelDroplessMoE, self).__init__()\n\n        if neox_args.moe_router_type == \"sinkhorn\":\n            self.router = SinkhornRouter(\n                neox_args,\n                init_method,\n            )\n        elif neox_args.moe_router_type == \"topk\":\n            self.router = TopKTokenChoiceRouter(\n                neox_args,\n                init_method,\n            )\n        else:\n            raise ValueError(f\"Invalid MoE Router type {neox_args.moe_router_type}\")\n\n        self.experts = ParallelDroplessMLP(\n            neox_args,\n            init_method,\n            output_layer_init_method,\n        )\n\n    def forward(self, x):\n        # we expect inputs as (sl, bs, hs)\n        # neox provides inputs as torch.Size([2048, 4, 768])\n        # (sl, bs, hs)\n\n        # NOTE: If we're going to cast the activations to lower precision\n        # do it before we permute the tokens to save bandwidth\n        x = cast_if_autocast_enabled(x)\n\n        # Compute the expert scores and assignments\n        expert_weights, expert_indices = self.router(x)\n\n        # return value should be\n        return self.experts(x, expert_weights, expert_indices), None\n",
        "megatron/model/moe_mlp.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2023 MegaBlocks authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom megatron.model.activations import get_activation\n\nfrom megatron.mpu.layers import _initialize_affine_weight_gpu\nfrom megatron.mpu.initialize import get_model_parallel_world_size\nfrom megatron.mpu.utils import divide\n\nfrom megatron.neox_arguments.arguments import NeoXArgs\n\nfrom megablocks import grouped_gemm_util as gg\n\n\nclass ScaleGradient(torch.autograd.Function):\n    @staticmethod\n    @torch.cuda.amp.custom_fwd\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        return x\n\n    @staticmethod\n    @torch.cuda.amp.custom_bwd\n    def backward(ctx, grad):\n        return grad * ctx.scale, None\n\n\nscale_gradient = ScaleGradient.apply\n\n\nclass MemoryOptimizedParallelGroupedMLP(torch.autograd.Function):\n    \"\"\"GroupedMLP with manually scheduled memory reuse.\"\"\"\n\n    @staticmethod\n    @torch.cuda.amp.custom_fwd\n    def forward(ctx, x, w1, w2, batch_sizes, activation_fn):\n        # x: [m, k], w1: [n, k], w2: [n, k]\n        if not x.is_contiguous() or not w1.is_contiguous() or not w2.is_contiguous():\n            raise ValueError(\"Expected contiguous 'x', 'w1' and 'w2'.\")\n\n        # Layer 0: x @ w1.t().\n        sdd_out = gg.backend.gmm(x, w1, batch_sizes, trans_b=True)\n\n        # activation_fn\n        activation_fn_out = activation_fn(sdd_out)\n\n        # Layer 1: x @ w2.\n        dsd_out = gg.backend.gmm(activation_fn_out, w2, batch_sizes)\n\n        # NOTE: Save the input to the layer and the activation_fn input for\n        # gradient computation. We'll re-compute the activation_fn forward\n        # pass in the backward pass to avoid materializing another\n        # intermediate.\n        ctx.x_shape = x.shape\n        ctx.sdd_out_shape = sdd_out.shape\n        ctx.dtype = x.dtype\n        ctx.activation_fn = activation_fn\n        ctx.save_for_backward(w1, w2, batch_sizes, x, sdd_out)\n        return dsd_out\n\n    @staticmethod\n    @torch.cuda.amp.custom_bwd\n    def backward(ctx, ddsd_out):\n        if (\n            not ctx.needs_input_grad[0]\n            or not ctx.needs_input_grad[1]\n            or not ctx.needs_input_grad[2]\n        ):\n            raise ValueError(\"Expected all MLP inputs to need grad.\")\n\n        # Unpack saved tensors\n        dtype = ctx.dtype\n        saved_tensors = ctx.saved_tensors\n        w1, w2 = saved_tensors[:2]\n        batch_sizes = saved_tensors[2]\n        x = saved_tensors[3]\n        sdd_out = saved_tensors[4]\n\n        # Rematerialize activation_fn output.\n        activation_fn = ctx.activation_fn\n        with torch.set_grad_enabled(True):\n            sdd_out.requires_grad = True\n            activation_fn_out = activation_fn(sdd_out)\n            activation_grad_fn = activation_fn_out.backward\n\n        # Compute dw2 with recomputed activation_fn output.\n        dw2 = gg.backend.gmm(activation_fn_out, ddsd_out, batch_sizes, trans_a=True)\n\n        # Compute dactivation_fn_out.\n        #\n        # NOTE: We reuse the activation_fn_out allocation.\n        dactivation_fn_out = activation_fn_out\n        gg.backend.gmm(ddsd_out, w2, batch_sizes, trans_b=True, c=dactivation_fn_out)\n\n        # Compute dsdd_out.\n        #\n        # NOTE: This reuses the dactivation_fn_out allocation.\n        if activation_fn is DEFAULT_ACTIVATION_FN:\n            dsdd_out = gelu.gelu_backward_(dactivation_fn_out, sdd_out)\n        else:\n            assert activation_grad_fn is not None\n            activation_grad_fn(dactivation_fn_out)\n            dsdd_out = sdd_out.grad\n\n        # Compute dw1.\n        dw1 = gg.backend.gmm(dsdd_out, x, batch_sizes, trans_a=True)\n\n        # Compute dx.\n        #\n        # NOTE: This reuses the ddsd_out allocation.\n        gg.backend.gmm(dsdd_out, w1, batch_sizes, c=ddsd_out)\n        dx = ddsd_out\n        return dx, dw1, dw2, None, None\n\n\nmemory_optimized_grouped_mlp = MemoryOptimizedParallelGroupedMLP.apply\n\n\nclass ParallelGroupedMLP(torch.nn.Module):\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n        output_layer_init_method,\n        stride=1,\n        multiple_of=256,\n    ):\n        \"\"\"\n        Copied from SparseMLP\n        \"\"\"\n        super(ParallelGroupedMLP, self).__init__()\n\n        self.activation_func, self.activation_fn_is_gated = get_activation(neox_args)\n        self.activation_type = neox_args.activation\n\n        self.multiple_of = multiple_of\n\n        world_size = get_model_parallel_world_size()\n        self.num_experts = neox_args.moe_num_experts\n        self.experts_per_rank = divide(self.num_experts, world_size)\n\n        self.hidden_size = neox_args.hidden_size\n\n        # Allow custom intermediate size\n        if neox_args.intermediate_size is not None:\n            per_expert_ff_dim = neox_args.intermediate_size\n        # Otherwise, 4 x hidden size, padded to multiple of 256\n        else:\n            per_expert_ff_dim = 4 * self.hidden_size\n            per_expert_ff_dim = self.multiple_of * (\n                (per_expert_ff_dim + multiple_of - 1) // multiple_of\n            )\n\n        self.per_expert_ff_dim = per_expert_ff_dim\n        # number of rows per rank is the number of experts * ff dimension\n        self.num_rows_per_rank = self.experts_per_rank * per_expert_ff_dim\n\n        # input\n        self.w1 = torch.nn.Parameter(\n            torch.empty(\n                self.num_rows_per_rank,\n                self.hidden_size,\n                device=torch.cuda.current_device(),\n                dtype=neox_args.params_dtype,\n            )\n        )\n        _initialize_affine_weight_gpu(\n            self.w1, init_method, partition_dim=0, stride=stride\n        )\n\n        # output\n        self.w2 = torch.nn.Parameter(\n            torch.empty(\n                self.num_rows_per_rank,\n                self.hidden_size,\n                device=torch.cuda.current_device(),\n                dtype=neox_args.params_dtype,\n            )\n        )\n        _initialize_affine_weight_gpu(\n            self.w2, output_layer_init_method, partition_dim=0, stride=stride\n        )\n\n        # TODO: why do we need this? was in original megablocks code\n        self.gradient_scale = None\n        if world_size > 1:\n            self.gradient_scale = 1 / world_size\n\n    def scale_grad(self, w: torch.Tensor):\n        \"\"\"\n        Copied from SparseMLP\n        \"\"\"\n        if self.gradient_scale is None:\n            return w\n        return scale_gradient(w, self.gradient_scale)\n\n    def forward(self, x: torch.Tensor, tokens_per_expert: torch.Tensor):\n        grouped_gemm_batch_sizes = tokens_per_expert.cpu().to(torch.long)\n        w1, w2 = (self.scale_grad(self.w1), self.scale_grad(self.w2))\n\n        # Re-shape the weights for the grouped GEMMs\n        w1 = w1.view(self.experts_per_rank, -1, self.hidden_size)\n        w2 = w2.view(self.experts_per_rank, -1, self.hidden_size)\n\n        # Compute the MLP\n        x = gg.ops.gmm(x, w1, grouped_gemm_batch_sizes, trans_b=True)\n        x = self.activation_func(x)\n        return gg.ops.gmm(x, w2, grouped_gemm_batch_sizes)\n\n\nclass MemoryOptimizedParallelGroupedLLaMAMLP(torch.autograd.Function):\n    \"\"\"GroupedMLP with manually scheduled memory reuse.\"\"\"\n\n    @staticmethod\n    @torch.cuda.amp.custom_fwd\n    def forward(ctx, x, w1, w3, w2, batch_sizes, activation_fn):\n        # x: [m, k], w1: [n, k], w3: [n, k], w2: [n, k]\n        if (\n            not x.is_contiguous()\n            or not w1.is_contiguous()\n            or not w3.is_contiguous()\n            or not w2.is_contiguous()\n        ):\n            raise ValueError(\"Expected contiguous 'x', 'w1', 'w3' and 'w2'.\")\n\n        # Layer 0: x @ w1.t().\n        sdd_out = gg.backend.gmm(x, w1, batch_sizes, trans_b=True)\n        w3_out = gg.backend.gmm(x, w3, batch_sizes, trans_b=True)\n\n        # GeLU.\n        activation_fn_out = activation_fn(sdd_out) * w3_out\n\n        # Layer 1: x @ w2.\n        dsd_out = gg.backend.gmm(activation_fn_out, w2, batch_sizes)\n\n        # NOTE: Save the input to the layer and the activation_fn input for\n        # gradient computation. We'll re-compute the activation_fn forward\n        # pass in the backward pass to avoid materializing another\n        # intermediate.\n        ctx.x_shape = x.shape\n        ctx.sdd_out_shape = sdd_out.shape\n        ctx.dtype = x.dtype\n        ctx.activation_fn = activation_fn\n        ctx.save_for_backward(w1, w3, w2, batch_sizes, x, sdd_out, w3_out)\n        return dsd_out\n\n    @staticmethod\n    @torch.cuda.amp.custom_bwd\n    def backward(ctx, ddsd_out):\n        if (\n            not ctx.needs_input_grad[0]\n            or not ctx.needs_input_grad[1]\n            or not ctx.needs_input_grad[2]\n        ):\n            raise ValueError(\"Expected all MLP inputs to need grad.\")\n\n        # Unpack saved tensors\n        dtype = ctx.dtype\n        saved_tensors = ctx.saved_tensors\n        w1, w3, w2 = saved_tensors[:3]\n        batch_sizes = saved_tensors[3]\n        x = saved_tensors[4]\n        sdd_out, w3_out = saved_tensors[5:7]\n\n        # Rematerialize activation_fn output.\n        activation_fn = ctx.activation_fn\n        with torch.set_grad_enabled(True):\n            sdd_out.requires_grad = True\n            w3_out.requires_grad = True\n            activation_fn_out = activation_fn(sdd_out) * w3_out\n            activation_grad_fn = activation_fn_out.backward\n\n        # Compute dw2 with recomputed activation_fn output.\n        dw2 = gg.backend.gmm(activation_fn_out, ddsd_out, batch_sizes, trans_a=True)\n\n        # Compute dactivation_fn_out.\n        #\n        # NOTE: We reuse the activation_fn_out allocation.\n        dactivation_fn_out = activation_fn_out\n        gg.backend.gmm(ddsd_out, w2, batch_sizes, trans_b=True, c=dactivation_fn_out)\n\n        # Compute dsdd_out.\n        #\n        # NOTE: This reuses the dactivation_fn_out allocation.\n        assert activation_grad_fn is not None\n        activation_grad_fn(dactivation_fn_out)\n        dsdd_out = sdd_out.grad\n        dw3_out = w3_out.grad\n\n        # Compute dw1.\n        dw1 = gg.backend.gmm(dsdd_out, x, batch_sizes, trans_a=True)\n\n        # Compute dw3.\n        dw3 = gg.backend.gmm(dw3_out, x, batch_sizes, trans_a=True)\n\n        # Compute dx.\n        #\n        # NOTE: This reuses the ddsd_out allocation.\n        dx = ddsd_out\n        gg.backend.gmm(dsdd_out, w1, batch_sizes, c=dx)\n        dx += gg.backend.gmm(dw3_out, w3, batch_sizes)\n        return dx, dw1, dw3, dw2, None, None\n\n\nmemory_optimized_grouped_llama_mlp = MemoryOptimizedParallelGroupedLLaMAMLP.apply\n\n\nclass ParallelGroupedLLaMAMLP(torch.nn.Module):\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n        output_layer_init_method,\n        stride=1,\n        multiple_of=256,\n    ):\n        \"\"\"\n        Copied from SparseMLP\n        \"\"\"\n        super(ParallelGroupedLLaMAMLP, self).__init__()\n\n        self.activation_func, self.activation_fn_is_gated = get_activation(neox_args)\n        self.activation_type = neox_args.activation\n\n        self.multiple_of = multiple_of\n\n        world_size = get_model_parallel_world_size()\n        self.num_experts = neox_args.moe_num_experts\n        self.experts_per_rank = divide(self.num_experts, world_size)\n\n        self.hidden_size = neox_args.hidden_size\n\n        # Allow custom intermediate size\n        if neox_args.intermediate_size is not None:\n            per_expert_ff_dim = neox_args.intermediate_size\n        # Otherwise, 8/3 x hidden size, padded to multiple of 256\n        # TODO: why is this how we formulate it this way?\n        else:\n            per_expert_ff_dim = int(2 * neox_args.hidden_size * 4 / 3)\n            per_expert_ff_dim = self.multiple_of * (\n                (per_expert_ff_dim + multiple_of - 1) // multiple_of\n            )\n\n        self.per_expert_ff_dim = per_expert_ff_dim\n        # number of rows per rank is the number of experts * ff dimension per expert\n        self.num_rows_per_rank = self.experts_per_rank * per_expert_ff_dim\n\n        # input\n        self.w1 = torch.nn.Parameter(\n            torch.empty(\n                self.num_rows_per_rank,\n                self.hidden_size,\n                device=torch.cuda.current_device(),\n                dtype=neox_args.params_dtype,\n            )\n        )\n        _initialize_affine_weight_gpu(\n            self.w1, init_method, partition_dim=0, stride=stride\n        )\n\n        # gate\n        self.w3 = torch.nn.Parameter(\n            torch.empty(\n                self.num_rows_per_rank,\n                self.hidden_size,\n                device=torch.cuda.current_device(),\n                dtype=neox_args.params_dtype,\n            )\n        )\n        _initialize_affine_weight_gpu(\n            self.w3, init_method, partition_dim=0, stride=stride\n        )\n\n        # output\n        self.w2 = torch.nn.Parameter(\n            torch.empty(\n                self.num_rows_per_rank,\n                self.hidden_size,\n                device=torch.cuda.current_device(),\n                dtype=neox_args.params_dtype,\n            )\n        )\n        _initialize_affine_weight_gpu(\n            self.w2, output_layer_init_method, partition_dim=0, stride=stride\n        )\n\n        # TODO: why do we need this? was in original megablocks code\n        self.gradient_scale = None\n        if world_size > 1:\n            self.gradient_scale = 1 / world_size\n\n    def scale_grad(self, w: torch.Tensor):\n        \"\"\"\n        Copied from SparseMLP\n        \"\"\"\n        if self.gradient_scale is None:\n            return w\n        return scale_gradient(w, self.gradient_scale)\n\n    def forward(self, x: torch.Tensor, tokens_per_expert: torch.Tensor):\n        grouped_gemm_batch_sizes = tokens_per_expert.cpu().to(torch.long)\n        w1, w3, w2 = (\n            self.scale_grad(self.w1),\n            self.scale_grad(self.w3),\n            self.scale_grad(self.w2),\n        )\n\n        w1 = self.w1.view(self.experts_per_rank, -1, self.hidden_size)\n        w3 = w3.view(self.experts_per_rank, -1, self.hidden_size)\n\n        w2 = w2.view(self.experts_per_rank, -1, self.hidden_size)\n\n        # return memory_optimized_grouped_llama_mlp(\n        #     x,\n        #     w1,\n        #     w3,\n        #     w2,\n        #     grouped_gemm_batch_sizes,\n        #     self.activation_func\n        # )\n\n        llama_x_w1T = gg.ops.gmm(x, w1, grouped_gemm_batch_sizes, trans_b=True)\n\n        llama_x_w3T = gg.ops.gmm(x, w3, grouped_gemm_batch_sizes, trans_b=True)\n\n        llama_act_x_w1T = self.activation_func(llama_x_w1T)\n\n        # self.w2(self.activation_func(w1_out) * w3_out)\n        llama_mlp_out = gg.ops.gmm(\n            llama_act_x_w1T\n            * llama_x_w3T,  # activation results gated (element-wise) with w3\n            w2,  # w2\n            grouped_gemm_batch_sizes,  # batch_sizes\n        )\n\n        return llama_mlp_out\n",
        "megatron/model/norms.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch.nn import LayerNorm as LayerNorm\n\n\ndef get_norm(neox_args):\n    if neox_args.norm == \"rmsnorm\":\n        eps = neox_args.rms_norm_epsilon\n        if neox_args.rmsnorm_fusion:\n            from .fused_layer_norm import MixedFusedRMSNorm\n\n            norm = MixedFusedRMSNorm\n        else:\n            norm = RMSNorm\n    elif neox_args.norm == \"layernorm\":\n        eps = neox_args.layernorm_epsilon\n        if neox_args.layernorm_fusion:\n            from .fused_layer_norm import MixedFusedLayerNorm\n\n            norm = MixedFusedLayerNorm\n        else:\n            norm = LayerNorm\n    elif neox_args.norm == \"non_parametric_layernorm\":\n        eps = neox_args.layernorm_epsilon\n        if neox_args.layernorm_fusion:\n            raise ValueError(\n                f\"neox_args.layernorm_fusion not supported for non_parametric_layernorm\"\n            )\n        else:\n            norm = NonParametricLayernorm\n    elif neox_args.norm == \"scalenorm\":\n        eps = neox_args.scalenorm_epsilon\n        norm = ScaleNorm\n    elif neox_args.norm == \"te_rmsnorm\":\n        from .transformer_engine import TERMSNorm\n\n        norm = TERMSNorm\n        eps = neox_args.rms_norm_epsilon\n    elif neox_args.norm == \"te_layernorm\":\n        from .transformer_engine import TELayerNorm\n\n        norm = TELayerNorm\n        eps = neox_args.layernorm_epsilon\n    else:\n        raise ValueError(f\"norm {neox_args.norm} not recognized\")\n    return norm, eps\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim, p=-1.0, eps=1e-8, bias=False):\n        \"\"\"\n            Root Mean Square Layer Normalization\n        :param dim: model size\n        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n        :param eps:  epsilon value, default 1e-8\n        :param bias: whether use bias term for RMSNorm, disabled by\n            default because RMSNorm doesn't enforce re-centering invariance.\n        \"\"\"\n        super(RMSNorm, self).__init__()\n\n        self.eps = eps\n        self.d = dim\n        self.p = p\n        self.bias = bias\n\n        self.scale = torch.nn.Parameter(torch.ones(dim))\n        self.register_parameter(\"scale\", self.scale)\n\n        if self.bias:\n            self.offset = torch.nn.Parameter(torch.zeros(dim))\n            self.register_parameter(\"offset\", self.offset)\n\n    def forward(self, x):\n        dtype = x.dtype\n        if self.p >= 0.0 and self.p <= 1.0:\n            partial_size = int(self.d * self.p)\n            x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n\n        x = x.to(torch.float32)\n        variance = x.pow(2).mean(-1, keepdim=True)\n        x_normed = x * torch.rsqrt(variance + self.eps)\n\n        if self.bias:\n            return self.scale * x_normed + self.offset\n\n        return (self.scale * x_normed).to(dtype)\n\n\nclass ScaleNorm(torch.nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.g = torch.nn.Parameter(torch.ones(1))\n        self.eps = eps\n\n    def forward(self, x):\n        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n        return x / n * self.g\n\n\nclass NonParametricLayernorm(torch.nn.LayerNorm):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__(\n            normalized_shape=dim, eps=eps, elementwise_affine=False, bias=False\n        )\n",
        "megatron/model/positional_embeddings.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport math\n\n\nclass SinusoidalPositionalEmbedding(torch.nn.Module):\n    def __init__(self, dim, base=10000, precision=torch.half):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        self.precision = precision\n\n    def forward(self, x, seq_dim=1):\n        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        if self.precision == torch.bfloat16:\n            sinusoid_inp = sinusoid_inp.float()\n        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n        if self.precision == torch.bfloat16:\n            sin, cos = sin.bfloat16(), cos.bfloat16()\n        emb = torch.cat((sin, cos), dim=-1)\n        return emb[None, :, :]\n\n\nclass RotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self, dim, max_seq_len, base=10000, precision=torch.half, save_inv_freqs=False\n    ):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n        self.precision = precision\n        self.max_seq_len = max_seq_len\n        self.base = base\n        self.dim = dim\n\n        # precompute cos_cached, sin_cached in fp32\n        cos_cached, sin_cached, inv_freq = self._prepare_cache(\n            max_seq_len, precision, base\n        )\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n        self.cos_cached = cos_cached\n        self.sin_cached = sin_cached\n\n    def _prepare_cache(self, seq_len, precision, base):\n        # precompute cos_cached, sin_cached in fp32\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n\n        t = torch.arange(seq_len).type_as(inv_freq)\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n\n        self.emb = emb.reshape(emb.size(0), 1, 1, emb.size(1))\n\n        cos_cached = emb.cos()[:, None, None, :]\n        sin_cached = emb.sin()[:, None, None, :]\n\n        return (\n            cos_cached.to(precision),\n            sin_cached.to(precision),\n            inv_freq.to(precision),\n        )\n\n    def get_emb(self):\n        return self.emb.to(self.precision).cuda()\n\n    def forward(self, x, seq_dim=0, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[seq_dim]\n\n        assert seq_len <= self.max_seq_len\n\n        if seq_len != self.max_seq_len:\n            # y, z, _ = self._prepare_cache(seq_len, self.precision, self.base)\n            return (\n                self.cos_cached[:seq_len, ...].to(x.device),\n                self.sin_cached[:seq_len, ...].to(x.device),\n            )\n        else:\n            return self.cos_cached.to(x.device), self.sin_cached.to(x.device)\n\n\n# rotary pos emb helpers:\n\n\ndef rotate_half(x):\n    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n    return torch.cat(\n        (-x2, x1), dim=x1.ndim - 1\n    )  # dim=-1 triggers a bug in earlier torch versions\n\n\n@torch.jit.script\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    cos, sin = (\n        cos[offset : q.shape[0] + offset, ...],\n        sin[offset : q.shape[0] + offset, ...],\n    )\n    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n\n\ndef apply_rotary_pos_emb_torch(\n    q, k, cos, sin, offset: int = 0\n):  # jitting fails with bf16\n    cos, sin = (\n        cos[offset : q.shape[0] + offset, ...],\n        sin[offset : q.shape[0] + offset, ...],\n    )\n    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n\n\nclass AliBi(torch.nn.Module):\n    def __init__(self, num_heads, mp_size=1, mp_rank=1):\n        super().__init__()\n        # megatron splits across heads, so we need to make sure each\n        # head receives the correct matrix\n        assert mp_size <= num_heads and mp_rank <= mp_size\n        self.mp_size = mp_size\n        self.mp_rank = mp_rank\n        self.num_heads = num_heads\n        self.slice_size = num_heads // mp_size\n        self.cached_matrix = None\n        self.cached_seq_len = None\n        slopes = torch.Tensor(self._get_slopes(num_heads))[\n            mp_rank * self.slice_size : (mp_rank + 1) * self.slice_size\n        ]\n        self.register_buffer(\"slopes\", slopes)\n\n    def _get_slopes(self, n):\n        \"\"\"\n        Get slopes for Alibi positional embedding\n        n : int = number of heads.\n        For best performance, restrict n to a power of 2.\n        \"\"\"\n\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio**i for i in range(n)]\n\n        if math.log2(n).is_integer():\n            return get_slopes_power_of_2(n)\n        else:\n            closest_power_of_2 = 2 ** math.floor(math.log2(n))\n            return (\n                get_slopes_power_of_2(closest_power_of_2)\n                + self._get_slopes(2 * closest_power_of_2)[0::2][\n                    : n - closest_power_of_2\n                ]\n            )\n\n    def bias(self, seq_len_q, seq_len_k, device, dtype):\n        # [b, np, sq, sk]\n        # seq_len_q = x.shape[-2]\n        # seq_len_k = x.shape[-1]\n\n        # Initialize the AliBi matrix to match the first provided key length; grow it exponentially\n        # afterwards if longer inputs are provided. This is important for inference, where we will\n        # encounter progressively longer samples; it should have no effect at training time.\n        if self.cached_seq_len is not None and self.cached_seq_len >= seq_len_k:\n            a = self.cached_matrix\n        else:\n            target_seq_len = (\n                seq_len_k if self.cached_seq_len is None else self.cached_seq_len * 4\n            )\n            a = -torch.tril(\n                torch.arange(target_seq_len)\n                .view(target_seq_len, 1)\n                .repeat(1, target_seq_len)\n                + torch.arange(0, -target_seq_len, -1)\n            )\n            a = a.to(device).to(dtype)\n            slopes = self.slopes.to(a.device).to(a.dtype)\n            a = a * slopes.view(self.slopes.shape[0], 1, 1)\n            self.cached_seq_len = target_seq_len\n            self.cached_matrix = a\n\n        # If the AliBi matrix is larger than the key length, clip it.\n        if self.cached_seq_len > seq_len_k:\n            a = self.cached_matrix[:, :seq_len_k, :seq_len_k]\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            a = a[:, seq_len_k - 1, :].view(\n                a.shape[0], 1, a.shape[2]\n            )  # seq_len_k - 1 points to the last token index in the current inference batch.\n\n        return a\n\n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n\n        # Initialize the AliBi matrix to match the first provided key length; grow it exponentially\n        # afterwards if longer inputs are provided. This is important for inference, where we will\n        # encounter progressively longer samples; it should have no effect at training time.\n        if self.cached_seq_len is not None and self.cached_seq_len >= seq_len_k:\n            a = self.cached_matrix\n        else:\n            target_seq_len = (\n                seq_len_k if self.cached_seq_len is None else self.cached_seq_len * 4\n            )\n            a = -torch.tril(\n                torch.arange(target_seq_len)\n                .view(target_seq_len, 1)\n                .repeat(1, target_seq_len)\n                + torch.arange(0, -target_seq_len, -1)\n            )\n            a = a.to(x.device).to(x.dtype)\n            slopes = self.slopes.to(a.device).to(a.dtype)\n            a = a * slopes.view(self.slopes.shape[0], 1, 1)\n            self.cached_seq_len = target_seq_len\n            self.cached_matrix = a\n\n        # If the AliBi matrix is larger than the key length, clip it.\n        if self.cached_seq_len > seq_len_k:\n            a = self.cached_matrix[:, :seq_len_k, :seq_len_k]\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            a = a[:, seq_len_k - 1, :].view(\n                a.shape[0], 1, a.shape[2]\n            )  # seq_len_k - 1 points to the last token index in the current inference batch.\n\n        return x + a\n",
        "megatron/model/router.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2023 MegaBlocks authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom megatron.neox_arguments.arguments import NeoXArgs\nfrom megatron.mpu import get_model_parallel_group, get_model_parallel_rank\n\n\nclass SinkhornRouter(torch.nn.Module):\n    # TODO: reduce precision on expert_indices? it looks like it's currently int64\n    # TODO: how do we ensure that all copies of the router get the same\n    # initializations and stay in sync over time? Or is this handled by RNG seeding?\n\n    ### Sinkhorn\n\n    # - https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/moe_utils.py\n    # - https://github.com/fanshiqing/grouped_gemm\n    #     - NVIDIA forked original implementation and is using this in Megatron Core now\n    # - https://github.com/NVIDIA/Megatron-LM/blob/cafda9529d9956578014d4cb89b69b741702b514/megatron/core/transformer/moe/router.py#L215: this his how megatron actually does its router forward pass\n\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n    ):\n        super().__init__()\n        self.top_k = neox_args.moe_top_k\n        self.params_dtype = neox_args.params_dtype\n\n        # expert parallel group rank, for purposes of deciding if I should compute the router or wait for the result to be broadcast to me\n        self.expert_parallel_group = get_model_parallel_group()\n        self.expert_parallel_rank = get_model_parallel_rank()\n\n        # Sinkhorn router parameters.\n        #\n        # NOTE: This weight matrix is not parallelized with expert tensor\n        # parallelism. Each device needs the entire router weight matrix\n        # so that it can route its batch of data correctly.\n        self.layer = torch.nn.Linear(\n            neox_args.hidden_size,\n            neox_args.moe_num_experts,\n            bias=False,\n            dtype=neox_args.params_dtype,\n            device=torch.cuda.current_device(),\n        )\n        init_method(self.layer.weight)\n\n    def sinkhorn(self, cost: torch.Tensor, tol: float = 0.0001, max_iter=3):\n        \"\"\"Sinkhorn based MoE routing function\"\"\"\n        cost = torch.exp(cost)\n        d0 = torch.ones(cost.size(0), device=cost.device, dtype=cost.dtype)\n        d1 = 1 / (cost.size(1) * torch.sum(cost, 0))\n\n        eps = 0.00000001\n        error = 1e9\n        d1_old = d1\n        for iteration in range(max_iter):\n            d0 = (1 / d0.size(0)) * 1 / (torch.sum(d1 * cost, 1) + eps)\n            d1 = (1 / d1.size(0)) * 1 / (torch.sum(d0.unsqueeze(1) * cost, 0) + eps)\n            error = torch.mean(torch.abs(d1_old - d1))\n            d1_old = d1\n            if error > tol:\n                break\n        return d1 * cost * d0.unsqueeze(1)\n\n    def sinkhorn_load_balancing(self, logits: torch.Tensor):\n        \"\"\"Apply sinkhorn routing to the logits tensor.\n\n        Args:\n            logits (torch.Tensor): The logits tensor, as (bs * sl, hidden_size)\n\n        Returns:\n            torch.Tensor: The logits tensor after applying sinkhorn routing.\n        \"\"\"\n\n        def _sinkhorn_activation(logits):\n            if self.top_k == 1:\n                logits = torch.sigmoid(logits)\n            else:  # k > 1\n                logits = torch.softmax(logits, dim=-1, dtype=torch.float32).type_as(\n                    logits\n                )\n            return logits\n\n        # assert self.config.moe_aux_loss_coeff == 0, \"Sinkhorn routing does not support aux loss.\"\n        if self.training:\n            with torch.no_grad():\n                norm_logits = self.sinkhorn(\n                    logits.to(dtype=torch.float32)\n                )  # explicit fp32 conversion for stability\n                _, indices = torch.topk(norm_logits, k=self.top_k, dim=1)\n            logits = _sinkhorn_activation(logits)\n            scores = torch.gather(logits, 1, indices)\n        # at inference, just top_k it...sinkhorn algorithm doesn't support autoregressive generation\n        else:\n            logits = _sinkhorn_activation(logits)\n            scores, indices = torch.topk(logits, k=self.top_k, dim=1)\n        return scores, indices\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the Sinkhorn Router.\n\n        Only compute on rank 0 in the expert parallel group and broadcast to everyone else to avoid weird states where things get out of sync.\n\n        Args:\n            x (torch.Tensor): Input tensor to be routed.\n                (sl, bs, hs)\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Tuple containing\n                - expert_weights (sl * bs, top_k): Weights assigned to the selected experts\n                - expert_indices (sl * bs, top_k): Indices of the selected experts\n        \"\"\"\n        if self.expert_parallel_rank == 0:\n            # x.view shape: (sl * bs, hs)...every token as a row\n            # router_logits (float) shape: (sl * bs, num_experts)...expert rankings for every token\n            router_logits = self.layer(x.view(-1, x.shape[-1]))\n\n            # expert_weights (float) shape: (sl * bs, top_k)...value(s) from scores corresponding to the top_k experts\n            # expert_indices (int) shape: (sl * bs, top_k)...index(indices) from scores corresponding to the top_k experts\n            expert_weights, expert_indices = self.sinkhorn_load_balancing(router_logits)\n\n            # broadcast the routing result to all ranks\n            expert_weights_broadcast = torch.distributed.broadcast(\n                expert_weights,\n                src=torch.distributed.get_global_rank(self.expert_parallel_group, 0),\n                group=self.expert_parallel_group,\n                async_op=True,\n            )\n            expert_indices_broadcast = torch.distributed.broadcast(\n                expert_indices,\n                src=torch.distributed.get_global_rank(self.expert_parallel_group, 0),\n                group=self.expert_parallel_group,\n                async_op=True,\n            )\n        else:\n            # sl * bs\n            num_rows = x.view(-1, x.shape[-1]).shape[0]\n            expert_weights = torch.empty(\n                num_rows,\n                self.top_k,\n                device=torch.cuda.current_device(),\n                dtype=self.params_dtype,\n            )\n            expert_indices = torch.empty(\n                num_rows,\n                self.top_k,\n                device=torch.cuda.current_device(),\n                dtype=torch.int64,\n            )\n\n            expert_weights_broadcast = torch.distributed.broadcast(\n                expert_weights,\n                src=torch.distributed.get_global_rank(self.expert_parallel_group, 0),\n                group=self.expert_parallel_group,\n                async_op=True,\n            )\n            expert_indices_broadcast = torch.distributed.broadcast(\n                expert_indices,\n                src=torch.distributed.get_global_rank(self.expert_parallel_group, 0),\n                group=self.expert_parallel_group,\n                async_op=True,\n            )\n\n        # since both are executing asynchronously, it doesn't matter which one\n        # we wait for first\n        expert_weights_broadcast.wait()\n        expert_indices_broadcast.wait()\n\n        return expert_weights, expert_indices\n\n\nclass TopKTokenChoiceRouter(torch.nn.Module):\n    # TODO: how do we ensure that all copies of the router get the same\n    # initializations and stay in sync over time? Or is this handled by RNG seeding?\n\n    def __init__(\n        self,\n        neox_args: NeoXArgs,\n        init_method,\n    ):\n        super().__init__()\n        self.jitter_eps = neox_args.moe_jitter_eps\n        self.top_k = neox_args.moe_top_k\n\n        # Learned router parameters.\n        #\n        # NOTE: This weight matrix is not parallelized with expert tensor\n        # parallelism. Each device needs the entire router weight matrix\n        # so that it can route its batch of data correctly.\n        self.layer = torch.nn.Linear(\n            neox_args.hidden_size,\n            neox_args.moe_num_experts,\n            bias=False,\n            dtype=neox_args.params_dtype,\n            device=torch.cuda.current_device(),\n        )\n        init_method(self.layer.weight)\n\n    def jitter(self, x):\n        \"\"\"\n        Apply jittering to the input tensor during training.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Jittered input tensor.\n        \"\"\"\n        low = 1.0 - self.args.moe_jitter_eps\n        high = 1.0 + self.args.moe_jitter_eps\n        noise = torch.rand(x.size(), dtype=x.dtype, device=x.device)\n        return low + noise * (high - low)\n\n    def _top_k(self, scores):\n        \"\"\"\n        Select the top-k experts based on input scores.\n\n        Args:\n            scores (torch.Tensor): Input scores from the router.\n                (sl * bs, num_experts)\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Tuple containing expert weightings and indices of selected experts.\n\n\n        \"\"\"\n        if self.top_k == 1:\n            return scores.max(dim=-1, keepdim=True)\n        return torch.topk(scores, self.top_k, dim=-1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the Learned Router.\n\n        Args:\n            x (torch.Tensor): Input tensor to be routed.\n                (sl, bs, hs)\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Tuple containing\n                - expert_weights (sl * bs, top_k): Weights assigned to the selected experts\n                - expert_indices (sl * bs, top_k): Indices of the selected experts\n        \"\"\"\n        if self.training and self.jitter_eps is not None:\n            x = x * self.jitter(x)\n\n        # x.view shape: (sl * bs, hs)...every token as a row\n        # scores (float) shape: (sl * bs, num_experts)...expert rankings for every token\n        scores = self.layer(x.view(-1, x.shape[-1])).softmax(dim=-1)\n\n        # expert_weights (float) shape: (sl * bs, top_k)...value(s) from scores corresponding to the top_k experts\n        # expert_indices (int) shape: (sl * bs, top_k)...index(indices) from scores corresponding to the top_k experts\n        expert_weights, expert_indices = self._top_k(scores)\n        # expert_weights probability mass won't add up to 1 because we took\n        # the topk scores from the softmax\n        # TODO: placeholder for moe_normalize_expert_weights if necessary\n\n        return expert_weights, expert_indices\n",
        "megatron/model/rwkv/__init__.py": "",
        "megatron/model/rwkv/v6/__init__.py": "from .rwkv import RWKVResidualLayerPipe, RWKVResidualLayer\n",
        "megatron/model/rwkv/v6/rwkv.py": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\n\nimport os, math, gc, importlib\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.cpp_extension import load\n\n\nclass WKV(torch.autograd.Function):\n    \"\"\"\n    WKV block, using cuda kernel.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, B, T, C, H, r, k, v, w, u):\n        with torch.no_grad():\n            assert r.dtype == torch.bfloat16\n            assert k.dtype == torch.bfloat16\n            assert v.dtype == torch.bfloat16\n            assert w.dtype == torch.bfloat16\n            assert u.dtype == torch.bfloat16\n            ctx.B = B\n            ctx.T = T\n            ctx.C = C\n            ctx.H = H\n            assert r.is_contiguous()\n            assert k.is_contiguous()\n            assert v.is_contiguous()\n            assert w.is_contiguous()\n            assert u.is_contiguous()\n            ew = (-torch.exp(w.float())).contiguous()\n            ctx.save_for_backward(r, k, v, ew, u)\n            y = torch.empty(\n                (B, T, C),\n                device=r.device,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            wkv_cuda.forward(B, T, C, H, r, k, v, ew, u, y)\n            return y\n\n    @staticmethod\n    def backward(ctx, gy):\n        with torch.no_grad():\n            assert gy.dtype == torch.bfloat16\n            B = ctx.B\n            T = ctx.T\n            C = ctx.C\n            H = ctx.H\n            assert gy.is_contiguous()\n            r, k, v, ew, u = ctx.saved_tensors\n            gr = torch.empty(\n                (B, T, C),\n                device=gy.device,\n                requires_grad=False,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            gk = torch.empty(\n                (B, T, C),\n                device=gy.device,\n                requires_grad=False,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            gv = torch.empty(\n                (B, T, C),\n                device=gy.device,\n                requires_grad=False,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            gw = torch.empty(\n                (B, T, C),\n                device=gy.device,\n                requires_grad=False,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            gu = torch.empty(\n                (B, C),\n                device=gy.device,\n                requires_grad=False,\n                dtype=torch.bfloat16,\n                memory_format=torch.contiguous_format,\n            )  # .uniform_(-100, 100)\n            wkv_cuda.backward(B, T, C, H, r, k, v, ew, u, gy, gr, gk, gv, gw, gu)\n            gu = torch.sum(gu, 0).view(H, C // H)\n            return (None, None, None, None, gr, gk, gv, gw, gu)\n\n\ndef RUN_CUDA_RWKV(B, T, C, H, r, k, v, w, u):\n    return WKV.apply(B, T, C, H, r, k, v, w, u)\n\n\n# RWKV6 time mix\nclass RWKV_TimeMix(nn.Module):\n    \"\"\"\n    Time Mixing Layer\n    The RWKV substitute for attention.\n    TODO: fix jit compiling.\n    \"\"\"\n\n    def __init__(self, neox_args, layer_number):\n        super().__init__()\n        self.neox_args = neox_args\n        self.layer_number = layer_number\n\n        with torch.no_grad():\n            ratio_0_to_1 = layer_number / (neox_args.num_layers - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_number / neox_args.num_layers)  # 1 to ~0\n            ddd = torch.ones(1, 1, neox_args.hidden_size)\n            for i in range(neox_args.hidden_size):\n                ddd[0, 0, i] = i / neox_args.hidden_size\n\n            # fancy time_mix\n            self.time_maa_x = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n            self.time_maa_w = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n            self.time_maa_v = nn.Parameter(\n                1.0 - (torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            )\n            self.time_maa_r = nn.Parameter(\n                1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0)\n            )\n            self.time_maa_g = nn.Parameter(\n                1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0)\n            )\n\n            TIME_MIX_EXTRA_DIM = 32  # generate TIME_MIX for w,k,v,r,g\n            self.time_maa_w1 = nn.Parameter(\n                torch.zeros(neox_args.hidden_size, TIME_MIX_EXTRA_DIM * 5).uniform_(\n                    -1e-4, 1e-4\n                )\n            )\n            self.time_maa_w2 = nn.Parameter(\n                torch.zeros(5, TIME_MIX_EXTRA_DIM, neox_args.hidden_size).uniform_(\n                    -1e-4, 1e-4\n                )\n            )\n\n            # fancy time_decay\n            decay_speed = torch.ones(neox_args.dim_att)\n            for n in range(neox_args.dim_att):\n                decay_speed[n] = -6 + 5 * (n / (neox_args.dim_att - 1)) ** (\n                    0.7 + 1.3 * ratio_0_to_1\n                )\n            self.time_decay = nn.Parameter(decay_speed.reshape(1, 1, neox_args.dim_att))\n\n            TIME_DECAY_EXTRA_DIM = 64\n            self.time_decay_w1 = nn.Parameter(\n                torch.zeros(neox_args.hidden_size, TIME_DECAY_EXTRA_DIM).uniform_(\n                    -1e-4, 1e-4\n                )\n            )\n            self.time_decay_w2 = nn.Parameter(\n                torch.zeros(TIME_DECAY_EXTRA_DIM, neox_args.dim_att).uniform_(\n                    -1e-4, 1e-4\n                )\n            )\n\n            tmp = torch.zeros(neox_args.dim_att)\n            for n in range(neox_args.dim_att):\n                zigzag = ((n + 1) % 3 - 1) * 0.1\n                tmp[n] = ratio_0_to_1 * (1 - (n / (neox_args.dim_att - 1))) + zigzag\n\n            self.time_faaaa = nn.Parameter(\n                tmp.reshape(neox_args.num_attention_heads, neox_args.head_size)\n            )\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.receptance = nn.Linear(\n            neox_args.hidden_size, neox_args.dim_att, bias=False\n        )\n        self.key = nn.Linear(neox_args.hidden_size, neox_args.dim_att, bias=False)\n\n        self.value = nn.Linear(neox_args.hidden_size, neox_args.dim_att, bias=False)\n        self.output = nn.Linear(neox_args.dim_att, neox_args.hidden_size, bias=False)\n        self.gate = nn.Linear(neox_args.hidden_size, neox_args.dim_att, bias=False)\n        self.ln_x = nn.GroupNorm(\n            neox_args.num_attention_heads, neox_args.dim_att, eps=(1e-5) * (8**2)\n        )\n\n    def jit_func(self, x):\n        B, T, C = x.size()\n\n        xx = self.time_shift(x) - x\n\n        xxx = x + xx * self.time_maa_x\n        xxx = torch.tanh(xxx @ self.time_maa_w1).view(B * T, 5, -1).transpose(0, 1)\n        xxx = torch.bmm(xxx, self.time_maa_w2).view(5, B, T, -1)\n        mw, mk, mv, mr, mg = xxx.unbind(dim=0)\n\n        xw = x + xx * (self.time_maa_w + mw)\n        xk = x + xx * (self.time_maa_k + mk)\n        xv = x + xx * (self.time_maa_v + mv)\n        xr = x + xx * (self.time_maa_r + mr)\n        xg = x + xx * (self.time_maa_g + mg)\n\n        r = self.receptance(xr)\n        k = self.key(xk)\n        v = self.value(xv)\n        g = F.silu(self.gate(xg))\n\n        ww = torch.tanh(xw @ self.time_decay_w1) @ self.time_decay_w2\n        w = self.time_decay + ww\n\n        return r, k, v, g, w\n\n    def jit_func_2(self, x, g):\n        B, T, C = x.size()\n        x = x.view(B * T, C)\n\n        x = self.ln_x(x).view(B, T, C)\n        x = self.output(x * g)\n        return x\n\n    def forward(self, x):\n        B, T, C = x.size()\n        H = self.neox_args.num_attention_heads\n\n        r, k, v, g, w = self.jit_func(x)\n        x = RUN_CUDA_RWKV(B, T, C, H, r, k, v, w, u=self.time_faaaa)\n\n        return self.jit_func_2(x, g)\n\n\nclass RWKV_ChannelMix(nn.Module):\n    \"\"\"\n    Channel Mix layer. The ffn in RWKV\n    \"\"\"\n\n    def __init__(self, neox_args, layer_number):\n        super().__init__()\n        self.neox_args = neox_args\n        self.layer_number = layer_number\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n\n        with torch.no_grad():  # fancy init of time_mix\n            ratio_1_to_almost0 = 1.0 - (layer_number / neox_args.num_layers)  # 1 to ~0\n            ddd = torch.ones(1, 1, neox_args.hidden_size)\n            for i in range(neox_args.hidden_size):\n                ddd[0, 0, i] = i / neox_args.hidden_size\n            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n            self.time_maa_r = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n\n        self.key = nn.Linear(neox_args.hidden_size, neox_args.ffn_dim, bias=False)\n        self.receptance = nn.Linear(\n            neox_args.hidden_size, neox_args.hidden_size, bias=False\n        )\n        self.value = nn.Linear(neox_args.ffn_dim, neox_args.hidden_size, bias=False)\n\n    def forward(self, x):\n        xx = self.time_shift(x) - x\n        xk = x + xx * self.time_maa_k\n        xr = x + xx * self.time_maa_r\n\n        k = self.key(xk)\n        k = torch.relu(k) ** 2\n        kv = self.value(k)\n        return torch.sigmoid(self.receptance(xr)) * kv\n\n\nclass RWKVResidualLayer(nn.Module):\n    \"\"\"\n    RWKV layer definition\n    \"\"\"\n\n    def __init__(self, neox_args, layer_number):\n        super().__init__()\n        self.neox_args = neox_args\n        self.layer_number = layer_number\n        self.fp16 = neox_args.precision == \"fp16\"\n        self.bf16 = neox_args.precision == \"bfloat16\"\n        assert (\n            neox_args.intermediate_size == None or neox_args.expansion_factor == None\n        ), \"Must pass either the absolute intermediate size or the relative expansion factor for the mamba projections\"\n        if not hasattr(neox_args, \"dim_att\"):\n            neox_args.dim_att = neox_args.hidden_size\n        if neox_args.intermediate_size:\n            neox_args.ffn_dim = neox_args.intermediate_size\n        else:\n            self.expand = (\n                neox_args.expansion_factor if neox_args.expansion_factor else 3.5\n            )\n            neox_args.ffn_dim = int(self.expand * neox_args.hidden_size)\n            # Make hidden size 3.5x by default. Round to nearest multiple of 32 until we add hdim rounding logic\n        neox_args.ffn_dim = int(neox_args.ffn_dim // 32 * 32)\n        assert neox_args.hidden_size % 32 == 0\n        assert neox_args.dim_att % 32 == 0\n        assert neox_args.ffn_dim % 32 == 0\n        self.neox_args.head_size = neox_args.dim_att // neox_args.num_attention_heads\n        self.head_size = self.neox_args.head_size\n        self.num_attention_heads = neox_args.num_attention_heads\n        assert neox_args.dim_att % self.num_attention_heads == 0\n\n        if neox_args.attention_dropout > 0:\n            self.drop0 = nn.Dropout(p=neox_args.attention_dropout)\n\n        self.ln1 = nn.LayerNorm(neox_args.hidden_size)\n        self.ln2 = nn.LayerNorm(neox_args.hidden_size)\n\n        self.att = RWKV_TimeMix(neox_args, layer_number)\n\n        self.ffn = RWKV_ChannelMix(neox_args, layer_number)\n\n        if neox_args.attention_dropout > 0:\n            self.drop0 = nn.Dropout(p=neox_args.attention_dropout)\n        if neox_args.hidden_dropout > 0:\n            self.drop1 = nn.Dropout(p=neox_args.hidden_dropout)\n\n        if layer_number == 0:\n            global wkv_cuda\n            \"\"\"\n            Load cuda kernel at runtime. The kernel uses run time variables to build, ideally it should not.\n            \"\"\"\n            wkv_cuda = load(\n                name=\"wkv6\",\n                sources=[\n                    \"megatron/model/rwkv/v6/cuda/wkv6_op.cpp\",\n                    f\"megatron/model/rwkv/v6/cuda/wkv6_cuda.cu\",\n                ],\n                verbose=True,\n                extra_cuda_cflags=[\n                    \"-res-usage\",\n                    \"--use_fast_math\",\n                    \"-O3\",\n                    \"-Xptxas -O3\",\n                    \"--extra-device-vectorization\",\n                    f\"-D_N_={self.neox_args.head_size}\",\n                    f\"-D_T_={self.neox_args.seq_length}\",\n                ],\n            )\n\n    def forward(self, x):\n        neox_args = self.neox_args\n        B, T, C = x.size()\n        if self.layer_number == 0:\n            x = self.ln1(x)\n\n        if self.neox_args.attention_dropout == 0:\n            x = x + self.att(self.ln1(x))\n        else:\n            x = self.drop0(x + self.att(self.ln1(x)))\n\n        if self.neox_args.hidden_dropout == 0:\n            x = x + self.ffn(self.ln2(x))\n        else:\n            x = self.drop1(x + self.ffn(self.ln2(x)))\n\n        return x\n\n\nclass RWKVResidualLayerPipe(RWKVResidualLayer):\n    \"\"\"\n    RWKV Pipeline Layer\n    \"\"\"\n\n    def forward(self, args):\n        assert len(args) == 2\n        hidden_states, mask = args\n        neox_args = self.neox_args\n        return super().forward(hidden_states), mask\n",
        "megatron/model/transformer.py": "# # Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Transformer.\"\"\"\n\nimport math\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom pkg_resources import packaging\nfrom importlib.metadata import version\n\nfrom .norms import get_norm\nfrom megatron import mpu\nfrom megatron.model.fused_softmax import FusedScaleMaskSoftmax\nfrom megatron.model.activations import get_activation\nfrom megatron.model.utils import exists, get_fusion_type\nfrom megatron.model.positional_embeddings import (\n    RotaryEmbedding,\n    apply_rotary_pos_emb_torch,\n    apply_rotary_pos_emb,\n    AliBi,\n)\nfrom megatron.model.fused_rope import (\n    FusedRoPEFunc,\n    fused_apply_rotary_pos_emb_cached,\n)\nfrom megatron.model.fused_bias_dropout import (\n    get_bias_dropout_add,\n    bias_dropout_add_fused_train,\n    bias_dropout_add_fused_inference,\n)\nfrom megatron.model.utils import configure_sparse_attention\n\ntry:\n    from flash_attn.ops.activations import swiglu\nexcept ImportError:\n    swiglu = None\n\nfrom .utils import get_parallel_linear\n\n# flags required to enable jit fusion kernels\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_override_can_fuse_on_cpu(True)\ntorch._C._jit_override_can_fuse_on_gpu(True)\n\n\"\"\" We use the following notation throughout this file:\n     h: hidden size\n     n: number of attention heads\n     kv: number of key or value heads\n     p: number of model parallel partitions\n     np: n/p\n     kvp: kv/p\n     hp: h/p\n     hn: h/n\n     b: batch size\n     s: sequence length\n     l: number of layers\n    Transformer takes input of size [s, b, h] and returns a\n    tensor of the same size. We use the following arguments:\n        hyperparameters: transformer hyperparameters\n        attention_mask_func: a function that takes `unmasked-attention-scores`\n            with size [b, np, s, s] and an `attention-mask` and will apply\n            the masking. The function should return a masked score of the\n            same size [b, np, s, s].\n               masked-attention-scores = attention_mask_func(\n                                     unmasked-attention-scores, attention-mask)\n\"\"\"\n\n\nclass ParallelMLP(nn.Module):\n    \"\"\"MLP.\n\n    MLP will take the input with h hidden state, project it to 4*h\n    hidden dimension, perform nonlinear transformation, and project the\n    state back into h hidden dimension. At the end, dropout is also\n    applied.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        init_method,\n        output_layer_init_method,\n        parallel_output=False,\n        multiple_of=256,\n    ):\n        super().__init__()\n        assert (\n            neox_args.intermediate_size == None or neox_args.expansion_factor == None\n        ), \"Must pass either the absolute intermediate size or the relative expansion factor for the mamba projections\"\n\n        self.activation_func, self.is_gated = get_activation(neox_args)\n        self.activation_type = neox_args.activation\n        self.bias_gelu_fusion = neox_args.bias_gelu_fusion\n        self.multiple_of = multiple_of\n\n        ColumnParallelLinear, RowParallelLinear = get_parallel_linear(neox_args)\n\n        if neox_args.intermediate_size:\n            ffn_dim = neox_args.intermediate_size\n        elif neox_args.expansion_factor:\n            ffn_dim = int(neox_args.expansion_factor * neox_args.hidden_size)\n        else:\n            # 4h is default for ffn_dim\n            ffn_dim = 4 * neox_args.hidden_size\n        ffn_dim_in = ffn_dim\n        if self.is_gated:\n            # set activation function to be gated implementation\n            self.activation_func = Gated_Activation(\n                self.activation_func,\n                (swiglu is not None)\n                and (neox_args.activation == \"swiglu\")\n                and neox_args.use_flashattn_swiglu,\n            )\n            # auto scale so gated activations has equal parameters\n            ffn_dim = int(ffn_dim * 2 / 3)\n            ffn_dim_in = ffn_dim // 2\n        # set multiple\n        ffn_dim = int(\n            (2 * self.multiple_of)\n            * ((ffn_dim + (2 * multiple_of) - 1) // (2 * multiple_of))\n        )\n        ffn_dim_in = int(\n            self.multiple_of * ((ffn_dim_in + multiple_of - 1) // multiple_of)\n        )\n        self.linear1 = ColumnParallelLinear(\n            neox_args=neox_args,\n            input_size=neox_args.hidden_size,\n            output_size=ffn_dim,\n            gather_output=False,\n            init_method=init_method,\n            skip_bias_add=True,\n            bias=neox_args.use_bias_in_mlp,\n        )\n        # Project back to h.\n        self.linear2 = RowParallelLinear(\n            neox_args=neox_args,\n            input_size=ffn_dim_in,\n            output_size=neox_args.hidden_size,\n            input_is_parallel=True,\n            init_method=output_layer_init_method,\n            parallel_output=parallel_output,\n            skip_bias_add=True,\n            bias=neox_args.use_bias_in_mlp,\n        )\n\n    def forward(self, hidden_states):\n        # [s, b, intermediate_size]\n        intermediate_parallel, bias_parallel = self.linear1(hidden_states)\n        if self.is_gated or (self.activation_type == \"gelu\" and self.bias_gelu_fusion):\n            intermediate_parallel = self.activation_func(\n                intermediate_parallel, bias_parallel\n            )\n        else:\n            intermediate_parallel = self.activation_func(\n                intermediate_parallel + bias_parallel\n            )\n\n        # [s, b, h]\n        output, output_bias = self.linear2(intermediate_parallel)\n        return output, output_bias\n\n\nclass Gated_Activation(torch.nn.Module):\n    def __init__(self, activation_func, use_swiglu=False):\n        super().__init__()\n        self.activation_func = activation_func\n        self.use_swiglu = use_swiglu\n\n    def forward(self, x, bias=None):\n        x, gate = x.chunk(2, dim=-1)\n        if bias is not None:\n            bias_1, bias_2 = bias.chunk(2, dim=-1)\n            x = x + bias_1\n            gate = gate + bias_2\n        if not self.use_swiglu:\n            intermediate_parallel = self.activation_func(gate)\n            return intermediate_parallel * x\n        else:\n            return swiglu(gate, x)\n\n\nclass ParallelLinear(nn.Module):\n    \"\"\"\n    A Parallel Linear Layer transforming the transformer outputs from hidden_size -> vocab_size\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        parallel_output=True,\n        init_method=nn.init.xavier_normal_,\n        is_last_layer=False,\n    ):\n        super().__init__()\n\n        ColumnParallelLinear, RowParallelLinear = get_parallel_linear(neox_args)\n\n        self.is_rm = neox_args.train_impl == \"rm\"\n        parallelism = neox_args.output_layer_parallelism if not self.is_rm else \"row\"\n        if parallelism == \"column\":\n            self.final_linear = ColumnParallelLinear(\n                neox_args=neox_args,\n                input_size=neox_args.hidden_size,\n                output_size=neox_args.padded_vocab_size,\n                bias=False,\n                init_method=init_method,\n                gather_output=not parallel_output,\n                skip_bias_add=False,\n                mup_rescale_parameters=is_last_layer,  # rescale params only called if neox_args.use_mup = True, despite it not being included here\n                seq_dim=1,  # important: must mark that this layer receives shape [b, s, h] not [s, b, h] and so Seq. Parallel comms must gather along dim=1 rather than dim=0\n            )\n        else:\n            if not self.is_rm:\n                print(\n                    'ERROR: Output layer parallelism over the hidden dim is currently broken (https://github.com/EleutherAI/gpt-neox/issues/905). Please run with output_layer_parallelism = \"column\" until this issue is fixed.'\n                )\n                exit()\n                # self.final_linear = mpu.RowParallelLinear(\n                #     neox_args=neox_args,\n                #     input_size=neox_args.hidden_size,\n                #     output_size=neox_args.padded_vocab_size,\n                #     bias=False,\n                #     input_is_parallel=False,\n                #     init_method=init_method,\n                #     parallel_output=parallel_output,\n                #     skip_bias_add=False,\n                #     mup_rescale_parameters=is_last_layer,  # only called if neox_args.use_mup = True, despite it not being included here\n                # )\n            else:  # Not using cross entropy loss for RMs\n                self.rm_linear = RowParallelLinear(\n                    neox_args=neox_args,\n                    input_size=neox_args.hidden_size,\n                    output_size=1,\n                    bias=False,\n                    input_is_parallel=False,\n                    init_method=init_method,\n                    parallel_output=False,\n                    skip_bias_add=False,\n                    mup_rescale_parameters=is_last_layer,  # only called if neox_args.use_mup = True, despite it not being included here\n                )\n\n    def forward(self, hidden_states):\n        if not self.is_rm:\n            return self.final_linear(hidden_states)\n        else:\n            return self.rm_linear(hidden_states)\n\n\nclass ParallelSelfAttention(nn.Module):\n    \"\"\"Parallel self-attention layer abstract class.\n\n    Self-attention layer takes input with size [b, s, h]\n    and returns output of the same size.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        attention_mask_func,\n        init_method,\n        output_layer_init_method,\n        layer_number,\n        rpe=None,\n        rotary=False,\n        use_cache=False,\n        parallel_output=False,\n    ):\n        super().__init__()\n\n        ColumnParallelLinear, RowParallelLinear = get_parallel_linear(neox_args)\n\n        self.fp16 = neox_args.precision == \"fp16\"\n        self.bf16 = neox_args.precision == \"bfloat16\"\n        self.attention_mask_func = attention_mask_func\n        self.apply_query_key_layer_scaling = neox_args.apply_query_key_layer_scaling\n        self.use_cache = use_cache\n        self.attention_softmax_in_fp32 = neox_args.attention_softmax_in_fp32\n        if self.apply_query_key_layer_scaling:\n            self.attention_softmax_in_fp32 = True\n        self.layer_number = layer_number\n        # Per attention head and per partition values.\n        world_size = mpu.get_model_parallel_world_size()\n        self.hidden_size_per_partition = mpu.divide(neox_args.hidden_size, world_size)\n        self.hidden_size_per_attention_head = mpu.divide(\n            neox_args.hidden_size, neox_args.num_attention_heads\n        )\n        self.num_attention_heads_per_partition = mpu.divide(\n            neox_args.num_attention_heads, world_size\n        )\n        self.pos_emb = neox_args.pos_emb\n\n        self.use_qk_layernorm = neox_args.use_qk_layernorm\n        if self.use_qk_layernorm:\n            norm, eps = get_norm(neox_args)\n            self.qk_layernorm = norm(\n                [\n                    self.num_attention_heads_per_partition,\n                    self.hidden_size_per_attention_head,\n                ],\n                eps=eps,\n            )\n\n        self.sliding_window_width = neox_args.sliding_window_width\n\n        if (\n            not neox_args.num_kv_heads\n            or neox_args.num_kv_heads == neox_args.num_attention_heads\n        ):\n            self.gqa = False\n        else:\n            self.gqa = True\n        if self.gqa:\n            self.num_kv_heads_per_partition = mpu.divide(\n                neox_args.num_kv_heads, world_size\n            )  # we do not yet clone KV heads in MQA across TP ranks...\n            self.kv_hidden_size = (\n                neox_args.num_kv_heads * self.hidden_size_per_attention_head\n            )  # how large the total hidden dim for each of K and V is\n        else:\n            self.num_kv_heads_per_partition = self.num_attention_heads_per_partition\n            self.kv_hidden_size = neox_args.hidden_size\n\n        if not self.gqa:\n            # Strided linear layer.\n            self.query_key_value = ColumnParallelLinear(\n                neox_args=neox_args,\n                input_size=neox_args.hidden_size,\n                output_size=3 * neox_args.hidden_size,\n                gather_output=False,\n                init_method=init_method,\n                bias=neox_args.use_bias_in_attn_linear,\n            )\n        else:\n            # QKV proj is smaller if we are using GQA / MQA\n            self.query_key_value = ColumnParallelLinear(\n                neox_args=neox_args,\n                input_size=neox_args.hidden_size,\n                output_size=neox_args.hidden_size + 2 * self.kv_hidden_size,\n                gather_output=False,\n                init_method=init_method,\n                bias=neox_args.use_bias_in_attn_linear,\n            )\n\n        coeff = None\n        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n        if self.apply_query_key_layer_scaling:\n            coeff = max(1, self.layer_number)\n            self.norm_factor *= coeff\n\n        if neox_args.use_mup:\n            self.norm_factor = self.hidden_size_per_attention_head\n\n        self.rpe = rpe\n\n        if self.pos_emb == \"alibi\":\n            self.alibi_embed = AliBi(\n                neox_args.num_attention_heads,\n                neox_args.model_parallel_size,\n                mpu.get_model_parallel_rank(),\n            )\n\n        # TODO: this arg shouldn't need to be passed in - get from neox_args\n        if rotary:\n            if neox_args.rotary_pct == 1:\n                self.rotary_ndims = None\n            else:\n                assert neox_args.rotary_pct < 1\n                self.rotary_ndims = int(\n                    self.hidden_size_per_attention_head * neox_args.rotary_pct\n                )\n            dim = (\n                self.rotary_ndims\n                if self.rotary_ndims is not None\n                else self.hidden_size_per_attention_head\n            )\n            self.rotary_emb = RotaryEmbedding(\n                dim,\n                base=neox_args.rotary_emb_base,\n                max_seq_len=neox_args.seq_length,\n                precision=neox_args.params_dtype,\n                save_inv_freqs=neox_args.rotary_save_freqs_buffer,\n            )\n        else:\n            self.rotary_emb = None\n\n        self.rope_fusion = neox_args.rope_fusion\n        self.attention_type = neox_args.attention_config[layer_number]\n        self.use_flash_attention = self.attention_type == \"flash\"\n        self.use_triton = (\n            self.use_flash_attention\n            and self.pos_emb == \"alibi\"\n            and (\n                not packaging.version.Version(version(\"flash-attn\"))\n                >= packaging.version.Version(\"2.4.0.post1\")\n            )\n        )\n        self.sparse = self.attention_type not in (\"global\", \"flash\")\n\n        if self.gqa:\n            assert not self.sparse\n\n        if self.sparse:\n            self.sparse_attn = configure_sparse_attention(\n                neox_args,\n                self.attention_type,\n                self.num_attention_heads_per_partition,\n                mpu=mpu,\n            )\n        else:\n            if self.use_flash_attention:\n                # we now use Flash Attention 2's provided interface.\n                # TODO: we no longer need to use flash_triton_fn since flash cuda supports alibi.\n                # consider adding OpenAI's more recent Flash-2 Triton kernel in future\n                # from https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py\n                from flash_attn.flash_attn_interface import (\n                    flash_attn_func,\n                    flash_attn_varlen_func,\n                )\n                from flash_attn.flash_attn_triton import (\n                    flash_attn_func as flash_attn_unpadded_unpacked_func_triton,\n                )\n\n                self.flash_triton_fn = flash_attn_unpadded_unpacked_func_triton\n                self.flash_qkv_fn = flash_attn_func\n                self.flash_varlen_qkv_fn = flash_attn_varlen_func\n            else:\n                self.scale_mask_softmax = FusedScaleMaskSoftmax(\n                    input_in_fp16=self.fp16,\n                    input_in_bf16=self.bf16,\n                    fusion_type=get_fusion_type(neox_args),\n                    mask_func=self.attention_mask_func,\n                    softmax_in_fp32=self.attention_softmax_in_fp32,\n                    scale=coeff,\n                )\n\n            # Dropout. Note that for a single iteration, this layer will generate\n            # different outputs on different number of parallel partitions but\n            # on average it should not be partition dependent.\n            self.dropout_p = neox_args.attention_dropout\n            self.attention_dropout = nn.Dropout(self.dropout_p)\n\n        # Output.\n        self.dense = RowParallelLinear(\n            neox_args=neox_args,\n            input_size=neox_args.hidden_size,\n            output_size=neox_args.hidden_size,\n            input_is_parallel=True,\n            init_method=output_layer_init_method,\n            skip_bias_add=True,\n            parallel_output=parallel_output,\n            bias=neox_args.use_bias_in_attn_linear,\n        )\n\n    def attention(\n        self, query_layer, key_layer, value_layer, layer_past, attention_mask\n    ):\n        # ===================================\n        # Raw attention scores. [b, np, s, s]\n        # ===================================\n\n        # [b, np, sq, sk]\n        output_size = (\n            query_layer.size(1),\n            query_layer.size(2),\n            query_layer.size(0),\n            key_layer.size(0),\n        )\n        # [sq, b, np, hn] -> [sq, b * np, hn]\n        query_layer = query_layer.view(\n            output_size[2], output_size[0] * output_size[1], -1\n        )\n        key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)\n        # preallocating result tensor: [b * np, sq, sk]\n        matmul_result = torch.empty(\n            output_size[0] * output_size[1],\n            output_size[2],\n            output_size[3],\n            dtype=query_layer.dtype,\n            device=torch.cuda.current_device(),\n        )\n\n        # Raw attention scores. [b * np, sq, sk]\n        matmul_result = torch.baddbmm(\n            matmul_result,\n            query_layer.transpose(0, 1),  # [b * np, sq, hn]\n            key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n            beta=0.0,\n            alpha=(1.0 / self.norm_factor),\n        )\n\n        # change view to [b, np, sq, sk]\n        attention_scores = matmul_result.view(*output_size)\n        # ==================================================\n        # Update attention mask for inference. [b, np, sq, sk]\n        # ==================================================\n\n        if self.use_cache:\n            with torch.no_grad():\n                attention_mask = attention_mask[\n                    ..., : attention_scores.size(3), : attention_scores.size(3)\n                ]\n\n        # ===========================\n        # Attention probs and dropout\n        # ===========================\n\n        if exists(self.rpe):\n            rpe = self.rpe(query_layer.size(0), key_layer.size(0))\n            attention_scores += rpe  # [1, np, sq, sk]\n\n        if self.pos_emb == \"alibi\":\n            attention_scores = self.alibi_embed(attention_scores)\n\n        # attention scores and attention mask [b, np, sq, sk]\n        attention_probs = self.scale_mask_softmax(attention_scores, attention_mask)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        with mpu.get_cuda_rng_tracker().fork():\n            attention_probs = self.attention_dropout(attention_probs)\n\n        # =========================\n        # Context layer. [sq, b, hp]\n        # =========================\n\n        # value_layer -> context layer.\n        # [sk, b, np, hn] --> [b, np, sq, hn]\n\n        # context layer shape: [b, np, sq, hn]\n        output_size = (\n            value_layer.size(1),\n            value_layer.size(2),\n            query_layer.size(0),\n            value_layer.size(3),\n        )\n\n        # change view [sk, b * np, hn]\n        value_layer = value_layer.view(\n            value_layer.size(0), output_size[0] * output_size[1], -1\n        )\n\n        # change view [b * np, sq, sk]\n        attention_probs = attention_probs.view(\n            output_size[0] * output_size[1], output_size[2], -1\n        )\n\n        # matmul: [b * np, sq, hn]\n        context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n\n        # change view [b, np, sq, hn]\n        context_layer = context_layer.view(*output_size)\n        return context_layer\n\n    def flash_attention(self, query_layer, key_layer, value_layer):\n        # [b, np, sq, sk]\n        output_size = (\n            query_layer.size(1),\n            query_layer.size(2),\n            query_layer.size(0),\n            key_layer.size(0),\n        )\n\n        if self.use_flash_attention and not self.use_triton:\n\n            # [sk, b, np, hn] -> [b, sk, np, hn] -> [b * sk, 1, np, hn]\n            key_layer = key_layer.transpose(0, 1).reshape(\n                output_size[0], output_size[3], self.num_kv_heads_per_partition, -1\n            )\n            value_layer = value_layer.transpose(0, 1).reshape(\n                output_size[0], output_size[3], self.num_kv_heads_per_partition, -1\n            )\n\n            # [sq, b, np, hn] -> [b, sq, np, hn]\n            query_layer = query_layer.transpose(0, 1).reshape(\n                output_size[0], output_size[2], output_size[1], -1\n            )\n\n            # only pass in window_size or alibi_slopes kwarg\n            # if we use Sliding Window Attention / AliBi.\n            # Flash attn defaults to (-1,-1), or\n            # does not have this kwarg prior to v2.3.0\n            extra_kwargs = (\n                {\"window_size\": (self.sliding_window_width, -1)}\n                if self.sliding_window_width is not None\n                else {}\n            )\n            if self.pos_emb == \"alibi\":\n                extra_kwargs[\"alibi_slopes\"] = self.alibi_embed.slopes.to(\n                    query_layer.device\n                ).to(torch.float32)\n\n            if not self.training:\n                batch_size = output_size[0]\n                max_seqlen_q = output_size[2]\n                max_seqlen_k = output_size[3]\n\n                cu_seqlens_q = torch.arange(\n                    0,\n                    (batch_size + 1) * max_seqlen_q,\n                    step=max_seqlen_q,\n                    dtype=torch.int32,\n                    device=query_layer.device,\n                )\n\n                cu_seqlens_k = torch.arange(\n                    0,\n                    (batch_size + 1) * max_seqlen_k,\n                    step=max_seqlen_k,\n                    dtype=torch.int32,\n                    device=key_layer.device,\n                )\n\n                q_shape = query_layer.shape\n                k_shape = key_layer.shape\n                v_shape = value_layer.shape\n                is_causal = max_seqlen_q == max_seqlen_k\n                output = self.flash_varlen_qkv_fn(\n                    query_layer.reshape(\n                        (q_shape[0] * q_shape[1], q_shape[2], q_shape[3])\n                    ),\n                    key_layer.reshape(\n                        (k_shape[0] * k_shape[1], k_shape[2], k_shape[3])\n                    ),\n                    value_layer.reshape(\n                        (v_shape[0] * v_shape[1], v_shape[2], v_shape[3])\n                    ),\n                    cu_seqlens_q,\n                    cu_seqlens_k,\n                    max_seqlen_q,\n                    max_seqlen_k,\n                    softmax_scale=None,\n                    causal=is_causal,\n                    **extra_kwargs,\n                )\n                output = output.reshape(q_shape)\n            else:\n                output = self.flash_qkv_fn(\n                    query_layer,\n                    key_layer,\n                    value_layer,\n                    self.dropout_p if self.training else 0.0,\n                    softmax_scale=None,\n                    causal=True,\n                    **extra_kwargs,\n                )\n\n            matmul_result = output\n            # [b, sq, np, hn] -> [b, np, sq, hn]\n            matmul_result = matmul_result.transpose(1, 2)\n\n        else:\n            # we still use Triton if using AliBi with flash-attn<2.4.0.post1.\n\n            # [sq, b, np, hn] -> [b, sq, np, hn]\n            sq = query_layer.size(0)\n            b = query_layer.size(1)\n            sk = key_layer.size(0)\n\n            query_layer = query_layer.transpose(0, 1)\n            key_layer = key_layer.transpose(0, 1)\n            value_layer = value_layer.transpose(0, 1)\n\n            bias = self.alibi_embed.bias(sq, sk, query_layer.device, query_layer.dtype)\n            bias = bias.unsqueeze(0).tile((b, 1, 1, 1))\n\n            matmul_result = self.flash_triton_fn(\n                query_layer, key_layer, value_layer, bias=bias, causal=True\n            )\n            matmul_result = matmul_result.transpose(1, 2)\n\n        return matmul_result\n\n    def sparse_attention(self, query_layer, key_layer, value_layer, attention_mask):\n        # TODO: sparse attn dropout?\n        # TODO: pad to block size\n        # shape of q/k/v is [sq, b, np, hn] and needs to be transposed to [b, np, sq, hn]\n        query_layer, key_layer, value_layer = map(\n            lambda t: t.permute(1, 2, 0, 3).contiguous(),\n            (query_layer, key_layer, value_layer),\n        )\n        # output shape [b, np(heads), sq, hn]\n        attn_mask = attention_mask.to(query_layer.dtype) * -10000\n        if exists(self.rpe):\n            rpe = self.rpe(query_layer.size(0), key_layer.size(0))\n        else:\n            rpe = None\n        attn_scores = self.sparse_attn(\n            query_layer, key_layer, value_layer, attn_mask=attn_mask, rpe=rpe\n        )\n        # apply dropout\n        if self.training:\n            attn_scores = self.attention_dropout(attn_scores)\n        return attn_scores\n\n    def gqa_project(self, hidden_states, attention_mask, layer_past=None):\n        # QKV projection and separation into separate Q/K/V layers for GQA,\n        # where KV projections may be smaller than Q projection.\n        # the logic for this is explained in comments of this function\n        # detailing the intermediate sizes of tensors at each reshape.\n\n        # pass through projection: [sq, b, h] --> [sq, b, ((np + 2 * kvp) * hn)]\n        mixed_x_layer, _ = self.query_key_value(hidden_states)\n\n        # split the last dim, so that the first (q head * head dim) dimensions go to Q,\n        # the last smaller 2 * (kv head * head dim) each divided between K and V separately\n        split_sizes = (\n            self.num_attention_heads_per_partition\n            * self.hidden_size_per_attention_head,\n            self.num_kv_heads_per_partition * self.hidden_size_per_attention_head,\n            self.num_kv_heads_per_partition * self.hidden_size_per_attention_head,\n        )\n\n        # [sq, b, ((np + 2 * kvp) * hn)] --> 1 x [sq, b, np * hn] , 2 x [sq, b, kvp * hn]\n        (query_layer, key_layer, value_layer) = [\n            x.contiguous()\n            for x in torch.split(\n                mixed_x_layer,\n                split_sizes,\n                dim=mixed_x_layer.dim() - 1,\n            )\n        ]\n\n        # reshape Q to proper output shape (last dim = correct full \"real\" head size again)\n        # [sq, b, np * hn] --> [sq, b, np, hn]\n        new_query_shape = (\n            query_layer.size(0),\n            query_layer.size(1),\n            self.num_attention_heads_per_partition,\n            self.hidden_size_per_attention_head,\n        )\n\n        query_layer = query_layer.view(*new_query_shape)\n\n        # reshape K/V to proper output shape (last dim = correct full \"real\" head size again)\n        # 2 x [sq, b, np, (hn * (kvp / np))] --> 2 x [sq, b, kvp, hn]\n        new_kv_shape = (\n            key_layer.size(0),\n            key_layer.size(1),\n            self.num_kv_heads_per_partition,\n            self.hidden_size_per_attention_head,\n        )\n\n        key_layer = key_layer.view(*new_kv_shape)\n\n        value_layer = value_layer.view(*new_kv_shape)\n\n        # if not using Flash attention, we repeat K/V heads to match Q head counts\n        if not self.use_flash_attention:\n            key_layer = torch.repeat_interleave(\n                key_layer,\n                repeats=int(\n                    self.num_attention_heads_per_partition\n                    // self.num_kv_heads_per_partition\n                ),\n                dim=2,\n            )\n            value_layer = torch.repeat_interleave(\n                value_layer,\n                repeats=int(\n                    self.num_attention_heads_per_partition\n                    // self.num_kv_heads_per_partition\n                ),\n                dim=2,\n            )\n\n        return query_layer, key_layer, value_layer\n\n    def forward(self, hidden_states, attention_mask, layer_past=None):\n\n        # hidden_states: [sq, b, h]\n\n        # =====================\n        # Query, Key, and Value\n        # =====================\n        if not self.gqa:\n            # QKV projection for MHA.\n\n            # Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]\n            mixed_x_layer, _ = self.query_key_value(hidden_states)\n            # [sq, b, (np * 3 * hn)] --> [sq, b, np, 3 * hn]\n            new_tensor_shape = mixed_x_layer.size()[:-1] + (\n                self.num_attention_heads_per_partition,\n                3 * self.hidden_size_per_attention_head,\n            )\n            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\n            # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]\n            (query_layer, key_layer, value_layer) = mpu.split_tensor_along_last_dim(\n                mixed_x_layer, 3\n            )\n        else:\n            # Grouped Query Attention (GQA) - specific logic for performing QKV proj\n            # and separating out Q, K, and V outputs.\n\n            # output shapes: 1 x [sq, b, np, hn], 2 x [sq, b, kvp, hn] if using flash\n            query_layer, key_layer, value_layer = self.gqa_project(\n                hidden_states, attention_mask, layer_past=layer_past\n            )\n        # QK Normalization https://arxiv.org/abs/2302.05442\n        if self.use_qk_layernorm:\n            query_layer = self.qk_layernorm(query_layer)\n            key_layer = self.qk_layernorm(key_layer)\n\n        if exists(self.rotary_emb):\n            if exists(self.rotary_ndims):\n                # partial rotary\n                query_rot, query_pass = (\n                    query_layer[..., : self.rotary_ndims],\n                    query_layer[..., self.rotary_ndims :],\n                )\n                key_rot, key_pass = (\n                    key_layer[..., : self.rotary_ndims],\n                    key_layer[..., self.rotary_ndims :],\n                )\n            else:\n                # full rotary\n                query_rot, key_rot = query_layer, key_layer\n\n            seq_len = key_layer.shape[0]\n            offset = 0\n            if exists(layer_past) and layer_past.numel() > 0:\n                offset = layer_past[0].shape[0]\n                seq_len += offset\n            cos, sin = self.rotary_emb(value_layer, seq_len=seq_len)\n            if self.rope_fusion:\n                query_layer, key_layer = (\n                    fused_apply_rotary_pos_emb_cached(rot, cos, sin)\n                    for rot in [query_rot, key_rot]\n                )\n            else:\n                if self.bf16:\n                    apply_rotary_fn = apply_rotary_pos_emb_torch\n                else:\n                    apply_rotary_fn = apply_rotary_pos_emb\n                query_layer, key_layer = apply_rotary_fn(\n                    query_rot, key_rot, cos, sin, offset=offset\n                )\n\n            if exists(self.rotary_ndims):\n                query_layer = torch.cat((query_layer, query_pass), dim=-1)\n                key_layer = torch.cat((key_layer, key_pass), dim=-1)\n\n        # ==================================\n        # Cache key and value for inference\n        # ==================================\n\n        if exists(layer_past) and layer_past.numel() > 0:\n            past_key, past_value = layer_past\n            key_layer = torch.cat((past_key.type_as(key_layer), key_layer), dim=0)\n            value_layer = torch.cat(\n                (past_value.type_as(value_layer), value_layer), dim=0\n            )\n\n        if self.use_cache:\n            present = torch.stack((key_layer, value_layer))\n\n        if self.use_flash_attention:\n            context_layer = self.flash_attention(query_layer, key_layer, value_layer)\n        elif not self.sparse:\n            context_layer = self.attention(\n                query_layer, key_layer, value_layer, layer_past, attention_mask\n            )\n        else:\n            context_layer = self.sparse_attention(\n                query_layer, key_layer, value_layer, attention_mask\n            )\n\n        # [b, np, sq, hn] --> [sq, b, np, hn]\n        context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n\n        # [sq, b, np, hn] --> [sq, b, hp]\n        new_context_layer_shape = context_layer.size()[:-2] + (\n            self.hidden_size_per_partition,\n        )\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output, bias = self.dense(context_layer)\n\n        if self.use_cache:\n            output = [output, present]\n\n        return output, bias\n\n\nclass ParallelTransformerLayer(nn.Module):\n    \"\"\"A single transformer layer.\n\n    Transformer layer takes input with size [b, s, h] and returns an\n    output of the same size.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        attention_mask_func,\n        init_method,\n        output_layer_init_method,\n        layer_number,\n        rpe=None,\n        rotary=False,\n        use_cache=False,\n    ):\n\n        super().__init__()\n        self.layer_number = layer_number\n        self.neox_args = neox_args\n\n        norm, eps = get_norm(neox_args)\n\n        # Layernorm on the input data.\n        self.input_layernorm = norm(neox_args.hidden_size, eps=eps)\n        self.use_cache = use_cache\n\n        self.hidden_dropout = neox_args.hidden_dropout\n        self.bias_dropout_fusion = neox_args.bias_dropout_fusion\n        self.gpt_j_residual = neox_args.gpt_j_residual\n        self.gpt_j_tied = neox_args.gpt_j_tied\n        self.activation = neox_args.activation\n        self.num_experts = (\n            neox_args.moe_num_experts\n            if layer_number % neox_args.moe_expert_interval == 0\n            else 1\n        )\n\n        if self.num_experts > 1:\n            from megatron.model.moe import ParallelDroplessMoE\n\n        if self.gpt_j_residual:\n            # GPT-J style layers allow us to defer the reduction of results across TP ranks until the end of the two sublayers.\n            # the reduction we use is a simple allreduce for pure Tensor Parallel,\n            # but needs to be a reduce-scatter when using Megatron-style Sequence Parallel (LN sharding.)\n            self.reduce = (\n                mpu.mappings.reduce_from_model_parallel_region\n                if not neox_args.sequence_parallel\n                else mpu.mappings.reduce_scatter_to_sequence_parallel_region\n            )\n\n        # Self attention.\n        if neox_args.te_mha or neox_args.te_fp8_mha:\n            from megatron.model.transformer_engine import TEMultiheadAttention\n\n            self.attention = TEMultiheadAttention(\n                neox_args=neox_args,\n                attention_mask_func=attention_mask_func,\n                init_method=init_method,\n                output_layer_init_method=output_layer_init_method,\n                layer_number=layer_number,\n                rpe=rpe,\n                use_cache=self.use_cache,\n                rotary=rotary,\n                parallel_output=self.gpt_j_residual,\n            )\n\n        else:\n            self.attention = ParallelSelfAttention(\n                neox_args=neox_args,\n                attention_mask_func=attention_mask_func,\n                init_method=init_method,\n                output_layer_init_method=output_layer_init_method,\n                layer_number=layer_number,\n                rpe=rpe,\n                use_cache=self.use_cache,\n                rotary=rotary,\n                parallel_output=self.gpt_j_residual,\n            )\n\n        # Layernorm on the output of the attention layer.\n        # If GPT-J residuals are used, this is surpurfulous but leaving it in\n        # leads to cleaner code\n        if not self.neox_args.te_layernorm_mlp:\n            self.post_attention_layernorm = norm(neox_args.hidden_size, eps=eps)\n\n        # MLP\n        def get_mlp(**kw):\n            return ParallelMLP(\n                neox_args=neox_args,\n                init_method=init_method,\n                output_layer_init_method=output_layer_init_method,\n                parallel_output=self.gpt_j_residual,\n                multiple_of=neox_args.mlp_multiple_of,\n                **kw,\n            )\n\n        # TE MLP+LN\n        def get_te_lnmlp(**kw):\n            from megatron.model.transformer_engine import TELayerNormMLP\n\n            return TELayerNormMLP(\n                neox_args=neox_args,\n                init_method=init_method,\n                output_layer_init_method=output_layer_init_method,\n                parallel_output=self.gpt_j_residual,\n                multiple_of=neox_args.mlp_multiple_of,\n                **kw,\n            )\n\n        if self.num_experts <= 1:\n            if neox_args.te_layernorm_mlp:\n                self.mlp = get_te_lnmlp()\n            else:\n                self.mlp = get_mlp()\n        else:\n            self.mlp = ParallelDroplessMoE(\n                neox_args=neox_args,\n                init_method=init_method,\n                output_layer_init_method=output_layer_init_method,\n            )\n\n        self.layer_past = None  # used to cache k/v pairs in inference\n\n    def _get_bias_dropout(self):\n        if self.bias_dropout_fusion:\n            fn = (\n                bias_dropout_add_fused_train\n                if self.training\n                else bias_dropout_add_fused_inference\n            )\n        else:\n            fn = get_bias_dropout_add(self.training)\n        return fn\n\n    def forward(self, x, attention_mask, layer_past=None):\n        layer_past = layer_past if layer_past is not None else self.layer_past\n        bias_dropout_fn = self._get_bias_dropout()\n\n        # x: [b, s, h]\n\n        # Enable delayedscaling if TransformerEngine's FP8 is used for MHA layer.\n        if self.neox_args.te_fp8_mha:\n            from megatron.model.transformer_engine import TEDelayedScaling\n\n            fp8_recipe = TEDelayedScaling(neox_args=self.neox_args)\n            fp8_context = fp8_recipe.get_context()\n        else:\n            from contextlib import nullcontext\n\n            fp8_context = nullcontext()\n\n        with fp8_context:\n            if self.gpt_j_residual:\n                # pseudocode:\n                # x = x + attn(ln(x)) + mlp(ln(x))\n                # this means we can avoid doing the allreduce in the attn / mlp outputs\n                # to save communication time (we can do a single allreduce after we add mlp / attn outputs).\n                # due to a bug, the two layernorms are not tied in GPT-NeoX-20B. This is non-desirable, but\n                # we preserve the functionality for backwards compatibility\n\n                residual = x\n                # applies the correct normalization depending on if the norms are tied\n                if self.gpt_j_tied and not self.neox_args.te_layernorm_mlp:\n                    x = self.input_layernorm(x)\n                    x1, x2 = x, x\n                elif self.gpt_j_tied and self.neox_args.te_layernorm_mlp:\n                    x2 = x\n                    x = self.input_layernorm(x)\n                    x1 = x\n                elif self.neox_args.te_layernorm_mlp:\n                    x1, x2 = self.input_layernorm(x), x\n                else:\n                    x1, x2 = self.input_layernorm(x), self.post_attention_layernorm(x)\n\n                # attention operator\n                attention_output, attention_bias = self.attention(\n                    x1, attention_mask, layer_past=layer_past\n                )\n                if self.use_cache:\n                    attention_output, presents = attention_output\n                    self.layer_past = presents\n\n                if attention_bias is not None:\n                    with torch.enable_grad() if not self.eval else nullcontext():\n                        attention_output = bias_dropout_fn(\n                            attention_output,\n                            bias=attention_bias.expand_as(attention_output),\n                            residual=None,\n                            prob=self.hidden_dropout,\n                        )\n\n                # mlp operator\n                mlp_output, mlp_bias = self.mlp(x2)\n                if mlp_bias is not None:\n                    with torch.enable_grad() if not self.eval else nullcontext():\n                        output = bias_dropout_fn(\n                            mlp_output,\n                            bias=mlp_bias.expand_as(mlp_output),\n                            residual=attention_output,\n                            prob=self.hidden_dropout,\n                        )\n                else:\n                    output = mlp_output\n\n                # output = (x + attn(ln(x)) + mlp(ln(x))\n                output = residual + self.reduce(output)\n            else:\n                # pseudocode:\n                # x = x + attn(ln1(x))\n                # x = x + mlp(ln2(x))\n\n                residual = x\n\n                # x = x + attn(ln1(x))\n                attention_output, attention_bias = self.attention(\n                    self.input_layernorm(x), attention_mask, layer_past=layer_past\n                )\n\n                if self.use_cache:\n                    attention_output, presents = attention_output\n                    self.layer_past = presents\n                with torch.enable_grad() if not self.eval else nullcontext():\n                    if attention_bias is not None:\n                        # Use special bias_dropout_fn if we have a bias term from the above attention layer\n                        attention_output = bias_dropout_fn(\n                            attention_output,\n                            bias=attention_bias.expand_as(residual),\n                            residual=residual,\n                            prob=self.hidden_dropout,\n                        )\n                    else:\n                        # Otherwise just apply dropout + residual\n                        attention_output = (\n                            torch.nn.functional.dropout(\n                                attention_output,\n                                p=self.hidden_dropout,\n                                training=self.training,\n                            )\n                            + residual\n                        )\n\n                # output = x + mlp(ln2(x))\n                if self.neox_args.te_layernorm_mlp:\n                    layernorm_output = attention_output\n                else:\n                    layernorm_output = self.post_attention_layernorm(attention_output)\n                mlp_bias = torch.tensor(\n                    0.0, device=layernorm_output.device, dtype=layernorm_output.dtype\n                )\n\n                # call signatures of both dense and MoE are the same\n                mlp_output, mlp_bias = self.mlp(layernorm_output)\n\n                with torch.enable_grad() if not self.eval else nullcontext():\n                    if mlp_bias == None or (self.num_experts > 1):\n                        # No dropout either\n                        assert mlp_bias is None\n                        output = mlp_output + attention_output\n                    else:\n                        output = bias_dropout_fn(\n                            mlp_output,\n                            bias=mlp_bias.expand_as(attention_output),\n                            residual=attention_output,\n                            prob=self.hidden_dropout,\n                        )\n\n            return output\n\n\nclass ParallelTransformerLayerPipe(ParallelTransformerLayer):\n    \"\"\"Extends ParallelTransformerLayer to forward attention_mask through the pipeline.\"\"\"\n\n    def forward(self, args):\n        assert (\n            len(args) == 2\n        ), \"ParallelTransformerLayerPipe expects 2 arguments - hidden_states and attention_mask\"\n        hidden_states, attention_mask = args\n        # we are returning just [hidden_states, mask]\n        return super().forward(hidden_states, attention_mask), attention_mask\n\n\nclass ParallelLinearPipe(ParallelLinear):\n    \"\"\"Another helper class to pass presents through to the output when doing inference with a Pipe Parallel model\"\"\"\n\n    def forward(self, args):\n        assert isinstance(\n            args, torch.Tensor\n        ), \"ParallelLinearPipe expects a single argument - hidden_states\"\n        hidden_state = args\n        logits, bias = super().forward(hidden_state)\n        return logits\n\n\nclass NormPipe(nn.Module):\n    \"\"\"Just a helper class to pass presents through to the output when doing inference with a Pipe Parallel model\"\"\"\n\n    def __init__(self, norm_class, hidden_size, eps):\n        super().__init__()\n        self.norm = norm_class(hidden_size, eps=eps)\n\n    def forward(self, args):\n        assert not isinstance(\n            args, tuple\n        ), \"NormPipe should only receive a single tensor as input\"\n        return self.norm(args)\n\n\ndef parallel_lm_logits(\n    input_,\n    word_embeddings_weight,\n    parallel_output,\n    seq_parallel=False,\n    seq_dim=1,\n    bias=None,\n):\n    \"\"\"LM logits using word embedding weights.\"\"\"\n    # Parallel logits.\n    if seq_parallel:\n        # if using Sequence Parallelism, our logits are sharded along the sequence dimension.\n        # gather them here. (backward pass: reduce-scatter)\n        input_parallel = mpu.gather_from_sequence_parallel_region(\n            input_, seq_dim=seq_dim\n        )\n    else:\n        # Set up backprop all-reduce.\n        input_parallel = mpu.copy_to_model_parallel_region(input_)\n\n    # Matrix multiply.\n    if bias is None:\n        logits_parallel = F.linear(input_parallel, word_embeddings_weight)\n    else:\n        logits_parallel = F.linear(input_parallel, word_embeddings_weight, bias)\n\n    # Gather if needed.\n    if parallel_output:\n        return logits_parallel\n\n    return mpu.gather_from_model_parallel_region(logits_parallel)\n",
        "megatron/model/transformer_engine.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\n\nfrom megatron.model.transformer import Gated_Activation\nfrom megatron.model.activations import get_activation\nfrom megatron.mpu.initialize import get_model_parallel_rank\nfrom megatron.mpu.initialize import get_model_parallel_world_size\nfrom megatron.mpu.initialize import get_tensor_model_parallel_group\nfrom megatron.mpu.mappings import copy_to_model_parallel_region\nfrom megatron.mpu.mappings import gather_from_model_parallel_region\nfrom megatron.mpu.mappings import reduce_from_model_parallel_region\nfrom megatron.mpu.mappings import scatter_to_model_parallel_region\nfrom megatron.mpu.mappings import reduce_scatter_to_sequence_parallel_region\nfrom megatron.mpu.mappings import gather_from_sequence_parallel_region\nfrom megatron.mpu.layers import (\n    _initialize_affine_weight_gpu,\n    _initialize_affine_weight_cpu,\n)\nfrom megatron.mpu.random import get_cuda_rng_tracker\nfrom megatron.mpu.utils import divide\nfrom megatron.mpu.utils import VocabUtility\nfrom functools import partial\nfrom megatron.model.positional_embeddings import RotaryEmbedding\nfrom megatron import mpu\n\n# https://github.com/NVIDIA/TransformerEngine/issues/405\nimport os\nos.environ['NVTE_TORCH_COMPILE'] = str(0)\n\ntry:\n    import transformer_engine as te\nexcept ImportError:\n    raise ImportError(\n        \"Unable to import transformer-engine. Please refer to \"\n        \"https://github.com/NVIDIA/TransformerEngine for installation instructions.\"\n    )\n\n\n\nclass TERMSNorm(torch.nn.Module):\n    def __init__(self, dim, eps=1e-8, **kwargs):\n        \"\"\"\n            A conditional wrapper to initialize an instance of Transformer-Engine's\n            `RMSNorm` based on input\n        :param dim: model size\n        :param eps:  epsilon value, default 1e-8\n        \"\"\"\n        super(TERMSNorm, self).__init__()\n\n        self.d = dim\n        self.eps = eps\n        self.norm = te.pytorch.RMSNorm(\n            hidden_size=self.d,\n            eps=self.eps,\n            **kwargs,\n        )\n\n    def forward(self, x):\n        return self.norm(x)\n\n\nclass TELayerNorm(torch.nn.Module):\n    def __init__(self, dim, eps=1.0e-5, **kwargs):\n        \"\"\"\n            A conditional wrapper to initialize an instance of Transformer-Engine's\n            `LayerNorm` based on input\n        :param dim: model size\n        :param eps:  epsilon value, default 1.0e-5\n        \"\"\"\n        super(TELayerNorm, self).__init__()\n\n        self.d = dim\n        self.eps = eps\n        self.norm = te.pytorch.LayerNorm(\n            hidden_size=self.d,\n            eps=self.eps,\n            **kwargs,\n        )\n\n    def forward(self, x):\n        return self.norm(x)\n\n\nclass TELinear(te.pytorch.Linear):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `Linear` layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        input_size,\n        output_size,\n        bias=True,\n        init_method=init.xavier_normal_,\n        stride=1,\n        skip_bias_add=False,\n        mup_rescale_parameters=False,\n        seq_dim=0,\n    ):\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.skip_bias_add = skip_bias_add\n        self.use_bias = bias\n\n        self.sequence_parallel = neox_args.sequence_parallel\n        self.seq_dim = seq_dim\n\n        self.init_method = init_method\n        self.stride = stride\n        self.mup_rescale_parameters = mup_rescale_parameters\n        self.use_mup = neox_args.use_mup\n        self.params_dtype = neox_args.params_dtype\n\n        super(TELinear, self).__init__(\n            in_features=self.input_size,\n            out_features=self.output_size,\n            bias=self.use_bias,\n            init_method=self.init_method,\n            get_rng_state_tracker=get_cuda_rng_tracker,\n            device=torch.cuda.current_device(),\n            return_bias=self.skip_bias_add,\n            params_dtype=self.params_dtype,\n        )\n\n    def forward(self, inp, **kwargs):\n        if self.use_mup and self.mup_rescale_parameters:\n            input_ /= self.width_mult()\n\n        output = super(TELinear, self).forward(inp, **kwargs)\n\n        if self.skip_bias_add:\n            return output\n        else:\n            return output, None\n\n\nclass TELayerNormMLP(te.pytorch.LayerNormMLP):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `LayerNormMLP` layer that combines\n    layernorm and followed by the MLP module, consisting of 2 successive\n    linear transformations, separated by the GeLU activation.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        init_method,\n        output_layer_init_method,\n        parallel_output=False,\n        multiple_of=256,\n        MOE=False,\n        MoE_mp_size=1,\n        bias=True,\n    ):\n        self.activation_type = neox_args.activation\n        self.multiple_of = multiple_of\n        self.bias = bias\n        self.init_method = init_method\n        self.output_layer_init_method = output_layer_init_method\n\n        world_size = MoE_mp_size if MOE else get_model_parallel_world_size()\n        self.world_size = world_size\n        self.tp_group = get_tensor_model_parallel_group()\n        self.sequence_parallel = neox_args.sequence_parallel\n        self.seq_len = neox_args.seq_length\n        self.micro_batch_size = neox_args.train_micro_batch_size_per_gpu\n        self.params_dtype = neox_args.params_dtype\n        self.set_parallel_mode = False\n        if world_size > 1:\n            self.set_parallel_mode = True\n\n        if neox_args.intermediate_size:\n            ffn_dim = neox_args.intermediate_size\n        elif neox_args.expansion_factor:\n            ffn_dim = int(neox_args.expansion_factor * neox_args.hidden_size)\n        else:\n            # 4h is default for ffn_dim\n            ffn_dim = 4 * neox_args.hidden_size\n\n        if neox_args.norm in [\"layernorm\", \"te_layernorm\"]:\n            self.eps = 1.0e-5\n            self.normalization = \"LayerNorm\"\n        elif neox_args.norm in [\"rmsnorm\", \"te_rmsnorm\"]:\n            self.eps = 1.0e-8\n            self.normalization = \"RMSNorm\"\n        else:\n            raise ValueError(\n                \"Only LayerNorm and RMSNorm are supported with TransformerEngine\"\n            )\n\n        if self.activation_type not in [\n            \"gelu\",\n            \"geglu\",\n            \"relu\",\n            \"reglu\",\n            \"squared_relu\",\n            \"swiglu\",\n            \"qgelu\",\n            \"srelu\",\n        ]:\n            raise ValueError(\n                \"Only gelu, geglu, relu, reglu, squared_relu, swiglu, qgelu, and srelu are supported with TransformerEngine\"\n            )\n\n        super(TELayerNormMLP, self).__init__(\n            hidden_size=neox_args.hidden_size,\n            ffn_hidden_size=ffn_dim,\n            eps=self.eps,\n            bias=self.bias,\n            normalization=self.normalization,\n            activation=self.activation_type,\n            init_method=self.init_method,\n            output_layer_init_method=self.output_layer_init_method,\n            device=torch.cuda.current_device(),\n            set_parallel_mode=self.set_parallel_mode,\n            sequence_parallel=self.sequence_parallel,\n            tp_group=self.tp_group,\n            tp_size=self.world_size,\n            return_bias=True,\n            params_dtype=self.params_dtype,\n            seq_length=self.seq_len,\n            get_rng_state_tracker=get_cuda_rng_tracker,\n            micro_batch_size=self.micro_batch_size,\n        )\n\n\nclass TEColumnParallelLinear(te.pytorch.Linear):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `Linear` layer but specialized similar\n    to megatron's `ColumnParallelLinear` layer.\n\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias\n        gather_output: If true, call all-gather on output and make Y available\n                       to all GPUs, otherwise, every GPU will have its output\n                       which is Y_i = XA_i\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimations where bias\n                       can be fused with other elementwise operations. we skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        input_size,\n        output_size,\n        bias=True,\n        gather_output=True,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        MOE=False,\n        MoE_mp_size=1,\n        mup_rescale_parameters=False,\n        seq_dim=0,\n    ):\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.gather_output = gather_output\n        # Divide the weight matrix along the last dimension.\n        world_size = MoE_mp_size if MOE else get_model_parallel_world_size()\n        self.world_size = world_size\n        self.tp_group = get_tensor_model_parallel_group()\n        self.output_size_per_partition = divide(output_size, world_size)\n        self.skip_bias_add = skip_bias_add\n        self.use_bias = bias\n\n        self.sequence_parallel = neox_args.sequence_parallel\n        self.seq_dim = seq_dim\n\n        self.init_method = init_method\n        self.stride = stride\n        self.mup_rescale_parameters = mup_rescale_parameters\n        self.use_mup = neox_args.use_mup\n        self.params_dtype = neox_args.params_dtype\n        self.parallel_mode = \"column\"\n\n        super(TEColumnParallelLinear, self).__init__(\n            in_features=self.input_size,\n            out_features=self.output_size,\n            bias=self.use_bias,\n            init_method=self.init_method,\n            get_rng_state_tracker=get_cuda_rng_tracker,\n            device=torch.cuda.current_device(),\n            sequence_parallel=self.sequence_parallel,\n            tp_group=self.tp_group,\n            tp_size=self.world_size,\n            parallel_mode=self.parallel_mode,\n            return_bias=self.skip_bias_add,\n            params_dtype=self.params_dtype,\n        )\n\n    # Copied from Mup\n    def width_mult(self):\n        assert hasattr(self.weight, \"infshape\"), (\n            \"Please call set_base_shapes(...). If using torch.nn.DataParallel, \"\n            \"switch to distributed training with \"\n            \"torch.nn.parallel.DistributedDataParallel instead\"\n        )\n        return self.weight.infshape.width_mult()\n\n    # Copied from Mup\n    def _rescale_parameters(self):\n        \"\"\"Rescale parameters to convert SP initialization to μP initialization.\n        Warning: This method is NOT idempotent and should be called only once\n        unless you know what you are doing.\n        \"\"\"\n        if hasattr(self, \"_has_rescaled_params\") and self._has_rescaled_params:\n            raise RuntimeError(\n                \"`_rescale_parameters` has been called once before already. \"\n                \"Unless you know what you are doing, usually you should not be calling `_rescale_parameters` more than once.\\n\"\n                \"If you called `set_base_shapes` on a model loaded from a checkpoint, \"\n                \"or just want to re-set the base shapes of an existing model, \"\n                \"make sure to set the flag `rescale_params=False`.\\n\"\n                \"To bypass this error and *still rescale parameters*, set `self._has_rescaled_params=False` before this call.\"\n            )\n        if self.bias is not None:\n            self.bias.data *= self.width_mult() ** 0.5\n        self.weight.data *= self.width_mult() ** 0.5\n        self._has_rescaled_params = True\n\n    def mup_reinitialize_weights(self, neox_args):\n        if neox_args.use_cpu_initialization:\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.input_size_per_partition,\n                1,\n                partial(self.init_method, use_mup=True),\n                stride=self.stride,\n                return_master_weight=self.keep_master_weight_for_test,\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=1,\n                stride=self.stride,\n            )\n\n    def forward(self, inp, **kwargs):\n        if self.use_mup and self.mup_rescale_parameters:\n            input_ /= self.width_mult()\n\n        output = super(TEColumnParallelLinear, self).forward(inp, **kwargs)\n        if self.skip_bias_add:\n            return output\n        else:\n            return output, None\n\n\nclass TERowParallelLinear(te.pytorch.Linear):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `Linear` layer but specialized similar\n    to megatron's `RowParallelLinear` layer.\n\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias. Note that bias is not parallelized.\n        input_is_parallel: If true, we assume that the input is already\n                           split across the GPUs and we do not split\n                           again.\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimations where bias\n                       can be fused with other elementwise operations. we skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        input_size,\n        output_size,\n        bias=True,\n        input_is_parallel=False,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        MOE=False,\n        MoE_mp_size=1,\n        parallel_output=False,\n        mup_rescale_parameters=False,\n    ):\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        # Divide the weight matrix along the last dimension.\n        world_size = MoE_mp_size if MOE else get_model_parallel_world_size()\n        self.world_size = world_size\n        self.tp_group = get_tensor_model_parallel_group()\n        self.output_size_per_partition = divide(output_size, world_size)\n        self.skip_bias_add = skip_bias_add\n        self.use_bias = bias\n        self.input_is_parallel = input_is_parallel\n        self.sequence_parallel = neox_args.sequence_parallel\n\n        self.init_method = init_method\n        self.stride = stride\n        self.mup_rescale_parameters = mup_rescale_parameters\n        self.use_mup = neox_args.use_mup\n        self.params_dtype = neox_args.params_dtype\n        self.parallel_mode = \"row\"\n\n        super(TERowParallelLinear, self).__init__(\n            in_features=self.input_size,\n            out_features=self.output_size,\n            bias=self.use_bias,\n            init_method=self.init_method,\n            get_rng_state_tracker=get_cuda_rng_tracker,\n            device=torch.cuda.current_device(),\n            sequence_parallel=self.sequence_parallel,\n            tp_group=self.tp_group,\n            tp_size=self.world_size,\n            parallel_mode=self.parallel_mode,\n            return_bias=self.skip_bias_add,\n            params_dtype=self.params_dtype,\n        )\n\n    # Copied from Mup\n    def width_mult(self):\n        assert hasattr(self.weight, \"infshape\"), (\n            \"Please call set_base_shapes(...). If using torch.nn.DataParallel, \"\n            \"switch to distributed training with \"\n            \"torch.nn.parallel.DistributedDataParallel instead\"\n        )\n        return self.weight.infshape.width_mult()\n\n    # Copied from Mup\n    def _rescale_parameters(self):\n        \"\"\"Rescale parameters to convert SP initialization to μP initialization.\n        Warning: This method is NOT idempotent and should be called only once\n        unless you know what you are doing.\n        \"\"\"\n        if hasattr(self, \"_has_rescaled_params\") and self._has_rescaled_params:\n            raise RuntimeError(\n                \"`_rescale_parameters` has been called once before already. \"\n                \"Unless you know what you are doing, usually you should not be calling `_rescale_parameters` more than once.\\n\"\n                \"If you called `set_base_shapes` on a model loaded from a checkpoint, \"\n                \"or just want to re-set the base shapes of an existing model, \"\n                \"make sure to set the flag `rescale_params=False`.\\n\"\n                \"To bypass this error and *still rescale parameters*, set `self._has_rescaled_params=False` before this call.\"\n            )\n        if self.bias is not None:\n            self.bias.data *= self.width_mult() ** 0.5\n        self.weight.data *= self.width_mult() ** 0.5\n        self._has_rescaled_params = True\n\n    def mup_reinitialize_weights(self, neox_args):\n        if neox_args.use_cpu_initialization:\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.input_size_per_partition,\n                1,\n                partial(self.init_method, use_mup=True),\n                stride=self.stride,\n                return_master_weight=self.keep_master_weight_for_test,\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=1,\n                stride=self.stride,\n            )\n\n    def forward(self, inp, **kwargs):\n        if self.use_mup and self.mup_rescale_parameters:\n            input_ /= self.width_mult()\n\n        output = super(TERowParallelLinear, self).forward(inp, **kwargs)\n\n        if self.skip_bias_add:\n            return output\n        else:\n            return output, None\n\n\nclass TEMultiheadAttention(te.pytorch.MultiheadAttention):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `MultiheadAttention` layer that also\n    has \"flash attention\" enabled.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        attention_mask_func,\n        init_method,\n        output_layer_init_method,\n        layer_number,\n        rpe=None,\n        rotary=False,\n        use_cache=False,\n        parallel_output=False,\n    ):\n\n        self.neox_args = neox_args\n        self.attention_mask_func = attention_mask_func\n        self.init_method = init_method\n        self.output_layer_init_method = output_layer_init_method\n        self.layer_number = layer_number + 1\n\n        world_size = get_model_parallel_world_size()\n        self.world_size = world_size\n        self.tp_group = get_tensor_model_parallel_group()\n        self.sequence_parallel = neox_args.sequence_parallel\n        self.seq_len = neox_args.seq_length\n        self.micro_batch_size = neox_args.train_micro_batch_size_per_gpu\n        self.params_dtype = neox_args.params_dtype\n        self.set_parallel_mode = False\n        if world_size > 1:\n            self.set_parallel_mode = True\n\n        if neox_args.norm in [\"layernorm\", \"te_layernorm\"]:\n            self.eps = 1.0e-5\n            self.normalization = \"LayerNorm\"\n        elif neox_args.norm == [\"rmsnorm\", \"te_rmsnorm\"]:\n            self.eps = 1.0e-8\n            self.normalization = \"RMSNorm\"\n\n        if (\n            not neox_args.num_kv_heads\n            or neox_args.num_kv_heads == neox_args.num_attention_heads\n        ):\n            self.gqa = False\n            self.num_kv_heads = None\n        else:\n            self.gqa = True\n            self.num_kv_heads = neox_args.num_kv_heads\n\n        super(TEMultiheadAttention, self).__init__(\n            hidden_size=neox_args.hidden_size,\n            num_attention_heads=neox_args.num_attention_heads,\n            attention_dropout=neox_args.attention_dropout,\n            layernorm_epsilon=self.eps,\n            init_method=self.init_method,\n            output_layer_init_method=self.output_layer_init_method,\n            layer_number=self.layer_number,\n            window_size=neox_args.sliding_window_width,\n            num_gqa_groups=self.num_kv_heads,\n            input_layernorm=False,\n            normalization=self.normalization,\n            bias=True,\n            device=torch.cuda.current_device(),\n            get_rng_state_tracker=get_cuda_rng_tracker,\n            set_parallel_mode=self.set_parallel_mode,\n            sequence_parallel=self.sequence_parallel,\n            tp_group=self.tp_group,\n            tp_size=self.world_size,\n            params_dtype=self.params_dtype,\n            return_bias=True,\n            qkv_format=\"sbhd\",\n            fuse_qkv_params=True,\n        )\n\n        if neox_args.pos_emb == \"rotary\":\n            self.hidden_size_per_attention_head = mpu.divide(\n                neox_args.hidden_size, neox_args.num_attention_heads\n            )\n\n            if neox_args.rotary_pct == 1:\n                self.rotary_ndims = None\n            else:\n                assert neox_args.rotary_pct < 1\n                self.rotary_ndims = int(\n                    self.hidden_size_per_attention_head * neox_args.rotary_pct\n                )\n            dim = (\n                self.rotary_ndims\n                if self.rotary_ndims is not None\n                else self.hidden_size_per_attention_head\n            )\n            self.rotary_embeddings = RotaryEmbedding(\n                dim,\n                base=neox_args.rotary_emb_base,\n                max_seq_len=neox_args.seq_length,\n                precision=neox_args.params_dtype,\n                save_inv_freqs=neox_args.rotary_save_freqs_buffer,\n            )\n            self.rope_emb = self.rotary_embeddings.get_emb()\n\n    def forward(\n        self, hidden_states, attention_mask, layer_past=None, rope_emb=None, **kwargs\n    ):\n        output = super(TEMultiheadAttention, self).forward(\n            hidden_states, attention_mask, rotary_pos_emb=self.rope_emb, **kwargs\n        )\n        return output\n\n\nclass TEDelayedScaling(te.common.recipe.DelayedScaling):\n    \"\"\"\n    Wrapper for the Transformer-Engine's `DelayedScaling` layer.\n    \"\"\"\n\n    ##TODO Test with H100\n    def __init__(self, neox_args):\n\n        self.neox_args = neox_args\n        self.tp_group = get_tensor_model_parallel_group()\n\n        if neox_args.te_fp8_format == \"e4m3\":\n            fp8_format = te.common.recipe.Format.E4M3\n        elif neox_args.te_fp8_format == \"hybrid\":\n            fp8_format = te.common.recipe.Format.HYBRID\n        else:\n            raise ValueError(\"E4M3 and HYBRID are the only supported FP8 formats.\")\n\n        override_linear_precision = (False, False, not neox_args.te_fp8_wgrad)\n\n        super().__init__(\n            margin=neox_args.fp8_margin,\n            fp8_format=te_fp8_format,\n            amax_compute_algo=neox_args.te_fp8_amax_compute_algo,\n            amax_history_len=neox_args.te_fp8_amax_history_len,\n            override_linear_precision=override_linear_precision,\n            fp8_mha=neox_args.te_fp8_mha,\n        )\n\n    def fp8_context(self):\n        fp8_group = None\n        if self.tp_group:\n            fp8_group = self.tp_group\n        fp8_context = te.pytorch.fp8_autocast(\n            enabled=True, fp8_recipe=self, fp8_group=fp8_group\n        )\n\n        return get_context\n",
        "megatron/model/utils.py": "# # Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for models.\"\"\"\n\nimport torch\nfrom megatron.model.fused_softmax import SoftmaxFusionTypes\nfrom megatron import mpu\nfrom types import GeneratorType\nimport torch.distributed as dist\n\nimport importlib\nfrom typing import List, Dict, Any\n\n\ndef get_params_for_weight_decay_optimization(module: Any, neox_args: Any):\n    \"\"\"\n    Divide params into with-weight-decay and without-weight-decay groups.\n    Layernorms and biases will have no weight decay but the rest will.\n    \"\"\"\n    weight_decay_params = {\"params\": [], \"name\": \"weight_decay_params\"}\n    no_weight_decay_params = {\n        \"params\": [],\n        \"weight_decay\": 0.0,\n        \"name\": \"no_weight_decay_params\",\n    }\n\n    def is_no_weight_decay_module(module_: Any) -> bool:\n        return (\n            type(module_).__name__\n            in [\n                \"LayerNorm\",\n                \"RMSNorm\",\n                \"ScaleNorm\",\n                \"TELayerNorm\",\n                \"TERMSNorm\",\n                \"MixedFusedLayerNorm\",\n                \"MixedFusedRMSNorm\",\n            ]\n            or neox_args.weight_decay == 0.0\n        )\n\n    for module_ in module.modules():\n        if is_no_weight_decay_module(module_):\n            no_weight_decay_params[\"params\"].extend(\n                [p for p in module_._parameters.values() if p is not None]\n            )\n        else:\n            for name, param in module_._parameters.items():\n                if param is None:\n                    continue\n                if name == \"bias\" or getattr(param, \"_no_weight_decay\", False):\n                    no_weight_decay_params[\"params\"].append(param)\n                else:\n                    weight_decay_params[\"params\"].append(param)\n\n    if neox_args.weight_decay == 0.0:\n        # Only return a single param group to minimize calls to compressed_allreduce with onebitadam\n        return [no_weight_decay_params]\n    return weight_decay_params, no_weight_decay_params\n\n\ndef exists(x):\n    return x is not None\n\n\nclass Lambda(torch.nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\nclass SequentialWrapper(torch.nn.Module):\n    \"\"\"\n    Used to convert a deepspeed PipelineModule to an nn.Sequential like model whilst retaining\n    activation checkpointing.\n    \"\"\"\n\n    def __init__(\n        self,\n        layers,\n        activation_checkpoint_interval,\n        activation_checkpoint_func,\n        parent_class_name=None,\n    ):\n        super().__init__()\n        self.sequential = torch.nn.Sequential(*layers)\n        self.activation_checkpoint_interval = activation_checkpoint_interval\n        self.parent_class_name = parent_class_name\n        self.activation_checkpoint_func = activation_checkpoint_func\n        self.batch_fn = None\n\n    def _is_checkpointable(self, funcs):\n        if self.parent_class_name == \"GPT2ModelPipe\":\n            return all(\n                \"ParallelTransformerLayerPipe\" in f.__class__.__name__ for f in funcs\n            )\n        params = [f.parameters() for f in funcs if isinstance(f, torch.nn.Module)]\n        return any(len(list(p)) > 0 for p in params)\n\n    def set_batch_fn(self, fn):\n        \"\"\"Execute a post-processing function on input data.\n\n        Args:\n            fn (function): The function to run.\n        \"\"\"\n        self.batch_fn = fn\n\n    def inference_mode(self, use_cache=True):\n        \"\"\"\n        Sets up the model for inference by turning on k/v caching (if specified) and setting `parallel output` of the final layer to false,\n        so logits are gathered across model parallel ranks.\n\n        :param cache: (bool) True if you want to use caching during inference, False otherwise\n        \"\"\"\n        _set_use_cache(self.sequential, use_cache)\n        recursive_setattr(self.sequential, \"training\", False)\n\n    def train_mode(self):\n        \"\"\"\n        Sets up the model for training by turning off k/v caching.\n        \"\"\"\n        _set_use_cache(self.sequential, False)\n        recursive_setattr(self.sequential, \"training\", True)\n\n    def forward(\n        self,\n        forward_input,\n        curriculum_seqlen=None,\n        labels=None,\n        neox_args=None,\n        return_moe_losses=False,\n    ):\n\n        if self.batch_fn:\n            forward_input = self.batch_fn(forward_input)\n\n        if (\n            curriculum_seqlen is not None\n            and isinstance(forward_input, tuple)\n            and len(forward_input) == 3\n        ):\n            neox_args.update_value(\"curriculum_seqlen\", curriculum_seqlen)\n            tokens = forward_input[0]\n            input_ids = forward_input[1]\n            attention_mask = forward_input[2]\n            if curriculum_seqlen < input_ids.size()[1]:\n                # seqlen-based curriculum learning\n                # input_ids, position_ids, labels have size [batch size, seqlen]\n                input_ids = input_ids[:, :curriculum_seqlen].contiguous()\n                tokens = tokens[:, :curriculum_seqlen].contiguous()\n                # position_ids = position_ids[:, :curriculum_seqlen].contiguous()\n                if labels is not None:\n                    labels = labels[:, :curriculum_seqlen].contiguous()\n                # attention_mask has size [1, 1, seqlen, seqlen]\n                attention_mask = attention_mask[\n                    :, :, :curriculum_seqlen, :curriculum_seqlen\n                ].contiguous()\n            forward_input = (tokens, input_ids, attention_mask)\n\n        moe_losses = []\n\n        def exec_range_func(start, end):\n            \"\"\"Helper function to be used with checkpoint()\n            Adapted from torch.utils.checkpoint:checkpoint_sequential()\n            \"\"\"\n\n            def exec_func(*inputs):\n                # Single tensor inputs need to be unwrapped\n                if len(inputs) == 1:\n                    inputs = inputs[0]\n                for idx, layer in enumerate(self.sequential[start:end]):\n                    inputs = layer(inputs)\n                    if hasattr(layer, \"last_moe_loss\"):\n                        moe_losses.append(layer.last_moe_loss)\n                return inputs\n\n            return exec_func\n\n        if self.activation_checkpoint_interval == 0:\n            func = exec_range_func(0, len(self.sequential))\n            x = func(forward_input)\n        else:\n            num_layers = len(self.sequential)\n            x = forward_input\n            for start_idx in range(0, num_layers, self.activation_checkpoint_interval):\n                end_idx = min(\n                    start_idx + self.activation_checkpoint_interval, num_layers\n                )\n\n                funcs = self.sequential[start_idx:end_idx]\n                # Since we either pass tensors or tuples of tensors without unpacking, we\n                # need to be careful not to double-wrap tensors with tuple.\n                if not isinstance(x, tuple):\n                    x = (x,)\n\n                if self._is_checkpointable(funcs):\n                    x = self.activation_checkpoint_func(\n                        exec_range_func(start_idx, end_idx), *x\n                    )\n                else:\n                    x = exec_range_func(start_idx, end_idx)(*x)\n        if return_moe_losses:\n            return x, moe_losses\n        else:\n            return x\n\n    def clear_cache(self):\n        \"\"\"\n        Recursively clears the kv cache on all layers\n        \"\"\"\n        recursive_setattr(self.sequential, \"layer_past\", None)\n\n\ndef recursive_setattr(m, attr, value, assert_type=None, type_filter=None):\n    \"\"\"\n    Recursively set attributes on a pytorch module or an iterable of modules.\n    If an assert_type is provided, it will assert that the type of the value is the same as the assert_type.\n    If a type_filter is provided, it will only set attributes on modules that match that type.\n    \"\"\"\n    if assert_type is not None:\n        assert isinstance(value, assert_type), \"Value is not the correct type.\"\n\n    # if m is a list or a generator, iterate over the elements\n    if isinstance(m, (list, GeneratorType)):\n        for i in m:\n            recursive_setattr(i, attr, value, assert_type, type_filter)\n    elif isinstance(m, torch.nn.Module):\n        if hasattr(m, attr):\n            if type_filter is None or isinstance(m, type_filter):\n                setattr(m, attr, value)\n        if hasattr(m, \"children\"):\n            recursive_setattr(m.children(), attr, value, assert_type, type_filter)\n\n\ndef _set_use_cache(modules, value: bool):\n    \"\"\"\n    Recursively sets an use_cache to `value` on a list of pytorch modules, if they have a use_cache attribute.\n    use_cache is used to decide whether we cache past key value activations or not in inference.\n    \"\"\"\n    recursive_setattr(modules, \"use_cache\", value, assert_type=bool)\n\n\ndef configure_sparse_attention(neox_args, attention_type, num_attention_heads, mpu):\n    from deepspeed.ops.sparse_attention import (\n        SparseSelfAttention,\n        VariableSparsityConfig,\n        FixedSparsityConfig,\n        BigBirdSparsityConfig,\n        BSLongformerSparsityConfig,\n    )\n    from deepspeed.ops.sparse_attention.sparsity_config import (\n        LocalSlidingWindowSparsityConfig,\n    )\n\n    if attention_type == \"sparse_fixed\":\n        # you can think of local window size as `block_size` * `num_local_blocks`.\n        # so if you wanted to set a local window size of 256, set block size to 16 and `num_local_blocks` to 16\n        sparsity_config = FixedSparsityConfig(\n            num_heads=num_attention_heads,\n            block=neox_args.sparsity_config.get(\"block\", 16),\n            different_layout_per_head=neox_args.sparsity_config.get(\n                \"different_layout_per_head\", False\n            ),\n            num_local_blocks=neox_args.sparsity_config.get(\"num_local_blocks\", 4),\n            num_global_blocks=neox_args.sparsity_config.get(\"num_global_blocks\", 1),\n            num_different_global_patterns=neox_args.sparsity_config.get(\n                \"num_different_global_patterns\", 1\n            ),\n            attention=\"unidirectional\",\n            horizontal_global_attention=False,\n        )\n    elif attention_type == \"sparse_variable\":\n        sparsity_config = VariableSparsityConfig(\n            num_heads=num_attention_heads,\n            block=neox_args.sparsity_config.get(\"block\", 16),\n            different_layout_per_head=neox_args.sparsity_config.get(\n                \"different_layout_per_head\", False\n            ),\n            num_random_blocks=neox_args.sparsity_config.get(\"num_random_blocks\", 0),\n            local_window_blocks=neox_args.sparsity_config.get(\n                \"local_window_blocks\", [4]\n            ),\n            global_block_indices=neox_args.sparsity_config.get(\n                \"global_block_indices\", [0]\n            ),\n            global_block_end_indices=neox_args.sparsity_config.get(\n                \"global_block_end_indices\", None\n            ),\n            attention=\"unidirectional\",\n            horizontal_global_attention=False,\n        )\n    elif attention_type == \"local\":\n        # can configure with `num_local_blocks` or `num_sliding_window_blocks`\n        num_local_blocks = neox_args.sparsity_config.get(\n            \"num_local_blocks\",\n            neox_args.sparsity_config.get(\"num_sliding_window_blocks\", 4),\n        )\n        sparsity_config = LocalSlidingWindowSparsityConfig(\n            num_heads=num_attention_heads,\n            block=neox_args.sparsity_config.get(\"block\", 16),\n            num_sliding_window_blocks=num_local_blocks,\n            attention=\"unidirectional\",\n        )\n    elif attention_type == \"bigbird\":\n        sparsity_config = BigBirdSparsityConfig(\n            num_heads=num_attention_heads,\n            block=neox_args.sparsity_config.get(\"block\", 16),\n            different_layout_per_head=neox_args.sparsity_config.get(\n                \"different_layout_per_head\", False\n            ),\n            num_random_blocks=neox_args.sparsity_config.get(\"num_random_blocks\", 1),\n            num_sliding_window_blocks=neox_args.sparsity_config.get(\n                \"num_sliding_window_blocks\", 3\n            ),\n            num_global_blocks=neox_args.sparsity_config.get(\"num_global_blocks\", 1),\n            attention=\"unidirectional\",\n        )\n    elif attention_type == \"bslongformer\":\n        sparsity_config = BSLongformerSparsityConfig(\n            num_heads=num_attention_heads,\n            block=neox_args.sparsity_config.get(\"block\", 16),\n            different_layout_per_head=neox_args.sparsity_config.get(\n                \"different_layout_per_head\", False\n            ),\n            num_sliding_window_blocks=neox_args.sparsity_config.get(\n                \"num_sliding_window_blocks\", 3\n            ),\n            global_block_indices=neox_args.sparsity_config.get(\n                \"global_block_indices\", [0]\n            ),\n            global_block_end_indices=neox_args.sparsity_config.get(\n                \"global_block_end_indices\", None\n            ),\n            attention=\"unidirectional\",\n        )\n    else:\n        raise ValueError(f\"Attention type {attention_type} not recognized\")\n    return SparseSelfAttention(\n        sparsity_config=sparsity_config,\n        max_seq_length=neox_args.seq_length,\n        attn_mask_mode=\"add\",\n        mpu=mpu,\n    )\n\n\ndef get_fusion_type(neox_args):\n    fusion_type = SoftmaxFusionTypes.none\n    if neox_args.scaled_upper_triang_masked_softmax_fusion:\n        fusion_type = SoftmaxFusionTypes.upper_triang\n    elif neox_args.scaled_masked_softmax_fusion:\n        fusion_type = SoftmaxFusionTypes.general\n    return fusion_type\n\n\ndef reduce_weight_grads_from_model_parallel_region(input_):\n    \"\"\"A hook that can be applied to any weight tensor via .register_hook().\n    Allreduces grads for e.g. LN weights across the model parallel group.\n    Needed to keep LNs in sync, despite them getting diff data -> diff gradients when using sequence parallel.\n    \"\"\"\n    # Bypass the function if no TP -> no comm needed.\n    if mpu.get_model_parallel_world_size() == 1:\n        return input_\n\n    # Bf16 convert\n    dt = input_.dtype\n    if dt == torch.bfloat16 and mpu.get_fp32_allreduce():\n        input_ = input_.float()\n\n    # All-reduce.\n    dist.all_reduce(input_, group=mpu.get_model_parallel_group())\n\n    # Bf16 convert\n    if dt == torch.bfloat16 and mpu.get_fp32_allreduce():\n        input_ = input_.bfloat16()\n\n    return input_\n\n\ndef mark_norms_for_sequence_parallel_grad_sync(module, neox_args):\n    \"\"\"Iterate through the modules in our model, and for any \"...Norm\" classnames,\n    register a hook on each of that module's parameters which will allreduce norms' weights' grads across\n    the model (sequence) parallel region.\n    \"\"\"\n\n    if not neox_args.sequence_parallel:\n        # if we aren't using sequence parallelism, this is a no-op\n        return\n\n    for module_ in module.modules():\n        if \"norm\" in type(module_).__name__.lower():\n            # this is a norm, we want to allreduce its weight grads across sequence parallel region\n            for name, param in module_.named_parameters():\n                if param.requires_grad:\n                    param.register_hook(reduce_weight_grads_from_model_parallel_region)\n\n\ndef get_parallel_linear(neox_args):\n    if neox_args.te_columnparallel:\n        from megatron.model.transformer_engine import (\n            TEColumnParallelLinear as ColumnParallelLinear,\n        )\n    else:\n        from megatron.mpu import ColumnParallelLinear\n    if neox_args.te_rowparallel:\n        from megatron.model.transformer_engine import (\n            TERowParallelLinear as RowParallelLinear,\n        )\n    else:\n        from megatron.mpu import RowParallelLinear\n\n    return ColumnParallelLinear, RowParallelLinear\n",
        "megatron/model/weight_server.py": "from typing import Union, List\n\nimport torch\nimport socket\nimport pickle\n\n\ndef send_tensor(state_dict_key, data, sock, end: bool):\n    storage = data.storage()\n    (\n        storage_device,\n        storage_handle,\n        storage_size_bytes,\n        storage_offset_bytes,\n        ref_counter_handle,\n        ref_counter_offset,\n        event_handle,\n        event_sync_required,\n    ) = storage._share_cuda_()\n    sock.send(\n        pickle.dumps(\n            {\n                \"state_dict_key\": state_dict_key,\n                \"dtype\": data.dtype,\n                \"tensor_size\": data.shape,\n                \"tensor_stride\": data.stride(),\n                \"tensor_offset\": data.storage_offset(),  # !Not sure about this one.\n                \"storage_cls\": type(storage),\n                \"storage_device\": storage_device,\n                \"storage_handle\": storage_handle,\n                \"storage_size_bytes\": storage_size_bytes,\n                \"storage_offset_bytes\": storage_offset_bytes,\n                \"requires_grad\": False,\n                \"ref_counter_handle\": ref_counter_handle,\n                \"ref_counter_offset\": ref_counter_offset,\n                \"event_handle\": event_handle,\n                \"event_sync_required\": event_sync_required,\n                \"end\": end,\n            }\n        )\n    )\n\n\ndef send_state_dict(state_dict, sock):\n    for i, key in enumerate(state_dict.keys()):\n        print(key)\n        end = i == len(state_dict.keys()) - 1\n        send_tensor(key, state_dict[key], sock, end)\n        sock.recv(4096)\n\n\ndef start_server(model, ports: Union[int, List[int]] = 6000):\n    global_rank = torch.distributed.get_rank()\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    if type(ports) == int:\n        port = ports + global_rank\n    else:\n        port = ports[global_rank]\n    s.bind((\"localhost\", port))\n    s.listen(1)\n    conn, addr = s.accept()\n    state_dict = model.state_dict()\n    send_state_dict(state_dict, conn)\n    conn.close()\n",
        "megatron/model/word_embeddings.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport math\nfrom torch.nn.parameter import Parameter\n\nfrom megatron import mpu\nfrom megatron.model.positional_embeddings import SinusoidalPositionalEmbedding\nfrom megatron.model.init_functions import get_init_methods\n\n\nclass Embedding(torch.nn.Module):\n    \"\"\"Language model embeddings.\n    Arguments:\n        hidden_size: hidden size\n        vocab_size: vocabulary size\n        max_sequence_length: maximum size of sequence. This\n                             is used for positional embedding\n        embedding_dropout_prob: dropout probability for embeddings\n        init_method: weight initialization method\n        num_tokentypes: size of the token-type embeddings. 0 value\n                        will ignore this embedding\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        hidden_size,\n        vocab_size,\n        max_sequence_length,\n        embedding_dropout_prob,\n        init_method,\n        num_tokentypes=0,\n        use_pos_emb=True,\n    ):\n        super(Embedding, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.init_method = init_method\n        self.num_tokentypes = num_tokentypes\n\n        self.sequence_parallel = (\n            neox_args.sequence_parallel\n        )  # if we are using sequence parallelism, then we'll want to scatter our inputs across the seqlen dim across TP ranks\n\n        self.use_mup = neox_args.use_mup\n        self.mup_embedding_mult = neox_args.mup_embedding_mult\n        self.mup_rp_embedding_mult = neox_args.mup_rp_embedding_mult\n\n        # Word embeddings (parallel).\n        self.word_embeddings = mpu.VocabParallelEmbedding(\n            neox_args=neox_args,\n            num_embeddings=vocab_size,\n            embedding_dim=self.hidden_size,\n            init_method=self.init_method,\n        )\n        self._word_embeddings_key = \"word_embeddings\"\n\n        if neox_args.use_bnb_optimizer:\n            try:\n                import bitsandbytes as bnb\n\n                self.embedding_module = bnb.nn.StableEmbedding\n            except ModuleNotFoundError:\n                print(\n                    \"Please install bitsandbytes following https://github.com/facebookresearch/bitsandbytes.\"\n                )\n                raise Exception\n        else:\n            self.embedding_module = torch.nn.Embedding\n\n        # Position embedding (serial).\n        self.use_pos_emb = use_pos_emb\n        if self.use_pos_emb:\n            self.embedding_type = neox_args.pos_emb\n            if self.embedding_type == \"learned\":\n                self.position_embeddings = self.embedding_module(\n                    max_sequence_length, self.hidden_size\n                )\n                self._position_embeddings_key = \"position_embeddings\"\n                # Initialize the position embeddings.\n                self.init_method(self.position_embeddings.weight)\n            elif self.embedding_type == \"sinusoidal\":\n                self.position_embeddings = SinusoidalPositionalEmbedding(\n                    self.hidden_size\n                )\n\n        # Token type embedding.\n        # Add this as an optional field that can be added through\n        # method call so we can load a pretrain model without\n        # token types and add them as needed.\n        self._tokentype_embeddings_key = \"tokentype_embeddings\"\n        if self.num_tokentypes > 0:\n            self.tokentype_embeddings = self.embedding_module(\n                self.num_tokentypes, self.hidden_size\n            )\n            # Initialize the token-type embeddings.\n            self.init_method(self.tokentype_embeddings.weight)\n        else:\n            self.tokentype_embeddings = None\n\n        # Embeddings dropout\n        self.embedding_dropout = torch.nn.Dropout(embedding_dropout_prob)\n        self.opt_pos_emb_offset = neox_args.opt_pos_emb_offset\n\n        # For ticking position ids forward\n        self.layer_past = None\n\n    def add_tokentype_embeddings(self, num_tokentypes):\n        \"\"\"Add token-type embedding. This function is provided so we can add\n        token-type embeddings in case the pretrained model does not have it.\n        This allows us to load the model normally and then add this embedding.\n        \"\"\"\n        if self.tokentype_embeddings is not None:\n            raise Exception(\"tokentype embeddings is already initialized\")\n        if torch.distributed.get_rank() == 0:\n            print(\n                \"adding embedding for {} tokentypes\".format(num_tokentypes), flush=True\n            )\n        self.num_tokentypes = num_tokentypes\n        self.tokentype_embeddings = self.embedding_module(\n            num_tokentypes, self.hidden_size\n        )\n        # Initialize the token-type embeddings.\n        self.init_method(self.tokentype_embeddings.weight)\n\n    def forward(self, input_ids, position_ids, tokentype_ids=None):\n        # Embeddings.\n        words_embeddings = self.word_embeddings(input_ids)\n        if self.use_pos_emb and self.embedding_type in [\"learned\", \"sinusoidal\"]:\n            if self.opt_pos_emb_offset:\n                if self.layer_past is not None:\n                    position_ids = position_ids + self.layer_past + 1\n                self.layer_past = position_ids[:, -1]\n                # OPT always adds 2 for some reason, according to the HF implementation\n                position_ids = position_ids + self.opt_pos_emb_offset\n            position_embeddings = self.position_embeddings(position_ids)\n            position_embeddings.mul_(self.mup_rp_embedding_mult)\n            embeddings = words_embeddings + position_embeddings\n        else:\n            embeddings = words_embeddings\n        if tokentype_ids is not None:\n            assert self.tokentype_embeddings is not None\n            embeddings = embeddings + self.tokentype_embeddings(tokentype_ids)\n        else:\n            assert self.tokentype_embeddings is None\n\n        # Dropout.\n        embeddings = self.embedding_dropout(embeddings)\n\n        if self.use_mup:\n            with torch.no_grad():\n                embeddings.mul_(self.mup_embedding_mult)\n\n        if self.sequence_parallel:\n            # TODO: megatron-lm does dropout using the scattered embs. This would save a tiny bit of time, perhaps?\n            # Not a priority since we don't often use dropout\n            embeddings = mpu.scatter_to_sequence_parallel_region(embeddings)\n\n        return embeddings\n\n\nclass EmbeddingPipe(Embedding):\n    \"\"\"Extends Embedding to forward attention_mask through the pipeline.\"\"\"\n\n    @property\n    def word_embeddings_weight(self):\n        \"\"\"Easy accessory for the pipeline engine to tie embeddings across stages.\"\"\"\n        return self.word_embeddings.weight\n\n    def forward(self, args):\n        assert (\n            len(args) == 3\n        ), f\"Expected 3 arguments (input_ids, position_ids, attention_mask), but got {len(args)}.\"\n\n        input_ids = args[0]\n        position_ids = args[1]\n        attention_mask = args[2]\n        embeddings = super().forward(input_ids, position_ids)\n        return embeddings, attention_mask\n\n\nclass SoftEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        neox_args,\n        wte,\n        n_tokens: int = 10,\n        init_range: float = 0.5,\n        init_string: str = \"\",\n    ):\n        super(SoftEmbedding, self).__init__()\n        self.n_tokens = n_tokens\n        self.neox_args = neox_args\n        self.init_range = init_range\n        self.init_string = init_string\n        self.soft_embedding_weight = torch.nn.parameter.Parameter(\n            self.initialize_embedding(wte)\n        )\n\n    def initialize_embedding(self):\n        if self.init_string:\n            embeds = torch.LongTensor(\n                self.neox_args.tokenizer.tokenize(self.init_string)\n            ).to(self.embedding_module.weight.device)\n            embeds = self.embedding_module(embeds)\n            if embeds.shape[0] >= self.n_tokens:\n                embeds = embeds[: self.n_tokens, :]  # slice\n            else:\n                embeds = embeds.repeat(math.ceil(self.n_tokens / embeds.shape[0]), 1)[\n                    : self.n_tokens, :\n                ]  # pad up to n_tokens\n            return embeds\n        return torch.Tensor(n_tokens, neox_args.hidden_size).uniform_(\n            -self.random_range, self.random_range\n        )\n\n    def forward(self, args: tuple):\n        in_inference = len(args) == 3  # embeddings, layer_past, attention_mask\n        in_train = len(args) == 2  # embeddings, attention_mask\n        if in_train:\n            embedding, attention_mask = args\n        else:\n            embedding, layer_past, attention_mask = args\n        soft_embedding = self.soft_embedding_weight.repeat(\n            embedding.shape[0], 1, 1\n        )  # repeat batch_size times\n        if in_train:\n            # append soft embedding at the beginning in training\n            embedding = torch.cat((soft_embedding, embedding), dim=1)\n            embedding = embedding[:, : self.neox_args.seq_length, ...]\n            return embedding, attention_mask\n        else:\n            if not (exists(layer_past) and layer_past.numel() > 0):\n                # if in inference, on the first forward pass, we want to do the same as in training (append soft embedding)\n                embedding = torch.cat((soft_embedding, embedding), dim=1)\n                embedding = embedding[:, : self.neox_args.seq_length, ...]\n            # otherwise, we're in incremental mode, and just want to forward the single embedding (since the soft prompt has already been cached)\n            return embedding, layer_past, attention_mask\n",
        "megatron/mpu/__init__.py": "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Model parallel utility interface.\"\"\"\n\nfrom .cross_entropy import vocab_parallel_cross_entropy\n\nfrom .data import broadcast_data\n\nfrom .initialize import is_unitialized\nfrom .initialize import destroy_model_parallel\nfrom .initialize import get_data_parallel_group\nfrom .initialize import get_data_parallel_rank\nfrom .initialize import get_data_parallel_world_size\nfrom .initialize import get_model_parallel_group\nfrom .initialize import get_model_parallel_rank, set_model_parallel_rank\nfrom .initialize import get_model_parallel_src_rank, get_data_parallel_src_rank\nfrom .initialize import get_model_parallel_world_size, set_model_parallel_world_size\nfrom .initialize import get_topology\nfrom .initialize import get_pipe_parallel_group\nfrom .initialize import get_pipe_parallel_rank\nfrom .initialize import get_pipe_parallel_world_size\nfrom .initialize import get_tensor_model_parallel_group\nfrom .initialize import get_tensor_model_parallel_rank\nfrom .initialize import get_tensor_model_parallel_world_size\nfrom .initialize import get_io_parallel_group\nfrom .initialize import get_expert_tokens_for_rank\nfrom .initialize import get_expert_token_counts_for_rank\nfrom .initialize import initialize_model_parallel\nfrom .initialize import model_parallel_is_initialized\nfrom .initialize import get_fp32_allreduce\n\nfrom .layers import ColumnParallelLinear\nfrom .layers import RowParallelLinear\nfrom .layers import VocabParallelEmbedding\nfrom .layers import ParallelRelativePositionBias\n\nfrom .mappings import copy_to_model_parallel_region\nfrom .mappings import copy_to_expert_model_parallel_region\nfrom .mappings import gather_from_model_parallel_region\nfrom .mappings import gather_from_expert_model_parallel_region\nfrom .mappings import reduce_from_model_parallel_region\nfrom .mappings import scatter_to_model_parallel_region\nfrom .mappings import reduce_scatter_to_sequence_parallel_region\nfrom .mappings import gather_from_sequence_parallel_region\nfrom .mappings import scatter_to_sequence_parallel_region\n\nfrom .random import checkpoint\nfrom .random import get_cuda_rng_tracker\nfrom .random import model_parallel_cuda_manual_seed\n\nfrom .utils import divide\nfrom .utils import split_tensor_along_last_dim\n",
        "megatron/mpu/cross_entropy.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\n\nfrom .initialize import get_model_parallel_group\nfrom .initialize import get_model_parallel_rank\nfrom .initialize import get_model_parallel_world_size\nfrom .utils import VocabUtility\n\n\nclass _VocabParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, vocab_parallel_logits, target):\n\n        # Maximum value along vocab dimension across all GPUs.\n        logits_max = torch.max(vocab_parallel_logits, dim=-1)[0]\n        torch.distributed.all_reduce(\n            logits_max,\n            op=torch.distributed.ReduceOp.MAX,\n            group=get_model_parallel_group(),\n        )\n        # Subtract the maximum value.\n        vocab_parallel_logits.sub_(logits_max.unsqueeze(dim=-1))\n\n        # Get the partition's vocab indices\n        get_vocab_range = VocabUtility.vocab_range_from_per_partition_vocab_size\n        partition_vocab_size = vocab_parallel_logits.size()[-1]\n        rank = get_model_parallel_rank()\n        world_size = get_model_parallel_world_size()\n        vocab_start_index, vocab_end_index = get_vocab_range(\n            partition_vocab_size, rank, world_size\n        )\n\n        # Create a mask of valid vocab ids (1 means it needs to be masked).\n        target_mask = (target < vocab_start_index) | (target >= vocab_end_index)\n        masked_target = target.clone() - vocab_start_index\n        masked_target[target_mask] = 0\n\n        # Get predicted-logits = logits[target].\n        # For Simplicity, we convert logits to a 2-D tensor with size\n        # [*, partition-vocab-size] and target to a 1-D tensor of size [*].\n        logits_2d = vocab_parallel_logits.view(-1, partition_vocab_size)\n        masked_target_1d = masked_target.view(-1)\n        arange_1d = torch.arange(\n            start=0, end=logits_2d.size()[0], device=logits_2d.device\n        )\n        predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]\n        predicted_logits_1d = predicted_logits_1d.clone().contiguous()\n        predicted_logits = predicted_logits_1d.view_as(target)\n        predicted_logits[target_mask] = 0.0\n        # All reduce is needed to get the chunks from other GPUs.\n        torch.distributed.all_reduce(\n            predicted_logits,\n            op=torch.distributed.ReduceOp.SUM,\n            group=get_model_parallel_group(),\n        )\n\n        # Sum of exponential of logits along vocab dimension across all GPUs.\n        exp_logits = vocab_parallel_logits\n        torch.exp(vocab_parallel_logits, out=exp_logits)\n        sum_exp_logits = exp_logits.sum(dim=-1)\n        torch.distributed.all_reduce(\n            sum_exp_logits,\n            op=torch.distributed.ReduceOp.SUM,\n            group=get_model_parallel_group(),\n        )\n\n        # Loss = log(sum(exp(logits))) - predicted-logit.\n        loss = torch.log(sum_exp_logits) - predicted_logits\n\n        # Store softmax, target-mask and masked-target for backward pass.\n        exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))\n        ctx.save_for_backward(exp_logits, target_mask, masked_target_1d)\n\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n\n        # Retrieve tensors from the forward path.\n        softmax, target_mask, masked_target_1d = ctx.saved_tensors\n\n        # All the inputs have softmax as their gradient.\n        grad_input = softmax\n        # For simplicity, work with the 2D gradient.\n        partition_vocab_size = softmax.size()[-1]\n        grad_2d = grad_input.view(-1, partition_vocab_size)\n\n        # Add the gradient from matching classes.\n        arange_1d = torch.arange(start=0, end=grad_2d.size()[0], device=grad_2d.device)\n        grad_2d[arange_1d, masked_target_1d] -= 1.0 - target_mask.view(-1).float()\n\n        # Finally elementwise multiplication with the output gradients.\n        grad_input.mul_(grad_output.unsqueeze(dim=-1))\n\n        return grad_input, None\n\n\ndef vocab_parallel_cross_entropy(vocab_parallel_logits, target):\n    \"\"\"Helper function for the cross entropy.\"\"\"\n    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target)\n",
        "megatron/mpu/data.py": "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .initialize import get_model_parallel_group\nfrom .initialize import get_model_parallel_rank\nfrom .initialize import get_model_parallel_src_rank\n\n\n_MAX_DATA_DIM = 4\n\n\ndef _check_data_types(keys, data, target_dtype):\n    \"\"\"Check that all the keys have the same target data type.\"\"\"\n    for key in keys:\n        assert (\n            data[key].dtype == target_dtype\n        ), \"{} has data type {} which \" \"is different than {}\".format(\n            key, data[key].dtype, target_dtype\n        )\n\n\ndef _build_key_size_numel_dictionaries(keys, data):\n    \"\"\"Build the size on rank 0 and broadcast.\"\"\"\n    max_dim = _MAX_DATA_DIM\n    sizes = [0 for _ in range(max_dim) for _ in keys]\n\n    # Pack the sizes on rank zero.\n    if get_model_parallel_rank() == 0:\n        offset = 0\n        for key in keys:\n            assert data[key].dim() < max_dim, \"you should increase MAX_DATA_DIM\"\n            size = data[key].size()\n            for i, s in enumerate(size):\n                sizes[i + offset] = s\n            offset += max_dim\n\n    # Move to GPU and broadcast.\n    sizes_cuda = torch.cuda.LongTensor(sizes)\n    torch.distributed.broadcast(\n        sizes_cuda, get_model_parallel_src_rank(), group=get_model_parallel_group()\n    )\n\n    # Move back to cpu and unpack.\n    sizes_cpu = sizes_cuda.cpu()\n    key_size = {}\n    key_numel = {}\n    total_numel = 0\n    offset = 0\n    for key in keys:\n        i = 0\n        size = []\n        numel = 1\n        while sizes_cpu[offset + i] > 0:\n            this_size = sizes_cpu[offset + i]\n            size.append(this_size)\n            numel *= this_size\n            i += 1\n        key_size[key] = size\n        key_numel[key] = numel\n        total_numel += numel\n        offset += max_dim\n\n    return key_size, key_numel, total_numel\n\n\ndef broadcast_data(keys, data, datatype):\n    \"\"\"Broadcast data from rank zero of each model parallel group to the\n    members of the same model parallel group.\n\n    Arguments:\n        keys: list of keys in the data dictionary to be broadcasted\n        data: data dictionary of string keys and cpu tensor values.\n        datatype: torch data type of all tensors in data associated\n                  with keys.\n    \"\"\"\n    # Build (key, size) and (key, number of elements) dictionaries along\n    # with the total number of elements on all ranks.\n    key_size, key_numel, total_numel = _build_key_size_numel_dictionaries(keys, data)\n\n    # Pack on rank zero.\n    if get_model_parallel_rank() == 0:\n        # Check that all keys have the same data type.\n        _check_data_types(keys, data, datatype)\n        # Flatten the data associated with the keys\n        flatten_data = torch.cat(\n            [data[key].contiguous().view(-1) for key in keys], dim=0\n        ).cuda()\n    else:\n        flatten_data = torch.empty(\n            total_numel, device=torch.cuda.current_device(), dtype=datatype\n        )\n\n    # Broadcast\n    torch.distributed.broadcast(\n        flatten_data, get_model_parallel_src_rank(), group=get_model_parallel_group()\n    )\n\n    # Unpack\n    output = {}\n    offset = 0\n    for key in keys:\n        size = key_size[key]\n        numel = key_numel[key]\n        output[key] = flatten_data.narrow(0, offset, numel).view(size)\n        offset += numel\n\n    return output\n",
        "megatron/mpu/initialize.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"Model and data parallel groups.\"\"\"\n\nfrom typing import Optional\nimport torch\n\nfrom .utils import ensure_divisibility\n\n# Model parallel group that the current rank belongs to.\n_MODEL_PARALLEL_GROUP = None\n# Data parallel group that the current rank belongs to.\n_DATA_PARALLEL_GROUP = None\n# Pipeline parallel group that the current rank belongs to.\n_PIPE_PARALLEL_GROUP = None\n\n# A group used to sync during the IO process. Usually this is data_parallel_group(),\n# but with pipeline parallelism it must also involve the last stage (which is not in the\n# DP group of rank 0)\n_IO_PARALLEL_GROUP = None\n\n# These values enable us to change the mpu sizes on the fly.\n_MPU_WORLD_SIZE = None\n_MPU_RANK = None\n\n# Used to query 3D topology\n_MPU_TOPOLOGY = None\n\n# Get fp32_allreduce flag\n_FP32_ALLREDUCE = None\n\n\ndef is_unitialized():\n    \"\"\"Useful for code segments that may be accessed with or without mpu initialization\"\"\"\n    return _DATA_PARALLEL_GROUP is None\n\n\ndef initialize_model_parallel(model_parallel_size, topology=None, fp32_allreduce=False):\n    \"\"\"\n    Initialize model data parallel groups.\n\n    Arguments:\n        model_parallel_size: number of GPUs used to parallelize model.\n\n    Let's say we have a total of 8 GPUs denoted by g0 ... g7 and we\n    use 2 GPUs to parallelize the model. The present function will\n    create 4 model parallel groups and 2 data parallel groups as:\n        4 model parallel groups:\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7]\n        2 data parallel groups:\n            [g0, g2, g4, g6], [g1, g3, g5, g7]\n    Note that for efficiency, the caller should make sure adjacent ranks\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\n    ranks 8 to 15 belong to the second box.\n    \"\"\"\n    if torch.distributed.get_rank() == 0:\n        print(\"> initializing model parallel with size {}\".format(model_parallel_size))\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size = torch.distributed.get_world_size()\n    if world_size < model_parallel_size:\n        raise ValueError(\"world size cannot be smaller than model parallel size\")\n    ensure_divisibility(world_size, model_parallel_size)\n    rank = torch.distributed.get_rank()\n\n    global _MPU_TOPOLOGY\n    if topology:\n        _MPU_TOPOLOGY = topology\n\n    # Build the data parallel groups.\n    global _DATA_PARALLEL_GROUP\n    assert _DATA_PARALLEL_GROUP is None, \"data parallel group is already initialized\"\n    if topology:\n        for dp_group in topology.get_axis_comm_lists(\"data\"):\n            group = torch.distributed.new_group(ranks=dp_group)\n            if rank == 0:\n                print(f\"MPU DP:\", dp_group)\n            if rank in dp_group:\n                _DATA_PARALLEL_GROUP = group\n    else:\n        for i in range(model_parallel_size):\n            ranks = range(i, world_size, model_parallel_size)\n            group = torch.distributed.new_group(ranks)\n            if i == (rank % model_parallel_size):\n                _DATA_PARALLEL_GROUP = group\n\n    # Build pipeline parallel group\n    if topology is not None:\n        global _PIPE_PARALLEL_GROUP\n        for pp_group in topology.get_axis_comm_lists(\"pipe\"):\n            group = torch.distributed.new_group(ranks=pp_group)\n            if rank == 0:\n                print(f\"MPU PP:\", pp_group)\n            if rank in pp_group:\n                _PIPE_PARALLEL_GROUP = group\n\n    # Build IO group\n    global _IO_PARALLEL_GROUP\n    if topology and topology.get_dim(\"pipe\") > 1:\n        io_stages = [0, topology.get_dim(\"pipe\") - 1]\n        io_group = []\n        for stage in io_stages:\n            io_group.extend(topology.filter_match(pipe=stage, model=0))\n        if rank == 0:\n            print(f\"MPU IO:\", io_group)\n        group = torch.distributed.new_group(ranks=io_group)\n        if rank in io_group:\n            _IO_PARALLEL_GROUP = group\n    else:\n        _IO_PARALLEL_GROUP = get_data_parallel_group()\n\n    # Build the model parallel groups.\n    global _MODEL_PARALLEL_GROUP\n    assert _MODEL_PARALLEL_GROUP is None, \"model parallel group is already initialized\"\n    if topology:\n        # Short circuit case without model parallelism.\n        # TODO: it would be nice  to avoid this branching case?\n        if model_parallel_size == 1:\n            for group_rank in range(world_size):\n                group = torch.distributed.new_group(ranks=[group_rank])\n                if rank == 0:\n                    print(f\"MPU MP:\", [group_rank])\n                if rank == group_rank:\n                    _MODEL_PARALLEL_GROUP = group\n            return\n\n        for mp_group in topology.get_axis_comm_lists(\"model\"):\n            group = torch.distributed.new_group(ranks=mp_group)\n            if rank == 0:\n                print(f\"MPU MP:\", mp_group)\n            if rank in mp_group:\n                _MODEL_PARALLEL_GROUP = group\n\n    else:\n        for i in range(world_size // model_parallel_size):\n            ranks = range(i * model_parallel_size, (i + 1) * model_parallel_size)\n            group = torch.distributed.new_group(ranks)\n            if i == (rank // model_parallel_size):\n                _MODEL_PARALLEL_GROUP = group\n\n    global _FP32_ALLREDUCE\n    assert _FP32_ALLREDUCE is None, \"fp32_allreduce is already initialized\"\n    _FP32_ALLREDUCE = fp32_allreduce\n\n\ndef model_parallel_is_initialized():\n    \"\"\"Check if model and data parallel groups are initialized.\"\"\"\n    if _MODEL_PARALLEL_GROUP is None or _DATA_PARALLEL_GROUP is None:\n        return False\n    return True\n\n\ndef get_model_parallel_group():\n    \"\"\"Get the model parallel group the caller rank belongs to.\"\"\"\n    assert _MODEL_PARALLEL_GROUP is not None, \"model parallel group is not initialized\"\n    return _MODEL_PARALLEL_GROUP\n\n\ndef get_data_parallel_group():\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\n    assert _DATA_PARALLEL_GROUP is not None, \"data parallel group is not initialized\"\n    return _DATA_PARALLEL_GROUP\n\n\ndef get_io_parallel_group():\n    \"\"\"Get the IO parallel group the caller rank belongs to.\"\"\"\n    assert _IO_PARALLEL_GROUP is not None, \"IO parallel group is not initialized\"\n    return _IO_PARALLEL_GROUP\n\n\ndef set_model_parallel_world_size(world_size):\n    \"\"\"Set the model parallel size\"\"\"\n    global _MPU_WORLD_SIZE\n    _MPU_WORLD_SIZE = world_size\n\n\ndef get_model_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    global _MPU_WORLD_SIZE\n    if _MPU_WORLD_SIZE is not None:\n        return _MPU_WORLD_SIZE\n    return torch.distributed.get_world_size(group=get_model_parallel_group())\n\n\ndef set_model_parallel_rank(rank):\n    \"\"\"Set model parallel rank.\"\"\"\n    global _MPU_RANK\n    _MPU_RANK = rank\n\n\ndef get_model_parallel_rank():\n    \"\"\"Return my rank for the model parallel group.\"\"\"\n    global _MPU_RANK\n    if _MPU_RANK is not None:\n        return _MPU_RANK\n    return torch.distributed.get_rank(group=get_model_parallel_group())\n\n\ndef get_model_parallel_src_rank():\n    \"\"\"Calculate the global rank corresponding to a local rank zero\n    in the model parallel group.\"\"\"\n    global_rank = torch.distributed.get_rank()\n    local_world_size = get_model_parallel_world_size()\n    return (global_rank // local_world_size) * local_world_size\n\n\ndef get_data_parallel_src_rank():\n    \"\"\"Calculate the global rank corresponding to a local rank zero\n    in the data parallel group.\"\"\"\n    global_rank = torch.distributed.get_rank()\n    topo = get_topology()\n    if topo is None:\n        # we are just using model parallel\n        return global_rank % get_model_parallel_world_size()\n    else:\n        # We are using pipeline parallel\n        d = topo.get_axis_comm_lists(\"data\")\n        for l in d:\n            if global_rank in l:\n                return l[0]\n\n\ndef get_data_parallel_world_size():\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return torch.distributed.get_world_size(group=get_data_parallel_group())\n\n\ndef get_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return torch.distributed.get_rank(group=get_data_parallel_group())\n\n\ndef get_topology():\n    return _MPU_TOPOLOGY\n\n\ndef get_pipe_parallel_group():\n    \"\"\"Get the pipe parallel group the caller rank belongs to.\"\"\"\n    assert _PIPE_PARALLEL_GROUP is not None, \"data parallel group is not initialized\"\n    return _PIPE_PARALLEL_GROUP\n\n\ndef get_pipe_parallel_rank():\n    \"\"\"Return my rank for the pipe parallel group.\"\"\"\n    return torch.distributed.get_rank(group=get_pipe_parallel_group())\n\n\ndef get_pipe_parallel_world_size():\n    \"\"\"Return world size for the pipe parallel group.\"\"\"\n    return torch.distributed.get_world_size(group=get_pipe_parallel_group())\n\n\ndef get_expert_tokens_for_rank(\n    routed_tokens: torch.Tensor,\n    tokens_per_expert: torch.Tensor,\n    rank: Optional[int] = None,\n):\n    \"\"\"\n    Allow user to specify rank, fall back on this device\n    \"\"\"\n    # Calculate cumulative sums of tokens_per_expert, ensure the shapes are correct\n    world_size = get_model_parallel_world_size()\n    if rank is None:\n        rank = get_model_parallel_rank()\n\n    # TODO: is this check necessary here/what does it cost us to redundantly do it in multiple places?\n    assert tokens_per_expert.shape[0] % world_size == 0\n\n    cumulative_sums = torch.cumsum(tokens_per_expert, dim=0)\n    assert cumulative_sums[-1] == routed_tokens.shape[0]\n\n    # select the right starting and ending indices from the cumsum to figure out what tokens to select\n    rank_expert_indices = cumulative_sums.chunk(world_size)\n    start_index = rank_expert_indices[rank - 1][-1] if rank > 0 else 0\n    end_index = rank_expert_indices[rank][-1]\n\n    # Use indices to select the chunk of the tokens matrix\n    selected_experts = routed_tokens[start_index:end_index]\n\n    return selected_experts\n\n\ndef get_expert_token_counts_for_rank(\n    tokens_per_expert: torch.Tensor, rank: Optional[int] = None\n):\n    \"\"\"\n    Allow user to specify rank, fall back on this device\n    \"\"\"\n    # TODO: add bounds checking of size is 1D for tokens_per_expert\n    # should be (num_experts) long\n    world_size = get_model_parallel_world_size()\n    if rank is None:\n        rank = get_model_parallel_rank()\n\n    return tokens_per_expert.chunk(world_size)[rank]\n\n\ndef set_tensor_model_parallel_world_size(world_size):\n    \"\"\"Set the tensor model parallel size\"\"\"\n    set_model_parallel_world_size(world_size)\n\n\ndef get_tensor_model_parallel_group():\n    \"\"\"Get the tensor model parallel group the caller rank belongs to.\"\"\"\n    return get_model_parallel_group()\n\n\ndef get_tensor_model_parallel_src_rank():\n    \"\"\"Calculate the global rank corresponding to the first local rank\n    in the tensor model parallel group.\"\"\"\n    return get_model_parallel_rank()\n\n\n# Needed for MOE. True tensor parallelism todo.\ndef get_tensor_model_parallel_world_size():\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    return get_model_parallel_world_size()\n\n\ndef set_tensor_model_parallel_rank(rank):\n    \"\"\"Set tensor model parallel rank.\"\"\"\n    set_model_parallel_rank(rank)\n\n\ndef get_tensor_model_parallel_rank():\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    return get_model_parallel_rank()\n\n\ndef destroy_model_parallel():\n    \"\"\"Set the groups to none.\"\"\"\n    global _MODEL_PARALLEL_GROUP\n    _MODEL_PARALLEL_GROUP = None\n    global _DATA_PARALLEL_GROUP\n    _DATA_PARALLEL_GROUP = None\n    global _PIPE_PARALLEL_GROUP\n    _PIPE_PARALLEL_GROUP = None\n    global _IO_PARALLEL_GROUP\n    _IO_PARALLEL_GROUP = None\n    global _MPU_WORLD_SIZE\n    global _MPU_RANK\n    _MPU_WORLD_SIZE = None\n    _MPU_RANK = None\n    global _MPU_TOPOLOGY\n    _MPU_TOPOLOGY = None\n    global _FP32_ALLREDUCE\n    _FP32_ALLREDUCE = None\n\n\ndef get_fp32_allreduce():\n    \"\"\"Get the fp32 allreduce flag\"\"\"\n    assert _FP32_ALLREDUCE is not None, \"fp32_allreduce is not Initialized\"\n    return _FP32_ALLREDUCE\n",
        "megatron/mpu/layers.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Parts of the code here are adapted from PyTorch\n# repo: https://github.com/pytorch/pytorch\n\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\n\nfrom .initialize import get_model_parallel_rank\nfrom .initialize import get_model_parallel_world_size\nfrom .mappings import copy_to_model_parallel_region\nfrom .mappings import gather_from_model_parallel_region\nfrom .mappings import reduce_from_model_parallel_region\nfrom .mappings import scatter_to_model_parallel_region\nfrom .mappings import reduce_scatter_to_sequence_parallel_region\nfrom .mappings import gather_from_sequence_parallel_region\nfrom .random import get_cuda_rng_tracker\nfrom .utils import divide\nfrom .utils import VocabUtility\nfrom functools import partial\n\n\ndef _initialize_affine_weight_gpu(weight, init_method, partition_dim, stride=1):\n    \"\"\"Initialize affine weight for model parallel on GPU.\"\"\"\n\n    weight.model_parallel = True\n    weight.partition_dim = partition_dim\n    weight.partition_stride = stride\n\n    with get_cuda_rng_tracker().fork():\n        init_method(weight)\n\n\ndef _initialize_affine_weight_cpu(\n    neox_args,\n    weight,\n    output_size,\n    input_size,\n    per_partition_size,\n    partition_dim,\n    init_method,\n    stride=1,\n    return_master_weight=False,\n):\n    \"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\"\"\"\n\n    weight.model_parallel = True\n    weight.partition_dim = partition_dim\n    weight.partition_stride = stride\n\n    # Initialize master weight\n    master_weight = torch.empty(\n        output_size, input_size, dtype=torch.float, requires_grad=False\n    )\n    init_method(master_weight)\n    master_weight = master_weight.to(dtype=neox_args.params_dtype)\n\n    # Split and copy\n    per_partition_per_stride_size = divide(per_partition_size, stride)\n    weight_list = torch.split(\n        master_weight, per_partition_per_stride_size, dim=partition_dim\n    )\n    rank = get_model_parallel_rank()\n    world_size = get_model_parallel_world_size()\n    my_weight_list = weight_list[rank::world_size]\n\n    with torch.no_grad():\n        torch.cat(my_weight_list, dim=partition_dim, out=weight)\n    if return_master_weight:\n        return master_weight\n    return None\n\n\nclass VocabParallelEmbedding(torch.nn.Module):\n    \"\"\"Embedding parallelized in the vocabulary dimension.\n\n    This is mainly adapted from torch.nn.Embedding and all the default\n    values are kept.\n    Arguments:\n        num_embeddings: vocabulary size.\n        embedding_dim: size of hidden state.\n        init_method: method to initialize weights.\n    \"\"\"\n\n    def __init__(\n        self, neox_args, num_embeddings, embedding_dim, init_method=init.xavier_normal_\n    ):\n        super(VocabParallelEmbedding, self).__init__()\n        # Keep the input dimensions.\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        # Set the detauls for compatibility.\n        self.padding_idx = None\n        self.max_norm = None\n        self.norm_type = 2.0\n        self.scale_grad_by_freq = False\n        self.sparse = False\n        self._weight = None\n        self.model_parallel_size = get_model_parallel_world_size()\n        # Divide the weight matrix along the vocabulary dimension.\n        (\n            self.vocab_start_index,\n            self.vocab_end_index,\n        ) = VocabUtility.vocab_range_from_global_vocab_size(\n            self.num_embeddings, get_model_parallel_rank(), self.model_parallel_size\n        )\n        self.num_embeddings_per_partition = (\n            self.vocab_end_index - self.vocab_start_index\n        )\n        self.init_method = init_method\n\n        # Allocate weights and initialize.\n        if neox_args.use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_embeddings_per_partition,\n                    self.embedding_dim,\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.num_embeddings,\n                self.embedding_dim,\n                self.num_embeddings_per_partition,\n                0,\n                init_method,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_embeddings_per_partition,\n                    self.embedding_dim,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=0, stride=1\n            )\n\n    def mup_reinitialize_weights(self, neox_args):\n        if neox_args.use_cpu_initialization:\n            _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.num_embeddings,\n                self.embedding_dim,\n                self.num_embeddings_per_partition,\n                0,\n                partial(self.init_method, use_mup=True),\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=0,\n                stride=1,\n            )\n\n    def forward(self, input_):\n        if self.model_parallel_size > 1:\n            # Build the mask.\n            input_mask = (input_ < self.vocab_start_index) | (\n                input_ >= self.vocab_end_index\n            )\n            # Mask the input.\n            masked_input = input_.clone() - self.vocab_start_index\n            masked_input[input_mask] = 0\n        else:\n            masked_input = input_\n            # Get the embeddings.\n        output_parallel = F.embedding(\n            masked_input,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n        # Mask the output embedding.\n        if self.model_parallel_size > 1:\n            output_parallel[input_mask, :] = 0.0\n        # Reduce across all the model parallel GPUs.\n        output = reduce_from_model_parallel_region(output_parallel)\n        return output\n\n\nclass ParallelRelativePositionBias(torch.nn.Module):\n    \"\"\"T5 Relative Position Bias parallelized in the heads dimension\n\n    Based on https://github.com/lucidrains/x-transformers/blob/6b93c21be0d0a679da6f7b9621d9bb638ab18428/x_transformers/x_transformers.py#L106 (14.12.2021)\n    and adapted for megatron's model parallelism\n\n    Arguments:\n        scale: scaling factor for the bias\n        causal: flag for causal/non-causal language modelling.\n        num_buckets: number of rp buckets.\n        max_distance: max distance in sequence dim for each bucket.\n        heads: number of attention heads (total)\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        scale,\n        causal=True,\n        num_buckets=32,\n        max_distance=128,\n        heads=8,\n        init_method=init.xavier_normal_,\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.heads = heads\n\n        # Set the defaults for compatibility.\n        self.padding_idx = None\n        self.max_norm = None\n        self.norm_type = 2.0\n        self.scale_grad_by_freq = False\n        self.sparse = False\n        self._weight = None\n        self.model_parallel_size = get_model_parallel_world_size()\n        self.model_parallel_rank = get_model_parallel_rank()\n\n        # Divide the weight matrix along the heads dimension.\n        self.head_start_index, self.head_end_index = self.get_heads_range(\n            self.heads, self.model_parallel_rank, self.model_parallel_size\n        )\n        self.num_heads_per_partition = self.head_end_index - self.head_start_index\n        self.init_method = init_method\n\n        # Allocate weights and initialize.\n        if neox_args.use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_buckets,\n                    self.num_heads_per_partition,\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.num_buckets,\n                self.heads,\n                self.num_heads_per_partition,\n                partition_dim=1,\n                init_method=init_method,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_buckets,\n                    self.num_heads_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=1, stride=1\n            )\n        self._q_len_cached = None\n        self._k_len_cached = None\n        self._rel_pos_bucket_cached = None\n\n    def mup_reinitialize_weights(self, neox_args):\n        if self.use_cpu_initialization:\n            _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.num_buckets,\n                self.heads,\n                self.num_heads_per_partition,\n                partition_dim=1,\n                init_method=partial(self.init_method, use_mup=True),\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=1,\n                stride=1,\n            )\n\n    @staticmethod\n    def get_heads_range(global_n_heads, rank, world_size):\n        per_partition_n_heads = divide(global_n_heads, world_size)\n        index_f = rank * per_partition_n_heads\n        index_l = index_f + per_partition_n_heads\n        return index_f, index_l\n\n    def _relative_position_bucket(\n        self, relative_position, num_buckets=32, max_distance=128\n    ):\n        ret = 0\n        n = -relative_position\n        if not self.causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = (\n            max_exact\n            + (\n                torch.log(n.float() / max_exact)\n                / math.log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n            ).long()\n        )\n        val_if_large = torch.min(\n            val_if_large, torch.full_like(val_if_large, num_buckets - 1)\n        )\n\n        ret += torch.where(is_small, n, val_if_large)\n        self._rel_pos_bucket_cached = ret\n        return self._rel_pos_bucket_cached\n\n    def forward(self, q_len, k_len):\n        if self._q_len_cached != q_len or self._k_len_cached != k_len:\n            # cache bucket if first step seq len stays constant\n            self._q_len_cached, self._k_len_cached = q_len, k_len\n            q_pos = torch.arange(\n                q_len, dtype=torch.long, device=torch.cuda.current_device()\n            )\n            k_pos = torch.arange(\n                k_len, dtype=torch.long, device=torch.cuda.current_device()\n            )\n            rel_pos = k_pos[None, :] - q_pos[:, None]\n            rp_bucket = self._relative_position_bucket(\n                rel_pos, num_buckets=self.num_buckets, max_distance=self.max_distance\n            )\n        else:\n            rp_bucket = self._rel_pos_bucket_cached\n        values = F.embedding(\n            rp_bucket,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n        bias = values.movedim(2, 0).unsqueeze(0)\n        return bias * self.scale\n\n\nclass ColumnParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with column parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its second dimension as A = [A_1, ..., A_p].\n\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias\n        gather_output: If true, call all-gather on output and make Y available\n                       to all GPUs, otherwise, every GPU will have its output\n                       which is Y_i = XA_i\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimations where bias\n                       can be fused with other elementwise operations. we skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        input_size,\n        output_size,\n        bias=True,\n        gather_output=True,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        mup_rescale_parameters=False,\n        seq_dim=0,  # Dimension which is the seq_len dimension. final ParallelLinear overrides this to be 1 ; otherwise, the default is used throughout.\n    ):\n        super(ColumnParallelLinear, self).__init__()\n\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.gather_output = gather_output\n        # Divide the weight matrix along the last dimension.\n        world_size = get_model_parallel_world_size()\n        self.output_size_per_partition = divide(output_size, world_size)\n        self.skip_bias_add = skip_bias_add\n\n        self.sequence_parallel = neox_args.sequence_parallel\n        self.seq_dim = seq_dim\n\n        self.init_method = init_method\n        self.stride = stride\n        self.mup_rescale_parameters = mup_rescale_parameters\n        self.use_mup = neox_args.use_mup\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        # Initialize weight.\n        if neox_args.use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size_per_partition,\n                    self.input_size,\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.output_size_per_partition,\n                0,\n                init_method,\n                stride=stride,\n                return_master_weight=keep_master_weight_for_test,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size_per_partition,\n                    self.input_size,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=0, stride=stride\n            )\n\n        if bias:\n            if neox_args.use_cpu_initialization:\n                self.bias = Parameter(\n                    torch.empty(\n                        self.output_size_per_partition, dtype=neox_args.params_dtype\n                    )\n                )\n            else:\n                self.bias = Parameter(\n                    torch.empty(\n                        self.output_size_per_partition,\n                        device=torch.cuda.current_device(),\n                        dtype=neox_args.params_dtype,\n                    )\n                )\n            self.bias.model_parallel = True\n            self.bias.partition_dim = 0\n            self.bias.stride = stride\n            # Always initialize bias to zero.\n            with torch.no_grad():\n                self.bias.zero_()\n        else:\n            self.register_parameter(\"bias\", None)\n\n    # Copied from Mup\n    def width_mult(self):\n        assert hasattr(self.weight, \"infshape\"), (\n            \"Please call set_base_shapes(...). If using torch.nn.DataParallel, \"\n            \"switch to distributed training with \"\n            \"torch.nn.parallel.DistributedDataParallel instead\"\n        )\n        return self.weight.infshape.width_mult()\n\n    # Copied from Mup\n    def _rescale_parameters(self):\n        \"\"\"Rescale parameters to convert SP initialization to μP initialization.\n        Warning: This method is NOT idempotent and should be called only once\n        unless you know what you are doing.\n        \"\"\"\n        if hasattr(self, \"_has_rescaled_params\") and self._has_rescaled_params:\n            raise RuntimeError(\n                \"`_rescale_parameters` has been called once before already. \"\n                \"Unless you know what you are doing, usually you should not be calling `_rescale_parameters` more than once.\\n\"\n                \"If you called `set_base_shapes` on a model loaded from a checkpoint, \"\n                \"or just want to re-set the base shapes of an existing model, \"\n                \"make sure to set the flag `rescale_params=False`.\\n\"\n                \"To bypass this error and *still rescale parameters*, set `self._has_rescaled_params=False` before this call.\"\n            )\n        if self.bias is not None:\n            self.bias.data *= self.width_mult() ** 0.5\n        self.weight.data *= self.width_mult() ** 0.5\n        self._has_rescaled_params = True\n\n    def mup_reinitialize_weights(self, neox_args):\n        if neox_args.use_cpu_initialization:\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.output_size_per_partition,\n                0,\n                partial(self.init_method, use_mup=True),\n                stride=self.stride,\n                return_master_weight=keep_master_weight_for_test,\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=0,\n                stride=self.stride,\n            )\n\n    def set_parallel_output(self, value: bool):\n        assert isinstance(value, bool)\n        self.gather_output = (\n            not value\n        )  # if gather_output is True, parallel output is False, so we set the opposite\n\n    def forward(self, input_):\n        if self.use_mup and self.mup_rescale_parameters:\n            input_ /= self.width_mult()\n\n        if self.sequence_parallel:\n            input_parallel = input_\n        else:\n            # Set up backprop all-reduce.\n            input_parallel = copy_to_model_parallel_region(input_)\n        # Matrix multiply.\n\n        if self.sequence_parallel:\n            # do an AG in the fwd pass, RS in bwd pass.\n            # gather / scatter portion happens across the sequence dim (self.seq_dim)--\n            # almost always is [s, b, h] and so dim 0, but for lm_head ParallelLinear it is seq_dim=1 and [b, s, h]\n            input_parallel = gather_from_sequence_parallel_region(\n                input_parallel, seq_dim=self.seq_dim\n            )\n\n        bias = self.bias if not self.skip_bias_add else None\n        output_parallel = F.linear(input_parallel, self.weight, bias)\n        if self.gather_output:\n            # All-gather across the partitions.\n            assert (\n                not self.sequence_parallel\n            ), \"sequence_parallel=True and gather_output=True are incompatible!\"\n            output = gather_from_model_parallel_region(output_parallel)\n        else:\n            output = output_parallel\n        output_bias = self.bias if self.skip_bias_add else None\n        return output, output_bias\n\n\nclass RowParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with row parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its first dimension and X along its second dimension as:\n               -   -\n              | A_1 |\n              | .   |\n          A = | .   |        X = [X_1, ..., X_p]\n              | .   |\n              | A_p |\n               -   -\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias. Note that bias is not parallelized.\n        input_is_parallel: If true, we assume that the input is already\n                           split across the GPUs and we do not split\n                           again.\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimations where bias\n                       can be fused with other elementwise operations. we skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n        input_size,\n        output_size,\n        bias=True,\n        input_is_parallel=False,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        parallel_output=False,\n        mup_rescale_parameters=False,\n    ):\n        super(RowParallelLinear, self).__init__()\n\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.input_is_parallel = input_is_parallel\n        # Divide the weight matrix along the last dimension.\n        world_size = get_model_parallel_world_size()\n        self.input_size_per_partition = divide(input_size, world_size)\n        self.skip_bias_add = skip_bias_add\n        self.parallel_output = parallel_output\n\n        self.sequence_parallel = neox_args.sequence_parallel\n        assert not (\n            self.sequence_parallel and not self.input_is_parallel\n        ), \"Cannot have self.input_is_parallel=False and self.sequence_parallel=True.\"\n\n        self.init_method = init_method\n        self.stride = stride\n        self.keep_master_weight_for_test = keep_master_weight_for_test\n        self.mup_rescale_parameters = mup_rescale_parameters\n        self.use_mup = neox_args.use_mup\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        # Initialize weight.\n        if neox_args.use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size,\n                    self.input_size_per_partition,\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.input_size_per_partition,\n                1,\n                init_method,\n                stride=stride,\n                return_master_weight=keep_master_weight_for_test,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size,\n                    self.input_size_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=1, stride=stride\n            )\n        if bias:\n            if neox_args.use_cpu_initialization:\n                self.bias = Parameter(\n                    torch.empty(self.output_size, dtype=neox_args.params_dtype)\n                )\n            else:\n                self.bias = Parameter(\n                    torch.empty(\n                        self.output_size,\n                        device=torch.cuda.current_device(),\n                        dtype=neox_args.params_dtype,\n                    )\n                )\n            # Always initialize bias to zero.\n            with torch.no_grad():\n                self.bias.zero_()\n        else:\n            self.register_parameter(\"bias\", None)\n\n    # Copied from Mup\n    def width_mult(self):\n        assert hasattr(self.weight, \"infshape\"), (\n            \"Please call set_base_shapes(...). If using torch.nn.DataParallel, \"\n            \"switch to distributed training with \"\n            \"torch.nn.parallel.DistributedDataParallel instead\"\n        )\n        return self.weight.infshape.width_mult()\n\n    # Copied from Mup\n    def _rescale_parameters(self):\n        \"\"\"Rescale parameters to convert SP initialization to μP initialization.\n        Warning: This method is NOT idempotent and should be called only once\n        unless you know what you are doing.\n        \"\"\"\n        if hasattr(self, \"_has_rescaled_params\") and self._has_rescaled_params:\n            raise RuntimeError(\n                \"`_rescale_parameters` has been called once before already. \"\n                \"Unless you know what you are doing, usually you should not be calling `_rescale_parameters` more than once.\\n\"\n                \"If you called `set_base_shapes` on a model loaded from a checkpoint, \"\n                \"or just want to re-set the base shapes of an existing model, \"\n                \"make sure to set the flag `rescale_params=False`.\\n\"\n                \"To bypass this error and *still rescale parameters*, set `self._has_rescaled_params=False` before this call.\"\n            )\n        if self.bias is not None:\n            self.bias.data *= self.width_mult() ** 0.5\n        self.weight.data *= self.width_mult() ** 0.5\n        self._has_rescaled_params = True\n\n    def mup_reinitialize_weights(self, neox_args):\n        if neox_args.use_cpu_initialization:\n            self.master_weight = _initialize_affine_weight_cpu(\n                neox_args,\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.input_size_per_partition,\n                1,\n                partial(self.init_method, use_mup=True),\n                stride=self.stride,\n                return_master_weight=self.keep_master_weight_for_test,\n            )\n        else:\n            _initialize_affine_weight_gpu(\n                self.weight,\n                partial(self.init_method, use_mup=True),\n                partition_dim=1,\n                stride=self.stride,\n            )\n\n    def set_parallel_output(self, parallel_output: bool):\n        assert isinstance(parallel_output, bool)\n        self.parallel_output = parallel_output\n\n    def forward(self, input_):\n        if self.use_mup and self.mup_rescale_parameters:\n            input_ /= self.width_mult()\n        # Set up backprop all-reduce.\n        if self.input_is_parallel:\n            input_parallel = input_\n        else:\n            input_parallel = scatter_to_model_parallel_region(input_)\n        # Matrix multiply.\n        output_parallel = F.linear(input_parallel, self.weight)\n        # All-reduce across all the partitions.\n        if self.sequence_parallel and not self.parallel_output:\n            # do an RS in the fwd pass, AG in bwd pass.\n            # skip in the gpt-j parallel sublayer case (self.parallel_output=True)\n            # (user responsible for calling reduce-scatter)\n            output_ = reduce_scatter_to_sequence_parallel_region(output_parallel)\n        elif not self.parallel_output:\n            output_ = reduce_from_model_parallel_region(output_parallel)\n        else:\n            output_ = output_parallel\n        if not self.skip_bias_add:\n            output = output_ + self.bias if self.bias is not None else output_\n            output_bias = None\n        else:\n            output = output_\n            output_bias = self.bias\n        return output, output_bias\n",
        "megatron/mpu/mappings.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .initialize import (\n    get_expert_tokens_for_rank,\n    get_model_parallel_group,\n    get_model_parallel_world_size,\n    get_model_parallel_rank,\n    get_fp32_allreduce,\n    get_expert_token_counts_for_rank,\n)\nfrom .utils import split_tensor_along_last_dim, split_tensor_along_any_dim\n\n\ndef _reduce(input_):\n    \"\"\"All-reduce the the input tensor across model parallel group.\"\"\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if get_model_parallel_world_size() == 1:\n        return input_\n\n    # upcast to fp32 if using fp32 allreduce\n    dt = input_.dtype\n    if get_fp32_allreduce():\n        input_ = input_.float()\n\n    # All-reduce.\n    torch.distributed.all_reduce(input_, group=get_model_parallel_group())\n\n    # reconvert to original Bf16/Fp16 dtype\n    if get_fp32_allreduce():\n        input_ = input_.to(dt)\n\n    return input_\n\n\ndef _split(input_):\n    \"\"\"Split the tensor along its last dimension and keep the\n    corresponding slice.\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along last dimension.\n    input_list = split_tensor_along_last_dim(input_, world_size)\n\n    # Note: torch.split does not create contiguous tensors by default.\n    rank = get_model_parallel_rank()\n    output = input_list[rank].contiguous()\n\n    return output\n\n\ndef _gather(input_):\n    \"\"\"Gather tensors and concatinate along the last dimension.\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Size and dimension.\n    last_dim = input_.dim() - 1\n    rank = get_model_parallel_rank()\n\n    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n    tensor_list[rank] = input_\n    torch.distributed.all_gather(tensor_list, input_, group=get_model_parallel_group())\n\n    # Note: torch.cat already creates a contiguous tensor.\n    output = torch.cat(tensor_list, dim=last_dim).contiguous()\n\n    return output\n\n\ndef _dmoe_reduce(input_, tokens_per_expert):\n    \"\"\"All-reduce the the dMoE input tensor across model parallel group.\"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if get_model_parallel_world_size() == 1:\n        return input_\n\n    # Bf16 convert\n    dt = input_.dtype\n    if dt == torch.bfloat16 and get_fp32_allreduce():\n        input_ = input_.float()\n\n    output = torch.zeros(\n        (sum(tokens_per_expert), input_.shape[-1]),\n        dtype=input_.dtype,\n        device=input_.device,\n    )\n    world_size = get_model_parallel_world_size()\n    rank = get_model_parallel_rank()\n\n    cumulative_sums = torch.cumsum(tokens_per_expert, dim=0)\n\n    # select the right starting and ending indices from the cumsum to figure out what tokens to select\n    rank_expert_indices = cumulative_sums.chunk(world_size)\n    start_index = rank_expert_indices[rank - 1][-1] if rank > 0 else 0\n    end_index = rank_expert_indices[rank][-1]\n\n    output[start_index:end_index] = input_\n\n    # All-reduce.\n    torch.distributed.all_reduce(output, group=get_model_parallel_group())\n\n    # Bf16 convert\n    if dt == torch.bfloat16 and get_fp32_allreduce():\n        output = output.bfloat16()\n\n    return output\n\n\ndef _dmoe_split(input_, tokens_per_expert):\n    \"\"\"Split the tensor along its first dimension according to where tokens\n    were routed, keeping the corresponding slice.\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along first dimension, getting the expert tokens\n    output = get_expert_tokens_for_rank(input_, tokens_per_expert)\n\n    return output\n\n\ndef _dmoe_gather(input_: torch.Tensor, tokens_per_expert: torch.Tensor):\n    \"\"\"Gather tensors and concatinate along the first dimension)\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Bf16 convert\n    dt = input_.dtype\n    if dt == torch.bfloat16 and get_fp32_allreduce():\n        input_ = input_.float()\n\n    # Gather along first dimension\n    gather_dim = 0\n    rank = get_model_parallel_rank()\n\n    tokens_by_rank = [\n        get_expert_token_counts_for_rank(tokens_per_expert, r)\n        for r in range(world_size)\n    ]\n    # print(f\"{torch.cuda.current_device()}: tokens_by_rank {tokens_by_rank}\")\n    tensor_list = [\n        torch.empty(sum(r), input_.shape[-1], device=input_.device, dtype=input_.dtype)\n        for r in tokens_by_rank\n    ]\n    tensor_list[rank] = input_\n    torch.distributed.all_gather(tensor_list, input_, group=get_model_parallel_group())\n\n    # Note: torch.cat already creates a contiguous tensor.\n    output = torch.cat(tensor_list, dim=gather_dim)\n\n    # Bf16 convert\n    if dt == torch.bfloat16 and get_fp32_allreduce():\n        output = output.bfloat16()\n\n    return output\n\n\ndef _reduce_scatter_along_seq_dim(input_, seq_dim):\n    \"\"\"Reduce-scatter the input tensor across model parallel group, scattering across sequence dim.\"\"\"\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # upcast to fp32 if using fp32 allreduce\n    dt = input_.dtype\n    if get_fp32_allreduce():\n        input_ = input_.float()\n\n    dim_size = list(input_.size())\n    assert (\n        isinstance(seq_dim, int) and seq_dim < len(dim_size) and seq_dim >= 0\n    ), \"seq_dim must be a valid tensor dim\"\n    assert dim_size[seq_dim] % world_size == 0\n\n    if seq_dim == 0:\n        # reduce_scatter_tensor is faster but only works correctly on dimension 0\n        dim_size[seq_dim] = dim_size[seq_dim] // world_size\n        output = torch.empty(\n            dim_size, dtype=input_.dtype, device=torch.cuda.current_device()\n        )\n        torch.distributed.reduce_scatter_tensor(\n            output, input_.contiguous(), group=get_model_parallel_group()\n        )\n    else:\n        tensor_list = list(\n            torch.split(input_, input_.shape[seq_dim] // world_size, seq_dim)\n        )\n        output = torch.empty_like(tensor_list[0])\n        torch.distributed.reduce_scatter(\n            output, tensor_list, group=get_model_parallel_group()\n        )\n\n    # reconvert to original Bf16/Fp16 dtype\n    if get_fp32_allreduce():\n        output = output.to(dt)\n\n    return output\n\n\ndef _gather_along_seq_dim(input_, seq_dim):\n    \"\"\"Gather tensors and concatinate along the (manually-specified) sequence dimension.\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    assert (\n        isinstance(seq_dim, int) and seq_dim < len(dim_size) and seq_dim >= 0\n    ), \"seq_dim must be a valid tensor dim\"\n    dim_size[seq_dim] = dim_size[seq_dim] * world_size\n\n    if seq_dim == 0:\n        # reduce_gather_tensor is faster but only works correctly on dimension 0\n        output = torch.empty(\n            dim_size, dtype=input_.dtype, device=torch.cuda.current_device()\n        )\n        torch.distributed.all_gather_into_tensor(\n            output, input_.contiguous(), group=get_model_parallel_group()\n        )\n    else:\n        input_ = input_.contiguous()\n        rank = get_model_parallel_rank()\n        tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n        tensor_list[rank] = input_\n        torch.distributed.all_gather(\n            tensor_list, input_, group=get_model_parallel_group()\n        )\n        output = torch.cat(tensor_list, dim=seq_dim)\n\n    return output\n\n\ndef _split_along_seq_dim(input_, seq_dim):\n    \"\"\"Split the tensor along the sequence dimension (as manually selected) and keep the\n    corresponding slice.\"\"\"\n\n    world_size = get_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along second dimension.\n    input_list = split_tensor_along_any_dim(input_, world_size, seq_dim)\n\n    # Note: torch.split does not create contiguous tensors by default.\n    rank = get_model_parallel_rank()\n    output = input_list[rank].contiguous()\n\n    return output\n\n\nclass _CopyToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Pass the input to the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return input_\n\n    @staticmethod\n    def forward(ctx, input_):\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _reduce(grad_output)\n\n\nclass _CopyToExpertModelParallelRegion(torch.autograd.Function):\n    \"\"\"Pass the input to the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, tokens_per_expert):\n        # TODO: not sure if this is sufficient? not sure how this gets used downstream...\n        return get_expert_tokens_for_rank(input_, tokens_per_expert)\n\n    @staticmethod\n    def forward(ctx, input_, tokens_per_expert):\n        # Save tokens_per_expert in the context for later use in the backward pass\n        ctx.save_for_backward(tokens_per_expert)\n\n        return get_expert_tokens_for_rank(input_, tokens_per_expert)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve the tokens_per_expert from the context\n        (tokens_per_expert,) = ctx.saved_tensors\n\n        # no grad for tokens_per_expert\n        # return _dmoe_reduce(grad_output, tokens_per_expert), None\n        return _dmoe_gather(grad_output, tokens_per_expert), None\n\n\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _reduce(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _reduce(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass _ScatterToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _split(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _split(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather(grad_output)\n\n\nclass _GatherFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from model parallel region and concatinate.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _gather(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _gather(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _split(grad_output)\n\n\nclass _GatherFromExpertModelParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from expert model parallel region and concatinate.\n\n    The major difference between this and _GatherFromModelParallelRegion is in the\n    dMoE case, we need to gather & split along the first dimension, not the last\n    \"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, tokens_per_expert):\n        # TODO: not sure if this is sufficient? not sure how this gets used downstream...\n        return _dmoe_gather(input_, tokens_per_expert)\n\n    @staticmethod\n    def forward(ctx, input_, tokens_per_expert):\n        # Save tokens_per_expert in the context for later use in the backward pass\n        ctx.save_for_backward(tokens_per_expert)\n\n        return _dmoe_gather(input_, tokens_per_expert)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve the tokens_per_expert from the context\n        (tokens_per_expert,) = ctx.saved_tensors\n\n        # no grad for tokens_per_expert\n        return _dmoe_split(grad_output, tokens_per_expert), None\n\n\nclass _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Reduce-Scatter across sequence parallel region (same as model parallel region.)\n    Note: same region as model parallel region\n    \"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, seq_dim):\n        return _reduce_scatter_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def forward(ctx, input_, seq_dim):\n        ctx.seq_dim = seq_dim\n        return _reduce_scatter_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        seq_dim = ctx.seq_dim\n        return _gather_along_seq_dim(grad_output, seq_dim=seq_dim), None\n\n\nclass _GatherFromSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"All-Gather across sequence parallel region (same region as model parallel region.)\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, seq_dim):\n        return _gather_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def forward(ctx, input_, seq_dim):\n        ctx.seq_dim = seq_dim\n        return _gather_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        seq_dim = ctx.seq_dim\n        return _reduce_scatter_along_seq_dim(grad_output, seq_dim=seq_dim), None\n\n\nclass _ScatterToSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Scatter (split) sequence length across sequence parallel region (=> same region as model parallel.)\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, seq_dim):\n        return _split_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def forward(ctx, input_, seq_dim):\n        ctx.seq_dim = seq_dim\n        return _split_along_seq_dim(input_, seq_dim=seq_dim)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        seq_dim = ctx.seq_dim\n        return (\n            _gather_along_seq_dim(grad_output, seq_dim=seq_dim),\n            None,\n        )\n\n\n# -----------------\n# Helper functions.\n# -----------------\n\n\ndef copy_to_model_parallel_region(input_):\n    return _CopyToModelParallelRegion.apply(input_)\n\n\ndef copy_to_expert_model_parallel_region(input_, tokens_per_expert):\n    return _CopyToExpertModelParallelRegion.apply(input_, tokens_per_expert)\n\n\ndef reduce_from_model_parallel_region(input_):\n    return _ReduceFromModelParallelRegion.apply(input_)\n\n\ndef scatter_to_model_parallel_region(input_):\n    return _ScatterToModelParallelRegion.apply(input_)\n\n\ndef gather_from_model_parallel_region(input_):\n    return _GatherFromModelParallelRegion.apply(input_)\n\n\ndef gather_from_expert_model_parallel_region(input_, tokens_per_expert):\n    return _GatherFromExpertModelParallelRegion.apply(input_, tokens_per_expert)\n\n\ndef reduce_scatter_to_sequence_parallel_region(input_, seq_dim=0):\n    return _ReduceScatterToSequenceParallelRegion.apply(input_, seq_dim)\n\n\ndef gather_from_sequence_parallel_region(input_, seq_dim=0):\n    return _GatherFromSequenceParallelRegion.apply(input_, seq_dim)\n\n\ndef scatter_to_sequence_parallel_region(\n    input_, seq_dim=1\n):  # use this fn in scattering input embeds across TP ranks. There, shape of inps is [b, s, h] instead of the usual [s, b, h]\n    return _ScatterToSequenceParallelRegion.apply(input_, seq_dim)\n",
        "megatron/mpu/random.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# mostly moving to using checkpointing from deepspeed (identical code anyway) so currently this file is only imports\n# TODO: should be able to get rid of this file entirely\n\nimport deepspeed\nimport deepspeed.runtime.activation_checkpointing.checkpointing as checkpointing\n\n# Default name for the model parallel rng tracker.\n_MODEL_PARALLEL_RNG_TRACKER_NAME = (\n    deepspeed.checkpointing._MODEL_PARALLEL_RNG_TRACKER_NAME\n)\n\n# Whether apply model parallelsim to checkpointed hidden states.\n_CHECKPOINTED_ACTIVATIONS_MEMORY_BUFFER = None\n\n# RNG tracker object.\n_CUDA_RNG_STATE_TRACKER = deepspeed.checkpointing._CUDA_RNG_STATE_TRACKER\n\n# Deepspeed checkpointing functions\n# TODO: replace calls to these in our codebase with calls to the deepspeed ones\n_set_cuda_rng_state = checkpointing._set_cuda_rng_state\ncheckpoint = checkpointing.checkpoint\nmodel_parallel_cuda_manual_seed = checkpointing.model_parallel_cuda_manual_seed\nget_cuda_rng_tracker = checkpointing.get_cuda_rng_tracker\n",
        "megatron/mpu/utils.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\n\n\ndef ensure_divisibility(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator\n    )\n\n\ndef divide(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n\n\ndef split_tensor_along_last_dim(tensor, num_partitions, contiguous_split_chunks=False):\n    \"\"\"Split a tensor along its last dimension.\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n\ndef split_tensor_along_any_dim(\n    tensor, num_partitions, seq_dim, contiguous_split_chunks=False\n):\n    \"\"\"Split a tensor along a user-specified dimension.\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        seq_dim: dimension along which to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n    \"\"\"\n    # Get the size and dimension.\n    seq_dim_size = divide(tensor.size()[seq_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, seq_dim_size, dim=seq_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n\nclass VocabUtility:\n    \"\"\"Split the vocabulary into `world_size` chunks amd return the\n    first and last index of the vocabulary belonging to the `rank`\n    partition: Note that indices in [first, last]\"\"\"\n\n    @staticmethod\n    def vocab_range_from_per_partition_vocab_size(\n        per_partition_vocab_size, rank, world_size\n    ):\n        index_f = rank * per_partition_vocab_size\n        index_l = index_f + per_partition_vocab_size\n        return index_f, index_l\n\n    @staticmethod\n    def vocab_range_from_global_vocab_size(global_vocab_size, rank, world_size):\n        per_partition_vocab_size = divide(global_vocab_size, world_size)\n        return VocabUtility.vocab_range_from_per_partition_vocab_size(\n            per_partition_vocab_size, rank, world_size\n        )\n",
        "megatron/mup_substitute.py": "\"\"\"\nHelper functions for performing coord check.\n\"\"\"\nimport os\nfrom copy import copy\nfrom itertools import product\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\n\nfrom mup import coord_check as mup_coord_check\nfrom megatron.training import train_step\n\n\ndef _get_coord_data(\n    neox_args,\n    timers,\n    lr_scheduler,\n    models,\n    dataloader,\n    optcls,\n    nsteps=3,\n    dict_in_out=False,\n    flatten_input=False,\n    flatten_output=False,\n    output_name=\"loss\",\n    lossfn=\"xent\",\n    filter_module_by_name=None,\n    fix_data=True,\n    cuda=True,\n    nseeds=1,\n    output_fdict=None,\n    input_fdict=None,\n    param_fdict=None,\n    show_progress=True,\n    one_hot_target=False,\n):\n    df = []\n\n    for i in range(nseeds):\n        torch.manual_seed(i)\n        for width, model in models.items():\n            model = model()\n            model.train()\n            optimizer = optcls(model)\n            for step in range(nsteps + 1):\n                remove_hooks = []\n                # add hooks\n                for name, module in model.named_modules():\n                    if filter_module_by_name and not filter_module_by_name(name):\n                        continue\n                    remove_hooks.append(\n                        module.register_forward_hook(\n                            mup_coord_check._record_coords(\n                                df,\n                                width,\n                                name,\n                                step + 1,\n                                output_fdict=output_fdict,\n                                input_fdict=input_fdict,\n                                param_fdict=param_fdict,\n                            )\n                        )\n                    )\n\n                # train for a step\n                loss_dict, skipped_iter = train_step(\n                    neox_args=neox_args,\n                    timers=timers,\n                    data_iterator=dataloader,\n                    model=model,\n                    optimizer=optimizer,\n                    lr_scheduler=lr_scheduler,\n                )\n\n                # remove hooks\n                for handle in remove_hooks:\n                    handle.remove()\n\n            import gc\n\n            del model\n            gc.collect()\n\n    return pd.DataFrame(df)\n\n\ndef get_coord_data(\n    neox_args,\n    timers,\n    lr_scheduler,\n    models,\n    dataloader,\n    optimizer=\"sgd\",\n    lr=None,\n    mup=True,\n    filter_trainable_by_name=None,\n    **kwargs\n):\n    \"\"\"Get coord data for coord check.\n    Train the models in `models` with data from `dataloader` and optimizer\n    specified by `optimizer` and `lr` for `nsteps` steps, and record coordinate\n    statistics specified by `output_fdict`, `input_fdict`, `param_fdict`. By\n    default, only `l1` is computed for output activations of each module.\n    This function wraps around `_get_coord_data`, with the main difference being\n    user can specify common optimizers via a more convenient interface.\n    Inputs:\n        models:\n            a dict of lazy models, where the keys are numbers indicating width.\n            Each entry of `models` is a function that instantiates a model given\n            nothing.\n        dataloader:\n            an iterator whose elements are either Huggingface style dicts, if\n            `dict_in_out` is True, or (input, label). If `fix_data` is True\n            (which is the default), then only the first element of `dataloader`\n            is used in a loop and the rest of `dataloder` is ignored.\n        optimizer:\n            a string in `['sgd', 'adam', 'adamw']`, with default being `'sgd'`.\n        lr:\n            learning rate. By default is 0.1 for `'sgd'` and 1e-3 for others.\n        mup:\n            If True, then use the optimizer from `mup.optim`; otherwise, use the\n            one from `torch.optim`.\n        filter_trainable_by_name:\n            a function that returns a bool given module names (from\n            `model.named_modules()`), or None. If not None, then only modules\n            whose name yields True will be trained.\n        nsteps:\n            number of steps to train the model\n        dict_in_out:\n            whether the data loader contains Huggingface-style dict input and\n            output. Default: False\n        flatten_input:\n            if not `dict_in_out`, reshape the input to be\n            `input.view(input.shape[0], -1)`. Typically used for testing MLPs.\n        flatten_output:\n            if not `dict_in_out`, reshape the label to be `label.view(-1,\n            input.shape[-1])`.\n        output_name:\n            if `dict_in_out`, this is the key for the loss value if the output\n            is a dict. If the output is not a dict, then we assume the first\n            element of the output is the loss.\n        lossfn:\n            loss function to use if not `dict_in_out`. Can be either a string from\n            [`xent`, 'mse', 'nll', 'l1'] or a python `callable` such that\n            `lossfn(output, target)` returns the loss value. Examples of valid\n            `callable`s are `F.cross_entropy`, `F.mse_loss`, etc, where `F` is\n            `torch.nn.functional`. Default: 'xent'\n        filter_module_by_name:\n            a function that returns a bool given module names (from\n            `model.named_modules()`), or None. If not None, then only modules\n            whose name yields True will be recorded.\n        cuda:\n            whether to use cuda or not. Default: True\n        nseeds:\n            number of times to repeat the training, each with different seeds.\n        output_fdict, input_fdict, param_fdict:\n            function dicts to be used in `_record_coords`. By default, only `l1`\n            is computed for output activations of each module.\n        show_progress:\n            show progress using tqdm. Default: True\n        one_hot_target:\n            convert target label into a one-hot vector. This typically is only\n            used for `'mse'` or `'l1'` losses in classification tasks.\n            Default: False\n    Output:\n        a pandas DataFrame containing recorded results. The column names are\n        `'width', 'module', 't'` as well as names of statistics recorded, such\n        as `'l1'` (see `FDICT` for other premade statistics that can be\n        collected).\n\n    Breaking Changes:\n        In v1.0.0, when `lossfn=='mse'`, the target is automatically converted\n        to a one hot vector before loss computation. Starting in v1.1.0, this\n        behavior is turned off, and the user needs to explicitly turn on this\n        behavior by setting `one_hot_target=True`.\n    \"\"\"\n    if lr is None:\n        lr = 0.1 if optimizer == \"sgd\" else 1e-3\n    if mup:\n        from mup.optim import MuAdam as Adam\n        from mup.optim import MuAdamW as AdamW\n        from mup.optim import MuSGD as SGD\n    else:\n        from torch.optim import SGD, Adam, AdamW\n\n    def get_trainable(model):\n        params = model.parameters()\n        if filter_trainable_by_name is not None:\n            params = []\n            for name, p in model.named_parameters():\n                if filter_trainable_by_name(name):\n                    params.append(p)\n        return params\n\n    if optimizer == \"sgd\":\n        optcls = lambda model: SGD(get_trainable(model), lr=lr)\n    elif optimizer == \"adam\":\n        optcls = lambda model: Adam(get_trainable(model), lr=lr)\n    elif optimizer == \"adamw\":\n        optcls = lambda model: AdamW(get_trainable(model), lr=lr)\n    elif optimizer is None:\n        raise ValueError(\"optimizer should be sgd|adam|adamw or a custom function\")\n\n    data = _get_coord_data(\n        neox_args, timers, lr_scheduler, models, dataloader, optcls, **kwargs\n    )\n    data[\"optimizer\"] = optimizer\n    data[\"lr\"] = lr\n    return data\n",
        "megatron/neox_arguments/__init__.py": "\"\"\"\nNeoX Arguments manages all configuration arguments.\n\n**general**\n\n* The implementation makes use of the python dataclass.\n* The main class 'NeoXArgs' (in ./arguments) exposes all configuration attributes that are relevant to GPT NeoX\n* No attributes are nested (apart from attributes with type dict)\n* Output functions (enable_logging, save_yml, print) are implemented\n* Instantiation always runs NeoXArgs.__post_init__(), which calculates derived values and performs a validation (values, types, keys).\n* it is possible to set undefined attributes (e.g. line of code 'NeoXArgs().my_undefined_config = 42' works fine); such set attributes are not validated\n* It is possible to update attributes (e.g. line of code 'NeoXArgs().do_train = True' works fine); a validation can be performed by calling the validation functions on the class instance\n* In order to avoid setting undefined attributes you can use the function NeoXArgs().update_value(); this function raises an error if the to be set attribute is not defined\n\n**instantiation**\nNeoX args can be instantiated with the following options\n\n* NeoXArgs.from_ymls([\"path_to_yaml1\", \"path_to_yaml2\", ...]): load yaml configuration files and instantiate with the values provided; checks for duplications and unknown arguments are performed\n* NeoXArgs.from_dict({\"num_layers\": 12, ...}): load attribute values from dict; checks unknown arguments are performed\n\n* NeoXArgs.consume_deepy_args(): entry point for deepy.py configuring and consuming command line arguments (i.e. user_script, conf_dir, conf_file, wandb_group, wandb_run_name, wandb_team); neox_args.get_deepspeed_main_args() produces a list of command line arguments to feed to deepspeed.launcher.runner.main\n* NeoXArgs.consume_neox_args(): In the call stack deepy.py -> deepspeed -> pretrain_gpt2.py; arguments are passed to pretrain_gpt2.py by neox_args.get_deepspeed_main_args(). So produced arguments can be read with consume_neox_args() to instantiate a NeoXArgs instance.\n\n\n**code structure**\n\n* NeoX args (in ./arguments) inherits from the following subclasses: NeoXArgsDeepspeedRunner, NeoXArgsDeepspeedConfig, NeoXArgsModel, NeoXArgsTokenizer, NeoXArgsTraining, NeoXArgsParallelism, NeoXArgsLogging, NeoXArgsOther, NeoXArgsTextgen, NeoXArgsMoE\n* The Subclasses group args according to their purpose\n* The attributes of NeoXArgsDeepspeedRunner are directly mapped to the expected command line args of deepspeed.launcher.runner.main; no attributes unknown to deepspeed should be included; no arguments relevant for deepspeed should be omitted\n* The attributes of NeoXArgsDeepspeedConfig are directly mapped to the expected keys of the deepspeed config; no arguments relevant for deepspeed should be omitted\n* calculated attributes (decorator '@property') are available as attribute, but would not be included in dataclass fields (e.g. NeoXArgs().__dataclass_fields__.items())\n* refer to docstrings in code for more information\n\"\"\"\n\n\nfrom .arguments import NeoXArgs\n",
        "megatron/neox_arguments/arguments.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport os\nfrom pathlib import Path\nimport yaml\nimport json\nimport logging\nimport copy\nimport torch\nimport argparse\nfrom pkg_resources import packaging\nfrom importlib.metadata import version\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nfrom socket import gethostname\n\ntry:\n    from typing import Literal, Union\nexcept ImportError:\n    from typing_extensions import Literal, Union\nfrom deepspeed.launcher.runner import DLTS_HOSTFILE\nfrom megatron.logging import Tee\nfrom megatron.tokenizer import build_tokenizer\nfrom megatron.utils import obtain_resource_pool, expand_attention_types\nfrom .deepspeed_args import NeoXArgsDeepspeedConfig, NeoXArgsDeepspeedRunner\nfrom .neox_args import (\n    NeoXArgsModel,\n    NeoXArgsTokenizer,\n    NeoXArgsTraining,\n    NeoXArgsParallelism,\n    NeoXArgsMoE,\n    NeoXArgsLogging,\n    NeoXArgsOther,\n    NeoXArgsTextgen,\n    NeoXArgsOptimizer,\n    NeoXArgsLRScheduler,\n    ATTENTION_TYPE_CHOICES,\n)\n\n### ANSI escape codes ###\nEND = \"\\033[0m\"\nGREEN = \"\\033[92m\"\nRED = \"\\033[91m\"\nYELLOW = \"\\033[93m\"\n\n### Formatted logging prefixes ###\nERROR = f\"{RED}[ERROR]{END} \"\nFAIL = f\"{RED}[FAIL]{END}\"\nINFO = \"[INFO]\"\nOKAY = f\"{GREEN}[OKAY]{END}\"\nSUCCESS = f\"{GREEN} [SUCCESS] {END}\"\nWARNING = f\"{YELLOW}[WARNING]{END}\"\n\n# ZERO defaults by deespeed\n# These values should not be changed unless defaults in deepspeed are changed\n# for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\nZERO_DEFAULTS = {\n    \"stage\": 0,\n    \"allgather_partitions\": True,\n    \"reduce_scatter\": True,\n    \"allgather_bucket_size\": int(5e8),\n    \"overlap_comm\": False,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": int(5e8),\n    \"contiguous_gradients\": False,\n}\n\n# NeoX optimizer defaults\nOPT_DEFAULT = \"Adam\"\nOPT_PARAMS_DEFAULTS = {\n    \"lr\": 0.001,\n    \"betas\": [0.9, 0.999],\n    \"eps\": 1.0e-8,\n    \"weight_decay\": 0,\n    \"freeze_step\": 400,\n    \"momentum\": 0.0,\n    \"cuda_aware\": False,\n}\n\n\nAUTOTUNING_ARGS = (\n    \"train_batch_size\",\n    \"train_micro_batch_size_per_gpu\",\n    \"gradient_accumulation_steps\",\n    \"zero_optimization\",\n    \"autotuning\",\n)\n\nBASE_CLASSES = [\n    NeoXArgsDeepspeedRunner,\n    NeoXArgsDeepspeedConfig,\n    NeoXArgsModel,\n    NeoXArgsMoE,\n    NeoXArgsLRScheduler,\n    NeoXArgsOptimizer,\n    NeoXArgsTokenizer,\n    NeoXArgsTraining,\n    NeoXArgsParallelism,\n    NeoXArgsLogging,\n    NeoXArgsTextgen,\n    NeoXArgsOther,\n]\n\nDEEPSPEED_ARG_CLASSES = [NeoXArgsDeepspeedRunner, NeoXArgsDeepspeedConfig]\nNEOX_ARG_CLASSES = [i for i in BASE_CLASSES if i not in DEEPSPEED_ARG_CLASSES]\n\nif \"DLTS_HOSTFILE\" in os.environ:\n    DLTS_HOSTFILE = os.environ[\"DLTS_HOSTFILE\"]\n\n\n@dataclass\nclass NeoXArgs(*BASE_CLASSES):\n    \"\"\"\n    data class containing all configurations\n\n    NeoXArgs inherits from a number of small configuration classes\n    \"\"\"\n\n    ############################################################################################################################\n    # start of instantiation\n\n    def __post_init__(self):\n        \"\"\"\n        after initialization of default or loaded values\n        a number of functions are performed in order to\n        calculate values, assert consistency and do typechecking.\n        \"\"\"\n        if not NeoXArgs.validate_keys():\n            raise ValueError(\n                self.__class__.__name__\n                + \".__post_init__() NeoXArgs keys cannot be validated\"\n            )\n\n        self.enable_logging()\n\n        self.calculate_derived()\n\n        if not self.validate_types():\n            raise ValueError(\n                self.__class__.__name__\n                + \".__post_init__() NeoXArgs types cannot be validated\"\n            )\n\n        if not self.validate_values():\n            raise ValueError(\n                self.__class__.__name__\n                + \".__post_init__() NeoXArgs values cannot be validated\"\n            )\n\n    def build_tokenizer(self):\n        self.tokenizer = build_tokenizer(self)\n\n    def initialize_tensorboard_writer(self):\n        if self.tensorboard_dir and self.rank == 0:\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                print(\"> setting up tensorboard ...\")\n                self.tensorboard_writer = SummaryWriter(log_dir=self.tensorboard_dir)\n            except (ModuleNotFoundError, ImportError):\n                print(\n                    \"WARNING: TensorBoard writing requested but is not \"\n                    \"available (are you using PyTorch 1.1.0 or later and do you have tensorboard installed?), \"\n                    \"no TensorBoard logs will be written.\",\n                    flush=True,\n                )\n\n    def initialize_comet(self):\n        if self.use_comet and self.rank == 0:\n            try:\n                import comet_ml\n\n                # Deactivate output logging to avoid any potential interference with Tee\n                self.comet_experiment = comet_ml.start(\n                    workspace=self.comet_workspace,\n                    project=self.comet_project,\n                    experiment_config=comet_ml.ExperimentConfig(\n                        auto_output_logging=False\n                    ),\n                )\n                self.comet_experiment.__internal_api__log_parameters__(\n                    self.all_config,\n                    framework=\"gpt-neox\",\n                    source=\"manual\",\n                    flatten_nested=True,\n                )\n\n                if self.comet_experiment_name:\n                    self.comet_experiment.set_name(self.comet_experiment_name)\n\n                if self.comet_tags:\n                    self.comet_experiment.add_tags(self.comet_tags)\n\n                if self.comet_others:\n                    self.comet_experiment.log_others(self.comet_others)\n\n                logging.info(\"> setting up comet ...\")\n            except ImportError as e:\n                logging.error(\n                    f'{FAIL} importing comet. Comet can be installed with \"pip install comet_llm\". See https://github.com/comet-ml/comet-llm for more info. Full error is:'\n                )\n                raise e\n            except Exception as e:\n                logging.error(\n                    f'{FAIL} Error setting up Comet. Either set \"use_comet: False\" in your configuration file, or resolve the issue with Comet. Full error is:',\n                )\n                raise e\n\n    @classmethod\n    def from_ymls(cls, paths_to_yml_files: List[str], overwrite_values: Dict = None):\n        \"\"\"\n        instantiates NeoXArgs while reading values from yml files\n\n        paths_to_yml_files: list of paths to yml files\n\n        overwrite_values: If provided, overwrite any values in the yamls with these values\n        \"\"\"\n\n        print(cls.__name__ + \".from_ymls() \" + str(paths_to_yml_files), flush=True)\n\n        # initialize an empty config dictionary to be filled by yamls\n        config = dict()\n        config_files = dict()\n        # iterate of all to be loaded yaml files\n        for conf_file_name in paths_to_yml_files:\n            # load file\n            with open(conf_file_name) as conf_file:\n                conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n\n            # check for key duplicates and load values\n            for conf_key, conf_value in conf.items():\n                if conf_key in config:\n                    raise ValueError(\n                        f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n                    )\n\n                conf_key_converted = conf_key.replace(\n                    \"-\", \"_\"\n                )  # TODO remove replace and update configuration files?\n                config[conf_key_converted] = conf_value\n\n            # load original config files to save unchanged with checkpoint\n            # saving the original config retains comments\n            filename = os.path.basename(conf_file_name)\n            assert (\n                filename not in config_files\n            ), \"At least two config files have the same filename. This will result in conflicts when saving out configs with the checkpoint in one single directory. Please use unique names for configs.\"\n            config_files[filename] = open(conf_file_name).read()\n\n        # add config file content to neox args to make them accessible in code\n        # this is used when saving checkpoints\n        config[\"config_files\"] = config_files\n\n        # Configuration parameters not specified\n        params_not_in_config = sorted(\n            list(set(cls.__dataclass_fields__.keys()) - set(config.keys()))\n        )\n        if len(params_not_in_config) > 0:\n            logging.debug(\n                cls.__name__\n                + \".from_ymls() Configuration parameters not specified (using defaults): \"\n                + \", \".join(params_not_in_config)\n            )\n\n        if overwrite_values is not None:\n            for k, v in overwrite_values.items():\n                config[k] = v\n\n        # instantiate class and return\n        # duplicate values and unrecognized keys are again checked upon instantiation\n        return cls(**config)\n\n    @classmethod\n    def from_dict(cls, args_dict: Dict):\n        \"\"\"\n        instantiates NeoXArgs while reading values from input dict\n        \"\"\"\n        return cls(**args_dict)\n\n    ############################################################################################################################\n    # start of command line args interface\n\n    @classmethod\n    def consume_deepy_args(cls, input_args=None):\n        \"\"\"\n        entry point for deepy.py configuring and consuming command line arguments.\n\n        We can use `--wandb_group` / `--wandb_team` to overwrite those args from the command line, otherwise the value from the config is taken.\n        \"\"\"\n\n        parser = argparse.ArgumentParser(\n            description=\"GPT-NeoX Configuration\", allow_abbrev=False\n        )\n\n        group = parser.add_argument_group(title=\"Training Configuration\")\n\n        group.add_argument(\n            \"user_script\",\n            type=str,\n            help=\"User script to launch, followed by any required \" \"arguments.\",\n        )\n\n        group.add_argument(\n            \"--conf_dir\",\n            \"-d\",\n            type=str,\n            default=None,\n            help=\"Directory to prefix to all configuration file paths\",\n        )\n\n        group.add_argument(\n            \"conf_file\",\n            type=str,\n            nargs=\"+\",\n            help=\"Configuration file path. Multiple files can be provided and will be merged.\",\n        )\n\n        group = parser.add_argument_group(title=\"Weights and Biases monitoring args\")\n\n        group.add_argument(\n            \"--wandb_group\",\n            type=str,\n            default=None,\n            help='Weights & Biases group name - used to group together \"runs\".',\n        )\n        group.add_argument(\n            \"--wandb_run_name\",\n            type=str,\n            default=None,\n            help=\"Weights & Biases run name for the current experiment.\",\n        )\n        group.add_argument(\n            \"--wandb_team\",\n            type=str,\n            default=None,\n            help=\"Weights & Biases team name.\",\n        )\n\n        group = parser.add_argument_group(title=\"Eval args\")\n\n        group.add_argument(\n            \"--eval_tasks\",\n            type=str,\n            nargs=\"+\",\n            default=None,\n            help=\"Optionally overwrite eval tasks to run for eval.py\",\n        )\n        group.add_argument(\n            \"--iteration\",\n            type=int,\n            default=None,\n            help=\"Iteration to load checkpoint from in the eval.py and generate.py scripts. If None is provided, uses the latest iteration.\",\n        )\n        group.add_argument(\n            \"--eval_results_prefix\",\n            type=str,\n            default=None,\n            help=\"prefix to append to eval results file\",\n        )\n        parser.add_argument(\n            \"-H\",\n            \"--hostfile\",\n            type=str,\n            help=\"Hostfile path (in MPI style) that defines the \"\n            \"resource pool available to the job (e.g., \"\n            \"worker-0 slots=4)\",\n        )\n        group = parser.add_argument_group(title=\"Generation args\")\n        group.add_argument(\n            \"-i\",\n            \"--sample_input_file\",\n            type=str,\n            default=None,\n            help=\"Optionally overwrite `sample_input_file` for generate.py\",\n        )\n        group.add_argument(\n            \"-o\",\n            \"--sample_output_file\",\n            type=str,\n            default=None,\n            help=\"Optionally overwrite `sample_output_file` for generate.py\",\n        )\n\n        tuning = parser.add_argument_group(title=\"DeepSpeed Autotuning\")\n        tuning.add_argument(\n            \"--autotuning\",\n            type=str,\n            default=None,\n            choices=(\"tune\", \"run\"),\n            help=\"Use DeepSpeed's autotuning feature to optimize certain hyperparameters. For more details refer to documentation here: https://www.deepspeed.ai/tutorials/autotuning/\",\n        )\n        args_parsed = parser.parse_args(input_args)\n\n        # Validate user_script exists\n        assert os.path.exists(\n            args_parsed.user_script\n        ), f\"User script could not be found: {args_parsed.user_script}\"\n\n        # load config files\n        conf_files = args_parsed.conf_file\n        if args_parsed.conf_dir:\n            conf_files = [os.path.join(args_parsed.conf_dir, f) for f in conf_files]\n\n        # enables us to pass in `125M` instead of `125M.yml`\n        conf_files = [\n            (cf if (cf.endswith(\".yml\") or cf.endswith(\".json\")) else cf + \".yml\")\n            for cf in conf_files\n        ]\n\n        # determine overwrite values\n        overwrite_values = dict()\n        for k, v in vars(args_parsed).items():\n            if k == \"autotuning\" and v is not None:\n                overwrite_values[\"autotuning_run\"] = v\n            elif k not in [\"conf_dir\", \"conf_file\"] and v is not None:\n                overwrite_values[k] = v\n\n        # load args\n        neox_args = cls.from_ymls(\n            paths_to_yml_files=conf_files, overwrite_values=overwrite_values\n        )\n\n        if neox_args.use_wandb:\n            try:\n                import wandb\n\n                # Check if the W&B group name is configured\n                if neox_args.wandb_group is None:\n                    # Set a randomized string as group name if no group name is provided\n                    neox_args.wandb_group = wandb.sdk.lib.runid.generate_id()\n                else:\n                    # Concatenate the W&B group name with a randomized string to ensure uniqueness.\n                    neox_args.wandb_group += \"_\" + wandb.sdk.lib.runid.generate_id()\n            except ModuleNotFoundError as e:\n                if e.name == \"wandb\":\n                    e.msg += \"\\nWeights & Biases monitoring was requested but `wandb` was not found. Install `wandb` to use Weights & Biases, or set the `use_wandb` configuration option to a boolean false to disable Weights & Biases logging.\"\n                raise e\n\n            neox_args.wandb_group += \"_\" + wandb.util.generate_id()\n\n        neox_args.print()\n\n        return neox_args\n\n    @classmethod\n    def consume_neox_args(cls, overwrite_values=None, input_args=None):\n        \"\"\"\n        Deepspeed launcher needs to pass the arguments for `pretrain_gpt2.py` across to all machines.\n\n        In order not to have any problems with different configs being mismatched across machines, we instead read the .yaml configuration file from the main rank,\n        then serialize the arguments to a dictionary, which the deepspeed launcher broadcasts to all machines (`--megatron_config`).\n\n        We then instantiate a new NeoXArgs from the dictionary (`.from_dict`). This should ensure args are never inconsistent across machines.\n        \"\"\"\n\n        parser = argparse.ArgumentParser(\n            description=\"GPT-NeoX Configuration\", allow_abbrev=False\n        )\n        parser.add_argument(\n            \"--megatron_config\",\n            type=str,\n            default=None,\n            help=\"json dict dumped as string in NeoXArgs.get_deepspeed_main_args()\",\n        )\n        parser.add_argument(\n            \"--deepspeed_config\",\n            type=str,\n            default=None,\n            help=\"Only need this (at this stage) for autotuning\",\n        )\n        args_parsed, _ = parser.parse_known_args(input_args)\n        megatron_config = json.loads(\n            base64.urlsafe_b64decode(args_parsed.megatron_config).decode(\"utf-8\")\n        )\n        if args_parsed.deepspeed_config is not None:\n            overwrite_values = cls.set_up_autotuning(\n                args_parsed.deepspeed_config, overwrite_values\n            )\n        if overwrite_values is not None:\n            megatron_config.update(overwrite_values)\n        return cls.from_dict(args_dict=megatron_config)\n\n    @staticmethod\n    def set_up_autotuning(encoded_config, overwrite_values):\n        config = json.loads(base64.urlsafe_b64decode(encoded_config).decode(\"utf-8\"))\n        overwrite_values = overwrite_values if overwrite_values else {}\n        for tuning_param in AUTOTUNING_ARGS:\n            # TODO: This is for autotuning specifically, may cause surprises for someone with a weird setup\n            if tuning_param in config:\n                overwrite_values[tuning_param] = config[tuning_param]\n        return overwrite_values\n\n    @staticmethod\n    def convert_key_value_to_command_line_arg(k, v):\n        if isinstance(v, bool):\n            if v:\n                return [f\"--{k}\"]\n            else:\n                return []\n        if v is None:\n            return []\n        return [f\"--{k}\", str(v)]\n\n    def get_extra_deepspeed_args(self):\n        \"\"\"\n        Sets up the extra arguments for deepspeed. This is done by reading in the `deepspeed_extra_args` dictionary from\n            the configuration file, and then adding any arguments where values differ from those specified in the dataclass.\n        \"\"\"\n        neox_args = self.get_parent_class_value_dict(\n            *self.__class__.__bases__, only_non_defaults=True\n        )\n\n        extra_ds_args = dict()\n\n        for key, value in self.deepspeed_extra_args.items():\n            # Check to make sure the key is not already changed from defaults, and raise an exception if it is\n            # This is to prevent users from accidentally writing arguments both in deepspeed_extra_args and in the base level\n            # of the configuration file\n            if hasattr(neox_args, key):\n                raise ValueError(\n                    f\"Key {key} is already specified elsewhere. Reading in a different value from the 'deepspeed_extra_args' option in the configuration file will cause undefined behavior.\"\n                )\n            extra_ds_args[key] = value\n\n        return extra_ds_args\n\n    def get_deepspeed_main_args(self):\n        args_list = list()\n\n        if self.autotuning_run is not None:\n            args_list.extend(\n                self.convert_key_value_to_command_line_arg(\n                    \"autotuning\", self.autotuning_run\n                )\n            )\n\n        # get deepspeed runner args, and only pass them in to deepspeed launcher if they differ from defaults\n        for key, default_value in NeoXArgsDeepspeedRunner().defaults():\n            if key == \"autotuning_run\":\n                continue\n            configured_value = getattr(self, key)\n\n            if key == \"force_multi\":\n                if self.deepspeed_slurm or self.deepspeed_mpi:\n                    configured_value = True\n            if configured_value != default_value:\n                args_list.extend(\n                    self.convert_key_value_to_command_line_arg(key, configured_value)\n                )\n\n        if self.deepspeed_slurm:\n            comment = getattr(self, \"comment\")\n            if comment:\n                args_list.extend(\n                    self.convert_key_value_to_command_line_arg(\"comment\", comment)\n                )\n            account = getattr(self, \"account\")\n            if account:\n                args_list.extend(\n                    self.convert_key_value_to_command_line_arg(\"account\", account)\n                )\n\n            # master_address = os.environ['SLURM_JOB_NODELIST'].split('\\n')[0]\n            # args_list.extend(\n            #    self.convert_key_value_to_command_line_arg('master_addr', master_address)\n            # )\n\n        if \"DLTS_HOSTFILE\" in os.environ:\n            args_list.extend(\n                self.convert_key_value_to_command_line_arg(\n                    \"hostfile\", os.environ[\"DLTS_HOSTFILE\"]\n                )\n            )\n\n        if \"MASTER_ADDR\" in os.environ:\n            args_list.extend(\n                self.convert_key_value_to_command_line_arg(\n                    \"master_addr\", os.environ[\"MASTER_ADDR\"]\n                )\n            )\n\n        if (\n            \"--include\" in args_list or \"--exclude\" in args_list\n        ) and \"--num_gpus\" in args_list:\n            print(\n                \"WARNING: both --include/--exclude and num_gpus were specified simultaneously - overriding num_gpus with --include/--exclude\"\n            )\n            # cannot specify these both simultaneously, remove num_gpus from list\n            idx = args_list.index(\"--num_gpus\")\n            # pop twice, once for the arg, once for its value\n            args_list.pop(idx)\n            args_list.pop(idx)\n\n        # add user script\n        args_list.append(self.user_script)\n\n        self.configure_distributed_args()\n        cwd = Path.cwd()\n\n        # get deepspeed_config\n        args_list.append(\"--deepspeed_config\")\n\n        if self.autotuning_run is not None:\n            ds_fp = cwd / Path(\"ds_config.json\")\n            if self.rank == 0:\n                with open(ds_fp, mode=\"w\") as ds_file:\n                    json.dump(self.deepspeed_config, ds_file)\n            args_list.append(str(ds_fp))\n        else:\n            encoded_ds_config = base64.urlsafe_b64encode(\n                json.dumps(self.deepspeed_config).encode(\"utf-8\")\n            ).decode(\"utf-8\")\n            args_list.append(encoded_ds_config)\n\n        # get all config values\n        args_list.append(\"--megatron_config\")\n        neox_args = self.get_parent_class_value_dict(\n            *self.__class__.__bases__, only_non_defaults=True\n        )\n        encoded_mega_config = base64.urlsafe_b64encode(\n            json.dumps(neox_args).encode(\"utf-8\")\n        ).decode(\"utf-8\")\n        args_list.append(str(encoded_mega_config))\n        return args_list\n\n    ############################################################################################################################\n    # start of calculated properties\n\n    @property\n    def deepspeed_config(self) -> dict:\n        \"\"\"\n        returns a dict containing variables within deepspeed config\n        \"\"\"\n        config = self.get_parent_class_value_dict_extra_ds(\n            NeoXArgsDeepspeedConfig, only_non_defaults=True\n        )\n        return config\n\n    @property\n    def deepspeed_runner(self) -> dict:\n        \"\"\"\n        returns variables within deepspeed runner\n        \"\"\"\n        return self.get_parent_class_value_dict(NeoXArgsDeepspeedRunner)\n\n    @property\n    def megatron_config(self) -> dict:\n        \"\"\"\n        returns variables within megatron args\n        \"\"\"\n        return self.get_parent_class_value_dict(*NEOX_ARG_CLASSES)\n\n    @property\n    def all_config(self) -> dict:\n        \"\"\"\n        returns variables of all args\n        \"\"\"\n        return self.get_parent_class_value_dict(*BASE_CLASSES)\n\n    def get_parent_class_value_dict(\n        self, *parent_classes, only_non_defaults=False\n    ) -> dict:\n        \"\"\"\n        takes a sequence of parent classes and returns corresponding values (with defaults set)\n        \"\"\"\n        # TODO no Nones or non-defaults\n        result = dict()\n        for parent in parent_classes:\n            for key, default_value in parent().defaults():\n                if key in [\"tokenizer\", \"tensorboard_writer\", \"adlr_autoresume_object\"]:\n                    continue\n                if only_non_defaults:\n                    value = getattr(self, key)\n                    if value == default_value:\n                        continue\n                result[key] = getattr(self, key)\n        return result\n\n    def get_parent_class_value_dict_extra_ds(\n        self, *parent_classes, only_non_defaults=False\n    ) -> dict:\n        \"\"\"\n        Takes a sequence of parent classes and returns corresponding values (with defaults set).\n        Also adds in any extra deepspeed arguments that are specified in the configuration file.\n\n        Args:\n            parent_classes: sequence of parent classes\n            only_non_defaults: if True, only returns values that differ from defaults\n\n        Returns:\n            dict of arguments and values\n\n        \"\"\"\n        # TODO no Nones or non-defaults\n        result = dict()\n        for parent in parent_classes:\n            for key, default_value in parent().defaults():\n                if key in [\n                    \"tokenizer\",\n                    \"tensorboard_writer\",\n                    \"adlr_autoresume_object\",\n                    \"deepspeed_extra_args\",\n                ]:\n                    continue\n                if only_non_defaults:\n                    value = getattr(self, key)\n                    if value == default_value:\n                        continue\n                result[key] = getattr(self, key)\n\n        if self.deepspeed_extra_args is not None:\n            extra_ds_args = self.get_extra_deepspeed_args()\n            result.update(extra_ds_args)\n\n        return result\n\n    @property\n    def params_dtype(self):\n        \"\"\"\n        returns the datatype on the basis of configured precision\n        \"\"\"\n        if self.precision == \"fp16\":\n            return torch.half\n        elif self.precision == \"bfloat16\":\n            return torch.bfloat16\n        else:\n            return torch.float\n\n    ############################################################################################################################\n    # start of logging and output\n\n    def enable_logging(self):\n        \"\"\"\n        enable Tee logs based on the configured logdir\n        \"\"\"\n        if self.log_dir:\n            os.makedirs(self.log_dir, exist_ok=True)\n            hostname = gethostname()\n            file_prefix = os.path.join(self.log_dir, hostname)\n            Tee(file_prefix + \"_stdout.txt\", err=False)\n            Tee(file_prefix + \"_stderr.txt\", err=True)\n\n    def print(self):\n        \"\"\"Print arguments.\"\"\"\n        if self.rank == 0 or self.rank is None:\n            print(\"-------------------- arguments --------------------\", flush=True)\n            str_list = []\n            for arg in vars(self):\n                # add arg + value\n                dots = \".\" * (32 - len(arg))\n                value = getattr(self, arg)\n                print_str = \"  {} {} {}\".format(arg, dots, value)\n\n                # add info 'default or updated'\n                field_def = self.__dataclass_fields__.get(arg)\n                if field_def is not None:\n                    default_info = (\n                        \"default\" if value == field_def.default else \"updated\"\n                    )\n                else:\n                    default_info = \"\"\n                dots = \".\" * (64 - len(print_str))\n                print_str += dots\n                str_list.append({\"print_str\": print_str, \"default_info\": default_info})\n\n            for arg in sorted(\n                sorted(str_list, key=lambda x: x[\"print_str\"].lower()),\n                key=lambda x: x[\"default_info\"],\n                reverse=True,\n            ):\n                print(arg[\"print_str\"] + arg[\"default_info\"], flush=True)\n            print(\"---------------- end of arguments ----------------\", flush=True)\n\n    ############################################################################################################################\n    # start of calculations and derived values\n\n    def configure_distributed_args(self):\n        \"\"\"\n        Configures distributed training arguments from local variables set by deepspeed launcher.\n        \"\"\"\n        if self.deepspeed_mpi:\n            from deepspeed.comm import mpi_discovery\n\n            mpi_discovery()\n\n        if self.deepspeed_slurm:\n            os.environ[\"LOCAL_RANK\"] = os.environ[\"SLURM_LOCALID\"]\n            os.environ[\"RANK\"] = os.environ[\"SLURM_PROCID\"]\n            os.environ[\"WORLD_SIZE\"] = (\n                os.environ[\"SLURM_NTASKS\"]\n                if os.environ.get(\"SLURM_NTASKS\") is not None\n                else str(\n                    int(os.environ[\"SLURM_NNODES\"])\n                    * int(os.environ[\"SLURM_NTASKS_PER_NODE\"])\n                )\n            )\n\n        self.update_value(\"local_rank\", int(os.getenv(\"LOCAL_RANK\", \"0\")))\n        self.update_value(\"rank\", int(os.getenv(\"RANK\", \"0\")))\n        self.update_value(\"world_size\", int(os.getenv(\"WORLD_SIZE\", \"1\")))\n\n        if self.rank == 0:\n            print(\n                self.__class__.__name__\n                + \".configure_distributed_args() using world size: {} and model-parallel size: {} \".format(\n                    self.world_size, self.model_parallel_size\n                ),\n                flush=True,\n            )\n\n    @staticmethod\n    def calculate_batch_parameters(\n        dp_world_size, train_batch=None, micro_batch=None, grad_acc=None\n    ):\n        # all values are provided nothing needs to be set\n        if train_batch is not None and micro_batch is not None and grad_acc is not None:\n            return train_batch, micro_batch, grad_acc\n\n        # gradient_accumulation_steps needs to be set\n        elif train_batch is not None and micro_batch is not None:\n            grad_acc = train_batch // micro_batch\n            grad_acc //= dp_world_size\n\n        # micro_batch_per_gpu needs to be set\n        elif train_batch is not None and grad_acc is not None:\n            micro_batch = train_batch // dp_world_size\n            micro_batch //= grad_acc\n\n        # train_batch_size needs to be set\n        elif micro_batch is not None and grad_acc is not None:\n            train_batch = micro_batch * grad_acc\n            train_batch *= dp_world_size\n\n        # gradient_accumulation_steps and micro_batch_per_gpus is set\n        elif train_batch is not None:\n            grad_acc = 1\n            micro_batch = train_batch // dp_world_size\n\n        # train_batch_size and gradient_accumulation_step is set\n        elif micro_batch is not None:\n            train_batch = micro_batch * dp_world_size\n            grad_acc = 1\n\n        # either none of the three parameters are provided or just gradient_accumulation_step is provided\n        else:\n            assert (\n                False\n            ), \"Either train_batch_size or train_micro_batch_size_per_gpu needs to be provided\"\n        return int(train_batch), int(micro_batch), int(grad_acc)\n\n    @staticmethod\n    def check_batch_parameters(dp_world_size, train_batch, micro_batch, grad_acc):\n        assert (\n            train_batch > 0\n        ), f\"Train batch size: {train_batch} has to be greater than 0\"\n\n        assert (\n            micro_batch > 0\n        ), f\"Micro batch size per gpu: {micro_batch} has to be greater than 0\"\n\n        assert (\n            grad_acc > 0\n        ), f\"Gradient accumulation steps: {grad_acc} has to be greater than 0\"\n\n        assert train_batch == micro_batch * grad_acc * dp_world_size, (\n            f\"Check batch related parameters. train_batch_size is not equal\"\n            \" to micro_batch_per_gpu * gradient_acc_step * world_size \\n\"\n            f\"{train_batch} != {micro_batch} * {grad_acc} * {dp_world_size}\"\n        )\n\n    def calculate_derived(self):\n        \"\"\"\n        Derives additional configuration values necessary for training from the current config\n        \"\"\"\n\n        # number of gpus\n        # Get number of GPUs param or hostfile to determine train_batch_size\n        global_num_gpus = getattr(self, \"global_num_gpus\", None)\n        if global_num_gpus is None:\n            if self.hostfile is not None or os.path.exists(DLTS_HOSTFILE):\n                hostfile_path = self.hostfile or DLTS_HOSTFILE\n                resources = obtain_resource_pool(\n                    hostfile_path, self.include or \"\", self.exclude or \"\"\n                )\n                if self.num_nodes is not None and self.num_nodes > 0:\n                    resources = {\n                        k: resources[k]\n                        for k in list(resources.keys())[: self.num_nodes]\n                    }\n                global_num_gpus = sum(map(len, resources.values()))\n                if self.num_gpus is not None and self.num_gpus > 0:\n                    global_num_gpus = self.num_gpus * len(resources)\n            else:\n                global_num_gpus = torch.cuda.device_count()\n            self.update_value(\"global_num_gpus\", global_num_gpus)\n\n        logging.info(\n            self.__class__.__name__\n            + \".calculate_derived() \"\n            + f\"Total number of GPUs determined to be: {global_num_gpus}\"\n        )\n\n        # get world size in the model/pipe parallel case, the actual `world size` deepspeed uses is the size of the\n        # data-parallel group, or (num_gpus / mp_size) / pp_size\n        pp_size = self.pipe_parallel_size\n        pp_size = pp_size if pp_size >= 1 else 1\n        mp_size = self.model_parallel_size\n        mp_size = mp_size if mp_size >= 1 else 1\n        self.update_value(\"model_parallel_size\", mp_size)\n\n        # pp_size and mp_size are only used here to compute dp world size and nowhere else.\n        dp_world_size = (global_num_gpus / pp_size) / mp_size\n        if not (dp_world_size % 1 == 0):\n            error_message = (\n                f\"{ERROR}\"\n                + self.__class__.__name__\n                + \".calculate_derived() \"\n                + f\"(global_num_gpus / pp_size) / mp_size [({global_num_gpus} / {pp_size}) / {mp_size}] must be a whole number\"\n            )\n            logging.error(error_message)\n            raise AssertionError(error_message)\n\n            # Automatically derive train_batch_size = train_micro_batch_size_per_gpu*global_num_gpus*gradient_accumulation_steps\n        (\n            train_batch_size,\n            train_micro_batch_size_per_gpu,\n            gradient_accumulation_steps,\n        ) = self.calculate_batch_parameters(\n            dp_world_size=dp_world_size,\n            train_batch=self.train_batch_size,\n            micro_batch=self.train_micro_batch_size_per_gpu,\n            grad_acc=self.gradient_accumulation_steps,\n        )\n        self.check_batch_parameters(\n            dp_world_size=dp_world_size,\n            train_batch=train_batch_size,\n            micro_batch=train_micro_batch_size_per_gpu,\n            grad_acc=gradient_accumulation_steps,\n        )\n        self.update_values(\n            {\n                # batch size params\n                \"train_batch_size\": train_batch_size,\n                \"train_micro_batch_size_per_gpu\": train_micro_batch_size_per_gpu,\n                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n                \"batch_size\": train_micro_batch_size_per_gpu,\n                # duplicate items\n                \"clip_grad\": self.gradient_clipping,\n            }\n        )\n\n        # derive precision\n        if self.fp16 and self.fp16.get(\"enabled\", False):\n            if self.precision is None:\n                self.update_value(\"precision\", \"fp16\")\n            else:\n                fp16_conflict = \"DeepSpeed fp16 field was set but precision conflicts\"\n                assert self.precision == \"fp16\", fp16_conflict\n\n        if self.bf16 and self.bf16.get(\"enabled\", False):\n            if self.precision is None:\n                self.update_value(\"precision\", \"bfloat16\")\n            else:\n                bf16_conflict = \"DeepSpeed bf16 field was set but precision conflicts\"\n                assert self.precision == \"bfloat16\", bf16_conflict\n\n        if self.precision == \"fp16\":\n            if isinstance(self.fp16, dict) and len(self.fp16) > 0:\n                fp16_args = copy.deepcopy(self.fp16)\n                fp16_args[\"enabled\"] = True\n            else:\n                fp16_args = {\"type\": \"fp16\", \"enabled\": True}\n            self.update_value(\"fp16\", fp16_args)\n        elif self.precision == \"bfloat16\":\n            if not self.bf16:\n                bf_config = {\"bf16\": {\"enabled\": True}}\n                # dt_config = {\"grad_accum_dtype\": \"fp32\"}\n                if self.deepspeed_extra_args is None:\n                    self.update_value(\"deepspeed_extra_args\", bf_config)\n                else:\n                    extra_args = copy.deepcopy(self.deepspeed_extra_args)\n                    extra_args.update(bf_config)\n                    self.update_value(\"deepspeed_extra_args\", extra_args)\n\n            zero_stage = self.zero_optimization[\"stage\"]\n            if self.data_types is None:\n                fp32_grad_accum = False\n            else:\n                fp32_grad_accum = self.data_types.get(\"grad_accum_dtype\") == \"fp32\"\n            if (zero_stage > 0) and (pp_size > 0) and not fp32_grad_accum:\n                # Remove this code when this issue is resolved\n                # https://github.com/microsoft/DeepSpeed/issues/1835\n                logging.warn(\n                    \"Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads\"\n                )\n        else:\n            self.update_value(\"precision\", \"fp32\")\n\n        # zero optimization\n        if self.zero_optimization is None:\n            self.zero_optimization = copy.deepcopy(\n                ZERO_DEFAULTS\n            )  # a dict is overwritten and not updated key by key\n        try:\n            stage = self.zero_optimization[\"stage\"]\n            if stage in (0, 1, 2, 3):\n                self.update_values(\n                    {\n                        \"zero_stage\": self.zero_optimization.get(\n                            \"stage\", ZERO_DEFAULTS[\"stage\"]\n                        ),\n                        \"zero_reduce_scatter\": self.zero_optimization.get(\n                            \"reduce_scatter\", ZERO_DEFAULTS[\"reduce_scatter\"]\n                        ),\n                        \"zero_contiguous_gradients\": self.zero_optimization.get(\n                            \"contiguous_gradients\",\n                            ZERO_DEFAULTS[\"contiguous_gradients\"],\n                        ),\n                        \"zero_reduce_bucket_size\": self.zero_optimization.get(\n                            \"reduce_bucket_size\", ZERO_DEFAULTS[\"reduce_bucket_size\"]\n                        ),\n                        \"zero_allgather_bucket_size\": self.zero_optimization.get(\n                            \"allgather_bucket_size\",\n                            ZERO_DEFAULTS[\"allgather_bucket_size\"],\n                        ),\n                    }\n                )\n            else:\n                assert (\n                    self.autotuning is not None\n                ), f\"Zero Stage must be an integer unless you are doing autotuning, not {stage}\"\n        except KeyError as ke:\n            print(f\"Zero Optimization config: {self.zero_optimization}\")\n            raise ke\n\n        # optimizer and scheduler\n        opt_params = self.optimizer or {\n            \"type\": OPT_DEFAULT,\n            \"params\": OPT_PARAMS_DEFAULTS,\n        }\n        self.update_values(\n            {\n                \"optimizer_type\": opt_params.get(\"type\", OPT_DEFAULT),\n                \"lr\": opt_params[\"params\"].get(\"lr\", OPT_PARAMS_DEFAULTS[\"lr\"]),\n            }\n        )\n\n        if self.optimizer_type.lower() == \"onebitadam\":\n            assert (\n                self.train_iters is not None\n            ), \"OneBitAdam requires train_iters to be specified\"\n\n            # onebitadam needs to instantiated by deepspeed, and so we need to pass deepspeed scheduler args\n            # for all other optimizers, the scheduling is handled by megatron\n            self.scheduler = {\n                \"type\": \"WarmupDecayLR\",  # for now this is the only ds scheduler offering decay\n                \"params\": {\n                    \"warmup_min_lr\": 0,\n                    \"warmup_max_lr\": self.lr,\n                    \"warmup_num_steps\": int(self.train_iters * self.warmup),\n                    \"total_num_steps\": self.lr_decay_iters or self.train_iters,\n                },\n            }\n\n        # Fp16 loss scaling.\n        self.update_value(\"dynamic_loss_scale\", self.loss_scale is None)\n\n        # Update 'is pipe parallel' flag\n        # if we set pipe_parallel_size to 0, GPT2ModelPipe.to_sequential() is called, and we run training with\n        # the sequential model without the PipelineModule wrapper to avoid the overhead it incurs\n        self.update_value(\"is_pipe_parallel\", self.pipe_parallel_size >= 1)\n\n        # MoE config\n        if self.moe_num_experts > 1:\n            assert self.zero_optimization[\"stage\"] < 2, \"MoE is not compatible with zero stages 2 and 3\"\n\n        # Attention config\n        if self.attention_config is None:\n            self.update_value(\"attention_config\", [[[\"global\"], self.num_layers]])\n        self.update_value(\n            \"attention_config\",\n            expand_attention_types(self.attention_config, self.num_layers),\n        )\n        assert (\n            len(self.attention_config) == self.num_layers\n        ), \"Length of attention config list must equal num_layers\"\n        for item in self.attention_config:\n            assert (\n                item in ATTENTION_TYPE_CHOICES\n            ), f\"Attention type {item} not recognized\"\n        if \"gmlp\" in self.attention_config or \"amlp\" in self.attention_config:\n            assert (\n                not self.partition_activations\n            ), \"GMLP Blocks are not compatible with partition activations\"\n        if \"mamba\" in self.attention_config:\n            if isinstance(self.zero_stage, int):\n                assert self.zero_stage <= 2, \"Zero stage 3 not compatible with Mamba\"\n            assert (\n                self.hidden_dropout == 0.0,\n            ), \"Mamba does not yet have dropout implemented\"\n        if \"rwkv\" in self.attention_config:\n            assert (\n                self.model_parallel_size == 1\n            ), \"RWKV not currently compatible with model parallelism\"\n            if isinstance(self.zero_stage, int):\n                assert self.zero_stage <= 2, \"Zero stage 3 not compatible with RWKV\"\n            assert (\n                self.hidden_dropout == 0.0,\n            ), \"RWKV does not yet have dropout implemented\"\n\n        # Sparsity config\n        if self.sparsity_config is None:\n            # Can't have a default value as an empty dict so need to set it here\n            self.update_value(\"sparsity_config\", {})\n\n        # Multi-query or grouped-query attention settings\n        if self.num_kv_heads is not None:\n            # need KV heads <= query heads, and KV heads dividing query heads evenly\n            assert (\n                self.num_attention_heads % self.num_kv_heads == 0\n            ), \"num_kv_heads must evenly divide num_attention_heads and be no greater than it\"\n\n            if self.num_kv_heads < self.num_attention_heads:\n                # GQA / MQA not compatible with sparse attention configurations\n                assert (\n                    not self.sparsity_config\n                ), \"Sparse attention not compatible with GQA or MQA\"\n                assert all(\n                    (attn_type == \"flash\") or (attn_type == \"global\")\n                    for attn_type in self.attention_config\n                ), \"GQA / MQA currently only compatible with Flash or standard global/sliding window Attention\"\n                assert (\n                    self.num_kv_heads % self.model_parallel_size == 0\n                ), \"Number of KV heads must be at least model_parallel_size for now!\"\n        # Flash attention version >=2.3.0 required to combine Flash + Sliding Window Attention\n        if \"flash\" in self.attention_config:\n            _flash_version = packaging.version.Version(version(\"flash-attn\"))\n            if self.sliding_window_width is not None:\n                assert _flash_version >= packaging.version.Version(\n                    \"2.3.0\"\n                ), f\"Flash-Attention version ({str(_flash_version)}) must be >= 2.3.0 to support sliding window attention.\"\n            if self.pos_emb == \"alibi\":\n                if not _flash_version >= packaging.version.Version(\"2.4.0.post1\"):\n                    print(\n                        f\"Warning: Flash-Attention version ({str(_flash_version)}) must be >= 2.4.0.post1 to support AliBi. Falling back to flash-attn triton backend, but version 2.4.0.post1 or later will be required in future.\"\n                    )\n\n        # Adding equal dataset weights if none are provided\n        if self.train_data_paths and (self.train_data_weights is None):\n            self.train_data_weights = [1.0] * len(self.train_data_paths)\n        elif self.pos_train_data_paths and (self.train_data_weights is None):\n            self.train_data_weights = [1.0] * len(self.pos_train_data_paths)\n        if self.valid_data_paths and (self.valid_data_weights is None):\n            self.valid_data_weights = [1.0] * len(self.valid_data_paths)\n        elif self.pos_valid_data_paths and (self.valid_data_weights is None):\n            self.valid_data_weights = [1.0] * len(self.pos_valid_data_paths)\n        if self.test_data_paths and (self.test_data_weights is None):\n            self.test_data_weights = [1.0] * len(self.test_data_paths)\n        elif self.pos_test_data_paths and (self.test_data_weights is None):\n            self.test_data_weights = [1.0] * len(self.pos_test_data_paths)\n\n        if self.train_label_data_paths:\n            err_str = \"Must use `train_label_data_paths` with `train_data_paths`, not `data_path`\"\n            assert self.train_data_paths and not self.data_path, err_str\n\n        # if a sample input file is provided, default text_gen_type type to input-file\n        if self.text_gen_type is None:\n            if self.sample_input_file:\n                self.update_value(\"text_gen_type\", \"input-file\")\n            else:\n                self.update_value(\"text_gen_type\", \"unconditional\")\n\n    ############################################################################################################################\n    # start of validation functions\n\n    @classmethod\n    def validate_keys(cls):\n        \"\"\"\n        test that there are no duplicate arguments\n        \"\"\"\n        source_classes = list(cls.__bases__)\n        defined_properties = dict()\n\n        for source_class in source_classes:\n            source_vars = list(source_class.__dataclass_fields__)\n            for item in source_vars:\n                if item in defined_properties.keys():\n                    logging.error(\n                        f\"({cls.__name__}) duplicate of item: {item}, in class {source_class.__name__} and {defined_properties[item]}\"\n                    )\n                    return False\n                else:\n                    defined_properties[item] = source_class.__name__\n        return True\n\n    def validate_values(self):\n        # the current codebase assumes running with deepspeed only\n        if not self.deepspeed:\n            return False\n\n        # learning rate\n        if self.lr is None:\n            error_message = (\n                f\"{FAIL} \" + self.__class__.__name__ + \".validate_values() lr is None\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n            return False\n\n        # required arguments\n        required_args = [\n            \"num_layers\",\n            \"hidden_size\",\n            \"num_attention_heads\",\n            \"max_position_embeddings\",\n        ]\n        for req_arg in required_args:\n            if getattr(self, req_arg) is None:\n                error_message = (\n                    f\"{FAIL}\"\n                    + self.__class__.__name__\n                    + \".validate_values() \"\n                    + req_arg\n                    + \" is None.\"\n                )\n                logging.error(error_message)\n                raise ValueError(error_message)\n                return False\n\n        # Checks.\n        if self.hidden_size % self.num_attention_heads != 0 and not (\n            \"mamba\" in self.attention_config\n        ):\n            error_message = (\n                f\"{FAIL}\"\n                + self.__class__.__name__\n                + \".validate_values() hidden_size must be divisible by num_attention_heads\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n            return False\n\n        if self.seq_length is not None:\n            if not (self.max_position_embeddings >= self.seq_length):\n                error_message = (\n                    f\"{FAIL}\"\n                    + self.__class__.__name__\n                    + \".validate_values() max_position_embeddings must be bigger or equal seq_length\"\n                )\n                logging.error(error_message)\n                raise ValueError(error_message)\n                return False\n\n        if not (self.min_lr <= self.lr):\n            error_message = (\n                \"{FAIL}\"\n                + self.__class__.__name__\n                + \".validate_values() min_lr must be smaller or equal lr\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n            return False\n\n        if (\n            self.save is not None\n            and self.checkpoint_factor is None\n            and self.extra_save_iters is None\n        ):\n            error_message = (\n                f\"{FAIL}\"\n                + self.__class__.__name__\n                + \".validate_values() checkpoint_factor or extra_save_iters must be defined if save is defined\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n            return False\n\n        # Parameters sharing does not work with torch DDP.\n        if (self.num_unique_layers is not None) and (self.num_layers is not None):\n            if not (self.num_unique_layers <= self.num_layers):\n                error_message = (\n                    f\"{FAIL}\"\n                    + self.__class__.__name__\n                    + \".validate_values() num-unique-layers must be smaller or equal num_layers\"\n                )\n                logging.error(error_message)\n                raise ValueError(error_message)\n                return False\n\n            if not (self.num_layers % self.num_unique_layers == 0):\n                error_message = (\n                    f\"{FAIL}\"\n                    + self.__class__.__name__\n                    + \".validate_values() num-layers should be divisible by num-unique-layers\"\n                )\n                logging.error(error_message)\n                raise ValueError(error_message)\n                return False\n\n        if self.fp16_lm_cross_entropy and self.precision != \"fp16\":\n            error_message = (\n                f\"{FAIL}\"\n                + self.__class__.__name__\n                + \".validate_values() lm cross entropy in fp16 only support in fp16 mode.\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n            return False\n\n        # assert that if one of train/test/valid_data_path are provided, data_path should not be\n        has_separate_path = [\n            data_path is not None\n            for data_path in [\n                self.train_data_paths,\n                self.valid_data_paths,\n                self.test_data_paths,\n            ]\n        ]\n        if all(has_separate_path):\n            assert self.data_path is None, (\n                f\"{FAIL} Please provide *either* `data_path` or `train/valid/test_data_path` \"\n                \"in args \"\n            )\n\n        # assert that if one of train/test/valid_data_path are provided, all should be\n        assert_error_mess = (\n            f\"{FAIL} One or more of train/valid/test data_path are not provided:\\n\\t\"\n        )\n        assert_error_mess += \"\\n\\t\".join(\n            [\n                f\"{name} data paths: {data_path},\"\n                for name, data_path in [\n                    [\"train\", self.train_data_paths],\n                    [\"valid\", self.valid_data_paths],\n                    [\"test\", self.test_data_paths],\n                ]\n            ]\n        )\n        assert any(has_separate_path) == all(has_separate_path), assert_error_mess\n\n        # assert that if train / valid / test data path(s) and weights are provided, that the paths and the weights should be equal length\n        if self.train_data_paths is not None:\n            assert len(self.train_data_paths) == len(self.train_data_weights)\n        if self.valid_data_paths is not None:\n            assert len(self.valid_data_paths) == len(self.valid_data_weights)\n        if self.test_data_paths is not None:\n            assert len(self.test_data_paths) == len(self.test_data_weights)\n\n        return True\n\n    def validate_types(self):\n        \"\"\"\n        At runtime, checks types are actually the type specified.\n        \"\"\"\n        for field_name, field_def in self.__dataclass_fields__.items():\n            actual_value = getattr(self, field_name)\n            if actual_value is None:\n                continue  # we allow for some values not to be configured\n\n            if self.autotuning is not None and actual_value == \"auto\":\n                continue\n\n            actual_type = type(actual_value)\n            if actual_type != field_def.type:\n                if (\n                    actual_type == int and field_def.type == float\n                ):  # floats should be able to be configured as ints\n                    continue\n\n                # for typing.Literal (i.e a list of choices) - checks that actual value is in accepted values\n                elif field_def.type.__origin__ == Literal:\n                    accepted_values = field_def.type.__args__\n                    if actual_value in accepted_values:\n                        continue\n                    elif type(actual_value) == str:\n                        # case insensitive checking\n                        lowercase_accepted_values = [\n                            i.lower() for i in accepted_values if isinstance(i, str)\n                        ]\n                        if actual_value.lower() in lowercase_accepted_values:\n                            continue\n                    logging.error(\n                        f\"{FAIL}\"\n                        + self.__class__.__name__\n                        + \".validate_types() \"\n                        + f\"{field_name}: '{actual_value}' Not in accepted values: '{accepted_values}'\"\n                    )\n                    return False\n                elif field_def.type.__origin__ == Union:\n                    accepted_types = field_def.type.__args__\n                    if actual_type in accepted_types:\n                        continue\n                    else:\n                        logging.error(\n                            f\"{FAIL}\"\n                            + self.__class__.__name__\n                            + \".validate_types() \"\n                            + f\"{field_name}: '{actual_type}' not in {accepted_types}\"\n                        )\n                        return False\n\n                logging.error(\n                    f\"{FAIL}\"\n                    + self.__class__.__name__\n                    + \".validate_types() \"\n                    + f\"{field_name}: '{actual_type}' instead of '{field_def.type}'\"\n                )\n                return False\n\n        # validate deepspeed dicts\n        for field_name in [\"optimizer\", \"scheduler\"]:\n            value = getattr(self, field_name)\n            if isinstance(\n                value, dict\n            ):  # dict is checked above, only fields are checked here\n                if \"type\" in value:\n                    if not isinstance(value[\"type\"], str):\n                        logging.error(\n                            self.__class__.__name__\n                            + \".validate_types() \"\n                            + f\"{field_name}: key 'type' must be a string\"\n                        )\n                        return False\n                else:\n                    logging.error(\n                        f\"{FAIL}\"\n                        + self.__class__.__name__\n                        + \".validate_types() \"\n                        + f\"{field_name}: must contain key 'type'\"\n                    )\n                    return False\n                if \"params\" in value:\n                    if not isinstance(value[\"params\"], dict):\n                        logging.error(\n                            f\"{FAIL}\"\n                            + self.__class__.__name__\n                            + \".validate_types() \"\n                            + f\"{field_name}: key 'params' must be a dict\"\n                        )\n                        return False\n                else:\n                    logging.error(\n                        f\"{FAIL}\"\n                        + self.__class__.__name__\n                        + \".validate_types() \"\n                        + f\"{field_name}: must contain key 'params'\"\n                    )\n                    return False\n\n        for field_name in [\"fp16\", \"amp\", \"flops_profiler\"]:\n            value = getattr(self, field_name)\n            if isinstance(value, dict):\n                if not \"enabled\" in value:\n                    error_message = (\n                        f\"{FAIL}\"\n                        + self.__class__.__name__\n                        + \".validate_types() \"\n                        + f\"{field_name}: must contain key 'enabled'\"\n                    )\n                    logging.error(error_message)\n                    return False\n\n        return True\n",
        "megatron/neox_arguments/deepspeed_args.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\n\ntry:\n    from .template import NeoXArgsTemplate\nexcept ImportError:\n    from template import NeoXArgsTemplate\n\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal\n\n\n@dataclass\nclass NeoXArgsDeepspeedConfig(NeoXArgsTemplate):\n    \"\"\"\n    Args for deepspeed config\n    Every argument included here will be included in deepspeed config json\n    As of Mar 8 2023, up to date compared to https://www.deepspeed.ai/docs/config-json/\n    \"\"\"\n\n    deepspeed: bool = True\n    \"\"\"boolean flag to enable DeepSpeed (Always True)\"\"\"\n\n    train_batch_size: int = None\n    \"\"\"\n    The effective training batch size. This is the amount of data samples that leads to one step of model update. train_batch_size is aggregated by the batch size that a single GPU processes in one forward/backward pass (a.k.a., train_step_batch_size), the gradient accumulation steps (a.k.a., gradient_accumulation_steps), and the number of GPUs.\n    \"\"\"\n\n    train_micro_batch_size_per_gpu: int = None\n    \"\"\"\n    Batch size to be processed by one GPU in one step (without gradient accumulation). When specified, gradient_accumulation_steps is automatically calculated using train_batch_size and number of GPUs. Should not be concurrently specified with gradient_accumulation_steps in the configuration JSON.\n    \"\"\"\n\n    gradient_accumulation_steps: int = 1\n    \"\"\"\n    Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. When specified, train_step_batch_size is automatically calculated using train_batch_size and number of GPUs. Should not be concurrently specified with train_step_batch_size in the configuration JSON.\n    \"\"\"\n\n    optimizer: dict = None\n    \"\"\"\n    dict containing the keys type and params\n\n    type: The optimizer name. DeepSpeed natively supports Adam, AdamW, OneBitAdam, Lamb, and OneBitLamb optimizers (See here for details) and will import other optimizers from torch.\n\n    params: Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for Adam).\n    \"\"\"\n\n    scheduler: dict = None\n    \"\"\"\n    dict containing the keys type and params\n\n    type: The scheduler name. See here (https://deepspeed.readthedocs.io/en/latest/schedulers.html) for list of support schedulers.\n\n    params: Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.\n    \"\"\"\n\n    fp32_allreduce: bool = False\n    \"\"\"\n    During gradient averaging perform allreduce with 32 bit values\n    \"\"\"\n\n    prescale_gradients: bool = False\n    \"\"\"\n    Scale gradients before doing allreduce\n    \"\"\"\n\n    gradient_predivide_factor: float = 1.0\n    \"\"\"\n    Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs\n    \"\"\"\n\n    sparse_gradients: bool = False\n    \"\"\"\n    Enable sparse compression of torch.nn.Embedding gradients.\n    \"\"\"\n\n    # ---FP16 Training Options---\n\n    fp16: dict = None\n    \"\"\"\n    Configuration for using mixed precision/FP16 training that leverages NVIDIA’s Apex package.\n\n    Dictionary options as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#fp16-training-options\n    \"\"\"\n\n    bf16: dict = None\n    \"\"\"\n    Configuration for using bfloat16 floating-point format as an alternative to FP16. BFLOAT16 requires hardware support (e.g., NVIDIA A100).\n\n    Dictionary options as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#bfloat16-training-options\n    \"\"\"\n\n    # ---Automatic Mixed Precision (AMP) Training Options---\n\n    amp: dict = None\n    \"\"\"\n    Configuration for using automatic mixed precision (AMP) training that leverages NVIDIA’s Apex AMP package.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options\n    \"\"\"\n\n    gradient_clipping: float = 1.0\n    \"\"\"\n    Enable gradient clipping with provided value\n    \"\"\"\n\n    # ---ZeRO Optimization Options---\n\n    zero_optimization: dict = None\n    \"\"\"\n    Configuration for using ZeRO optimization.\n\n    Multi-level dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#zero-optimization-options\n    \"\"\"\n\n    # ---Logging Options---\n\n    curriculum_learning: dict = None\n    \"\"\"\"\"\"\n\n    curriculum_seqlen: int = 0\n    \"\"\"\n    Internal var for tracking the current seqlen\n    \"\"\"\n\n    steps_per_print: int = 10\n    \"\"\"\n    Print train loss every N steps.\n    \"\"\"\n\n    wall_clock_breakdown: bool = False\n    \"\"\"\n    Enable timing of the latency of forward/backward/update training phases.\n    \"\"\"\n\n    dump_state: bool = False\n    \"\"\"\n    Print out state information of DeepSpeed object after initialization.\n    \"\"\"\n\n    # ---FLOPS Profiler Options---\n\n    flops_profiler: dict = None\n    \"\"\"\n    Configuration for using FLOPS profiler.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#flops-profiler\n    \"\"\"\n\n    # ---Communication Options---\n\n    communication_data_type: bool = None\n    \"\"\"\n    During gradient averaging, perform communication with selected data type. By default it will be determined by selected regime\n    \"\"\"\n\n    # ---Autotuning Options---\n    autotuning: dict = None\n    \"\"\"\n    Configuration for using autotuning.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#autotuning\n    \"\"\"\n\n    # ---Activation Checkpointing Options---\n\n    activation_checkpointing: dict = None\n    \"\"\"\n    Configuration for using activation checkpointing.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#activation-checkpointing\n    \"\"\"\n\n    # ---Sparse Attention Options---\n\n    sparse_attention: dict = None\n    \"\"\"\n    Configuration for using sparse attention.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#sparse-attention\n\n    \"\"\"\n\n    # ---Data Efficiency Options---\n\n    data_efficiency: dict = None\n    \"\"\"\n    Configuration for using data efficiency.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#data-efficiency\n    \"\"\"\n\n    # ---Monitoring Module Options---\n\n    tensorboard: dict = None\n    \"\"\"\n    Configuration for using tensorboard.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#monitoring-module-tensorboard-wandb-csv\n    \"\"\"\n\n    wandb: dict = None\n    \"\"\"\n    Configuration for using wandb.\n    \"\"\"\n\n    csv_monitor: dict = None\n    \"\"\"\n    Configuration for using csv_monitor.\n    \"\"\"\n\n    # ---Elastic Training Options---\n\n    elasticity: dict = None\n    \"\"\"\n    Configuration for using elastic training.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#elastic-training-config-v01-and-v02\n    \"\"\"\n\n    # ---Communication Logging Options---\n\n    comms_logger: dict = None\n    \"\"\"\n    Configuration for using communication logger.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#communication-logging\n    \"\"\"\n\n    # ---Compression Options---\n\n    compression_training: dict = None\n    \"\"\"\n    Configuration for using compression training.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#compression\n    \"\"\"\n\n    # ---Checkpointing Options---\n\n    checkpoint: dict = None\n    \"\"\"\n    Configuration for using checkpointing.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#checkpoint-options\n    \"\"\"\n\n    # ---Data Type Options---\n\n    data_types: dict = None\n    \"\"\"\n    Configuration for using data types.\n\n    Dictionary as described in Deepspeed documentation: https://www.deepspeed.ai/docs/config-json/#data-type-options\n    \"\"\"\n\n    # ---EXTRA ARGUMENTS---\n\n    deepspeed_extra_args: dict = None\n    \"\"\"\n    Dictionary of extra arguments to be included in the yaml config file. This can be used for any argument not included in the above list.\n    \"\"\"\n\n    autotuning: dict = None\n    \"\"\"Dictionary as described in DeepSpeed autotuning documentation: https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/autotuning\"\"\"\n\n\n@dataclass\nclass NeoXArgsDeepspeedRunner(NeoXArgsTemplate):\n    \"\"\"\n    Args for deepspeed runner (deepspeed.launcher.runner).\n    Every argument included here will be passed as command line argument to deepspeed.launcher.runner\n    \"\"\"\n\n    hostfile: str = None\n    \"\"\"\n    list of hostnames / ssh aliases and the number of GPUs per host\n\n    example file contents:\n    worker-1 slots=4\n    worker-2 slots=4\n    127.0.0 slots=4\n    127.0.1 slots=4\n    \"\"\"\n\n    include: str = None\n    \"\"\"\n    Specify hardware resources to use during execution. String format is `NODE_SPEC[@NODE_SPEC ...]` where `NODE_SPEC=NAME[:SLOT[,SLOT ...]]`. If `:SLOT` is omitted, include all slots on that host. Example: `\"worker-0@worker-1:0,2\"` will use all slots. on `worker-0` and slots `[0, 2]` on `worker-1`.\n    \"\"\"\n\n    exclude: str = None\n    \"\"\"\n    Specify hardware resources to NOT use during execution. Same format as include\n    \"\"\"\n\n    num_nodes: int = -1\n    \"\"\"\n    Total number of worker nodes to run on, this will use the top N hosts from the given hostfile. -1 will use all.\n    \"\"\"\n\n    num_gpus: int = None\n    \"\"\"\n    Max number of GPUs to use on each node, will use [0:N) GPU ids on each node. None / not specifying a value will use all.\n    \"\"\"\n\n    master_port: int = 29500\n    \"\"\"\n    Port used by PyTorch distributed for communication during training.\n    \"\"\"\n\n    master_addr: str = None\n    \"\"\"\n    IP address of node 0, will be inferred via 'hostname -I' if not specified.\n    \"\"\"\n\n    launcher: Literal[\"pdsh\", \"openmpi\", \"mvapich\", \"slurm\"] = \"pdsh\"\n    \"\"\"\n    Launcher backend for multi-node training. Options currently include PDSH, OpenMPI, MVAPICH.\n    \"\"\"\n\n    force_multi: bool = False\n    \"\"\"\n    Force multi-node training even if only one node is specified.\n    \"\"\"\n\n    autotuning_run: str = None\n    \"\"\"\n    Either \"tune\", \"run\", or `None`.\n    \"\"\"\n\n    no_ssh_check: bool = False\n    \"\"\"\n    If true, overrides the default check where DeepSpeed confirms that the headnode is accessible via ssh.\n    \"\"\"\n\n    force_multi: bool = False\n    \"\"\"\n    If true, Force multi-node launcher mode, helps in cases where user wants to launch on single remote node.\n    \"\"\"\n\n    comment: str = None\n    \"\"\"\n    Adds a `--comment` to the DeepSpeed launch command. In DeeperSpeed this is passed on to the SlurmLauncher as well. Sometimes necessary for cluster rules, or so I've heard.\n    \"\"\"\n\n    account: str = None\n    \"\"\"\n    Adds a `--account` to the DeepSpeed launch command. In DeeperSpeed this is passed on to the SlurmLauncher as well. Sometimes necessary for cluster rules, or so I've heard.\n    \"\"\"\n",
        "megatron/neox_arguments/neox_args.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport subprocess\nfrom dataclasses import dataclass\n\ntry:\n    from .template import NeoXArgsTemplate\nexcept ImportError:\n    from template import NeoXArgsTemplate\n\ntry:\n    from typing import List, Literal, Union, Optional, Any\nexcept ImportError:\n    from typing_extensions import List, Literal, Union, Optional\n\n\nATTENTION_TYPE_CHOICES = [\n    \"global\",\n    \"local\",\n    \"sparse_fixed\",\n    \"sparse_variable\",\n    \"bigbird\",\n    \"bslongformer\",\n    \"gmlp\",\n    \"amlp\",\n    \"flash\",\n    \"rwkv\",\n    \"mamba\",\n]\n\n\ndef get_git_commit_hash():\n    \"\"\"Gets the git commit hash of your current repo (if it exists)\"\"\"\n    try:\n        git_hash = subprocess.check_output([\"git\", \"describe\", \"--always\"]).strip()\n        git_hash = git_hash.decode()\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        git_hash = None\n    return git_hash\n\n\n@dataclass\nclass NeoXArgsParallelism(NeoXArgsTemplate):\n    \"\"\"\n    Parallelism Arguments\n    \"\"\"\n\n    pipe_parallel_size: int = 0\n    \"\"\"\n    Number of pipeline parallel stages. Disable with 0.\n    \"\"\"\n\n    model_parallel_size: int = 1\n    \"\"\"\n    Size of the model parallelism.\n    \"\"\"\n\n    pipe_partition_method: str = \"type:transformer|mlp\"\n    \"\"\"\n    method used to distribute model layers across pipeline stages. Choose from \"parameters\", which balances the number\n    of parameters on each pipeline stage, \"uniform\", which naively balances the number of layers per stage, or\n    \"type:[regex]\", which balances layers whose class names match [regex]\n    \"\"\"\n\n    world_size: int = None\n    \"\"\"\n    Total world size (i.e number of gpus in cluster). Configured post-launch using distributed launcher\n    \"\"\"\n\n    is_pipe_parallel: bool = False\n    \"\"\"\n    flag to determine whether pipeline parallelism is on - shouldn't be set by user, is automatically determined\n    according to pipeline parallel size.\n    \"\"\"\n\n    sequence_parallel: bool = False\n    \"\"\"\n    flag to determine whether Megatron-style Sequence Parallelism (https://arxiv.org/abs/2205.05198)\n    (Layernorm inputs and activations are sharded across model parallel group) will be used. Has no effect when model_parallel_size is 1.\n    **Set by user, in contrast to neox_args.is_pipe_parallel.**\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsModel(NeoXArgsTemplate):\n    \"\"\"\n    Model Arguments\n    \"\"\"\n\n    precision: Literal[\"fp16\", \"fp32\", \"bfloat16\"] = None\n    \"\"\"\n    description of the used precision, either one of fp16 or fp32 (and in the future bf16).\n    \"\"\"\n\n    num_layers: int = None\n    \"\"\"\n    Number of transformer layers.\n    \"\"\"\n\n    hidden_size: int = None\n    \"\"\"\n    Transformer hidden size.\n    \"\"\"\n\n    intermediate_size: int = None\n    \"\"\"\n    Transformer intermediate size. Default = 4h\n    \"\"\"\n\n    mlp_multiple_of: int = 1\n    \"\"\"\n    force mlp size to be a multiple of this value\n    \"\"\"\n\n    expansion_factor: float = None\n    \"\"\"\n    Transformer intermediate size. Default = 4\n    \"\"\"\n\n    num_attention_heads: int = None\n    \"\"\"\n    Number of transformer attention heads.\n\n    If num_kv_heads is set, will control only number of query heads.\n    \"\"\"\n\n    num_kv_heads: int = None\n    \"\"\"\n    Number of transformer key/value attention heads.\n\n    If set to None or the same value as num_attention_heads, will perform multi-head attention (MHA).\n    If set to < num_attention_heads but > 1, will perform grouped-query attention (GQA) (https://arxiv.org/pdf/2305.13245.pdf)\n    If set to 1, will perform multi-query attention.\n\n    Must be < num_attention_heads and divide num_attention_heads evenly.\n    \"\"\"\n\n    seq_length: int = None\n    \"\"\"\n    Maximum sequence length to process.\n    \"\"\"\n\n    sliding_window_width: int = None\n    \"\"\"\n    Width of the attention sliding window. Only supported with Flash Attention 2.\n    \"\"\"\n\n    max_position_embeddings: int = None\n    \"\"\"\n    Maximum number of position embeddings to use. This is the size of position embedding.\n    \"\"\"\n\n    norm: Literal[\n        \"layernorm\",\n        \"rmsnorm\",\n        \"non_parametric_layernorm\",\n        \"scalenorm\",\n        \"te_rmsnorm\",\n        \"te_layernorm\",\n    ] = \"layernorm\"\n    \"\"\"\n    Normalization layer to use. Choose from \"layernorm\", \"rmsnorm\", \"non_parametric_layernorm\", \"scalenorm\", \"te_rmsnorm\", \"te_layernorm\".\n    \"\"\"\n\n    layernorm_fusion: bool = False\n    \"\"\"\n    Use fused layer norm kernel (if `norm` is `layernorm`).\n    \"\"\"\n\n    rmsnorm_fusion: bool = False\n    \"\"\"\n    Use fused RMS norm kernel (if `norm` is `rmsnorm`).\n    \"\"\"\n\n    use_qk_layernorm: bool = False\n    \"\"\"\n    Use QK Normalization\n    \"\"\"\n\n    layernorm_epsilon: float = 1.0e-5\n    \"\"\"\n    Layer norm epsilon.\n    \"\"\"\n\n    rms_norm_epsilon: float = 1.0e-8\n    \"\"\"\n    Root mean squared norm epsilon\n    \"\"\"\n\n    scalenorm_epsilon: float = 1.0e-8\n    \"\"\"\n    Scalenorm epsilon\n    \"\"\"\n\n    pos_emb: Literal[\n        \"learned\", \"rotary\", \"sinusoidal\", \"rpe\", \"alibi\", \"none\"\n    ] = \"learned\"\n    \"\"\"\n    Type of positional embedding to use - choose from 'learned', 'rotary', 'sinusoidal', 'rpe', 'none'\n    \"\"\"\n\n    rpe_num_buckets: int = 32\n    \"\"\"\n    T5 relative positional encoding number of buckets, default 32.\n    \"\"\"\n\n    rpe_max_distance: int = 128\n    \"\"\"\n    T5 relative positional encoding max distance, default 128.\n    \"\"\"\n\n    opt_pos_emb_offset: int = 0\n    \"\"\"\n    Learned position embedding offset (only used by OPT, where it should be set to 2).\n    \"\"\"\n\n    no_weight_tying: bool = False\n    \"\"\"\n    Disables weight tying between embedding weights and final Linear layer\n    \"\"\"\n\n    attention_config: list = None\n\n    \"\"\"\n    Attention configuration for gpt-neox\n\n    The first item in the list specifies the attention type(s), and should be a list of strings. The second item\n    specifies the number of times to repeat those attention types in the full list.\n\n    attention type choices:  [global, local, sparse_fixed, sparse_variable, bslongformer, bigbird, \"gmlp\", \"amlp\", \"flash\", \"mamba\", \"rwkv\"]\n\n    So a 12 layer network with only global attention could be specified like:\n        [[[`global`], 12]]\n\n    or a 12 layer network with alternating global / local like:\n        [[[`global`, `local`], 6]]\n\n    If none is specified, this defaults to\n        [[[`global`], n_layers]]\n    \"\"\"\n\n    sparsity_config: dict = None\n\n    \"\"\"\n    Sparsity configuration dict as defined in https://www.deepspeed.ai/docs/config-json/#sparse-attention\n\n    Note that since neox is autoregressive, attention is always \"unidirectional\" and `horizontal_global_attention` is\n    always false.\n\n    The main difference between our sparsity config and deepspeed's is that `mode` is ignored - since it is instead\n    specified in attention_config defining each layer.\n\n    An example config is given below:\n          \"sparse_attention\": {\n            \"block\": 16,\n            \"different_layout_per_head\": true,\n            \"num_local_blocks\": 4,\n            \"num_global_blocks\": 1,\n            \"num_different_global_patterns\": 4,\n            \"num_random_blocks\": 0,\n            \"local_window_blocks\": [4],\n            \"global_block_indices\": [0],\n            \"global_block_end_indices\": None,\n            \"num_sliding_window_blocks\": 3\n          }\n    \"\"\"\n\n    num_unique_layers: int = None\n    \"\"\"\n    Number of unique transformer layers. num-layers should be divisible by this value. Currently only has an effect when pipe_parallel_size=0.\n    \"\"\"\n\n    param_sharing_style: str = \"grouped\"\n    \"\"\"\n    Ordering of the shared parameters. For example, for a num-layers=4 and --num-unique-layers=2, we will have the following ordering for two unique layers 1 and 2-: grouped: [1, 2, 1, 2] and spaced: [1, 1, 2, 2].\n    \"\"\"\n\n    make_vocab_size_divisible_by: int = 128\n    \"\"\"\n    Pad the vocab size to be divisible by this value. This is added for computational efficiency reasons.\n    \"\"\"\n\n    activation: Literal[\n        \"gelu\",\n        \"geglu\",\n        \"relu\",\n        \"softsign\",\n        \"swish\",\n        \"mish\",\n        \"silu\",\n        \"reglu\",\n        \"swiglu\",\n        \"bilinear\",\n        \"glu\",\n    ] = \"gelu\"\n    \"\"\"\n    Activation function to use - choose from [\"gelu\", \"geglu\", \"relu\", \"softsign\", \"swish\", \"mish\", \"silu\", \"reglu\", \"swiglu\", \"bilinear\", \"glu\"]\n    \"\"\"\n\n    use_flashattn_swiglu: bool = False\n    \"\"\"\n    Use flash attention's version of swiglu\n    \"\"\"\n\n    scaled_upper_triang_masked_softmax_fusion: bool = False\n    \"\"\"\n    Enable fusion of query_key_value_scaling time (upper diagonal) masking and softmax.\n    \"\"\"\n\n    scaled_masked_softmax_fusion: bool = False\n    \"\"\"\n    Enable fusion of query_key_value_scaling general masking and softmax.\n    \"\"\"\n\n    bias_gelu_fusion: bool = False\n    \"\"\"\n    Enable bias and gelu fusion.\n    \"\"\"\n\n    bias_dropout_fusion: bool = False\n    \"\"\"\n    Enable bias and dropout fusion.\n    \"\"\"\n\n    rope_fusion: bool = False\n    \"\"\"\n    Enable rotary embedding fusion.\n    \"\"\"\n\n    fp16_lm_cross_entropy: bool = False\n    \"\"\"\n    Move the cross entropy unreduced loss calculation for lm head to fp16.\n    \"\"\"\n\n    init_method_std: float = 0.02\n    \"\"\"\n    Standard deviation of the zero mean normal distribution used for weight initialization.\n    \"\"\"\n\n    apply_query_key_layer_scaling: bool = False\n    \"\"\"\n    Scale Q * K^T by 1 / layer-number. If this flag is set, then it will automatically set attention-softmax-in-fp32 to true\n    \"\"\"\n\n    use_cpu_initialization: bool = False\n    \"\"\"\n    If set, affine parallel weights initialization uses CPU\n    \"\"\"\n\n    attention_softmax_in_fp32: bool = False\n    \"\"\"\n    Run attention masking and softmax in fp32.\n    \"\"\"\n\n    rotary_pct: float = 1.0\n    \"\"\"\n    pct of hidden dims to apply rotary positional embedding to\n    \"\"\"\n\n    rotary_emb_base: int = 10000\n    \"\"\"\n    Base for rotary positional embedding\n    \"\"\"\n\n    rotary_save_freqs_buffer: bool = False\n    \"\"\"\n    Used to control whether the `inv_freqs` buffer in rotary embeddings\n    will be stored in checkpoints (persistent=True) or not.\n\n    Defaults to false, but is left configurable to maintain backward-compatibility\n    with GPT-NeoX checkpoints that were trained with this flag.\n    \"\"\"\n\n    init_method: Literal[\n        \"normal\",\n        \"scaled_normal\",\n        \"orthogonal\",\n        \"scaled_orthogonal\",\n        \"xavier_uniform\",\n        \"xavier_normal\",\n        \"wang_init\",\n        \"small_init\",\n        \"single_residual_scaled_normal\",\n    ] = \"normal\"\n    \"\"\"\n    Init function used on all layers except ff residual outputs - choose from\n    [\"normal\", \"scaled_normal\", \"orthogonal\", \"scaled_orthogonal\", \"xavier_uniform\", \"xavier_normal\", \"wang_init\", \"small_init\"]\n    \"\"\"\n\n    output_layer_init_method: Literal[\n        \"normal\",\n        \"scaled_normal\",\n        \"orthogonal\",\n        \"scaled_orthogonal\",\n        \"xavier_uniform\",\n        \"xavier_normal\",\n        \"wang_init\",\n        \"small_init\",\n        \"single_residual_scaled_normal\",\n    ] = \"scaled_normal\"\n    \"\"\"\n    Init function used for ff residual outputs - choose from\n    [\"normal\", \"scaled_normal\", \"orthogonal\", \"scaled_orthogonal\", \"xavier_uniform\", \"xavier_normal\", \"wang_init\", \"small_init\"]\n    \"\"\"\n\n    gmlp_attn_dim: int = 64\n    \"\"\"\n    the dimension of the single head self attention in gmlp model (not used in gpt models).\n    If None - gmlp model doesn't use attention.\n    \"\"\"\n\n    gpt_j_residual: bool = False\n    \"\"\"\n    If false, we use the conventional residual path:\n      x = x + attn(ln1(x))\n      x = x + mlp(ln2(x))\n    Otherwise, we use the residual path from GPT-J, which offers a slight speedup:\n      x = ln(x)\n      x = x + attn(x) + mlp(x)\n    \"\"\"\n\n    gpt_j_tied: bool = False\n    \"\"\"\n    If false, we use\n      x = x + attn(ln1(x)) + mlp(ln2(x))\n    Otherwise, we tie the layer norms\n      y = ln(x)\n      x = x + attn(y) + mlp(y)\n    \"\"\"\n\n    use_bias_in_norms: bool = True\n    \"\"\"\n    If false, norms (e.g. LayerNorm) will not have bias terms\n    \"\"\"\n    use_bias_in_attn_linear: bool = True\n    \"\"\"\n    If false, attn_linear (e.g. QKVO) will not have bias terms\n    \"\"\"\n    use_bias_in_mlp: bool = True\n    \"\"\"\n    If false, mlps will not have bias terms\n    \"\"\"\n\n    soft_prompt_tuning: dict = None\n    \"\"\"\n    Dictionary configuring the soft prompt tuning parameters.\n    If enabled, will train *only* the soft prompt, and freezes the rest of the model.\n    parameters in the dict are:\n        'enabled': bool = True # enables soft prompting\n        'num_tokens': int = 10 # length of the soft prompt in tokens\n        'init_string': str = '' # if provided, initialize the soft prompt with the word embeddings of this string\n        'init_range': float = 0.5 # if no init string is provided, initialize the soft prompt with a uniform distribution between -init_range and init_rang\n    \"\"\"\n\n    mamba_selective_scan_fusion: bool = False\n    \"\"\"\n    Enable fused kernels for Mamba selective scan.\n    \"\"\"\n\n    mamba_causal_conv_fusion: bool = False\n    \"\"\"\n    Enable fused kernels for Mamba causal Conv1d.\n    \"\"\"\n\n    mamba_inner_func_fusion: bool = False\n    \"\"\"\n    Enable fused inner operator for Mamba. (Supersedes conv. and selective scan fusion flags, requires each of those kernels to be installed.)\n    \"\"\"\n\n    mamba_selective_fp32_params: bool = True\n    \"\"\"\n    Keep selected parameters in fp32 for Mamba (A and D).\n    Requires https://github.com/EleutherAI/DeeperSpeed/pull/61 .\n    \"\"\"\n\n    mamba_use_bias_in_conv: bool = True\n    \"\"\"\n    If false, conv1d in mamba block will not have bias term\n    \"\"\"\n\n    mamba_use_bias_in_linears: bool = False\n    \"\"\"\n    Enable bias terms in mamba block up- and down- projections (in_proj and out_proj).\n    \"\"\"\n\n    # Output layer parallelism over the hidden dim is currently broken (https://github.com/EleutherAI/gpt-neox/issues/905)\n    output_layer_parallelism: Literal[\"column\"] = \"column\"\n\n    \"\"\"\n    Parameter controlling whether the output layer is parallelized over the hidden dim (row) or the vocab dim (column)\n    \"\"\"\n\n    serve_model_weights: bool = False\n    \"\"\"\n    If true, serve model weight pointers over a socket connection\n    \"\"\"\n\n    weight_server_port: Union[int, List[int]] = 6000\n    \"\"\"\n    Port(s) to serve model weights over\n    If an integer is provided, the port for each GPU will be 6000 + global rank\n    If a list is provided, the ports will be used in order, e.g. rank0 will be weight_server_port[0]\n    \"\"\"\n\n    online_dataserver_ips: Union[str, List[str]] = \"localhost\"\n    \"\"\"\n    ip addresses to connect to for online data serving, defaults to localhost\n    \"\"\"\n\n    online_dataserver_ports: Union[int, List[int]] = 10000\n    \"\"\"\n    Port(s) to connect to for online data serving, defaults to 10000\n    \"\"\"\n\n    te_columnparallel: bool = False\n    \"\"\"\n    Use TransformerEngine for RowParallelLinear layer.\n    \"\"\"\n\n    te_rowparallel: bool = False\n    \"\"\"\n    Use TransformerEngine for ColumnParallelLinear layer.\n    \"\"\"\n\n    te_layernorm_mlp: bool = False\n    \"\"\"\n    Use TransformerEngine for LayerNormMLP layer.\n    \"\"\"\n\n    te_mha: bool = False\n    \"\"\"\n    Use TransformerEngine for MultiheadAttention layer.\n    \"\"\"\n\n    te_fp8_format: Literal[\"e4m3\", \"hybrid\"] = \"hybrid\"\n    \"\"\"\n    Controls the FP8 data format used during forward and backward pass by TransformerEngine.\n    Hybrid uses E4M3 during forward pass, E5M2 during backward pass.\n    \"\"\"\n\n    te_fp8_wgrad: bool = True\n    \"\"\"\n    When set to False, override FP8 config options and do the wgrad computation\n    in higher precision.\n    \"\"\"\n\n    te_fp8_amax_history_len: int = 1\n    \"\"\"\n    The length of the amax history window used for scaling factor computation.\n    \"\"\"\n\n    te_fp8_amax_compute_algo: str = \"most_recent\"\n    \"\"\"\n    Algorithm used for choosing the `amax` value for the scaling factor computation. There are 2\n    predefined choices: `max` chooses the largest `amax` in the history window, while `most_recent`\n    always chooses the most recently seen value.\n    \"\"\"\n\n    te_fp8_margin: int = 0\n    \"\"\"\n    Margin for the scaling factor computation.\n    \"\"\"\n\n    te_fp8_mha: bool = False\n    \"\"\"\n    When set to True, use the FP8 implementation of Multi Head Attention.\n    \"\"\"\n\n    dim_att: int = None\n    \"\"\"\n    Total dimension of the attention mechanism for RWKV. If not set, defaults to hidden_size.\n    \"\"\"\n\n    head_size: int = None\n    \"\"\"\n    Size of each attention head for RWKV. Calculated as dim_att // num_attention_heads.\n    \"\"\"\n\n    ffn_dim: int = None\n    \"\"\"\n    Dimension of the feed-forward network for RWKV. If not set, calculated based on hidden_size and expansion_factor.\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsOptimizer(NeoXArgsTemplate):\n    \"\"\"\n    Optimizer Arguments\n    \"\"\"\n\n    optimizer_type: Literal[\n        \"adam\",\n        \"onebitadam\",\n        \"cpu_adam\",\n        \"cpu_torch_adam\",\n        \"sm3\",\n        \"madgrad_wd\",\n        \"sgd\",\n        \"lion\",\n    ] = \"adam\"\n    \"\"\"\n    Type of optimizer to use. Choose from ['adam', 'onebitadam', 'cpu_adam', 'cpu_torch_adam', 'sm3', 'madgrad_wd', 'sgd', 'lion']\n    NOTE: sgd will use MuSGD from Mup. Mup must be enabled for this optimizer.\n    \"\"\"\n\n    use_bnb_optimizer: bool = False\n    \"\"\"\n    Whether to enable the bitsandbytes optimizers\n    \"\"\"\n\n    zero_stage: Union[int, List[int], Literal[\"all\"]] = None\n    \"\"\"\n    Zero Optimizer stage\n    \"\"\"\n\n    zero_reduce_scatter: bool = None\n    \"\"\"\n    Zero: Uses reduce or reduce scatter instead of allreduce to average gradients\n    \"\"\"\n\n    zero_contiguous_gradients: bool = None\n    \"\"\"\n    Zero: Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models.\n    \"\"\"\n\n    zero_reduce_bucket_size: int = None\n    \"\"\"\n    Zero: Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes\n    \"\"\"\n\n    zero_allgather_bucket_size: int = None\n    \"\"\"\n    Zero: Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes\n    \"\"\"\n\n    lr: float = None\n    \"\"\"\n    Max Learning rate during training\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsLRScheduler(NeoXArgsTemplate):\n    \"\"\"\n    LR Scheduler Arguments\n    \"\"\"\n\n    lr_decay_style: Literal[\"constant\", \"linear\", \"cosine\", \"exponential\"] = \"linear\"\n    \"\"\"\n    Learning rate decay function. Choose from 'constant', 'linear', 'cosine', 'exponential'.\n    \"\"\"\n\n    lr_decay_iters: int = None\n    \"\"\"\n    Number of iterations to decay learning rate over, If None defaults to\n    --train-iters or the equivalent inferred valued from train_epochs.\n    \"\"\"\n\n    lr_decay_fraction: float = None\n    \"\"\"\n    Effective fraction of training over which to decay lr, overrides lr_decay_iters, useful when specifying train_epochs\n    \"\"\"\n\n    min_lr: float = 0.0\n    \"\"\"\n    Minimum value for learning rate. The scheduler clips values below this threshold.\n    \"\"\"\n\n    warmup: float = 0.01\n    \"\"\"\n    Percentage of total iterations to warmup on (.01 = 1 percent of all training iters).\n    \"\"\"\n\n    override_lr_scheduler: bool = False\n    \"\"\"\n    Reset the values of the scheduler (learning rate,warmup iterations, minimum learning rate, maximum number of iterations, and decay style from input arguments and ignore values from checkpoints. Note that all the above values will be reset.\n    \"\"\"\n\n    use_checkpoint_lr_scheduler: bool = False\n    \"\"\"\n    Use checkpoint to set the values of the scheduler (learning rate, warmup iterations, minimum learning rate, maximum number of iterations, and decay style from checkpoint and ignore input arguments.\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsLogging(NeoXArgsTemplate):\n    \"\"\"\n    Logging Arguments\n    \"\"\"\n\n    ### BEGIN WANDB ARGS ###\n    use_wandb: bool = None\n    \"\"\"Flag indicating if wandb is to be used.\"\"\"\n\n    wandb_group: str = None\n    \"\"\"Weights and Biases group name - used to group together \"runs\".\"\"\"\n\n    wandb_run_name: str = None\n    \"\"\"Weights and Biases run name for the current experiment\"\"\"\n\n    wandb_team: str = None\n    \"\"\"Team name for Weights and Biases.\"\"\"\n\n    wandb_project: str = \"neox\"\n    \"\"\"wandb project name\"\"\"\n\n    wandb_host: str = \"https://api.wandb.ai\"\n    \"\"\"url of the wandb host\"\"\"\n\n    wandb_init_all_ranks: bool = False\n    \"\"\"Initialize wandb on all ranks.\"\"\"\n    ### END WANDB ARGS ###\n\n    git_hash: str = get_git_commit_hash()\n    \"\"\"current git hash of repository\"\"\"\n\n    log_dir: str = None\n    \"\"\"\n    Directory to save logs to.\n    \"\"\"\n\n    ### BEGIN TENSORBOARD ARGS ###\n    tensorboard_writer = None\n    \"\"\"\n    initialized tensorboard writer\n    \"\"\"\n\n    tensorboard_dir: str = None\n    \"\"\"\n    Write TensorBoard logs to this directory.\n    \"\"\"\n    ### END TENSORBOARD ARGS ###\n\n    ### BEGIN COMET ARGS ###\n    use_comet: bool = None\n    \"\"\"Flag indicating if comet is to be used.\"\"\"\n\n    comet_workspace: Optional[str] = None\n    \"\"\"\n    Comet workspace name, if not configured Comet Experiments will be created in the user configured default workspace.\n    \"\"\"\n\n    comet_project: Optional[str] = None\n    \"\"\"\n    Comet project name, if not configured Comet Experiments will be created in the Uncategorized Experiments project.\n    \"\"\"\n\n    comet_experiment_name: Optional[str] = None\n    \"\"\"\n    Custom name for the Comet experiment. If not provided, a random name is used.\n    \"\"\"\n\n    comet_tags: Optional[list] = None\n    \"\"\"\n    List of tags to attach to the created Comet Experiment.\n    \"\"\"\n\n    comet_others: Optional[dict] = None\n    \"\"\"\n    Custom metadata to attach to the created Comet Experiment.\n    \"\"\"\n\n    comet_experiment: Any = None\n    \"\"\"\n    Initialized comet experiment object used to log data\n    \"\"\"\n    ### END COMET ARGS ###\n\n    peak_theoretical_tflops: float = None\n    \"\"\"\n    The peak hardware flops with which to compute MFU and HFU, in units of teraflops. Automatic detection is more trouble than it's worth, so this is left to the user. Helpful table listed at https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#tflops-comparison-table\n    \"\"\"\n\n    log_interval: int = 100\n    \"\"\"\n    Interval between logging.\n    \"\"\"\n\n    log_grad_pct_zeros: bool = False\n    \"\"\"\n    Log the percentage of zeros for the gradient of each parameter to wandb / tensorboard (useful for debugging). Needs wandb_init_all_ranks set to True if using pipeline parallelism to log all ranks.\n    \"\"\"\n\n    log_param_norm: bool = False\n    \"\"\"\n    Log the frob norm of the parameters to wandb / tensorboard (useful for debugging). Needs wandb_init_all_ranks set to True if using pipeline parallelism to log all ranks.\n    \"\"\"\n\n    log_grad_norm: bool = False\n    \"\"\"\n    Log the frob norm of the gradients to wandb / tensorboard (useful for debugging).\n    (N.B - this will only work with pp = 0 for now, as we don't have access to the gradients of the model because deepspeed.)\n    \"\"\"\n\n    log_optimizer_states: bool = False\n    \"\"\"\n    Log the frob norm of the optimizer states to wandb / tensorboard (useful for debugging).\n    \"\"\"\n\n    log_gradient_noise_scale: bool = False\n    \"\"\"\n    Whether to log the gradient noise scale when training (cf. https://arxiv.org/abs/1812.06162 for explanation)\n    \"\"\"\n\n    gradient_noise_scale_n_batches: int = 5\n    \"\"\"\n    Number of batches to accumulate gradients for in the gradient noise scale logger.\n    \"\"\"\n\n    gradient_noise_scale_cpu_offload: bool = False\n    \"\"\"\n    Whether to offload the buffered gradients to cpu when measuring gradient noise scale.\n    \"\"\"\n\n    ### BEGIN PROFILING ARGS\n    memory_profiling: bool = False\n    \"\"\"\n    Whether to take a memory snapshot of the model. Useful for debugging memory issues.\n    \"\"\"\n\n    memory_profiling_path: str = None\n    \"\"\"\n    Path to save memory snapshot to.\n    \"\"\"\n\n    profile: bool = False\n    \"\"\"\n    Enable nsys and pytorch profiling. When using this option with nsys,\n    nsys options should be directly specified in commandline.\n    An example nsys commandline is\n    ```\n    nsys profile -s none -t nvtx,cuda -o <path/to/output_file>\n    --force-overwrite true\n    --capture-range=cudaProfilerApi\n    --capture-range-end=stop\n    ```\n    \"\"\"\n\n    profile_step_start: int = 10\n    \"\"\"\n    Step to start profiling at.\n    \"\"\"\n\n    profile_step_stop: int = 12\n    \"\"\"\n    Step to stop profiling at.\n    \"\"\"\n    ### END PROFILING ARGS ###\n\n\n@dataclass\nclass NeoXArgsOther(NeoXArgsTemplate):\n    \"\"\"\n    Misc. Arguments\n    \"\"\"\n\n    distributed_backend: str = \"nccl\"\n    \"\"\"\n    Which backend to use for distributed training.\n    \"\"\"\n\n    local_rank: int = None\n    \"\"\"\n    local rank passed from distributed launcher.\n    \"\"\"\n\n    rank: int = None\n    \"\"\"\n    global rank of process being run (passed in via distributed launcher)\n    \"\"\"\n\n    lazy_mpu_init: bool = False\n    \"\"\"\n    If set to True, initialize_megatron() skips DDP initialization and returns function to complete it instead. Also turns on use-cpu-initialization flag. This is for external DDP manager.\n    \"\"\"\n\n    short_seq_prob: float = 0.1\n    \"\"\"\n    Probability of producing a short sequence.\n    \"\"\"\n\n    eod_mask_loss: bool = False\n    \"\"\"\n    Mask loss for the end of document tokens.\n    \"\"\"\n\n    adlr_autoresume: bool = False\n    \"\"\"\n    Enable auto-resume on adlr cluster.\n    \"\"\"\n\n    adlr_autoresume_object = None\n    \"\"\"\n    imported autoresume\n    \"\"\"\n\n    adlr_autoresume_interval: int = 1000\n    \"\"\"\n    Intervals over which check for auto-resume termination signal\n    \"\"\"\n\n    seed: int = 1234\n    \"\"\"\n    Random seed used for python, numpy, pytorch, and cuda.\n    \"\"\"\n\n    onnx_safe: bool = False\n    \"\"\"\n    Use workarounds for known problems with Torch ONNX exporter\n    \"\"\"\n\n    deepscale: bool = False\n    \"\"\"\n    (Deprecated) enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)'\n    \"\"\"\n\n    deepscale_config: str = None\n    \"\"\"(Deprecated) deepscale json configuration file.\"\"\"\n\n    deepspeed_mpi: bool = False\n    \"\"\"\n    Run via MPI, this will attempt to discover the necessary variables to initialize torch distributed from the MPI environment\n    \"\"\"\n\n    deepspeed_slurm: bool = False\n    \"\"\"\n    Run via SLURM, this will attempt to discover the necessary variables to initialize torch distributed from the SLURM environment\n    \"\"\"\n\n    user_script: str = None\n    \"\"\"\n    user script to be run\n    \"\"\"\n\n    iteration: int = None\n    \"\"\"\n    Set during training\n    \"\"\"\n\n    do_train: bool = None\n    \"\"\"\n    Set during training\n    \"\"\"\n\n    do_valid: bool = None\n    \"\"\"\n    Set during training\n    \"\"\"\n\n    do_test: bool = None\n    \"\"\"\n    Set during training\n    \"\"\"\n\n    global_num_gpus: int = None\n    \"\"\"\n    Set during launching\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsTokenizer(NeoXArgsTemplate):\n    \"\"\"\n    Tokenizer Arguments\n    \"\"\"\n\n    tokenizer_type: Literal[\n        \"GPT2BPETokenizer\",\n        \"HFTokenizer\",\n        \"HFGPT2Tokenizer\",\n        \"SPMTokenizer\",\n        \"CharLevelTokenizer\",\n        \"TiktokenTokenizer\",\n    ] = \"GPT2BPETokenizer\"\n    \"\"\"\n    Type of tokenizer to use - should be one of [\"GPT2BPETokenizer\", \"HFTokenizer\", \"HFGPT2Tokenizer\", \"SPMTokenizer\", \"CharLevelTokenizer\", \"TiktokenTokenizer\"]\n    \"\"\"\n\n    padded_vocab_size: int = None\n    \"\"\"\n    Total (padded) vocabulary size of tokenizer. Configured after launching of training,\n    as it's dependent on the parallelism size.\n    \"\"\"\n\n    tokenizer = None\n    \"\"\"\n    tokenizer object loaded into memory and accessible by other functions\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsTraining(NeoXArgsTemplate):\n    \"\"\"\n    Training Arguments\n    \"\"\"\n\n    data_path: str = None\n    \"\"\"\n    Path to combined dataset to split.\n    \"\"\"\n\n    use_shared_fs: bool = True\n    \"\"\"\n    Whether to use a shared filesystem for data loading. If False, local rank 0 on all nodes will preprocess the data,\n    otherwise only global rank 0 will preprocess the data. This is implemented in megatron/data/gpt2_dataset.py::_build_index_mappings.\n    \"\"\"\n\n    train_data_paths: list = None\n    \"\"\"\n    List of paths to train datasets.\n    \"\"\"\n\n    train_label_data_paths: list = None\n    \"\"\"\n    List of paths to train label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    train_reward_data_paths: list = None\n    \"\"\"\n    List of paths to train reward datasets\n    \"\"\"\n\n    test_data_paths: list = None\n    \"\"\"\n    List of paths to test datasets.\n    \"\"\"\n\n    test_label_data_paths: list = None\n    \"\"\"\n    List of paths to test label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    test_reward_data_paths: list = None\n    \"\"\"\n    List of paths to test reward datasets\n    \"\"\"\n\n    valid_data_paths: list = None\n    \"\"\"\n    List of paths to validation datasets.\n    \"\"\"\n\n    valid_label_data_paths: list = None\n    \"\"\"\n    List of paths to validation label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    valid_reward_data_paths: list = None\n    \"\"\"\n    List of paths to validation reward datasets\n    \"\"\"\n\n    pos_train_data_paths: list = None\n    neg_train_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative training datasets.\n    \"\"\"\n\n    pos_train_label_data_paths: list = None\n    neg_train_label_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative training label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    pos_valid_data_paths: list = None\n    neg_valid_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative validation datasets.\n    \"\"\"\n\n    pos_valid_label_data_paths: list = None\n    neg_valid_label_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative validation label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    pos_test_data_paths: list = None\n    neg_test_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative test datasets.\n    \"\"\"\n\n    pos_test_label_data_paths: list = None\n    neg_test_label_data_paths: list = None\n    \"\"\"\n    List of paths to positive and negative test label datasets (not shifted by 1 yet!).\n    \"\"\"\n\n    train_data_weights: list = None\n    \"\"\"\n    List of 'weights' that decide how often to sample from each training dataset when blending datasets. If None, defaults to equal weighting.\n    Should be a list the same length as `train_data_paths`\n    \"\"\"\n\n    valid_data_weights: list = None\n    \"\"\"\n    List of 'weights' that decide how often to sample from each validation dataset when blending datasets. If None, defaults to equal weighting.\n    Should be a list the same length as `valid_data_paths`\n    \"\"\"\n\n    test_data_weights: list = None\n    \"\"\"\n    List of 'weights' that decide how often to sample from each test dataset when blending datasets. If None, defaults to equal weighting.\n    Should be a list the same length as `test_data_paths`\n    \"\"\"\n\n    weight_by_num_documents: bool = False\n    \"\"\"\n    If True, Builds dataset weights from a multinomial distribution over groups of data according to the number of\n    documents in each group.\n\n    WARNING: setting this to True will override any user provided weights\n\n    We sample from a group according to the probability p(L) ∝ |L| ** α,\n    where p(L) is the probability of sampling from a given group,\n          |L| is the number of examples in that datapoint,\n          and α is a coefficient that acts to upsample data from underrepresented groups\n\n    Hence α (`alpha`) allows us to control how much to 'boost' the probability of training on low-resource groups.\n\n    See https://arxiv.org/abs/1911.02116 for more details\n    \"\"\"\n\n    weighted_sampler_alpha: float = 1.0\n    \"\"\"\n    Alpha value for `weight_by_num_documents`. Only has an effect if `weight_by_num_documents` = True.\n\n    when alpha = 1, the probability of sampling from a given group = n_samples / total_samples\n    as alpha -> 0, the probability of sampling from all groups becomes equal, and number of documents has no effect\n    as alpha -> inf, the probability of sampling from the groups with *the most samples* -> 1\n    \"\"\"\n\n    data_impl: Literal[\"infer\", \"mmap\", \"cached\"] = \"infer\"\n    \"\"\"\n    Implementation of indexed datasets, can be one of \"infer\", \"cached\", or \"mmap\"\n    \"\"\"\n\n    pack_impl: Literal[\"packed\", \"pack_until_overflow\", \"unpacked\"] = \"packed\"\n    \"\"\"\n    Packing implementation, can be one of \"packed\", \"pack_until_overflow\", or \"unpacked\".\n\n    warning: pack_until_overflow is very naive and will likely have issues with pretraining scale datasets\n    \"\"\"\n\n    dataset_impl: Literal[\"gpt2\", \"pairwise\", \"online\"] = \"gpt2\"\n    \"\"\"\n    Dataset implementation, can be one of \"gpt2\", \"pairwise\", or \"online\"\n    \"\"\"\n\n    train_impl: Literal[\"normal\", \"dpo\", \"rm\", \"kto\", \"reinforce\"] = \"normal\"\n    \"\"\"\n    Training implementation, can be one of \"normal\", \"dpo\", \"kto\", \"reinforce\", or \"rm\"\n    \"\"\"\n\n    dpo_fp32: bool = True\n    \"\"\"\n    Whether to cast logits to fp32 for DPO loss calculation.\n    \"\"\"\n\n    dpo_reference_free: bool = False\n    \"\"\"\n    Whether to use reference-free DPO.\n    \"\"\"\n\n    dpo_beta: float = 0.1\n    \"\"\"\n    Beta value for DPO\n    \"\"\"\n\n    kto_fp32: bool = True\n    \"\"\"\n    Whether to cast logits to fp32 for KTO loss calculation.\n    \"\"\"\n\n    kto_desirable_weight: float = 1.0\n    \"\"\"\n    Weight for desirable loss in KTO. Might help if you have unbalanced desirable and undesirable classes.\n    \"\"\"\n\n    kto_undesirable_weight: float = 1.0\n    \"\"\"\n    Weight for undesirable loss in KTO. Might help if you have unbalanced desirable and undesirable classes.\n    \"\"\"\n\n    z_loss: float = 0.0\n    \"\"\"\n    Z-loss parameter, only implemented for RM training currently.\n    https://arxiv.org/pdf/2204.02311\n    https://arxiv.org/pdf/2309.10305\n    \"\"\"\n\n    kto_beta: float = 0.1\n    \"\"\"\n    Beta value for KTO\n    \"\"\"\n\n    fp32_reinforce: bool = True\n    \"\"\"\n    Whether to cast logits to fp32 for Reinforce loss calculation.\n    \"\"\"\n\n    kl_impl: Literal[\"abs\", \"mse\", \"kl\", \"full\"] = \"mse\"\n    \"\"\"\n    KL divergence implementation, can be one of \"abs\", \"mse\", \"kl\", or \"full\"\n    \"\"\"\n\n    kl_div_beta: float = 0.1\n    \"\"\"\n    Beta value for KL divergence in Reinforce loss calculation.\n    \"\"\"\n\n    reinforce_leave_one_out: bool = False\n    \"\"\"\n    Whether to use reinforce leave one out for training\n    (from https://arxiv.org/abs/2402.14740 and https://api.semanticscholar.org/CorpusID:198489118)\n    \"\"\"\n\n    allow_chopped: bool = True\n    \"\"\"\n    WARNING: if your packing impl is packed, this is ignored.\n\n    Allow chopped samples in the dataset.\n    (e.g if your sequence length is 1024 and you have a sample of length 1026, it will be chopped to 1024)\n    \"\"\"\n\n    mmap_warmup: bool = False\n    \"\"\"\n    Warm up mmap files.\n    \"\"\"\n\n    save: str = None\n    \"\"\"\n    Output directory to save checkpoints to.\n    \"\"\"\n\n    s3_path: str = None\n    \"\"\"\n    Path to s3 bucket for saving checkpoints.\n    \"\"\"\n\n    s3_chunk_size: int = 104_857_600\n    \"\"\"\n    The number of bytes in each file chunk when uploading to s3. Defaults to 100MiB.\n    \"\"\"\n\n    config_files: dict = None\n    \"\"\"\n    Store of original config files mapping config filename to file contents\n    \"\"\"\n\n    load: str = None\n    \"\"\"\n    Directory containing a model checkpoint.\n    \"\"\"\n\n    checkpoint_validation_with_forward_pass: bool = False\n    \"\"\"\n    save input and output of a forward pass with the checkpoint and validate after load\n    \"\"\"\n\n    checkpoint_scale: Literal[\"linear\", \"log\"] = \"linear\"\n    \"\"\"\n    How step at which checkpoints are saved should scale. \"linear\" implies 1 checkpoint will be saved at every multiple of `checkpoint-factor`,\n    while \"log\" implies that the number of steps between each checkpoint will be multiplied by `checkpoint-factor` at each step, starting from step 1.\n    \"\"\"\n\n    checkpoint_factor: Union[int, float] = None\n    \"\"\"\n    Acts as a multiplier on either the \"log\" or \"linear\" checkpoint spacing.\n\n    With `checkpoint-scale=\"linear\"`, `checkpoint-factor=20`, and `train-iters=100`, checkpoints will be saved at\n    steps [20, 40, 60, 80, 100].\n\n    With `checkpoint-scale=\"log\"`, `checkpoint-factor=2`, and `train-iters=100`, checkpoints will be saved at\n    steps [1, 2, 4, 8, 16, 32, 64, 100].\n\n    Note that the last checkpoint step is always saved.\n    \"\"\"\n\n    extra_save_iters: list = None\n    \"\"\"\n    Additional iterations when a checkpoint should be saved.\n    Must be a list of ints or `None`.\n    \"\"\"\n\n    no_save_optim: bool = False\n    \"\"\"\n    Do not save current optimizer.\n    \"\"\"\n\n    no_save_rng: bool = False\n    \"\"\"\n    Do not save current rng state.\n    \"\"\"\n\n    no_load_optim: bool = False\n    \"\"\"\n    Do not load optimizer when loading checkpoint.\n    \"\"\"\n\n    no_load_rng: bool = False\n    \"\"\"\n    Do not load rng state when loading checkpoint.\n    \"\"\"\n\n    finetune: bool = False\n    \"\"\"\n    Load model for finetuning. Do not load optimizer or rng state from checkpoint and set iteration to 0. Assumed when loading a release checkpoint.\n    \"\"\"\n\n    batch_size: int = None\n    \"\"\"\n    training microbatch size per gpu\n    \"\"\"\n\n    train_iters: int = None\n    \"\"\"\n    Number of iterations to run for training.\n    \"\"\"\n\n    train_epochs: int = None\n    \"\"\"\n    Number of epochs to run for training. Do not specify both train_epochs and train_iters.\n    Not currently compatible with data reweighing, pairwise datasets, and packing other than 'packed'\n    \"\"\"\n\n    eval_iters: int = 100\n    \"\"\"\n    Number of iterations to run for evaluation validation/test for.\n    \"\"\"\n\n    keep_last_n_checkpoints: int = None\n    \"\"\"\n    Number of last checkpoints to keep\n    \"\"\"\n\n    eval_interval: int = 1000\n    \"\"\"\n    Interval between running evaluation on validation set.\n    \"\"\"\n\n    split: str = \"969, 30, 1\"\n    \"\"\"\n    Comma_separated list of proportions for training, validation, and test split. For example the split 90,5,5 will use 90% of data for training, 5% for validation and 5% for test.\n    \"\"\"\n\n    vocab_file: str = None\n    \"\"\"\n    Path to the vocab file.\n    \"\"\"\n\n    merge_file: str = None\n    \"\"\"\n    Path to the BPE merge file.\n    \"\"\"\n\n    num_workers: int = 2\n    \"\"\"\n    Dataloader number of workers.\n    \"\"\"\n\n    exit_interval: int = None\n    \"\"\"\n    Exit the program after the iteration is divisible by this value.\n    \"\"\"\n\n    attention_dropout: float = 0.0\n    \"\"\"\n    Post attention dropout probability.\n    \"\"\"\n\n    hidden_dropout: float = 0.0\n    \"\"\"\n    Dropout probability for hidden state transformer.\n    \"\"\"\n\n    weight_decay: float = 0.1\n    \"\"\"\n    Weight decay coefficient for L2 regularization.\n    \"\"\"\n\n    checkpoint_activations: bool = False\n    \"\"\"\n    Checkpoint activation to allow for training with larger models, sequences, and batch sizes.\n    \"\"\"\n\n    checkpoint_num_layers: int = 1\n    \"\"\"\n    Chunk size (number of layers) for checkpointing.\n    \"\"\"\n\n    deepspeed_activation_checkpointing: bool = True\n    \"\"\"\n    DEPRECATED - TODO: remove\n    Uses activation checkpointing from deepspeed\n    \"\"\"\n\n    contiguous_checkpointing: bool = False\n    \"\"\"\n    Contiguous memory checkpointing for activations.\n    \"\"\"\n\n    checkpoint_in_cpu: bool = False\n    \"\"\"\n    Move the activation checkpoints to CPU.\n    \"\"\"\n\n    synchronize_each_layer: bool = False\n    \"\"\"\n    does a synchronize at the beginning and end of each checkpointed layer.\n    \"\"\"\n\n    profile_backward: bool = False\n    \"\"\"\n    Enables backward pass profiling for checkpointed layers.\n    \"\"\"\n\n    partition_activations: bool = False\n    \"\"\"\n    Partition Activations across GPUs before checkpointing.\n    \"\"\"\n\n    clip_grad: float = 1.0\n    \"\"\"\n    Gradient clipping based on global L2 norm.\n    \"\"\"\n\n    hysteresis: int = 2\n    \"\"\"\n    hysteresis for dynamic loss scaling\n    \"\"\"\n\n    dynamic_loss_scale: bool = None\n    \"\"\"\n    flag indicating whether dynamic loss scale is used\n    \"\"\"\n\n    loss_scale: float = None\n    \"\"\"\n    Static loss scaling, positive power of 2\n    values can improve fp16 convergence. If None, dynamic loss scaling is used.\n    \"\"\"\n\n    loss_scale_window: float = 1000.0\n    \"\"\"\n    Window over which to raise/lower dynamic scale.\n    \"\"\"\n\n    min_scale: float = 1.0\n    \"\"\"\n    Minimum loss scale for dynamic loss scale.\n    \"\"\"\n\n    char_level_ppl: bool = False\n    \"\"\"\n    Whether to calculate character level perplexity as well as token level perplexity. (may incur a time cost)\n    \"\"\"\n\n    use_mup: bool = False\n    \"\"\"\n    Whether to use Microsoft's Mup https://github.com/microsoft/mup\n    \"\"\"\n\n    coord_check: bool = False\n    \"\"\"\n    Whether to generate a \"coord check\" plot to verify mup's implementation in neox\n    \"\"\"\n\n    save_base_shapes: bool = False\n    \"\"\"\n    Whether to save base shapes for mup. This will save the shapes to the path specified in base-shapes-file.\n    \"\"\"\n\n    base_shapes_file: str = None\n    \"\"\"\n    Path to the base shapes to save to/load from\n    \"\"\"\n\n    mup_init_scale: float = 1.0\n    \"\"\"\n    Initialization scale: All the parameters are multiplied by this value\n    \"\"\"\n\n    mup_attn_temp: float = 1.0\n    \"\"\"\n    Attention temperature: Reciprocal of the multiplier applied to the input to attention softmax\n    \"\"\"\n\n    mup_output_temp: float = 1.0\n    \"\"\"\n    Output temperature: Reciprocal of the multiplier applied to the input to softmax that\n    produces the distribution over output tokens.\n    \"\"\"\n\n    mup_embedding_mult: float = 1.0\n    \"\"\"\n    Scalar by which we multiply the output of the embedding layer\n    \"\"\"\n\n    mup_rp_embedding_mult: float = 1.0\n    \"\"\"\n    Scalar by which we multiply vectors representing relative position\n    \"\"\"\n\n    mup_width_scale: int = 2\n    \"\"\"\n    What to scale width by when creating the delta model for mup\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsTextgen(NeoXArgsTemplate):\n    \"\"\"\n    Text Generation arguments\n    \"\"\"\n\n    text_gen_type: str = None\n    \"\"\"\n    How to generate text/sample the model.\n    Options: `unconditional`, `input-file`, `interactive`, `precompute`\n    \"\"\"\n\n    precompute_model_name: str = None\n    \"\"\"\n    Model name to use for saving precomputed logprobs\n    \"\"\"\n\n    temperature: float = 0.0\n    \"\"\"\n    exponential scaling output distribution (\"higher == more risk\")\n    \"\"\"\n\n    top_p: float = 0.0\n    \"\"\"\n    Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n    \"\"\"\n\n    top_k: int = 0\n    \"\"\"\n    integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    \"\"\"\n\n    return_logits: bool = False\n    \"\"\"\n    Boolean for whether to return the logits for generated tokens\n    \"\"\"\n\n    maximum_tokens: int = 64\n    \"\"\"\n    maximum number of tokens to be generated\n    \"\"\"\n\n    prompt_end: str = \"\\n\"\n    \"\"\"\n    a single prompt's end. Defaults to newline\n    \"\"\"\n\n    sample_input_file: str = None\n    \"\"\"\n    Get input from file instead of interactive mode, each line is an input.\n    \"\"\"\n\n    sample_output_file: str = \"samples.txt\"\n    \"\"\"\n    Output file\n    \"\"\"\n\n    num_samples: int = 1\n    \"\"\"\n    Number of samples to generate unconditionally, defaults to 1 and interactive conditional sampling\n    \"\"\"\n\n    recompute: bool = False\n    \"\"\"\n    During generation recompute all attention instead of using previously computed keys/values.\n    Should be set to true for sparse attention models\n    \"\"\"\n\n    eval_results_prefix: str = \"\"\n    \"\"\"\n    prefix to which to save evaluation results - final fp will be {eval_results_prefix}_eval_results_yy-mm-dd-HH-MM.json\n    \"\"\"\n\n    eval_tasks: list = None\n    \"\"\"\n    Tasks to evaluate on using lm_eval_harness\n\n    NOTE: Requires internet connection\n    \"\"\"\n\n\n@dataclass\nclass NeoXArgsMoE(NeoXArgsTemplate):\n    \"\"\"\n    Mixture of Expert (MoE) Arguments\n    \"\"\"\n\n    moe_num_experts: int = 1\n    \"\"\"\n    The number of experts in MoE layers. MoE layers not used if set to 1\n    \"\"\"\n\n    moe_expert_interval: int = 1\n    \"\"\"\n    Have one MoE layer every expert_interval layers\n    \"\"\"\n\n    moe_top_k: int = 1\n    \"\"\"\n    The number of experts each token is routed to in MoE layers.\n    \"\"\"\n\n    moe_router_type: Literal[\"sinkhorn\", \"topk\"] = \"sinkhorn\"\n    \"\"\"\n    What token routing algorithm to use. Currently only sinkhorn is supported for training.\n    TopK is only used for inference/eval.\n    \"\"\"\n\n    moe_lbl_in_fp32: bool = False\n    \"\"\"\n    Whether to compute the load balancing loss in fp32.\n    \"\"\"\n\n    moe_jitter_eps: float = None\n    \"\"\"\n    Coefficient for MoE routing jitter. Jitter is\n    not used if set to None\n    \"\"\"\n",
        "megatron/neox_arguments/template.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\nimport logging\n\n\n@dataclass\nclass NeoXArgsTemplate:\n    def defaults(self):\n        \"\"\"\n        generator for getting default values.\n        \"\"\"\n        for key, field_def in self.__dataclass_fields__.items():\n            yield key, field_def.default\n\n    def update_value(self, key: str, value):\n        \"\"\"\n        updates a property value if the key already exists\n\n        Problem: a previously non-existing property can be added to the class instance without error.\n        \"\"\"\n        if hasattr(self, key):\n            setattr(self, key, value)\n        else:\n            error_message = (\n                self.__class__.__name__\n                + \".update_value() to be updated property \"\n                + str(key)\n                + \" does not exist\"\n            )\n            logging.error(error_message)\n            raise ValueError(error_message)\n\n    def update_values(self, d):\n        \"\"\"\n        Updates multiple values in self if the keys already exists\n        \"\"\"\n        for k, v in d.items():\n            self.update_value(k, v)\n",
        "megatron/optimizers.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass SM3(Optimizer):\n    \"\"\"Implements SM3 algorithm.\n    It has been proposed in `Memory-Efficient Adaptive Optimization`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): coefficient that scale delta before it is applied\n            to the parameters (default: 0.1)\n        momentum (float, optional): coefficient used to scale prior updates\n            before adding. This drastically increases memory usage if\n            `momentum > 0.0`. This is ignored if the parameter's gradient\n            is sparse. (default: 0.0)\n        beta (float, optional): coefficient used for exponential moving\n            averages (default: 0.0)\n        eps (float, optional): Term added to square-root in denominator to\n            improve numerical stability (default: 1e-30)\n    .. _Memory-Efficient Adaptive Optimization:\n        https://arxiv.org/abs/1901.11150\n    \"\"\"\n\n    def __init__(self, params, lr=0.1, momentum=0.0, beta=0.0, eps=1e-30):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {0}\".format(lr))\n        if not 0.0 <= momentum < 1.0:\n            raise ValueError(\"Invalid momentum: {0}\".format(momentum))\n        if not 0.0 <= beta < 1.0:\n            raise ValueError(\"Invalid beta: {0}\".format(beta))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid eps: {0}\".format(eps))\n\n        defaults = {\"lr\": lr, \"momentum\": momentum, \"beta\": beta, \"eps\": eps}\n        super(SM3, self).__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            momentum = group[\"momentum\"]\n            beta = group[\"beta\"]\n            eps = group[\"eps\"]\n            for p in group[\"params\"]:\n                if p is None:\n                    continue\n                grad = p.grad\n\n                state = self.state[p]\n                shape = grad.shape\n                rank = len(shape)\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"momentum_buffer\"] = 0.0\n                    _add_initial_accumulators(state, grad)\n\n                if grad.is_sparse:\n                    # the update is non-linear so indices must be unique\n                    grad.coalesce()\n                    grad_indices = grad._indices()\n                    grad_values = grad._values()\n\n                    # Transform update_values into sparse tensor\n                    def make_sparse(values):\n                        constructor = grad.new\n                        if grad_indices.dim() == 0 or values.dim() == 0:\n                            return constructor().resize_as_(grad)\n                        return constructor(grad_indices, values, grad.size())\n\n                    acc = state[_key(0)]\n                    update_values = _compute_sparse_update(\n                        beta, acc, grad_values, grad_indices\n                    )\n\n                    self._update_sparse_accumulator(\n                        beta, acc, make_sparse(update_values)\n                    )\n\n                    # Add small amount for numerical stability\n                    update_values.add_(eps).rsqrt_().mul_(grad_values)\n\n                    update = make_sparse(update_values)\n                else:\n                    # Get previous accumulators mu_{t-1}\n                    if rank > 1:\n                        acc_list = [state[_key(i)] for i in range(rank)]\n                    else:\n                        acc_list = [state[_key(0)]]\n\n                    # Get update from accumulators and gradients\n                    update = _compute_update(beta, acc_list, grad)\n\n                    # Update accumulators.\n                    self._update_accumulator(beta, acc_list, update)\n\n                    # Add small amount for numerical stability\n                    update.add_(eps).rsqrt_().mul_(grad)\n\n                    if momentum > 0.0:\n                        m = state[\"momentum_buffer\"]\n                        update.mul_(1.0 - momentum).add_(m, alpha=momentum)\n                        state[\"momentum_buffer\"] = update.detach()\n\n                p.sub_(update, alpha=group[\"lr\"])\n                state[\"step\"] += 1\n        return loss\n\n    @staticmethod\n    def _update_accumulator(beta, acc_list, update):\n        for i, acc in enumerate(acc_list):\n            nu_max = _max_reduce_except_dim(update, i)\n            if beta > 0.0:\n                torch.max(acc, nu_max, out=acc)\n            else:\n                # No need to compare - nu_max is bigger because of grad ** 2\n                acc.copy_(nu_max)\n\n    @staticmethod\n    def _update_sparse_accumulator(beta, acc, update):\n        nu_max = _max_reduce_except_dim(update.to_dense(), 0).squeeze()\n        if beta > 0.0:\n            torch.max(acc, nu_max, out=acc)\n        else:\n            # No need to compare - nu_max is bigger because of grad ** 2\n            acc.copy_(nu_max)\n\n\ndef _compute_sparse_update(beta, acc, grad_values, grad_indices):\n    # In the sparse case, a single accumulator is used.\n    update_values = torch.gather(acc, 0, grad_indices[0])\n    if beta > 0.0:\n        update_values.mul_(beta)\n    update_values.addcmul_(grad_values, grad_values, value=1.0 - beta)\n    return update_values\n\n\ndef _compute_update(beta, acc_list, grad):\n    rank = len(acc_list)\n    update = acc_list[0].clone()\n    for i in range(1, rank):\n        # We rely on broadcasting to get the proper end shape.\n        update = torch.min(update, acc_list[i])\n    if beta > 0.0:\n        update.mul_(beta)\n    update.addcmul_(grad, grad, value=1.0 - beta)\n\n    return update\n\n\ndef _key(i):\n    # Returns key used for accessing accumulators\n    return \"accumulator_\" + str(i)\n\n\ndef _add_initial_accumulators(state, grad):\n    # Creates initial accumulators. For a dense tensor of shape (n1, n2, n3),\n    # then our initial accumulators are of shape (n1, 1, 1), (1, n2, 1) and\n    # (1, 1, n3). For a sparse tensor of shape (n, *), we use a single\n    # accumulator of shape (n,).\n    shape = grad.shape\n    rank = len(shape)\n    defaults = {\"device\": grad.device, \"dtype\": grad.dtype}\n    acc = {}\n\n    if grad.is_sparse:\n        acc[_key(0)] = torch.zeros(shape[0], **defaults)\n    elif rank == 0:\n        # The scalar case is handled separately\n        acc[_key(0)] = torch.zeros(shape, **defaults)\n    else:\n        for i in range(rank):\n            acc_shape = [1] * i + [shape[i]] + [1] * (rank - 1 - i)\n            acc[_key(i)] = torch.zeros(acc_shape, **defaults)\n\n    state.update(acc)\n\n\ndef _max_reduce_except_dim(tensor, dim):\n    # Computes max along all dimensions except the given dim.\n    # If tensor is a scalar, it returns tensor.\n    rank = len(tensor.shape)\n    result = tensor\n    if rank > 0:\n        assert dim < rank\n        for d in range(rank):\n            if d != dim:\n                result = result.max(dim=d, keepdim=True).values\n    return result\n\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# modifications  - 4/4/2021  @lessw2020  (decay issue spotted by @nestordemeure )\n# weight decay has been implemented AdamW style instead of the original madgrad Adam style.\n# in initial image classification testing, this outperformed 0 weight decay or original style weight decay.\n\n# closure is checked if callable or not since some code passes loss directly, rather than in closure param\n\nimport math\nfrom typing import Collection, TYPE_CHECKING, Any, Callable, Optional, Tuple\n\nimport torch\nimport torch.optim\nimport collections\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\n\nclass madgrad_wd(torch.optim.Optimizer):\n    \"\"\"\n    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic\n    Optimization.\n\n    .. _MADGRAD: https://arxiv.org/abs/2101.11075\n\n    MADGRAD is a general purpose optimizer that can be used in place of SGD or\n    Adam may converge faster and generalize better. Currently GPU-only.\n    Typically, the same learning rate schedule that is used for SGD or Adam may\n    be used. The overall learning rate is not comparable to either method and\n    should be determined by a hyper-parameter sweep.\n\n    MADGRAD requires less weight decay than other methods, often as little as\n    zero. Momentum values used for SGD or Adam's beta1 should work here also.\n\n    On sparse problems both weight_decay and momentum should be set to 0.\n\n    Arguments:\n        params (iterable):\n            Iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float):\n            Learning rate (default: 1e-2).\n        momentum (float):\n            Momentum value in  the range [0,1) (default: 0.9).\n        weight_decay (float):\n            Weight decay, i.e. a L2 penalty (default: 0).\n        eps (float):\n            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).\n    \"\"\"\n\n    def __init__(\n        self,\n        params: _params_t,\n        lr: float = 1e-2,\n        momentum: float = 0.9,\n        weight_decay: float = 0,\n        eps: float = 1e-6,\n    ):\n        if momentum < 0 or momentum >= 1:\n            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if eps < 0:\n            raise ValueError(f\"Eps must be non-negative\")\n\n        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self) -> bool:\n        return False\n\n    @property\n    def supports_flat_params(self) -> bool:\n        return True\n\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None and isinstance(closure, collections.Callable):\n            loss = closure()\n\n        # step counter must be stored in state to ensure correct behavior under\n        # optimizer sharding\n        if \"k\" not in self.state:\n            self.state[\"k\"] = torch.tensor([0], dtype=torch.long)\n        k = self.state[\"k\"].item()\n\n        for group in self.param_groups:\n            eps = group[\"eps\"]\n            lr = group[\"lr\"] + eps\n            decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n\n            ck = 1 - momentum\n            lamb = lr * math.pow(k + 1, 0.5)\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                if \"grad_sum_sq\" not in state:\n                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n                    state[\"s\"] = torch.zeros_like(p.data).detach()\n                    if momentum != 0:\n                        state[\"x0\"] = torch.clone(p.data).detach()\n\n                if momentum != 0.0 and grad.is_sparse:\n                    raise RuntimeError(\n                        \"momentum != 0 is not compatible with sparse gradients\"\n                    )\n\n                grad_sum_sq = state[\"grad_sum_sq\"]\n                s = state[\"s\"]\n\n                # Apply weight decay - L2 / AdamW style\n                if decay:\n                    p.data.mul_(1 - lr * decay)\n\n                \"\"\" original impl:\n                if decay != 0:\n                    if grad.is_sparse:\n                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n\n                    grad.add_(p.data, alpha=decay)\n                \"\"\"\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()\n                    grad_val = grad._values()\n\n                    p_masked = p.sparse_mask(grad)\n                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n                    s_masked = s.sparse_mask(grad)\n\n                    # Compute x_0 from other known quantities\n                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n                    x0_masked_vals = p_masked._values().addcdiv(\n                        s_masked._values(), rms_masked_vals, value=1\n                    )\n\n                    # Dense + sparse op\n                    grad_sq = grad * grad\n                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n\n                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n\n                    s.add_(grad, alpha=lamb)\n                    s_masked._values().add_(grad_val, alpha=lamb)\n\n                    # update masked copy of p\n                    p_kp1_masked_vals = x0_masked_vals.addcdiv(\n                        s_masked._values(), rms_masked_vals, value=-1\n                    )\n                    # Copy updated masked p to dense p using an add operation\n                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n                    p.data.add_(p_masked, alpha=-1)\n                else:\n                    if momentum == 0:\n                        # Compute x_0 from other known quantities\n                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n                        x0 = p.data.addcdiv(s, rms, value=1)\n                    else:\n                        x0 = state[\"x0\"]\n\n                    # Accumulate second moments\n                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n\n                    # Update s\n                    s.data.add_(grad, alpha=lamb)\n\n                    # Step\n                    if momentum == 0:\n                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n                    else:\n                        z = x0.addcdiv(s, rms, value=-1)\n\n                        # p is a moving average of z\n                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n\n        self.state[\"k\"] += 1\n        return loss\n\n\nclass Lion(Optimizer):\n    \"\"\"\n    Implements the Lion Algorithm\n\n    .. / _Lion: https://arxiv.org/abs/2302.06675\n\n    Compared to AdamW and various adaptive optimizers that need to save both first and second moments,\n    Lion only needs the momentum, halving the additional memory footprint. This is beneficial when training large models\n    and / or with a large batch size.\n\n    Arguments:\n        params (iterable):\n            Iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float):\n            Learning rate (default: 1e-2).\n        beta (float):\n            coefficients used for computing running averages of gradient and its square (default: (0.9, 0.99))\n        weight_decay (float):\n            Weight decay, i.e. a L2 penalty (default: 0).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-4,\n        betas: Tuple[float, float] = (0.9, 0.99),\n        weight_decay: float = 0.0,\n    ):\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if not (0 <= betas[0] <= 1 and 0 <= betas[1] <= 1):\n            raise ValueError(f\"Betas {betas} must be in range [0, 1)\")\n\n        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    def update(self, p, grad, exp_avg, lr, wd, beta1, beta2):\n        \"\"\"https://arxiv.org/pdf/2302.06675.pdf#appendix.A\"\"\"\n\n        # update model parameters\n        p.mul_(1 - lr * wd)\n        sign = exp_avg.clone().mul_(beta1).add(grad, alpha=1 - beta1).sign_()\n        p.add_(sign, alpha=-lr)\n\n        # update EMA\n        exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n\n    @torch.no_grad()\n    def step(self, closure: Optional[Callable] = None):\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                state = self.state[p]\n\n                # init state - exponential moving average of gradient values\n                if len(state) == 0:\n                    state[\"exp_avg\"] = torch.zeros_like(p.data).detach()\n\n                self.update(\n                    p,\n                    p.grad,\n                    state[\"exp_avg\"],\n                    group[\"lr\"],\n                    group[\"weight_decay\"],\n                    group[\"betas\"][0],\n                    group[\"betas\"][1],\n                )\n\n        return loss\n",
        "megatron/text_generation_utils.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for generating text.\"\"\"\n\nimport copy\nimport json\nimport math\nimport os\nimport time\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom megatron import print_rank_0\nfrom megatron import mpu\nfrom megatron.utils import get_ltor_masks_and_position_ids, is_mp_rank_0\nfrom megatron.data.indexed_dataset import make_builder, make_dataset\nfrom megatron.mpu.mappings import gather_from_model_parallel_region\n\n\ndef get_batch(neox_args, context_tokens: torch.Tensor):\n    \"\"\"\n    Generate batch from context tokens. Attention mask and position ids are created. Returned tensors will be on CUDA.\n\n    neox_args: NeoXArgs.\n    context_tokens: torch tensor with dimensions [batch, context_size]\n\n    returns: tuple of torch tensors (tokens, attention_mask, position_ids) on CUDA\n    \"\"\"\n\n    # Move to GPU.\n    tokens = context_tokens.contiguous().cuda()\n    # Get the attention mask and position ids.\n    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n        data=tokens,\n        eod_token=neox_args.tokenizer.eod,\n        eod_mask_loss=neox_args.eod_mask_loss,\n    )\n    return tokens, attention_mask, position_ids\n\n\ndef pad_batch(\n    context_tokens: List[List[int]], pad_id: int, pad_len: int, truncate: bool = False\n):\n    \"\"\"\n    pads context lengths in context_tokens with pad_id to equal neox_args.seq_length,\n    and returns the padded batch and the new lengths.\n\n    context_tokens: list of lists of tokens\n    pad_id: int, integer to use as padding token\n    pad_len: int, context length to be padded; all batch items will be padded to the same length\n    truncate: bool, if True, truncate context tokens to pad_len if they are longer than pad_len\n\n    returns: tuple of padded context tokens and a list of unpadded token count\n    \"\"\"\n\n    context_lengths = []\n    for i, tokens in enumerate(context_tokens):\n        context_length = len(tokens)\n        if context_length < pad_len:\n            tokens.extend([pad_id] * (pad_len - context_length))\n        elif context_length > pad_len:\n            if not truncate:\n                raise ValueError(\"context_length is bigger than to be padded length\")\n            context_tokens[i] = tokens[:pad_len]\n            context_length = pad_len\n        context_lengths.append(context_length)\n    return context_tokens, context_lengths\n\n\ndef filter_logits(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):\n    \"\"\"\n    Filters the logits using top_k / top_p, filling any filtered vocab items with filter_value (defaults to -inf).\n\n    This function has been mostly taken from huggingface conversational ai code at\n    https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n\n    When both top_k and top_p are specified, tokens are first filtered according to top_k, renormalized, and then filtered according to top_p.\n\n    logits: torch.Tensor -> logits of megatron model.\n    top_k: integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p: float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n\n    returns: (filtered) logits\"\"\"\n\n    if top_k > 0:\n        # Remove all tokens with a probability less than the\n        # last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        # convert to 1D\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token\n        # above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n        for i in range(sorted_indices.size(0)):\n            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n            logits[i][indices_to_remove] = filter_value\n\n    return logits\n\n\ndef switch(val1, val2, boolean):\n    \"\"\"\n    replaces items in val1 with items in val2 where boolean = True\n    \"\"\"\n    boolean = boolean.type_as(val1)\n    return (1 - boolean) * val1 + boolean * val2\n\n\ndef forward_model(model, model_inputs, is_pipe_parallel=False) -> torch.Tensor:\n    \"\"\"\n    Runs model.forward(model_inputs)\n\n    We need to create a wrapper for this function because deepspeed pipe parallel modules operate differently to normal models.\n\n    model: a Megatron model.\n    model_inputs: tuple containing model args\n\n    returns: torch.Tensor containing the logits of the model\n    \"\"\"\n    # because someone at deepspeed decided pipeline modules couldn't use kwargs,\n    # we need to forward a pipe model differently to a normal model\n    if not is_pipe_parallel:\n        return model.module(model_inputs)\n    else:\n        # we need to format inputs this way because:\n        # a) deepspeed pipeline only accepts iterables\n        # b) deepspeed pipeline *requires* that you pass in labels for the loss, it's not easy to get around this\n        # so we wrap the inputs in an iterable, and pad them (because internally, we get labels as inputs[:, 1:] and inputs as inputs[:, :-1])\n        model_inputs = iter([{\"text\": F.pad(model_inputs[0], pad=(0, 1))}])\n\n        # set num microbatches to 1 at inference time\n        micro_batches_before = model.micro_batches\n        model.micro_batches = 1\n\n        # deepspeed sends metadata across pipeline stages only once in the first step, then assumes it will stay\n        # constant. In inference, the metadata of the tensors being sent across pipe stages may change, so we need to set\n        # these two flags in order for deepspeed to send the metadata every step, otherwise torch.distributed hangs\n        # silently. Fun stuff.\n        model.first_output_send = True\n        model.pipe_recv_buf = None\n\n        loss, logits = model.eval_batch(model_inputs, return_logits=True)\n        model.micro_batches = micro_batches_before\n        return logits\n\n\ndef broadcast_terminate_signal(terminate_runs: int):\n    \"\"\"Send signal to all workers to terminate if we've finished the process\"\"\"\n    terminate_runs_tensor = torch.cuda.LongTensor([terminate_runs])\n    torch.distributed.broadcast(\n        terminate_runs_tensor,\n        mpu.get_model_parallel_src_rank(),\n        group=mpu.get_model_parallel_group(),\n    )\n    return terminate_runs_tensor[0].item()\n\n\ndef stop_tokens_in_completion(stop_tokens, context_tokens, batch_index, current_index):\n    if stop_tokens is None:\n        return False\n    res = []\n    for token_group in stop_tokens:\n        context = context_tokens[batch_index, : current_index + 1]\n        context = context[-len(token_group) :]\n        if context.shape[0] == token_group.shape[0]:\n            res.append(all(token_group == context))\n        else:\n            res.append(False)\n    return any(res)\n\n\ndef stream_tokens(\n    neox_args,\n    model,\n    context_tokens: List[List[int]],\n    eos_token_id: int = None,\n    maximum_tokens: int = None,\n    recompute: bool = False,\n    temperature: float = 0.0,\n    top_k: int = 0,\n    top_p: float = 0.0,\n    stop_tokens=None,\n):\n    \"\"\"\n    iterator producing text completions\n\n    neox_args: NeoXArgs.\n    model: a Megatron model.\n    context_tokens: the prompt to complete; unpadded list of lists of tokens ids\n    context_lengths: lengths of context tokens of dimension [batch]; the context length records for each bach item how many non-padded tokens are provided\n    eos_token_id: end of text token at which completion is terminated, even if max_tokes count has not been reached\n    attention_mask: attention mask for megatron model.\n    position_ids: position ids for positional encoding.\n    maximum_tokens: maximum number of tokens to be generated; careful! if a batch input is provided maximum_tokens specifies the maximum number of forwards.\n                    longer batch items get less generated tokens.\n    recompute: flag indicating whether a cache is used for already forwarded tokens (true) or whether all tokens are recomputed at every iteration (false)\n    temperature (default 0.0): exponential scaling output distribution (\"higher == more risk\")\n    top_k (default 0): integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p (default 0.0): float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n    note: greedy decoding is used if temperature is 0.0, top_k is 0 and top_p is 0.0\n    yields: (\n                tokens (completions from model),\n                token_generation_start_index (token index per batch item for the first generated token),\n                token_generation_end_index (token index per batch item for the last generated token),\n                logits (logits which are so far computed, zeros otherwise),\n                is_done (flag for each bach item indicating whether an eod token was generated)\n            )\n\n            * each iteration adds a generated token to the context_tokens\n            * output contains both context_tokens from input and generated tokens\n            * if batch items have different lengths, the iterator will start at the first completion and return the unchanged input context token otherwise\n    \"\"\"\n\n    model.eval()\n\n    # pad batch in order to allow conversion to tensor\n    context_tokens, context_lengths = pad_batch(\n        copy.deepcopy(context_tokens),\n        pad_id=neox_args.tokenizer.eod,\n        pad_len=neox_args.seq_length,\n    )\n\n    # convert to tensor and broadcast\n    context_tokens = torch.cuda.LongTensor(context_tokens)\n    if stop_tokens:\n        if len(stop_tokens) > 0 and type(stop_tokens[0]) is not list:\n            stop_tokens = [stop_tokens]\n        for i in range(0, len(stop_tokens)):\n            stop_tokens[i] = torch.cuda.LongTensor(stop_tokens[i])\n\n    # Make sure context tokens + start tokens are the same across all ranks\n    token_generation_start_index = torch.cuda.LongTensor(context_lengths)\n    torch.distributed.broadcast(\n        context_tokens,\n        mpu.get_model_parallel_src_rank(),\n        group=mpu.get_model_parallel_group(),\n    )\n    torch.distributed.broadcast(\n        token_generation_start_index,\n        mpu.get_model_parallel_src_rank(),\n        group=mpu.get_model_parallel_group(),\n    )\n\n    # get attention mask / position ids\n    context_tokens, attention_mask, position_ids = get_batch(neox_args, context_tokens)\n\n    # set variables\n    eos_token_id = eos_token_id or neox_args.tokenizer.eod\n    maximum_tokens = maximum_tokens or (\n        neox_args.seq_length - token_generation_start_index.max().item() - 1\n    )\n    batch_size = context_tokens.size(0)\n\n    # get the context_index at which generation is to start\n    # we start generation at the position where the smallest context ends\n    token_index_to_generate = token_generation_start_index.min().item()\n    first_token_index_to_generate = token_index_to_generate\n    last_token_index_to_generate = min(\n        neox_args.seq_length\n        - 1,  # never generate more than the model's sequence length\n        token_index_to_generate + maximum_tokens - 1,\n    )\n\n    with torch.no_grad():\n        # initialize generation variables\n        state_is_done = torch.zeros([batch_size]).byte().cuda()\n        token_generation_end_index = torch.ones([batch_size]).long().cuda() * (-1)\n        generation_logits = (\n            torch.empty(maximum_tokens, neox_args.padded_vocab_size).float().cuda()\n        )\n\n        while token_index_to_generate <= last_token_index_to_generate:\n            if recompute:  # recompute all tokens\n                model_inputs = (\n                    context_tokens,\n                    position_ids,\n                    attention_mask,\n                )\n                logits = forward_model(model, model_inputs, neox_args.is_pipe_parallel)\n                if logits is not None:  # if pipe parallel, not all ranks return logits\n                    generated_token_logits = logits[\n                        :, token_index_to_generate - 1, :\n                    ]  # [bs, seq, vocab_size] -> [bs, vocab_size]\n            else:  # use kv cache\n                if token_index_to_generate == first_token_index_to_generate:\n                    tokens_to_use = context_tokens[:, :token_index_to_generate]\n                    positions_to_use = position_ids[:, :token_index_to_generate]\n                else:\n                    tokens_to_use = context_tokens[:, token_index_to_generate - 1].view(\n                        batch_size, -1\n                    )\n                    positions_to_use = position_ids[\n                        :, token_index_to_generate - 1\n                    ].view(batch_size, -1)\n\n                model_inputs = (\n                    tokens_to_use,  # input_ids\n                    positions_to_use,  # position_ids\n                    attention_mask,  # attention_mask\n                )\n\n                logits = forward_model(model, model_inputs, neox_args.is_pipe_parallel)\n                if logits is not None:  # if pipe parallel, not all ranks return logits\n                    generated_token_logits = (\n                        logits[:, -1].view(batch_size, -1).contiguous()\n                    )  # [bs, seq, vocab_size] -> [bs, vocab_size]\n\n            if logits is not None:\n                # sample token id of the to be generated token\n                if temperature == 0.0 and top_k == 0 and top_p == 0.0:\n                    generated_tokens = torch.argmax(\n                        generated_token_logits, dim=-1\n                    ).view(-1)\n                else:\n                    generated_token_logits = generated_token_logits.float()\n                    if temperature > 0.0:\n                        generated_token_logits /= temperature\n                    generated_token_logits = filter_logits(\n                        generated_token_logits, top_k=top_k, top_p=top_p\n                    )\n                    next_token_log_probs = F.softmax(generated_token_logits, dim=-1)\n                    generated_tokens = torch.multinomial(\n                        next_token_log_probs, num_samples=1\n                    ).view(-1)\n\n                if neox_args.return_logits:\n                    generation_logits[\n                        token_index_to_generate - 1\n                    ] = generated_token_logits[0]\n\n            if neox_args.is_pipe_parallel:\n                # broadcast generated tokens to pipe parallel group\n                src_rank = model.grid.stage_to_global(model.num_stages - 1)\n                generated_tokens = (\n                    generated_tokens\n                    if logits is not None\n                    else torch.zeros(batch_size, dtype=torch.long).cuda()\n                )\n                torch.distributed.broadcast(\n                    tensor=generated_tokens,\n                    src=src_rank,\n                    group=mpu.get_pipe_parallel_group(),\n                )\n\n            # determine if state has started for each batch item\n            state_started = (\n                token_generation_start_index <= token_index_to_generate\n            )  # check which batch items have been started\n\n            # switch out padding tokens for generated tokens\n            context_tokens[:, token_index_to_generate] = switch(\n                context_tokens[:, token_index_to_generate].view(-1),\n                generated_tokens,\n                state_started,\n            )\n\n            # determine if state has finished for each batch item\n            state_done = (\n                generated_tokens == eos_token_id\n            ).byte() & state_started.byte()  # check which batch items produce an eos_token in the current iteration\n            state_just_finished = (state_done & ~state_is_done).bool()\n            state_is_done = state_is_done | state_done\n            stop_tokens_produced = torch.zeros_like(state_is_done)\n            for batch_idx, ctx in enumerate(context_tokens):\n                stop_tokens_produced[batch_idx] = stop_tokens_in_completion(\n                    stop_tokens, context_tokens, batch_idx, token_index_to_generate\n                )\n            state_is_done = state_is_done | stop_tokens_produced\n\n            token_generation_end_index[\n                (state_started.byte() & ~state_is_done).bool()\n            ] = token_index_to_generate\n\n            token_index_to_generate += 1\n\n            yield context_tokens, token_generation_start_index, token_generation_end_index, generation_logits, state_is_done.bool()\n            if torch.all(state_is_done):\n                break\n\n\ndef generate_samples_from_prompt(\n    neox_args,\n    model,\n    text: Union[List[str], str],\n    eos_token_id: int = None,\n    maximum_tokens: int = 64,\n    recompute: bool = False,\n    temperature: float = 0.0,\n    top_k: int = 0,\n    top_p: float = 0.0,\n    stop_tokens=None,\n):\n    \"\"\"\n    Generates samples from raw text and returns them in a dictionary.\n\n    neox_args: NeoXArgs.\n    model: a Megatron model\n    text: either a single prompt (str) or a list of prompts (List[str]).\n\n    eos_token_id: end of text token at which completion is terminated, even if max_tokes count has not been reached\n    maximum_tokens: maximum number of tokens to be generated\n\n    recompute: flag indicating whether a cache is used for already forwarded tokens (true) or whether all tokens are recomputed at every iteration (false)\n\n    temperature (default 0.0): exponential scaling output distribution (\"higher == more risk\")\n    top_k (default 0): integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p (default 0.0): float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n    note: greedy decoding is used if temperature is 0.0, top_k is 0 and top_p is 0.0\n\n    returns: List[dict] -> a list of dicts containing the following fields:\n        - 'context' (the input)\n        - 'text' (the completion)\n        - 'length' (the length of the completion in number of tokens)\n        - 'finished':\n        - 'message': a messaged associated with the generation procedure, can be a warning or error\n        - 'duration_seconds': duration of the generation in seconds\n\n    \"\"\"\n    eos_token_id = eos_token_id or neox_args.tokenizer.eod\n\n    # type check\n    assert any(\n        [isinstance(text, str), isinstance(text, list)]\n    ), \"Text should be in string or list form\"\n    if isinstance(text, str):\n        text = [text]\n\n    input_count = len(text)\n    input_pos = 0\n\n    # generate completions\n    generated_texts = []\n    while True:\n\n        start_time = time.time()\n        # Tokenize text, and check whether we should terminate process\n        terminate_runs = 0\n        if input_pos == input_count:\n            terminate_runs = 1\n        else:\n            raw_text = text[input_pos]\n            input_pos += 1\n\n            if raw_text == \"\":\n                context_tokens = [eos_token_id]\n            else:\n                context_tokens = neox_args.tokenizer.tokenize(raw_text)\n            context_length = len(context_tokens)\n\n            if context_length >= (neox_args.seq_length // 2):\n                print_rank_0(\n                    \"\\nWarning! Context length\",\n                    context_length,\n                    \"\\nPlease give smaller context (e.g. half of the \"\n                    \"max sequence length)!\",\n                )\n        if not is_mp_rank_0():\n            context_tokens = neox_args.tokenizer.tokenize(\"EMPTY TEXT\")\n            context_length = len(context_tokens)\n            terminate_runs = 0\n\n        terminate_runs = broadcast_terminate_signal(terminate_runs)\n        if terminate_runs == 1:\n            return generated_texts\n\n        for (\n            batch_context_tokens,\n            batch_token_generation_start_index,\n            batch_token_generation_end_index,\n            batch_generated_token_logits,\n            is_done,\n        ) in stream_tokens(\n            neox_args=neox_args,\n            model=model,\n            context_tokens=[context_tokens],\n            eos_token_id=eos_token_id,\n            maximum_tokens=maximum_tokens,\n            recompute=recompute,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            stop_tokens=stop_tokens,\n        ):\n            pass  # finish generation and use all results below\n\n        batch_context_tokens = batch_context_tokens.cpu().numpy().tolist()\n        batch_token_generation_start_index = (\n            batch_token_generation_start_index.cpu().numpy().tolist()\n        )\n        batch_token_generation_end_index = (\n            batch_token_generation_end_index.cpu().numpy().tolist()\n        )\n        batch_is_done = is_done.cpu().numpy().tolist()\n\n        for tokens, start_index, end_index, is_done in zip(\n            batch_context_tokens,\n            batch_token_generation_start_index,\n            batch_token_generation_end_index,\n            batch_is_done,\n        ):\n\n            if end_index >= start_index:\n                generated_tokens = tokens[start_index : end_index + 1]\n                try:\n                    generated_text = neox_args.tokenizer.detokenize(generated_tokens)\n                    message = None\n                except KeyError:\n                    generated_text = None\n                    message = \"WARNING: generated token which doesn't exist.\"\n            else:\n                generated_text = None\n                generated_tokens = []\n                # this will happen if the first generated token is a stop token or eos token\n                message = \"WARNING: text generation did not start; try different batching or adjust parameters\"\n            if is_mp_rank_0():\n                data = {\n                    \"context\": raw_text,\n                    \"text\": generated_text,\n                    \"length\": len(generated_tokens),\n                    \"finished\": is_done,\n                    \"message\": message,\n                    \"duration_seconds\": float(time.time() - start_time),\n                }\n\n                if neox_args.return_logits:\n                    data[\"logits\"] = batch_generated_token_logits.cpu().numpy().tolist()\n\n                generated_texts.append(data)\n\n    return generated_texts\n\n\ndef generate_samples_input_from_file(\n    neox_args,\n    model,\n    input_file,\n    output_file=None,\n    eos_token_id: int = None,\n    maximum_tokens: int = 64,\n    prompt_end: str = \"\\n\",\n    recompute: bool = False,\n    temperature: float = 0.0,\n    top_k: int = 0,\n    top_p: float = 0.0,\n):\n    \"\"\"\n    Generates samples from an input file and writes them to an output file.\n\n    Reads prompts from neox_args.sample_input_file and writes completions to neox_args.sample_output_file\n\n    neox_args: NeoXArgs.\n    model: a Megatron model\n\n    input_file: path to input file. Each line in the input file will be treated as separate prompt. The line break at the end of the line is not included in the prompt.\n    output_file: file where generation results are to be stored in jsonl format. defaults to input_file+'.output.jsonl' if not defined\n\n    eos_token_id: end of text token at which completion is terminated, even if max_tokes count has not been reached\n    maximum_tokens: maximum number of tokens to be generated\n    prompt_end: end of a single input prompt. Defaults to newline character '\\n'. Other prompt-end sequences may be useful when generating indent-aware completions (e.g. code)\n\n    recompute: flag indicating whether a cache is used for already forwarded tokens (true) or whether all tokens are recomputed at every iteration (false)\n\n    temperature (default 0.0): exponential scaling output distribution (\"higher == more risk\")\n    top_k (default 0): integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p (default 0.0): float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n\n    note: greedy decoding is used if temperature is 0.0, top_k is 0 and top_p is 0.0\n\n\n    returns: List[dict] -> a list of dicts containing the following fields:\n        - 'context' (the input)\n        - 'text' (the completion)\n        - 'length' (the length of the completion in number of tokens)\n        - 'finished':\n        - 'message': a messaged associated with the generation procedure, can be a warning or error\n        - 'duration_seconds': duration of the generation in seconds\n    \"\"\"\n    # Read the sample file\n    print_rank_0(\n        \"generate_samples_input_from_file() loading input from {}\".format(input_file)\n    )\n    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n        prompts = f.read()\n        prompts = prompts.split(prompt_end)\n    prompts = [p.strip() for p in prompts]\n    prompts = [p for p in prompts if len(p) > 0]\n    print_rank_0(\n        \"generate_samples_input_from_file() prompts loaded: {}\".format(len(prompts))\n    )\n\n    if is_mp_rank_0():\n        if output_file is None:\n            output_file = str(input_file) + \".output.jsonl\"\n            print_rank_0(\n                \"generate_samples_input_from_file() setting default output file to {}\".format(\n                    output_file\n                )\n            )\n\n    print_rank_0(\"generate_samples_input_from_file() generating...\")\n    generated_texts = generate_samples_from_prompt(\n        neox_args=neox_args,\n        model=model,\n        text=prompts,\n        eos_token_id=eos_token_id,\n        maximum_tokens=maximum_tokens,\n        recompute=recompute,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    if is_mp_rank_0():\n        with open(output_file, \"w\") as f_out:\n            for item in generated_texts:\n                f_out.write(json.dumps(item) + \"\\n\")\n    print_rank_0(\"generate_samples_input_from_file() done\")\n    return generated_texts\n\n\ndef generate_samples_unconditional(\n    neox_args,\n    model,\n    number_of_samples: int = 10,\n    output_file=None,\n    eos_token_id: int = None,\n    maximum_tokens: int = 64,\n    recompute: bool = False,\n    temperature: float = 0.0,\n    top_k: int = 0,\n    top_p: float = 0.0,\n):\n    \"\"\"\n    Generates samples unconditionially (no prompt) and yields them in a dictionary.\n\n    neox_args: NeoXArgs.\n    model: a Megatron model\n\n    number_of_samples (default 10): number of unconditional samples to be generated\n\n    output_file: file where generation results are to be stored in jsonl format. no file will be stored if omitted\n\n    eos_token_id: end of text token at which completion is terminated, even if max_tokes count has not been reached\n    maximum_tokens: maximum number of tokens to be generated\n    prompt_end: end of a single input prompt. Defaults to newline character '\\n'. Other prompt-end sequences may be useful when generating indent-aware completions (e.g. code). The interactive mode will reroll the user-input request until the stop-char is met\n\n    recompute: flag indicating whether a cache is used for already forwarded tokens (true) or whether all tokens are recomputed at every iteration (false)\n\n    temperature (default 0.0): exponential scaling output distribution (\"higher == more risk\")\n    top_k (default 0): integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p (default 0.0): float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n\n    note: greedy decoding is used if temperature is 0.0, top_k is 0 and top_p is 0.0\n\n    yields: dict containing the following fields:\n        - 'context' (the input)\n        - 'text' (the completion)\n        - 'length' (the length of the completion in number of tokens)\n        - 'finished':\n        - 'message': a messaged associated with the generation procedure, can be a warning or error\n        - 'duration_seconds': duration of the generation in seconds\n    \"\"\"\n\n    print_rank_0(\"generate_samples_unconditional() generating...\")\n    assert number_of_samples > 0, \"number_of_samples must be > 0\"\n    generated_texts = generate_samples_from_prompt(\n        neox_args=neox_args,\n        model=model,\n        text=[\"\" for _ in range(number_of_samples)],\n        eos_token_id=eos_token_id,\n        maximum_tokens=maximum_tokens,\n        recompute=recompute,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    if is_mp_rank_0():\n        if output_file is not None:\n            with open(output_file, \"w\") as f_out:\n                for item in generated_texts:\n                    f_out.write(json.dumps(item) + \"\\n\")\n    print_rank_0(\"generate_samples_unconditional() done\")\n    return generated_texts\n\n\ndef generate_samples_interactive(\n    neox_args,\n    model,\n    maximum_tokens: int = 64,\n    prompt_end: str = \"\\n\",\n    eos_token_id: int = None,\n    recompute: bool = False,\n    temperature: float = 0.0,\n    top_k: int = 0,\n    top_p: float = 0.0,\n):\n    \"\"\"\n    Generates samples unconditionially (no prompt) and yields them in a dictionary.\n\n    neox_args: NeoXArgs.\n    model: a Megatron model\n\n    maximum_tokens: maximum number of tokens to be generated\n    eos_token_id: end of text token at which completion is terminated, even if max_tokes count has not been reached\n\n    recompute: flag indicating whether a cache is used for already forwarded tokens (true) or whether all tokens are recomputed at every iteration (false)\n\n    temperature (default 0.0): exponential scaling output distribution (\"higher == more risk\")\n    top_k (default 0): integer -> integer between 0 and the models vocab size. Filters out any logits with a probability less than that of the top_kth token.\n    top_p (default 0.0): float -> Top-p (nucleus) sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p.\n\n    note: greedy decoding is used if temperature is 0.0, top_k is 0 and top_p is 0.0\n\n    yields: dict containing the following fields:\n        - 'context' (the input)\n        - 'text' (the completion)\n        - 'length' (the length of the completion in number of tokens)\n        - 'finished':\n        - 'message': a messaged associated with the generation procedure, can be a warning or error\n        - 'duration_seconds': duration of the generation in seconds\n    \"\"\"\n\n    while True:\n        model.module.clear_cache()  # clear kv cache between batches\n        torch.distributed.barrier(group=mpu.get_model_parallel_group())\n        terminate_runs = 0\n\n        if torch.distributed.is_initialized() and torch.distributed.get_rank() == 0:\n            os.system(\"clear\")\n            raw_text = \"\"\n            while True:\n                current_input = input(\"Context prompt >>> \")\n                if (\n                    prompt_end == \"\\n\"\n                ):  # we need to handle '\\n' case as 'input' strips it and leads to lines being squashed\n                    raw_text += current_input\n                    break\n                if prompt_end in current_input:\n                    raw_text += current_input.split(prompt_end)[0]\n                    break\n                raw_text += (\n                    current_input + \"\\n\"\n                )  # re-add newline since we stripped it on input\n            context_tokens = neox_args.tokenizer.tokenize(raw_text)\n            if len(context_tokens) == 0:\n                context_tokens = [neox_args.tokenizer.eod]\n            context_length = len(context_tokens)\n            if context_length >= (neox_args.seq_length - 1):\n                print_rank_0(\n                    \"\\nContext length\"\n                    + str(context_length)\n                    + \"\\nReached max sequence length!\"\n                )\n                terminate_runs = 1\n        else:\n            context_tokens = neox_args.tokenizer.tokenize(\"EMPTY TEXT\")\n            context_length = len(context_tokens)\n\n        terminate_runs = broadcast_terminate_signal(terminate_runs)\n        if terminate_runs == 1:\n            return\n        for (\n            batch_context_tokens,\n            batch_token_generation_start_index,\n            batch_token_generation_end_index,\n            batch_generated_token_logits,\n            is_done,\n        ) in stream_tokens(\n            neox_args=neox_args,\n            model=model,\n            context_tokens=[context_tokens],\n            eos_token_id=eos_token_id,\n            maximum_tokens=maximum_tokens,\n            recompute=recompute,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n        ):\n            if mpu.get_model_parallel_rank() == 0:\n                generated_tokens = (\n                    batch_context_tokens[0]\n                    .cpu()\n                    .numpy()\n                    .tolist()[\n                        batch_token_generation_start_index[0]\n                        .item() : batch_token_generation_end_index[0]\n                        .item()\n                        + 1\n                    ]\n                )\n                generated_text = neox_args.tokenizer.detokenize(generated_tokens)\n                print_rank_0(\"Generated Text: \" + generated_text)\n        if torch.distributed.is_initialized() and torch.distributed.get_rank() == 0:\n            _ = input(\"\\n<press enter to continue>\")\n\n\ndef get_logp(logits, labels, force_fp32=False):\n    if force_fp32:\n        logits = logits.float()\n    logp = logits.log_softmax(dim=-1)\n    return torch.gather(logp, dim=2, index=labels.unsqueeze(2)).squeeze(2)\n\n\ndef precompute_logits(neox_args, model):\n    \"\"\"\n    Precomputes logprobs from training/testing/validation datasets\n\n    Saves it to the same directory as the dataset with the model name appended to it\n\n    neox_args: NeoXArgs.\n    model: a Megatron model\n\n    \"\"\"\n    if neox_args.precompute_model_name is None:\n        mdl_name = str(hash(neox_args.load))\n    else:\n        mdl_name = neox_args.precompute_model_name\n    print_rank_0(\"Precomputing logprobs...\")\n    model.eval()\n    data_paths = list()\n    if neox_args.train_data_paths is not None:\n        for path in neox_args.train_data_paths:\n            data_paths.append(path)\n        for path in neox_args.test_data_paths:\n            data_paths.append(path)\n        for path in neox_args.valid_data_paths:\n            data_paths.append(path)\n    elif neox_args.pos_train_data_paths is not None:\n        # Pairwise data...\n        for path in neox_args.pos_train_data_paths:\n            data_paths.append(path)\n        for path in neox_args.neg_train_data_paths:\n            data_paths.append(path)\n        for path in neox_args.pos_valid_data_paths:\n            data_paths.append(path)\n        for path in neox_args.neg_valid_data_paths:\n            data_paths.append(path)\n        for path in neox_args.pos_test_data_paths:\n            data_paths.append(path)\n        for path in neox_args.neg_test_data_paths:\n            data_paths.append(path)\n    for path in data_paths:\n        print_rank_0(f\"Precomputing logits for {path}\")\n        # Add hash to path...\n        out_path = path + f\"_{mdl_name}\"\n        if os.path.exists(out_path + \".idx\"):\n            continue\n        dataset = make_dataset(path, neox_args.data_impl, not neox_args.mmap_warmup)\n        if is_mp_rank_0():\n            out_dataset = make_builder(out_path + \".bin\", neox_args.data_impl)\n            out_dataset._dtype = np.float32\n        i = 0\n\n        # TODO: Not sure why this requires a multiple of 8? Investigate later.\n        while i < int(math.ceil(len(dataset) / 8.0) * 8):\n            start = time.time()\n            model.module.clear_cache()  # clear kv cache between batches\n            if is_mp_rank_0():\n                offset = (\n                    mpu.get_data_parallel_rank()\n                    * neox_args.train_micro_batch_size_per_gpu\n                )\n                context_tokens = [\n                    [int(x) for x in dataset.get(j % len(dataset)).tolist()]\n                    for j in range(\n                        i + offset,\n                        i + (neox_args.train_micro_batch_size_per_gpu + offset),\n                    )\n                ]\n                # grab microbatch\n                # pad batch in order to allow conversion to tensor\n                context_tokens, context_lengths = pad_batch(\n                    copy.deepcopy(context_tokens),\n                    pad_id=0,\n                    pad_len=neox_args.seq_length + 1,\n                    truncate=True,\n                )\n                # print(context_tokens)\n                label_tokens = [tokens[1:] for tokens in context_tokens]\n                context_tokens = [tokens[:-1] for tokens in context_tokens]\n            else:\n                context_tokens = [\n                    [0 for _ in range(neox_args.seq_length)]\n                    for _ in range(neox_args.batch_size)\n                ]\n                label_tokens = [\n                    [0 for _ in range(neox_args.seq_length)]\n                    for _ in range(neox_args.batch_size)\n                ]\n                context_lengths = [0 for _ in range(neox_args.batch_size)]\n            i += (\n                neox_args.train_micro_batch_size_per_gpu\n                * mpu.get_data_parallel_world_size()\n            )\n            # print(context_tokens)\n            # convert to tensor and broadcast\n            context_tokens = torch.cuda.LongTensor(context_tokens)\n            label_tokens = torch.cuda.LongTensor(label_tokens)\n            # Make sure context tokens + start tokens are the same across all ranks\n            token_generation_start_index = torch.cuda.LongTensor(context_lengths)\n            torch.distributed.broadcast(\n                context_tokens,\n                mpu.get_model_parallel_src_rank(),\n                group=mpu.get_model_parallel_group(),\n            )\n            torch.distributed.broadcast(\n                token_generation_start_index,\n                mpu.get_model_parallel_src_rank(),\n                group=mpu.get_model_parallel_group(),\n            )\n            torch.distributed.broadcast(\n                label_tokens,\n                mpu.get_model_parallel_src_rank(),\n                group=mpu.get_model_parallel_group(),\n            )\n            # context_tokens = context_tokens[:, :chop_len].contiguous()\n            # label_tokens = label_tokens[:, :chop_len].contiguous()\n            with torch.no_grad():\n                # get attention mask / position ids\n                context_tokens, attention_mask, position_ids = get_batch(\n                    neox_args, context_tokens\n                )\n                model_inputs = (\n                    context_tokens,\n                    position_ids,\n                    attention_mask,\n                )\n                maybe_tuple = forward_model(\n                    model, model_inputs, neox_args.is_pipe_parallel\n                )\n                if isinstance(maybe_tuple, tuple):\n                    logits, _ = maybe_tuple\n                else:\n                    logits = maybe_tuple\n                if logits is not None:  # if pipe parallel, not all ranks return logits\n                    logits = gather_from_model_parallel_region(logits)\n                    logp = get_logp(logits, label_tokens, True).squeeze()\n                if neox_args.is_pipe_parallel:\n                    # broadcast generated tokens to pipe parallel group\n                    src_rank = model.grid.stage_to_global(model.num_stages - 1)\n                    logp = (\n                        logp\n                        if logits is not None\n                        else torch.zeros(\n                            neox_args.batch_size, dtype=torch.float32\n                        ).cuda()\n                    )\n                    torch.distributed.broadcast(\n                        tensor=logp,\n                        src=src_rank,\n                        group=mpu.get_pipe_parallel_group(),\n                    )\n                    logp = logp.squeeze()\n                logp_list = [\n                    torch.zeros_like(logp)\n                    for _ in range(mpu.get_data_parallel_world_size())\n                ]\n                torch.distributed.all_gather(\n                    logp_list, logp, group=mpu.get_data_parallel_group()\n                )\n                logp = torch.cat(logp_list, dim=0).cpu().numpy()\n                if (mpu.get_model_parallel_rank() == 0) and (\n                    mpu.get_data_parallel_rank() == 0\n                ):\n                    for j in range(logp.shape[0]):\n                        out_dataset.add_item(logp[j])\n                        out_dataset.end_document()\n            print_rank_0(f\"Processed {i} / {len(dataset)} in {time.time() - start}\")\n        if is_mp_rank_0():\n            out_dataset.finalize(\n                out_path + \".idx\",\n            )\n        torch.distributed.barrier()\n",
        "megatron/tokenizer/__init__.py": "# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom .tokenizer import build_tokenizer\n",
        "megatron/tokenizer/tokenizer.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Megatron tokenizers.\"\"\"\n\nfrom abc import ABC\nfrom abc import abstractmethod\n\nfrom tokenizers import Tokenizer\nfrom transformers import GPT2Tokenizer, GPT2TokenizerFast\nimport numpy as np\nimport sentencepiece as spm\nfrom typing import List, Union\n\n\ndef build_tokenizer(args):\n    \"\"\"Initialize tokenizer.\"\"\"\n    if args.rank == 0:\n        print(\"> building {} tokenizer ...\".format(args.tokenizer_type), flush=True)\n\n    assert (\n        args.tokenizer_type is not None\n    ), \"tokenizer_type must be specified in the .yml config\"\n\n    # Select and instantiate the tokenizer.\n    if args.tokenizer_type.lower() == \"GPT2BPETokenizer\".lower():\n        assert args.vocab_file is not None\n        assert args.merge_file is not None\n        tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)\n    elif args.tokenizer_type.lower() == \"SPMTokenizer\".lower():\n        assert args.vocab_file is not None\n        tokenizer = SentencePieceTokenizer(args.vocab_file)\n    elif args.tokenizer_type.lower() == \"HFTokenizer\".lower():\n        assert args.vocab_file is not None\n        tokenizer = HFTokenizer(args.vocab_file)\n    elif args.tokenizer_type.lower() == \"HFGPT2Tokenizer\".lower():\n        if args.vocab_file is None:\n            print(\n                \"WARNING: No vocab file found, loading Huggingface's pretrained GPT2Tokenizer\"\n            )\n        tokenizer = HFGPT2Tokenizer(args.vocab_file)\n    elif args.tokenizer_type.lower() == \"CharLevelTokenizer\".lower():\n        tokenizer = CharLevelTokenizer(vocab_size=512)\n    elif args.tokenizer_type.lower() == \"TiktokenTokenizer\".lower():\n        assert args.vocab_file is not None\n        tokenizer = TiktokenTokenizer(args.vocab_file)\n    else:\n        raise NotImplementedError(\n            \"{} tokenizer is not \" \"implemented.\".format(args.tokenizer_type)\n        )\n\n    # Add vocab size.\n    args.padded_vocab_size = _vocab_size_with_padding(tokenizer.vocab_size, args)\n\n    return tokenizer\n\n\ndef _vocab_size_with_padding(orig_vocab_size, args):\n    \"\"\"Pad vocab size so it is divisible by model parallel size and\n    still having GPU friendly size.\"\"\"\n\n    after = orig_vocab_size\n    multiple = args.make_vocab_size_divisible_by * args.model_parallel_size\n    while (after % multiple) != 0:\n        after += 1\n    if args.rank == 0:\n        print(\n            \" > padded vocab (size: {}) with {} dummy tokens \"\n            \"(new size: {})\".format(orig_vocab_size, after - orig_vocab_size, after),\n            flush=True,\n        )\n    return after\n\n\nclass AbstractTokenizer(ABC):\n    \"\"\"Abstract class for tokenizer.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        pass\n\n    @property\n    @abstractmethod\n    def vocab(self):\n        \"\"\"Dictionary from vocab text token to id token.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def inv_vocab(self):\n        \"\"\"Dictionary from vocab id token to text token.\"\"\"\n        pass\n\n    @abstractmethod\n    def tokenize(self, text):\n        pass\n\n    def detokenize(self, token_ids):\n        raise NotImplementedError(\n            \"detokenizer is not implemented for {} \" \"tokenizer\".format(self.name)\n        )\n\n    @property\n    def cls(self):\n        raise NotImplementedError(\n            \"CLS is not provided for {} \" \"tokenizer\".format(self.name)\n        )\n\n    @property\n    def sep(self):\n        raise NotImplementedError(\n            \"SEP is not provided for {} \" \"tokenizer\".format(self.name)\n        )\n\n    @property\n    def pad(self):\n        raise NotImplementedError(\n            \"PAD is not provided for {} \" \"tokenizer\".format(self.name)\n        )\n\n    @property\n    def eod(self):\n        raise NotImplementedError(\n            \"EOD is not provided for {} \" \"tokenizer\".format(self.name)\n        )\n\n    @property\n    def mask(self):\n        raise NotImplementedError(\n            \"MASK is not provided for {} \" \"tokenizer\".format(self.name)\n        )\n\n\nclass _GPT2BPETokenizer(AbstractTokenizer):\n    \"\"\"Original GPT2 BPE tokenizer.\"\"\"\n\n    def __init__(self, vocab_file, merge_file):\n        name = \"GPT2 BPE\"\n        super().__init__(name)\n\n        self.tokenizer = GPT2Tokenizer(\n            vocab_file, merge_file, errors=\"replace\", special_tokens=[], max_len=None\n        )\n        self.eod_id = self.tokenizer.encoder[\"<|endoftext|>\"]\n\n    @property\n    def vocab_size(self):\n        return len(self.tokenizer.encoder)\n\n    @property\n    def vocab(self):\n        return self.tokenizer.encoder\n\n    @property\n    def inv_vocab(self):\n        return self.tokenizer.decoder\n\n    def tokenize(self, text):\n        return self.tokenizer.encode(text)\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n\nclass SentencePieceTokenizer(AbstractTokenizer):\n    \"\"\"Designed to Integrate SP's Tokenizer.\"\"\"\n\n    def __init__(self, vocab_file):\n        name = \"SPM\"\n        super().__init__(name)\n\n        self.tokenizer = spm.SentencePieceProcessor(model_file=vocab_file)\n        self.eod_id = self.tokenizer.piece_to_id(\"<|endoftext|>\")\n\n    @property\n    def vocab_size(self):\n        return self.tokenizer.get_piece_size()\n\n    @property\n    def vocab(self):\n        return {\n            self.tokenizer.id_to_piece(idx): idx\n            for idx in range(self.tokenizer.get_piece_size())\n        }\n\n    @property\n    def inv_vocab(self):\n        return {\n            idx: self.tokenizer.id_to_piece(idx)\n            for idx in range(self.tokenizer.get_piece_size())\n        }\n\n    def tokenize(self, text):\n        return self.tokenizer.encode(text)\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n\nclass HFTokenizer(AbstractTokenizer):\n    \"\"\"Designed to Integrate HF's Tokenizer library.\"\"\"\n\n    def __init__(self, vocab_file):\n        name = \"HFTokenizer\"\n        super().__init__(name)\n        self.tokenizer = Tokenizer.from_file(vocab_file)\n        self.eod_id = self.tokenizer.token_to_id(\"<|endoftext|>\")\n        self.pad_id = self.tokenizer.token_to_id(\"<|padding|>\")\n\n    @property\n    def vocab_size(self):\n        return self.tokenizer.get_vocab_size()\n\n    @property\n    def vocab(self):\n        return self.tokenizer.get_vocab()\n\n    @property\n    def inv_vocab(self):\n        return self.tokenizer.decoder\n\n    def tokenize(self, text: str):\n        return self.tokenizer.encode(text).ids\n\n    def tokenize_batch(self, text_batch: Union[List[str], str]):\n        return self.tokenizer.encode_batch(text_batch)\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n\nclass HFGPT2Tokenizer(AbstractTokenizer):\n    \"\"\"Designed to Integrate the pretrained OpenAI GPT2 Tokenizers from HF\"\"\"\n\n    def __init__(self, vocab_file=None, fast=True):\n        name = \"HFGPT2Tokenizer\"\n        if fast:\n            name += \"Fast\"\n        super().__init__(name)\n        if vocab_file is None:\n            vocab_file = \"gpt2\"\n        if fast:\n            self.tokenizer = GPT2TokenizerFast.from_pretrained(vocab_file)\n        else:\n            self.tokenizer = GPT2Tokenizer.from_pretrained(vocab_file)\n\n        self.tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n        self.eod_id = self.tokenizer.eos_token_id\n        self.pad_id = self.tokenizer.pad_token_id\n\n    @property\n    def vocab_size(self):\n        return len(self.tokenizer)\n\n    @property\n    def vocab(self):\n        return self.tokenizer.get_vocab()\n\n    @property\n    def inv_vocab(self):\n        return self.tokenizer._tokenizer.decoder\n\n    def tokenize(self, text: str):\n        return self.tokenizer.encode(text)\n\n    def tokenize_batch(self, text_batch: Union[List[str], str]):\n        if isinstance(text_batch, str):\n            text_batch = [text_batch]\n        return [self.tokenize(t) for t in text_batch]\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n\nclass CharLevelTokenizer(AbstractTokenizer):\n    \"\"\"Character Level Tokenizer\"\"\"\n\n    def __init__(self, vocab_size):\n        name = \"CharLevelTokenizer\"\n        super().__init__(name)\n        self._vocab_size = vocab_size\n        self.eod_id = 0\n        self.pad_id = 1\n\n    def clamp(self, n):\n        return max(32, min(n, self.vocab_size))\n\n    @property\n    def vocab_size(self):\n        return self._vocab_size\n\n    @property\n    def vocab(self):\n        raise NotImplementedError\n\n    @property\n    def inv_vocab(self):\n        raise NotImplementedError\n\n    def decode_token(self, token: int):\n        return str(chr(self.clamp(token)))\n\n    def tokenize(self, text: str):\n        return list(np.fromstring(text, dtype=np.uint8))\n\n    def tokenize_batch(self, text_batch: Union[List[str], str]):\n        if isinstance(text_batch, list):\n            return [self.tokenize(s) for s in text_batch]\n        else:\n            return self.tokenize(text_batch)\n\n    def detokenize(self, token_ids):\n        return \"\".join(list(map(self.decode_token, token_ids)))\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n\nclass TiktokenTokenizer(AbstractTokenizer):\n    \"\"\"Tokenizer from OpenAI's tiktoken implementation\"\"\"\n\n    def __init__(self, vocab_file):\n        try:\n            import tiktoken\n        except ModuleNotFoundError:\n            print(\"Please install tiktoken: (https://github.com/openai/tiktoken)\")\n            raise Exception\n\n        name = \"TiktokenTokenizer\"\n        super().__init__(name)\n\n        self.tokenizer = tiktoken.get_encoding(vocab_file)\n        self.eod_id = self.tokenizer.eot_token\n        self.pad_id = None\n\n    @property\n    def vocab_size(self):\n        return self.tokenizer.n_vocab\n\n    @property\n    def vocab(self):\n        raise NotImplementedError(\n            \"TiktokenTokenizer does not implement vocabulary access.\"\n        )\n\n    @property\n    def inv_vocab(self):\n        raise NotImplementedError(\n            \"TiktokenTokenizer does not implement vocabulary access. \\\n                To get the idx-th token in vocabulary, use tokenizer.decode([idx]) .\"\n        )\n\n    def tokenize(self, text: str):\n        return self.tokenizer.encode(text)  # ,  allowed_special=\"all\")\n\n    def tokenize_batch(self, text_batch: List[str]):\n        return self.tokenizer.encode_batch(text_batch, allowed_special=\"all\")\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(tokens=token_ids, errors=\"strict\")\n\n    @property\n    def eod(self):\n        return self.eod_id\n\n    @property\n    def pad(self):\n        raise NotImplementedError\n",
        "megatron/tokenizer/train_tokenizer.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nAssumes a dataset of jsonl files in the same format as the neox training set.\n\"\"\"\n\nfrom tokenizers import Tokenizer, decoders, models, pre_tokenizers, processors, trainers\nfrom tokenizers.normalizers import NFKC\n\nfrom glob import glob\nimport os\nimport json\nimport argparse\n\n\ndef load_jsonl(input_path, quiet=True) -> list:\n    \"\"\"\n    Read list of objects from a JSON lines file.\n    \"\"\"\n    data = []\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            data.append(json.loads(line.rstrip(\"\\n|\\r\")))\n    if not quiet:\n        print(\"Loaded {} records from {}\".format(len(data), input_path))\n    return data\n\n\ndef json_iterator(input_dir, text_key=\"text\"):\n    all_jsonls = glob(f\"{input_dir}/*.jsonl\") + glob(f\"{input_dir}/*.json\")\n    for j in all_jsonls:\n        data = load_jsonl(j)\n        for doc in data:\n            yield doc[text_key]\n\n\ndef train_tokenizer(\n    input_dir: str, save_path: str, tokenizer_type: str = \"BPE\", vocab_size: int = 52000\n):\n    \"\"\"\n    Trains a tokenizer on all the json files in `input_dir` and saves it to `save_path`\n\n    :param input_dir: input directory containing jsonl files\n    :param save_path: path to save tokenizer to\n    :param tokenizer_type: type of tokenizer to train.\n    :param vocab_size: int, size of tokenizer's vocab\n    :return:\n    \"\"\"\n\n    if tokenizer_type == \"BPE\":\n        model = models.BPE()\n    else:\n        raise NotImplementedError(f\"Tokenizer type {tokenizer_type} not implemented\")\n    tokenizer = Tokenizer(model)\n\n    # Customize pre-tokenization and decoding\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n    tokenizer.decoder = decoders.ByteLevel()\n    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n    tokenizer.normalizer = NFKC()\n\n    # And then train\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\", \"<|padding|>\"]\n    )\n    tokenizer.train_from_iterator(json_iterator(input_dir), trainer)\n\n    # And Save it\n    if save_path:\n        tokenizer.save(save_path, pretty=True)\n        print(f\"Tokenizer saved at {save_path}\")\n\n\ndef parse_args(input_args=None):\n    parser = argparse.ArgumentParser(\n        description=\"script for training a multilingual \"\n        \"HF tokenizer on CC dumps with upweighting for low resource languages\"\n    )\n    parser.add_argument(\n        \"--json_input_dir\",\n        type=str,\n        help=\"Path to folder containing tokenizer training data in jsonl format\",\n    )\n    parser.add_argument(\n        \"--tokenizer_output_path\",\n        type=str,\n        help=\"Path to which your trained tokenizer will be saved (should end in .json)\",\n    )\n    parser.add_argument(\n        \"--tokenizer_type\",\n        type=str,\n        help=\"type of tokenizer to train, currently only BPE is supported\",\n        choices=[\"BPE\"],\n        default=\"BPE\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--vocab_size\",\n        help=\"vocabulary size of tokenizer, default=52k\",\n        type=int,\n        default=52000,\n    )\n    args_parsed = parser.parse_args(input_args)\n    return args_parsed\n\n\ndef main(args):\n    train_tokenizer(\n        args.json_input_dir,\n        save_path=args.tokenizer_output_path,\n        tokenizer_type=args.tokenizer_type,\n        vocab_size=args.vocab_size,\n    )\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args)\n",
        "megatron/training.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This file has been modified from its original version\n#\n\n\"\"\"Pretrain utilities.\"\"\"\nfrom datetime import datetime\nfrom functools import partial\nfrom collections import defaultdict\n\nimport math\nimport sys\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.nn.functional as F\nimport deepspeed\nfrom deepspeed.runtime.data_pipeline.curriculum_scheduler import CurriculumScheduler\nimport numpy as np\n\nfrom megatron.utils import (\n    Timers,\n    init_wandb,\n    get_ltor_masks_and_position_ids,\n    reduce_losses,\n)\n\nfrom megatron import print_rank_0, mpu\nfrom megatron.model import (\n    GPT2ModelPipe,\n    SoftEmbedding,\n    get_params_for_weight_decay_optimization,\n    mark_norms_for_sequence_parallel_grad_sync,\n)\nfrom megatron.mpu.mappings import gather_from_model_parallel_region\nfrom megatron.checkpointing import load_checkpoint, save_checkpoint\nfrom megatron.data.data_utils import (\n    build_train_valid_test_data_loaders,\n    shift_and_wrap_data_loaders,\n)\nfrom megatron.initialize import initialize_megatron\nfrom megatron.learning_rates import AnnealingLR\nfrom megatron.logging import tb_wandb_log, training_log\nfrom megatron.utils import (\n    OverflowMonitor,\n    get_noise_scale_logger,\n    get_total_params,\n    CharCounter,\n)\nfrom megatron.model.weight_server import start_server\nfrom megatron.model.gpt2_model import cross_entropy\nfrom megatron.mpu import vocab_parallel_cross_entropy\n\nfrom pickle import dump\nimport os\n\n\ndef mup_weights_reinit(neox_args, model):\n    def has_method(o, name):\n        return callable(getattr(o, name, None))\n\n    for layer in model.modules():\n        # This normally would happen in set_base_shapes if we actually were able to use the MuReadout class\n        if hasattr(layer, \"mup_rescale_parameters\") and layer.mup_rescale_parameters:\n            layer._rescale_parameters()\n\n        if has_method(layer, \"mup_reinitialize_weights\"):\n            layer.mup_reinitialize_weights(neox_args)\n\n\ndef save_base_shapes(neox_args, base_shapes, use_cache):\n\n    # Instantiation of the base model fails in the init function (init_functions.py) because we haven't called set_base_shapes on it at this point, so disable it temporarily here\n    neox_args.use_mup = False\n\n    base_model = GPT2ModelPipe(\n        neox_args=neox_args,\n        num_tokentypes=0,\n        parallel_output=True if neox_args.train_impl != \"rm\" else False,\n        topology=mpu.get_topology(),\n        use_cache=use_cache,\n    )\n\n    if not neox_args.is_pipe_parallel:\n        base_model = base_model.to_sequential()\n\n    try:\n        import mup\n    except ModuleNotFoundError:\n        print(\"Please install mup https://github.com/microsoft/mup\")\n        raise Exception\n\n    base_shapes = mup.get_shapes(base_model)\n\n    del base_model\n\n    old_hidden_size = neox_args.hidden_size\n    neox_args.hidden_size = neox_args.hidden_size * neox_args.mup_width_scale\n\n    delta_model = GPT2ModelPipe(\n        neox_args=neox_args,\n        num_tokentypes=0,\n        parallel_output=True if neox_args.train_impl != \"rm\" else False,\n        topology=mpu.get_topology(),\n        use_cache=use_cache,\n    )\n\n    if not neox_args.is_pipe_parallel:\n        delta_model = delta_model.to_sequential()\n\n    delta_shapes = mup.get_shapes(delta_model)\n\n    # change back\n    neox_args.use_mup = True\n    neox_args.hidden_size = old_hidden_size\n\n    save_shapes = f\"{neox_args.base_shapes_file}.{torch.distributed.get_rank()}\"\n    print(f\"saving base shapes at {save_shapes}\")\n    mup.make_base_shapes(base_shapes, delta_shapes, savefile=save_shapes)\n    print(f\"base shapes saved...exiting\")\n    sys.exit(1)\n\n\ndef mup_coord_check(neox_args, timers, lr_scheduler, train_data_iterator):\n    from megatron.mup_substitute import get_coord_data\n    from mup.coord_check import plot_coord_data\n\n    def lazy_model(hidden_size):\n        def gen():\n            old_hidden_size = neox_args.hidden_size\n            neox_args.hidden_size = hidden_size\n\n            model, optimizer, _, _ = setup_model_and_optimizer(\n                neox_args=neox_args, use_cache=False\n            )\n\n            neox_args.hidden_size = old_hidden_size\n\n            return model\n\n        return gen\n\n    models = {}\n\n    # Hidden size needs to be divisible by num attention heads\n    for hidden_size in (neox_args.num_attention_heads * (2**p) for p in range(2, 9)):\n        models[hidden_size] = lazy_model(hidden_size)\n\n    neox_args.use_mup = True\n    df_up = get_coord_data(\n        neox_args, timers, lr_scheduler, models, train_data_iterator, mup=True\n    )\n    neox_args.use_mup = False\n    df_sp = get_coord_data(\n        neox_args, timers, lr_scheduler, models, train_data_iterator, mup=False\n    )\n\n    plot_coord_data(df_up, save_to=f\"coord_check_up.{torch.distributed.get_rank()}.jpg\")\n    plot_coord_data(df_sp, save_to=f\"coord_check_sp.{torch.distributed.get_rank()}.jpg\")\n\n    print_rank_0(\"Saved coord check plots... exiting\")\n    sys.exit(1)\n\n\ndef update_iterations(neox_args, data_loaders):\n    \"\"\"\n    Compute the number of train iterations if not specified and num_epochs, updates the neox_args object.\n    Note that if len(train_dataloader) % gradient_accumulation_steps != 0, this will configure neox\n    to do as many iterations as possible while ensuring that each example is seen *at most* train_epochs\n    times.\n    \"\"\"\n    if (not neox_args.do_train) or (neox_args.train_iters is not None):\n        pass\n    elif neox_args.train_iters is None and neox_args.train_epochs is None:\n        print_rank_0(\n            \"ERROR:Failed to specify either train_epochs or train_iters in config file\"\n        )\n    else:\n        global_rank = torch.distributed.get_rank()\n\n        if global_rank == 0:\n            train_dataloader = data_loaders[\"train\"]\n            train_epochs = neox_args.train_epochs\n            gradient_accumulation_steps = neox_args.gradient_accumulation_steps\n\n            train_dataloader_len = len(train_dataloader)\n            train_iterations = (\n                train_dataloader_len * train_epochs\n            ) // gradient_accumulation_steps\n\n            train_iters_tensor = torch.cuda.LongTensor([train_iterations])\n        else:\n            train_iters_tensor = torch.cuda.LongTensor([0])\n\n        torch.distributed.broadcast(train_iters_tensor, src=0)\n\n        neox_args.train_iters = train_iters_tensor[0].item()\n\n        print_rank_0(\n            f\"Training for a total of {neox_args.train_iters} iterations, corresponding to {neox_args.train_epochs} epochs.\"\n        )\n\n\ndef pretrain(neox_args):\n    \"\"\"Main training program.\n\n    This function will run the following in the order provided:\n        1) initialize Megatron.\n        2) get train/val/test datasets.\n        3) setup model, optimizer and lr schedule.\n        4) configure data loading\n        5) train the model.\n\n    Arguments:\n        neox_args: an instance of NeoXArgs containing the configuration for pretrain\n\n    \"\"\"\n    # setup logging and timers\n    init_wandb(neox_args=neox_args)\n    timers = Timers(\n        use_wandb=neox_args.use_wandb,\n        tensorboard_writer=neox_args.tensorboard_writer,\n        comet_experiment=neox_args.comet_experiment,\n    )\n\n    # Initialize and get arguments, timers, and Tensorboard writer.\n    initialize_megatron(neox_args=neox_args)\n\n    # Create data loaders\n    timers(\"train/valid/test data loaders\").start()\n    data_loaders = build_train_valid_test_data_loaders(neox_args=neox_args)\n    update_iterations(neox_args=neox_args, data_loaders=data_loaders)\n    timers(\"train/valid/test data loaders\").stop()\n\n    # Model, optimizer, and learning rate.\n    timers(\"model and optimizer\").start()\n    model, optimizer, lr_scheduler, reference_model = setup_model_and_optimizer(\n        neox_args=neox_args, use_cache=False, iteration=neox_args.iteration\n    )\n    timers(\"model and optimizer\").stop()\n\n    if neox_args.serve_model_weights:\n        start_server(model)\n        # sync...\n        torch.distributed.barrier()\n\n    # Start data stuff:\n\n    # Make and configure iterators\n    timers(\"train/valid/test data iterators\").start()\n    (\n        train_data_iterator,\n        valid_data_iterator,\n        test_data_iterator,\n    ) = shift_and_wrap_data_loaders(neox_args=neox_args, data_loaders=data_loaders)\n    timers(\"train/valid/test data iterators\").stop()\n\n    if neox_args.use_mup and neox_args.coord_check:\n        mup_coord_check(neox_args, timers, lr_scheduler, train_data_iterator)\n\n    # Print setup timing.\n    print_rank_0(\"done with setups ...\")\n    timers.log(\n        [\n            \"train/valid/test data loaders\",\n            \"model and optimizer\",\n            \"train/valid/test data iterators\",\n        ]\n    )\n    print_rank_0(\"training ...\")\n\n    iteration = neox_args.iteration\n    # edge case: save step 0 checkpoint if requested and we're starting from step 0\n    if (\n        neox_args.save\n        and neox_args.extra_save_iters\n        and 0 in neox_args.extra_save_iters\n        and iteration == 0\n    ):\n        save_checkpoint(\n            neox_args=neox_args,\n            iteration=iteration,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n        )\n\n    if neox_args.do_train and neox_args.train_iters > 0:\n        iteration = train(\n            neox_args=neox_args,\n            timers=timers,\n            model=model,\n            reference_model=reference_model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            train_data_iterator=train_data_iterator,\n            valid_data_iterator=valid_data_iterator,\n        )\n\n    if neox_args.do_valid:\n        prefix = \"the end of training for val data\"\n        evaluate_and_print_results(\n            neox_args=neox_args,\n            prefix=prefix,\n            forward_step_func=forward_step,\n            data_iterator=valid_data_iterator,\n            model=model,\n            iteration=iteration,\n            verbose=False,\n            timers=timers,\n            reference_model=reference_model,\n        )\n\n    if neox_args.save and iteration != 0:\n        save_checkpoint(\n            neox_args=neox_args,\n            iteration=iteration,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n        )\n\n    if neox_args.do_test:\n        # Run on test data.\n        prefix = \"the end of training for test data\"\n        evaluate_and_print_results(\n            neox_args=neox_args,\n            prefix=prefix,\n            forward_step_func=forward_step,\n            data_iterator=test_data_iterator,\n            model=model,\n            iteration=iteration,\n            verbose=True,\n            timers=timers,\n            chart_name=\"test\",\n            reference_model=reference_model,\n        )\n\n\ndef _get_batch(neox_args, tokenizer, keys, data, datatype, label_mask_zero=False):\n    \"\"\"Support function for get_batch / get_batch pipe (to avoid code repetition)\"\"\"\n    data_b = mpu.broadcast_data(keys, data, datatype)\n    token_key = keys[0]\n    label_key = keys[1] if len(keys) > 1 else None\n    # Unpack.\n    tokens_ = data_b[token_key].long()\n    if label_key in data_b:\n        label_mask = (data_b[label_key].long() >= 0)[:, 1:].contiguous()\n        labels = torch.where(\n            data_b[label_key].long() >= 0,\n            data_b[label_key].long(),\n            torch.zeros_like(data_b[label_key].long()),\n        )[:, 1:].contiguous()\n    else:\n        label_mask = (tokens_.long() >= 0)[:, 1:].contiguous()\n        labels = tokens_[:, 1:].contiguous()\n        if label_mask_zero:\n            labels = labels * label_mask\n    tokens = tokens_[:, :-1].contiguous()\n\n    # Get the masks and position ids.\n    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(\n        data=tokens,\n        eod_token=neox_args.tokenizer.eod,\n        eod_mask_loss=neox_args.eod_mask_loss,\n        sliding_window_width=neox_args.sliding_window_width,\n    )\n\n    # combine loss masks from get_ltor_masks_and_position_ids with loss masks from data\n    loss_mask = label_mask.to(loss_mask.dtype) * loss_mask\n    return tokens, labels, loss_mask, attention_mask, position_ids\n\n\ndef get_batch(neox_args, data_iterator):\n    \"\"\"Generate a batch\"\"\"\n\n    # Items and their type.\n    if neox_args.train_impl in [\"normal\", \"kto\", \"reinforce\"]:\n        keys = [\"text\", \"label\"] if neox_args.train_label_data_paths else [\"text\"]\n    elif neox_args.train_impl in [\"dpo\", \"rm\"]:\n        keys = (\n            [[\"pos\", \"pos_label\"], [\"neg\", \"neg_label\"]]\n            if neox_args.pos_train_label_data_paths\n            else [[\"pos\"], [\"neg\"]]\n        )\n    datatype = torch.int64\n\n    # Broadcast data.\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    if neox_args.train_impl == \"normal\":\n        return _get_batch(\n            neox_args=neox_args,\n            tokenizer=neox_args.tokenizer,\n            keys=keys,\n            data=data,\n            datatype=datatype,\n        )\n    elif neox_args.train_impl == \"kto\":\n        assert (\n            neox_args.train_micro_batch_size_per_gpu > 1\n        ), \"For KTO training, the train_micro_batch_size_per_gpu must be greater than 1.\"\n        tup = _get_batch(\n            neox_args=neox_args,\n            tokenizer=neox_args.tokenizer,\n            keys=keys,\n            data=data,\n            datatype=datatype,\n        )\n        # Remove the last token from the reward since we predict the next token, so\n        # Reward of <current prediction> will be based on the label of <next token>\n        rw_data = mpu.broadcast_data([\"reward\"], data, torch.float)[\"reward\"][\n            :, :-1\n        ].contiguous()\n        ref_data = (\n            mpu.broadcast_data([\"ref\"], data, torch.float)[\"ref\"][:, :-1].contiguous()\n            if neox_args.precompute_model_name\n            else None\n        )\n        return tup + (rw_data, ref_data)\n    elif neox_args.train_impl == \"reinforce\":\n\n        tup = _get_batch(\n            neox_args=neox_args,\n            tokenizer=neox_args.tokenizer,\n            keys=keys,\n            data=data,\n            datatype=datatype,\n        )\n        rw_data = mpu.broadcast_data([\"reward\"], data, torch.float)[\"reward\"]\n        raw_rw_data = mpu.broadcast_data([\"raw_reward\"], data, torch.float)[\n            \"raw_reward\"\n        ]\n        return tup + (rw_data, raw_rw_data)\n    elif neox_args.train_impl in [\"dpo\", \"rm\"]:\n        pos_tup = _get_batch(\n            neox_args=neox_args,\n            tokenizer=neox_args.tokenizer,\n            keys=keys[0],\n            data=data,\n            datatype=datatype,\n            label_mask_zero=True,\n        )\n        neg_tup = _get_batch(\n            neox_args=neox_args,\n            tokenizer=neox_args.tokenizer,\n            keys=keys[1],\n            data=data,\n            datatype=datatype,\n            label_mask_zero=True,\n        )\n        if neox_args.precompute_model_name:\n            ref_data = mpu.broadcast_data([\"pos_ref\", \"neg_ref\"], data, torch.float)\n        else:\n            ref_data = {\"pos_ref\": None}\n        return [\n            torch.cat((pos_item, neg_item), dim=0)\n            for pos_item, neg_item in zip(pos_tup, neg_tup)\n        ] + [\n            torch.cat((ref_data[\"pos_ref\"], ref_data[\"neg_ref\"]), dim=0)[\n                :, :-1\n            ].contiguous()\n            if ref_data[\"pos_ref\"] is not None\n            else None\n        ]\n\n\ndef get_batch_pipe(data, neox_args, curr_scheduler=None):\n    \"\"\"A modification of get_batch() to work with the latest batch instead of an iterator.\"\"\"\n\n    assert neox_args.train_impl not in [\n        \"kto\",\n        \"dpo\",\n        \"rm\",\n    ], \"Pipeline parallel is currently unsupported when using any of kto, dpo, rm. Set pipe_parallel_size to 0\"\n\n    # Items and their type.\n    keys = [\"text\", \"label\"] if neox_args.train_label_data_paths else [\"text\"]\n    datatype = torch.int64\n\n    tokens, labels, loss_mask, attention_mask, position_ids = _get_batch(\n        neox_args, neox_args.tokenizer, keys, data, datatype\n    )\n    if curr_scheduler is not None:\n        # iteration + 1 to align with how/when DeepSpeed updates the buffers\n        curriculum_seqlen = curr_scheduler.update_difficulty(neox_args.iteration + 1)\n        if curriculum_seqlen < tokens.size()[1]:\n            # seqlen-based curriculum learning\n            # input_ids, position_ids, labels have size [batch size, seqlen]\n            # input_ids = input_ids[:, :curriculum_seqlen].contiguous()\n            tokens = tokens[:, :curriculum_seqlen].contiguous()\n            position_ids = position_ids[:, :curriculum_seqlen].contiguous()\n            if labels is not None:\n                labels = labels[:, :curriculum_seqlen].contiguous()\n            if loss_mask is not None:\n                loss_mask = loss_mask[:, :curriculum_seqlen].contiguous()\n            # attention_mask has size [1, 1, seqlen, seqlen]\n            attention_mask = attention_mask[\n                :, :, :curriculum_seqlen, :curriculum_seqlen\n            ].contiguous()\n\n    # unpack data\n    return (tokens, position_ids, attention_mask), (labels, loss_mask)\n\n\ndef get_batch_sequential(forward_input, neox_args):\n    \"\"\"A modification of get_batch() to work with the latest batch instead of an iterator.\"\"\"\n    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(\n        data=forward_input[0],\n        eod_token=neox_args.tokenizer.eod,\n        eod_mask_loss=neox_args.eod_mask_loss,\n    )\n    return (forward_input[0], forward_input[1], attention_mask)\n\n\ndef forward_step(\n    data_iterator,\n    model,\n    neox_args,\n    timers,\n    return_logits=False,\n    is_train=False,\n    reference_model=None,\n):\n    \"\"\"Forward step.\"\"\"\n    if neox_args.is_pipe_parallel:\n        return model.eval_batch(data_iterator, return_logits=return_logits)\n\n    # Get the batch.\n    if neox_args.memory_profiling and neox_args.iteration:\n        torch.cuda.nvtx.range_push(f\"Get batch\")\n    if timers is not None:\n        timers(\"batch generator\").start()\n    if neox_args.train_impl == \"normal\":\n        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(\n            neox_args=neox_args, data_iterator=data_iterator\n        )\n    elif neox_args.train_impl == \"kto\":\n        (\n            tokens,\n            labels,\n            loss_mask,\n            attention_mask,\n            position_ids,\n            rewards,\n            ref_logp,\n        ) = get_batch(neox_args=neox_args, data_iterator=data_iterator)\n    elif neox_args.train_impl == \"reinforce\":\n        (\n            tokens,\n            labels,\n            loss_mask,\n            attention_mask,\n            position_ids,\n            rewards,\n            raw_rewards,\n        ) = get_batch(neox_args=neox_args, data_iterator=data_iterator)\n    if neox_args.train_impl in [\"dpo\", \"rm\"]:\n        tokens, labels, loss_mask, attention_mask, position_ids, ref_logp = get_batch(\n            neox_args=neox_args, data_iterator=data_iterator\n        )\n\n    if timers is not None:\n        timers(\"batch generator\").stop()\n    if neox_args.memory_profiling:\n        torch.cuda.nvtx.range_pop()\n\n    if neox_args.memory_profiling:\n        torch.cuda.nvtx.range_push(f\"Forward pass\")\n    metrics = {}\n    if neox_args.train_impl == \"normal\":\n        outputs = model((tokens, position_ids, attention_mask), neox_args=neox_args)\n        if (\n            is_train\n            and neox_args.curriculum_learning\n            and neox_args.curriculum_seqlen < neox_args.seq_length\n        ):\n            loss_mask = loss_mask[:, : neox_args.curriculum_seqlen].contiguous()\n            labels = labels[:, : neox_args.curriculum_seqlen].contiguous()\n        loss = cross_entropy(\n            outputs, (labels, loss_mask), _fp16=neox_args.fp16_lm_cross_entropy\n        )\n    elif neox_args.train_impl == \"rm\":\n        maybe_tuple = model((tokens, position_ids, attention_mask), neox_args=neox_args)\n        if type(maybe_tuple) is tuple:\n            outputs, _ = maybe_tuple\n        else:\n            outputs = maybe_tuple\n        pos, neg = torch.chunk(outputs, 2, 0)\n        pos_loss_mask, neg_loss_mask = torch.chunk(loss_mask, 2, 0)\n        # We assume that each pos, neg pair occur in the same order\n        # e.g. second nonzero pos is the corresponding second nonzero neg\n        # and that there are also an equal number of pos and neg in each sequence.\n        pos_indx = pos_loss_mask.nonzero()\n        neg_indx = neg_loss_mask.nonzero()\n        # indx[:, 0] is the batch index, indx[:, 1] is the token index, we only care about the token index.\n        pos_indx = pos_indx[:, 1].unsqueeze(1)\n        neg_indx = neg_indx[:, 1].unsqueeze(1)\n        pos = torch.gather(pos.squeeze(), dim=1, index=pos_indx)\n        neg = torch.gather(neg.squeeze(), dim=1, index=neg_indx)\n        with torch.no_grad():\n            metrics[\"pos_values\"] = pos.clone().detach().mean()\n            metrics[\"neg_values\"] = neg.clone().detach().mean()\n            metrics[\"margin\"] = (pos - neg).clone().detach().mean()\n            metrics[\"accuracy\"] = ((pos - neg) > 0).clone().detach().float().mean()\n        loss = (-F.logsigmoid(pos - neg).mean()) + (\n            (neox_args.z_loss * (pos**2 + neg**2)).mean()\n        )\n    elif neox_args.train_impl == \"dpo\":\n        # Based on https://github.com/eric-mitchell/direct-preference-optimization/blob/main/trainers.py#L90\n        with torch.inference_mode():\n            # So we can gather token logps...\n            token_logp_labels = labels.clone()\n            pos_loss_mask, neg_loss_mask = torch.chunk(loss_mask, 2, 0)\n            if neox_args.dpo_reference_free:\n                ref_pos = 0\n                ref_neg = 0\n            elif ref_logp is None:\n                ref_maybe_tuple = reference_model(\n                    (tokens, position_ids, attention_mask), neox_args=neox_args\n                )\n                if type(ref_maybe_tuple) is tuple:\n                    # We should ignore MoE losses yeah?\n                    ref_outputs, _ = ref_maybe_tuple\n                else:\n                    ref_outputs = ref_maybe_tuple\n                ref_pos, ref_neg = get_pos_neg_logp(\n                    ref_outputs, token_logp_labels, neox_args.dpo_fp32\n                )\n            else:\n                ref_pos, ref_neg = torch.chunk(ref_logp, 2, 0)\n            ref_pos = (ref_pos * pos_loss_mask).sum(-1)\n            ref_neg = (ref_neg * neg_loss_mask).sum(-1)\n        chosen_maybe_tuple = model(\n            (tokens, position_ids, attention_mask), neox_args=neox_args\n        )\n        if type(chosen_maybe_tuple) is tuple:\n            # We should ignore MoE losses yeah?\n            chosen_outputs, _ = chosen_maybe_tuple\n        else:\n            chosen_outputs = chosen_maybe_tuple\n        chosen_pos, chosen_neg = get_pos_neg_logp(\n            chosen_outputs, token_logp_labels, neox_args.dpo_fp32\n        )\n        chosen_pos = (chosen_pos * pos_loss_mask).sum(-1)\n        chosen_neg = (chosen_neg * neg_loss_mask).sum(-1)\n        with torch.no_grad():\n            # Collect metrics...\n            if not neox_args.dpo_reference_free:\n                metrics[\"ref_neg\"] = ref_neg.clone().detach().mean()\n                metrics[\"ref_pos\"] = ref_pos.clone().detach().mean()\n            metrics[\"chosen_neg\"] = chosen_neg.clone().detach().mean()\n            metrics[\"chosen_pos\"] = chosen_pos.clone().detach().mean()\n            if not neox_args.dpo_reference_free:\n                chosen_rewards = neox_args.dpo_beta * (\n                    chosen_pos.clone().detach() - ref_pos.clone().detach()\n                )\n                rejected_rewards = neox_args.dpo_beta * (\n                    chosen_neg.clone().detach() - ref_neg.clone().detach()\n                )\n                metrics[\"chosen_rewards\"] = chosen_rewards.mean()\n                metrics[\"rejected_rewards\"] = rejected_rewards.mean()\n                reward_acc = (chosen_rewards > rejected_rewards).float()\n                metrics[\"reward_acc\"] = reward_acc.mean()\n                metrics[\"margins\"] = (chosen_rewards - rejected_rewards).mean()\n        pi_logrations = chosen_pos - chosen_neg\n        ref_logrations = ref_pos - ref_neg\n        logits = pi_logrations - ref_logrations\n        loss = -F.logsigmoid(neox_args.dpo_beta * logits).mean()\n    elif neox_args.train_impl == \"kto\":\n        # Based on https://github.com/huggingface/trl/blob/main/trl/trainer/kto_trainer.py\n        # Except we don't have an extra input for KL logp, we just split the batch in half\n        with torch.no_grad():\n            # So we can gather token logps...\n            token_logp_labels = labels.clone()\n            token_logp_labels[token_logp_labels == -100] = 0\n            if ref_logp is None:\n                # Did not precompute logits....\n                ref_maybe_tuple = reference_model(\n                    (tokens, position_ids, attention_mask), neox_args=neox_args\n                )\n                if type(ref_maybe_tuple) is tuple:\n                    # We should ignore MoE losses yeah?\n                    ref_outputs, _ = ref_maybe_tuple\n                else:\n                    ref_outputs = ref_maybe_tuple\n                # gather across tensor parallel group\n                ref_outputs = gather_from_model_parallel_region(ref_outputs)\n\n                ref_logp = get_logp(ref_outputs, token_logp_labels, neox_args.kto_fp32)\n            else:\n                print(f\"REF LOGP: {ref_logp.clone().detach().mean()}\")\n            ref_logp = ref_logp * loss_mask\n            scaling = (rewards.sum(-1) > 0.001).float() * neox_args.kto_desirable_weight\n            scaling += (\n                rewards.sum(-1) < -0.001\n            ).float() * neox_args.kto_undesirable_weight\n            pos_mask = (rewards > 0.001).float()\n            neg_mask = (rewards < -0.001).float()\n        chosen_maybe_tuple = model(\n            (tokens, position_ids, attention_mask), neox_args=neox_args\n        )\n        if type(chosen_maybe_tuple) is tuple:\n            # We should ignore MoE losses yeah?\n            chosen_outputs, _ = chosen_maybe_tuple\n        else:\n            chosen_outputs = chosen_maybe_tuple\n        chosen_outputs = gather_from_model_parallel_region(chosen_outputs)\n        chosen_logp = get_logp(chosen_outputs, token_logp_labels, neox_args.kto_fp32)\n        chosen_logp = chosen_logp * loss_mask\n        with torch.no_grad():\n            # Collect metrics...\n            metrics[\"ref_logp\"] = ref_logp.clone().detach().sum(-1).mean()\n            metrics[\"policy_logp\"] = chosen_logp.clone().detach().sum(-1).mean()\n            metrics[\"pos_ref_logp\"] = (\n                (ref_logp * pos_mask).clone().detach().sum(-1).mean()\n            )\n            metrics[\"neg_ref_logp\"] = (\n                (ref_logp * neg_mask).clone().detach().sum(-1).mean()\n            )\n            metrics[\"pos_policy_logp\"] = (\n                (chosen_logp * pos_mask).clone().detach().sum(-1).mean()\n            )\n            metrics[\"neg_policy_logp\"] = (\n                (chosen_logp * neg_mask).clone().detach().sum(-1).mean()\n            )\n            metrics[\"kl\"] = (\n                chosen_logp.clone().detach() - ref_logp.clone().detach()\n            ).sum() / loss_mask.sum()\n            policy_rewards = (\n                neox_args.kto_beta\n                * rewards\n                * (chosen_logp.clone().detach() - ref_logp.clone().detach())\n            )\n            reward_acc = (policy_rewards.sum(-1) > 0.0).float()\n            metrics[\"reward_acc\"] = reward_acc.mean()\n            metrics[\"policy_rewards\"] = policy_rewards.sum()\n            print(metrics)\n        pol_logp1, pol_logp2 = torch.chunk(chosen_logp, 2, 0)\n        ref_logp1, ref_logp2 = torch.chunk(ref_logp, 2, 0)\n        reward1, reward2 = torch.chunk(rewards, 2, 0)\n        scaling1, scaling2 = torch.chunk(scaling, 2, 0)\n        kl1 = torch.clamp((pol_logp1 - ref_logp1).sum(-1), min=0).mean()\n        kl2 = torch.clamp((pol_logp2 - ref_logp2).sum(-1), min=0).mean()\n        log_ratio1 = pol_logp1 - ref_logp1\n        log_ratio2 = pol_logp2 - ref_logp2\n\n        # TODO: Add pack_until_overflow sequence support\n        loss = (\n            0.5\n            * scaling1.mean(-1)\n            * (\n                1\n                - F.sigmoid(\n                    (\n                        neox_args.kto_beta\n                        * reward1.mean(-1)\n                        * (log_ratio1.sum(-1) - kl2.clone().detach())\n                    )\n                )\n            )\n        ) + (\n            0.5\n            * scaling2.mean(-1)\n            * (\n                1\n                - F.sigmoid(\n                    (\n                        neox_args.kto_beta\n                        * reward2.mean(-1)\n                        * (log_ratio2.sum(-1) - kl1.clone().detach())\n                    )\n                )\n            )\n        )\n        # print(loss.shape)\n        loss = loss.mean()\n        # print(loss.shape)\n    elif neox_args.train_impl == \"reinforce\":\n        if reference_model is not None:\n            with torch.no_grad():\n                ref_outputs = reference_model(\n                    (tokens, position_ids, attention_mask), neox_args=neox_args\n                )\n                if type(ref_outputs) is tuple:\n                    ref_outputs, _ = ref_outputs\n                ref_outputs = ref_outputs\n                if neox_args.kl_impl == \"full\":\n                    # Have to do the loss over all tokens...\n                    ref_outputs = gather_from_model_parallel_region(ref_outputs)\n                    if neox_args.fp32_reinforce:\n                        ref_outputs = ref_outputs.float()\n                    ref_logp = ref_outputs.log_softmax(dim=-1).detach()\n                    ref_per_token_logp = torch.gather(\n                        ref_logp.clone(), dim=2, index=labels.unsqueeze(2)\n                    ).squeeze(2)\n                else:\n                    ref_per_token_logp = get_logp(\n                        ref_outputs, labels, neox_args.fp32_reinforce\n                    )\n                metrics[\"ref_logp\"] = ref_per_token_logp.clone().detach().mean()\n        outputs = model((tokens, position_ids, attention_mask), neox_args=neox_args)\n        if type(outputs) is tuple:\n            outputs, _ = outputs\n        if neox_args.kl_impl == \"full\":\n            # Have to do the loss over all tokens...\n            outputs = gather_from_model_parallel_region(outputs)\n            if neox_args.fp32_reinforce:\n                outputs = outputs.float()\n            logp = outputs.log_softmax(dim=-1)\n            per_token_logp = torch.gather(\n                logp.clone(), dim=2, index=labels.unsqueeze(2)\n            ).squeeze(2)\n        else:\n            per_token_logp = get_logp(outputs, labels, neox_args.fp32_reinforce)\n        with torch.no_grad():\n            metrics[\"logp\"] = per_token_logp.clone().detach().mean()\n            metrics[\"reward\"] = raw_rewards.clone().detach().mean()\n            metrics[\"reward_std\"] = raw_rewards.clone().detach().std()\n        loss_mask_sum = loss_mask.sum()\n        if reference_model is not None:\n            if neox_args.kl_impl == \"full\":\n                # Following along with\n                # https://github.com/huggingface/trl/blob/104a02d207b63a4a062882aaff68f2d275493399/trl/trainer/ppo_trainer.py#L1109\n                kl = F.kl_div(ref_logp, logp, log_target=True, reduction=\"none\").sum(-1)\n            else:\n                kl = per_token_logp - ref_per_token_logp\n                if neox_args.kl_impl == \"abs\":\n                    kl = kl.abs()\n                elif neox_args.kl_impl == \"mse\":\n                    kl = 0.5 * (kl).square()\n                elif neox_args.kl_impl == \"kl\":\n                    pass\n            with torch.no_grad():\n                metrics[\"kl\"] = kl.clone().detach().mean()\n            loss = (-per_token_logp * rewards) + (neox_args.kl_div_beta * kl)\n            loss = (loss * loss_mask).sum(-1) / loss_mask_sum\n            loss = loss.mean()\n        else:\n            loss = -(rewards * per_token_logp)\n            loss = (loss * loss_mask).sum(-1) / loss_mask_sum\n            loss = loss.mean()\n    if neox_args.memory_profiling:\n        torch.cuda.nvtx.range_pop()\n    if return_logits:\n        return loss, outputs, metrics\n    return loss, metrics\n\n\ndef get_model(neox_args, use_cache=False):\n    \"\"\"Build the model.\"\"\"\n\n    # Build model on cpu.\n    print_rank_0(\"building GPT2 model ...\")\n\n    # Temporarily disable mup so that the base model does not use the mup init functions before set_base_shapes is called below.\n    # If mup isn't being used anyways, this has no effect.\n    old_use_mup = neox_args.use_mup\n    neox_args.use_mup = False\n\n    if neox_args.zero_stage in [2, 3]:\n        if neox_args.pipe_parallel_size == 1:\n            print_rank_0(\n                \"ZeRO stage 2/3 and the PipelineModule are incompatible, please set 'pipe_parallel_size' to 0 instead\"\n            )\n            exit()\n        if neox_args.pipe_parallel_size > 1:\n            print_rank_0(\n                \"ZeRO stage 2/3 and pipeline paralleism are not supported simultaneously\"\n            )\n            exit()\n        if neox_args.model_parallel_size > 1:\n            print_rank_0(\n                \"ZeRO stage 2/3 and model paralleism are not currently supported simultaneously\"\n            )\n            exit()\n\n    with deepspeed.zero.Init(\n        config_dict_or_path=neox_args.deepspeed_config\n    ) if neox_args.zero_stage == 3 else nullcontext() as gs:\n        model = GPT2ModelPipe(\n            neox_args=neox_args,\n            num_tokentypes=0,\n            parallel_output=True if neox_args.train_impl != \"rm\" else False,\n            topology=mpu.get_topology(),\n            use_cache=use_cache,\n        )\n\n    ### soft prompt tuning stuff ###\n    if neox_args.soft_prompt_tuning is not None and neox_args.soft_prompt_tuning.get(\n        \"enabled\", False\n    ):\n        soft_prompt = SoftEmbedding(\n            neox_args,\n            wte=getattr(model, \"0\").word_embeddings,\n            n_tokens=neox_args.soft_prompt_tuning.get(\"n_tokens\", 10),\n            init_string=neox_args.soft_prompt_tuning.get(\"init_string\", \"\"),\n            init_range=neox_args.soft_prompt_tuning.get(\"init_range\", 0.5),\n        )\n        model.insert_layers(\n            layers=soft_prompt, idx=1\n        )  # insert the soft prompt layer directly after the word embeddings\n\n        # freeze everything but the soft prompt\n        for name, param in model.named_parameters():\n            if not \"soft_embedding\" in name:\n                param.requires_grad = False\n\n    if not neox_args.is_pipe_parallel:\n        # Export PipeParallel model to nn.Sequential model to avoid the overhead of deepspeed's pipe parallel training\n        model = model.to_sequential()\n\n    neox_args.use_mup = old_use_mup\n\n    if neox_args.use_mup:\n        try:\n            import mup\n        except ModuleNotFoundError:\n            print(\"Please install mup https://github.com/microsoft/mup\")\n            raise Exception\n\n        base_shapes = f\"{neox_args.base_shapes_file}.{torch.distributed.get_rank()}\"\n\n        if neox_args.save_base_shapes:\n            save_base_shapes(neox_args, base_shapes, use_cache)\n\n        mup.set_base_shapes(model, base_shapes)\n\n        # Call the mup replacement init functions on the model now that set_base_shapes has given each weight a .infshape attribute\n        mup_weights_reinit(neox_args, model)\n\n    if neox_args.deepspeed:\n        # DeepSpeed handles CUDA, FP16, and DDP components.\n        return model\n    else:\n        raise ValueError(\"Must be using deepspeed to run neox\")\n\n\ndef get_optimizer(model, neox_args, dummy=False):\n    \"\"\"Set up the optimizer.\"\"\"\n    if neox_args.no_load_optim and neox_args.deepspeed:\n        # Required to have something so...\n        dummy = True\n        neox_args.optimizer = {\"params\": {\"lr\": 0.0}}\n        neox_args.optimizer_type = \"adam\"\n    elif neox_args.no_load_optim:\n        return None, None\n\n    if neox_args.optimizer is None:\n        print_rank_0(\n            f\"ERROR: Optimizer is None. Either set the optimizer dict in your config (if training) or set no_load_optim in your config (if inference)\"\n        )\n        exit()\n    # Build parameter groups (weight decay and non-decay).\n    param_groups = get_params_for_weight_decay_optimization(model, neox_args)\n    print_rank_0(\n        f'Configuring Optimizer type: {neox_args.optimizer_type} with params: {neox_args.optimizer[\"params\"]}'\n    )\n\n    # Add model parallel attribute if it is not set.\n    for param_group in param_groups:\n        for param in param_group[\"params\"]:\n            if not hasattr(param, \"model_parallel\"):\n                param.model_parallel = False\n\n    # Filter out params that don't require a grad (for soft prompt tuning, etc.)\n    _param_groups = []\n    for param_group in param_groups:\n        trainable_params = [p for p in param_group[\"params\"] if p.requires_grad]\n        if dummy:\n            trainable_params = [trainable_params[0]]  # just take the first one\n        param_group[\"params\"] = trainable_params\n        _param_groups.append(param_group)\n        if dummy:\n            # Only need one.\n            break\n    param_groups = _param_groups\n\n    # If we're using mup, then the optimizer must be adam or sgd\n    assert not neox_args.use_mup or (\n        neox_args.optimizer_type.lower() == \"adam\"\n        or neox_args.optimizer_type.lower() == \"sgd\"\n    ), f\"If use_mup == True, you must specify either the adam or sgd optimizers. You passed: {neox_args.optimizer_type.lower()}\"\n\n    if neox_args.optimizer_type.lower() in [\"cpu_adam\", \"cpu_torch_adam\"]:\n        if neox_args.optimizer == \"cpu_torch_adam\":\n            cpu_adam_optimizer = torch.optim.Adam\n        else:\n            from deepspeed.ops.adam import DeepSpeedCPUAdam\n\n            cpu_adam_optimizer = DeepSpeedCPUAdam\n        optimizer = cpu_adam_optimizer(\n            param_groups,\n            weight_decay=neox_args.weight_decay,\n            **neox_args.optimizer[\"params\"],\n        )\n    elif neox_args.optimizer_type.lower() == \"onebitadam\":\n        assert neox_args.deepspeed\n        optimizer = None\n        # onebitadam needs to be instantiated within the deepspeed engine to work :|\n    elif neox_args.optimizer_type.lower() == \"sm3\":\n        from .optimizers import SM3\n\n        optimizer = SM3(param_groups, **neox_args.optimizer[\"params\"])\n    elif neox_args.optimizer_type.lower() == \"madgrad_wd\":\n        from .optimizers import madgrad_wd\n\n        optimizer = madgrad_wd(\n            param_groups,\n            weight_decay=neox_args.weight_decay,\n            **neox_args.optimizer[\"params\"],\n        )\n    elif neox_args.optimizer_type.lower() == \"lion\":\n        # if we want the deepspeed zero lion...megatron lion will throw DeepSpeed Error\n        if neox_args.zero_optimization[\"stage\"] != 0:\n            from deepspeed.ops.lion import FusedLion\n\n            lion_optimizer = FusedLion\n        # if not zero\n        else:\n            from .optimizers import Lion\n\n            lion_optimizer = Lion\n\n        optimizer = lion_optimizer(\n            param_groups,\n            weight_decay=neox_args.weight_decay,\n            **neox_args.optimizer[\"params\"],\n        )\n    elif neox_args.optimizer_type.lower() == \"adam\":\n        # Use Adam\n        if neox_args.use_mup:\n            try:\n                from mup import MuAdam\n\n                adam_optimizer = MuAdam\n            except ModuleNotFoundError:\n                print(\"Please install mup https://github.com/microsoft/mup\")\n                raise Exception\n        else:\n            if neox_args.use_bnb_optimizer:\n                try:\n                    import bitsandbytes as bnb\n\n                    adam_optimizer = bnb.optim.Adam8bit\n                except ModuleNotFoundError:\n                    print(\n                        \"Please install bitsandbytes following https://github.com/facebookresearch/bitsandbytes.\"\n                    )\n                    raise Exception\n            else:\n                try:\n                    # default to apex as it's slightly faster\n                    from apex.optimizers import FusedAdam as Adam\n                except ImportError:\n                    # if apex isn't installed, use deepspeed's FusedAdam\n                    print(\n                        \"WARNING: APEX not installed - defaulting to deepspeed's fused adam\"\n                    )\n                    from deepspeed.ops.adam import FusedAdam as Adam\n                adam_optimizer = Adam\n        optimizer = adam_optimizer(\n            param_groups,\n            weight_decay=neox_args.weight_decay,\n            **neox_args.optimizer[\"params\"],\n        )\n    elif neox_args.optimizer_type.lower() == \"sgd\":\n        try:\n            from mup import MuSGD\n        except ModuleNotFoundError:\n            print(\"Please install mup https://github.com/microsoft/mup\")\n            raise Exception\n        optimizer = MuSGD(\n            param_groups,\n            weight_decay=neox_args.weight_decay,\n            **neox_args.optimizer[\"params\"],\n        )\n    else:\n        raise ValueError(f\"Optimizer type {neox_args.optimizer_type} not recognized\")\n\n    if neox_args.deepspeed:\n        # fp16 wrapper is not required for DeepSpeed.\n        return optimizer, param_groups\n    else:\n        raise ValueError(\"Must be using deepspeed to run neox\")\n\n\ndef get_learning_rate_scheduler(optimizer, neox_args):\n    \"\"\"Build the learning rate scheduler.\"\"\"\n    if (neox_args.no_load_optim) and not neox_args.deepspeed:\n        # TODO: this should be configured as a separate arg\n        return None\n    if neox_args.deepspeed and neox_args.optimizer_type.lower() == \"onebitadam\":\n        print_rank_0(\n            \"WARNING: onebitadam requires the lr scheduler be built by deepspeed - \"\n            \"Make sure one is added to your deepspeed config\"\n        )\n        return None\n\n    # Add linear learning rate scheduler.\n    if neox_args.lr_decay_iters is not None:\n        num_iters = neox_args.lr_decay_iters\n    elif neox_args.lr_decay_fraction is not None:\n        num_iters = math.floor(neox_args.train_iters * neox_args.lr_decay_fraction)\n    else:\n        num_iters = neox_args.train_iters\n    num_iters = max(1, num_iters)\n    init_step = 0\n    warmup_iter = neox_args.warmup * num_iters\n    lr_scheduler = AnnealingLR(\n        optimizer,\n        start_lr=neox_args.lr,\n        warmup_iter=warmup_iter,\n        total_iters=num_iters,\n        decay_style=neox_args.lr_decay_style,\n        last_iter=init_step,\n        min_lr=neox_args.min_lr,\n        use_checkpoint_lr_scheduler=neox_args.use_checkpoint_lr_scheduler,\n        override_lr_scheduler=neox_args.override_lr_scheduler,\n        use_mup=neox_args.use_mup,\n    )\n\n    return lr_scheduler\n\n\ndef setup_model_and_optimizer(neox_args, use_cache=False, iteration=None):\n    \"\"\"Setup memory profiler\"\"\"\n    if neox_args.memory_profiling:\n        torch.cuda.memory._record_memory_history(\n            True,\n            # keep a maximum 100,000 alloc/free events from before the snapshot\n            trace_alloc_max_entries=100000,\n            trace_alloc_record_context=True,\n        )\n\n    \"\"\"Setup model and optimizer.\"\"\"\n    needs_reference_model = (\n        (\n            (neox_args.train_impl == \"dpo\")\n            and (neox_args.precompute_model_name is None)\n            and (not neox_args.dpo_reference_free)\n        )\n        or (\n            (neox_args.train_impl == \"kto\")\n            and (neox_args.precompute_model_name is None)\n        )\n        or ((neox_args.train_impl == \"reinforce\") and (neox_args.kl_div_beta > 0.0))\n    )\n    model = get_model(neox_args=neox_args, use_cache=use_cache)\n    if needs_reference_model:\n        reference_model = get_model(neox_args=neox_args, use_cache=use_cache)\n    else:\n        reference_model = None\n    optimizer, param_groups = get_optimizer(model=model, neox_args=neox_args)\n    lr_scheduler = get_learning_rate_scheduler(optimizer=optimizer, neox_args=neox_args)\n    if neox_args.deepspeed and needs_reference_model:\n        # Need an optimizer & lr_scheduler so make a very small one to keep deepspeed happy...\n        ref_optimizer, ref_param_groups = get_optimizer(\n            model=reference_model, neox_args=neox_args, dummy=True\n        )\n        ref_lr_scheduler = get_learning_rate_scheduler(\n            optimizer=ref_optimizer, neox_args=neox_args\n        )\n    else:\n        ref_optimizer, ref_param_groups, ref_lr_scheduler = None, None, None\n    if neox_args.deepspeed:\n        print_rank_0(\"DeepSpeed is enabled.\")\n        _model_params = param_groups if optimizer is None else None\n        _lr_scheduler = lr_scheduler\n\n        model, optimizer, _, lr_scheduler = deepspeed.initialize(\n            model=model,\n            optimizer=optimizer,\n            args=neox_args,\n            lr_scheduler=_lr_scheduler,\n            dist_init_required=False,\n            model_parameters=_model_params,\n            # Need to remove the below so that it doesn't conflict with --deepspeed_config required by autotuning\n            # config_params=neox_args.deepspeed_config,\n            mpu=mpu if not neox_args.is_pipe_parallel else None,\n        )\n        if needs_reference_model:\n            reference_model, _, _, _ = deepspeed.initialize(\n                model=reference_model,\n                optimizer=ref_optimizer,\n                args=neox_args,\n                lr_scheduler=ref_lr_scheduler,\n                dist_init_required=False,\n                model_parameters=ref_param_groups,\n                mpu=mpu if not neox_args.is_pipe_parallel else None,\n            )\n        mark_norms_for_sequence_parallel_grad_sync(model, neox_args)\n        model.total_params = get_total_params(model.module)\n        print_rank_0(f' > total params: {\"{:,}\".format(model.total_params)}')\n\n        if neox_args.is_pipe_parallel:\n            model.set_has_attention_mask(True)\n            if neox_args.curriculum_learning:\n                curr_scheduler = CurriculumScheduler(neox_args.curriculum_learning)\n                if iteration is not None and iteration > 0:\n                    curr_scheduler.update_difficulty(iteration)\n            else:\n                curr_scheduler = None\n            model.set_batch_fn(\n                partial(\n                    get_batch_pipe, neox_args=neox_args, curr_scheduler=curr_scheduler\n                )\n            )\n        else:\n            model.module.set_batch_fn(\n                partial(get_batch_sequential, neox_args=neox_args)\n            )\n\n    else:\n        raise ValueError(\"Must be using deepspeed to run neox\")\n\n    if neox_args.load is not None:\n        neox_args.iteration = load_checkpoint(\n            neox_args=neox_args,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            iteration=iteration,\n        )\n        if needs_reference_model:\n            _ = load_checkpoint(\n                neox_args=neox_args,\n                model=reference_model,\n                optimizer=ref_optimizer,\n                lr_scheduler=ref_lr_scheduler,\n                iteration=iteration,\n            )\n            reference_model.eval()\n        print_rank_0(\n            f\"Loading checkpoint and starting from iteration {neox_args.iteration}\"\n        )\n    else:\n        neox_args.iteration = 0\n\n    # need this for correct lr scheduling resume from ckpt\n    # but it will not exist if this is being called for inference\n    if lr_scheduler is not None:\n        lr_scheduler.optimizer = model.optimizer\n\n    return model, optimizer, lr_scheduler, reference_model\n\n\ndef backward_step(neox_args, timers, optimizer, model, loss):\n    \"\"\"Backward step.\"\"\"\n\n    # Backward pass.\n    timers(\"backward-backward\").start()\n    if neox_args.deepspeed:\n        model.backward(loss)\n    else:\n        raise ValueError(\"Must be using deepspeed to run neox\")\n    timers(\"backward-backward\").stop()\n\n    if neox_args.deepspeed:\n        # DeepSpeed backward propagation already addressed all reduce communication.\n        # Reset the timer to avoid breaking timer logs below.\n        timers(\"backward-allreduce\").reset()\n    else:\n        raise ValueError(\"Must be using deepspeed to run neox\")\n\n\ndef train_step(\n    neox_args,\n    timers,\n    data_iterator,\n    model,\n    optimizer,\n    lr_scheduler,\n    reference_model=None,\n):\n    \"\"\"Single training step.\"\"\"\n    # Pipeline parallelism schedules forward/backward/step\n    if neox_args.is_pipe_parallel:\n        reduced_loss = train_step_pipe(\n            neox_args=neox_args, timers=timers, model=model, data_iterator=data_iterator\n        )\n        reduce_metrics = reduced_loss\n        if (\n            neox_args.memory_profiling\n            and neox_args.iteration >= neox_args.profile_step_start\n            and neox_args.iteration <= neox_args.profile_step_stop\n            and torch.distributed.get_rank() == 0\n        ):\n            save_snapshot(neox_args)\n    else:\n        losses = []\n        metric_dicts = defaultdict(list)\n        for _ in range(neox_args.gradient_accumulation_steps):\n            # Forward model for one step.\n            timers(\"forward\").start()\n            loss, metric_dict = forward_step(\n                neox_args=neox_args,\n                timers=timers,\n                data_iterator=data_iterator,\n                model=model,\n                is_train=True,\n                reference_model=reference_model,\n            )\n            timers(\"forward\").stop()\n            losses.append(loss)\n            for key in metric_dict.keys():\n                metric_dicts[key].append(metric_dict[key])\n            # Calculate gradients, reduce across processes, and clip.\n            if (\n                neox_args.profile\n                and neox_args.iteration >= neox_args.profile_step_start\n                and neox_args.iteration <= neox_args.profile_step_stop\n            ):\n                torch.cuda.nvtx.range_push(f\"Backward pass\")\n            timers(\"backward\").start()\n            backward_step(\n                neox_args=neox_args,\n                timers=timers,\n                optimizer=optimizer,\n                model=model,\n                loss=loss,\n            )\n            timers(\"backward\").stop()\n            if (\n                neox_args.profile\n                and neox_args.iteration >= neox_args.profile_step_start\n                and neox_args.iteration <= neox_args.profile_step_stop\n            ):\n                torch.cuda.nvtx.range_pop()\n            # Update parameters.\n            if (\n                neox_args.profile\n                and neox_args.iteration >= neox_args.profile_step_start\n                and neox_args.iteration <= neox_args.profile_step_stop\n            ):\n                torch.cuda.nvtx.range_push(f\"Optimizer step\")\n\n            timers(\"optimizer\").start()\n            if neox_args.deepspeed:\n                model.step()\n            else:\n                raise ValueError(\"Must be using deepspeed to run neox\")\n            timers(\"optimizer\").stop()\n            if (\n                neox_args.profile\n                and neox_args.iteration >= neox_args.profile_step_start\n                and neox_args.iteration <= neox_args.profile_step_stop\n            ):\n                torch.cuda.nvtx.range_pop()\n            if (\n                neox_args.profile\n                and neox_args.iteration >= neox_args.profile_step_start\n                and neox_args.iteration <= neox_args.profile_step_stop\n                and torch.distributed.get_rank() == 0\n            ):\n                save_snapshot(neox_args)\n        # reduces metrics across machines for logging\n        reduce_metrics = {\n            key: reduce_losses(metric_dicts[key]).mean() for key in metric_dicts.keys()\n        }\n        reduce_metrics[\"lm_loss\"] = reduce_losses(losses).mean()\n\n    if neox_args.precision == \"fp16\" and model.optimizer.overflow:\n        skipped_iter = 1\n    else:\n        skipped_iter = 0\n\n    collect_loss_for_unit_test(reduce_metrics[\"lm_loss\"])\n    return reduce_metrics, skipped_iter\n\n\ndef train_step_pipe(neox_args, timers, model, data_iterator):\n    \"\"\"Single training step with DeepSpeed's pipeline parallel engine.\"\"\"\n\n    assert neox_args.deepspeed\n    loss = model.train_batch(data_iter=data_iterator)\n    loss_dict = {\"lm_loss\": loss}\n    # Don't break Megatron's timers because we changed code paths.\n    for t in [\n        \"forward\",\n        \"backward\",\n        \"allreduce\",\n        \"optimizer\",\n        \"batch generator\",\n        \"data loader\",\n    ]:\n        timers(t).reset()\n    return loss_dict\n\n\ndef is_save_iter(neox_args, iteration):\n    if neox_args.extra_save_iters and iteration in neox_args.extra_save_iters:\n        return True\n\n    if neox_args.checkpoint_factor:\n        if neox_args.checkpoint_scale == \"linear\":\n            assert float(\n                neox_args.checkpoint_factor\n            ).is_integer(), \"checkpoint_factor must be a whole number when using linear checkpoint_scale\"\n            return iteration % neox_args.checkpoint_factor == 0\n        elif neox_args.checkpoint_scale == \"log\":\n            # Check if iteration is a power of checkpoint_factor\n            assert neox_args.checkpoint_factor > 1\n            power = 1\n            while power < iteration + 1:\n                if int(power) == iteration:\n                    return True\n                power *= neox_args.checkpoint_factor\n            return False\n\n    return False\n\n\ndef train(\n    neox_args,\n    timers,\n    model,\n    reference_model,\n    optimizer,\n    lr_scheduler,\n    train_data_iterator,\n    valid_data_iterator,\n):\n    \"\"\"Train the model function.\"\"\"\n\n    # Turn on training mode which enables dropout.\n    model.train()\n\n    # Tracking loss.\n    total_loss_dict = {}\n\n    # Iterations.\n    iteration = neox_args.iteration\n\n    timers(\"interval time\").start()\n    report_memory_flag = True\n\n    # get noise scale logger (if neox_args.log_gradient_noise_scale is True)\n    noise_scale_logger = get_noise_scale_logger(neox_args)\n\n    # to monitor if we've skipped many iterations in a row and trigger an early exit\n    overflow_monitor = OverflowMonitor(optimizer)\n\n    if neox_args.profile:\n        schedule = torch.profiler.schedule(\n            wait=neox_args.profile_step_start,\n            warmup=1,\n            active=neox_args.profile_step_stop - neox_args.profile_step_start,\n        )\n        prof = torch.profiler.profile(\n            schedule=schedule,\n            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n                neox_args.tensorboard_dir\n            ),\n            record_shapes=True,\n            profile_memory=True,\n            with_flops=True,\n            with_modules=True,\n            with_stack=True,\n        )\n        prof.start()\n    while iteration < neox_args.train_iters:\n        if neox_args.profile:\n            prof.step()\n        if neox_args.profile and iteration == neox_args.profile_step_start:\n            torch.cuda.cudart().cudaProfilerStart()\n        loss_dict, skipped_iter = train_step(\n            neox_args=neox_args,\n            timers=timers,\n            data_iterator=train_data_iterator,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            reference_model=reference_model,\n        )\n        if neox_args.profile and iteration == neox_args.profile_step_stop:\n            torch.cuda.cudart().cudaProfilerStop()\n            prof.stop()\n        iteration += 1\n        neox_args.iteration = iteration\n        if neox_args.precision == \"fp16\":\n            overflow_monitor.check(skipped_iter)  # check for repeated overflow\n        if neox_args.log_gradient_noise_scale:  # log noise scale if applicable\n            noise_scale_logger.update()\n\n        # get learning rate (if present) - if doing soft prompt tuning + pipe parallel, you\n        # may have no tunable parameters on a specific rank\n        if optimizer.param_groups:\n            lr = optimizer.param_groups[0].get(\"lr\", 0)\n        else:\n            lr = 0\n\n        # Logging.\n        report_memory_flag = training_log(\n            neox_args=neox_args,\n            timers=timers,\n            loss_dict=loss_dict,\n            total_loss_dict=total_loss_dict,\n            learning_rate=lr,\n            iteration=iteration,\n            loss_scale=optimizer.cur_scale if neox_args.precision == \"fp16\" else None,\n            report_memory_flag=report_memory_flag,\n            skipped_iter=skipped_iter,\n            model=model,\n            optimizer=optimizer,\n            noise_scale_logger=noise_scale_logger,\n        )\n\n        # Checkpointing\n        if neox_args.save and is_save_iter(neox_args, iteration):\n            save_checkpoint(\n                neox_args=neox_args,\n                iteration=iteration,\n                model=model,\n                optimizer=optimizer,\n                lr_scheduler=lr_scheduler,\n            )\n        # Evaluation\n        if (\n            neox_args.eval_interval\n            and iteration % neox_args.eval_interval == 0\n            and neox_args.do_valid\n        ):\n            prefix = \"iteration {}\".format(iteration)\n            evaluate_and_print_results(\n                neox_args=neox_args,\n                prefix=prefix,\n                forward_step_func=forward_step,\n                data_iterator=valid_data_iterator,\n                model=model,\n                iteration=iteration,\n                verbose=False,\n                timers=timers,\n                reference_model=reference_model,\n            )\n\n        if neox_args.exit_interval and iteration % neox_args.exit_interval == 0:\n            torch.distributed.barrier()\n            time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            rank = torch.distributed.get_rank()\n            print_rank_0(\n                \"rank: {} | time: {} | exiting the program at iteration {}\".format(\n                    rank, time_str, iteration\n                )\n            )\n            sys.exit()\n\n    return iteration\n\n\ndef evaluate(\n    neox_args,\n    forward_step_fn,\n    data_iterator,\n    model,\n    verbose=False,\n    timers=None,\n    reference_model=None,\n):\n    \"\"\"Evaluation.\n    neox_args: NeoX Arguments\n    forward_step_fn: function with args `neox_args, timers,\n                    data_iterator & model that will run a forward pass on the model\n    data_iterator: Iterator that iterates over batches of data. Should return data in the form:\n                    {'text': np.array([tokens], dtype=np.int64)}\n                    where the size of the array is the model's context size + 1\n                    (`get_batch` transforms it into inputs / labels)\n    \"\"\"\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    losses = []\n    metric_dicts = defaultdict(list)\n    if neox_args.char_level_ppl:\n        data_iterator = CharCounter(data_iterator, neox_args.tokenizer)\n\n    with torch.no_grad():\n        iteration = 0\n        while iteration < neox_args.eval_iters:\n            iteration += 1\n            if verbose and iteration % neox_args.log_interval == 0:\n                print_rank_0(\n                    \"Evaluating iter {}/{}\".format(iteration, neox_args.eval_iters)\n                )\n\n            # although we're not accumulating gradients here, we count one iter as train_batch_size_per_gpu * g.a.s\n            # to be consistent with deepspeed's pipe parallel engine\n            # since pipe parallel already takes gradient_accumulation_steps into account - default to 1 here if pipe parallel is true\n            for _ in range(\n                1\n                if neox_args.is_pipe_parallel\n                else neox_args.gradient_accumulation_steps\n            ):\n                # Forward evaluation\n                loss, metric_dict = forward_step_fn(\n                    model=model,\n                    data_iterator=data_iterator,\n                    neox_args=neox_args,\n                    timers=timers,\n                    reference_model=reference_model,\n                )\n                losses.append(loss)\n                for key in metric_dict.keys():\n                    metric_dicts[key].append(metric_dict[key])\n            # When contiguous memory optimizations are enabled, the buffers\n            # allocated by the optimizations are deallocated during backward pass\n            # in the absence of backward pass the buffers should be reset after each\n            # forward pass\n            if neox_args.deepspeed and neox_args.deepspeed_activation_checkpointing:\n                deepspeed.checkpointing.reset()\n\n    # reduces losses across processes for logging & run eval harness tasks\n    eval_results = {\"lm_loss\": reduce_losses(losses).mean().item()}\n    for key in metric_dicts.keys():\n        eval_results[key] = reduce_losses(metric_dicts[key]).mean().item()\n    eval_results[\"lm_loss_ppl\"] = math.exp(eval_results[\"lm_loss\"])\n\n    if neox_args.char_level_ppl:\n        # calculate character level perplexity, if specified\n        # if neox_args.char_level_ppl:\n        # unwrap the data_iterator\n        tokens_per_char = data_iterator.tokens_per_char()\n        print_rank_0(f\"Counting chars took {data_iterator.total_time} seconds\")\n\n        data_iterator = data_iterator.data_iterator\n        eval_results[\"lm_loss_char_lvl_ppl\"] = math.exp(\n            eval_results[\"lm_loss\"] * tokens_per_char\n        )\n\n    if neox_args.eval_tasks:\n        from eval_tasks import run_eval_harness\n\n        eval_results.update(\n            run_eval_harness(\n                model, forward_step_fn, neox_args, eval_tasks=neox_args.eval_tasks\n            ).get(\"results\")\n        )\n    # Move model back to the train mode.\n    model.train()\n    return eval_results\n\n\ndef collect_loss_for_unit_test(lm_ss):\n    # Logic moved to separate function to allow tracking in unit tests with unittest.mock.patch\n    pass\n\n\ndef evaluate_and_print_results(\n    neox_args,\n    prefix,\n    forward_step_func,\n    data_iterator,\n    model,\n    iteration,\n    verbose=False,\n    timers=None,\n    chart_name=\"validation\",\n    reference_model=None,\n):\n    \"\"\"Helper function to evaluate and dump results on screen.\"\"\"\n    total_loss_dict = evaluate(\n        neox_args=neox_args,\n        forward_step_fn=forward_step_func,\n        data_iterator=data_iterator,\n        model=model,\n        verbose=verbose,\n        timers=timers,\n        reference_model=reference_model,\n    )\n    string = f\" {chart_name} results at {prefix} | \"\n    for k, v in total_loss_dict.items():\n        if isinstance(v, dict):\n            if neox_args.eval_tasks and \"results\" in v:\n                v = v[\"results\"]\n                print(v)\n            for k2, v2 in v.items():\n                k3 = \"_\".join([k, k2])\n                string += f\"{k3} value: {v2:.6E} | \"\n                tb_wandb_log(\n                    f\"{chart_name}/{k3}\",\n                    v2,\n                    iteration,\n                    use_wandb=neox_args.use_wandb,\n                    tensorboard_writer=neox_args.tensorboard_writer,\n                    comet_experiment=neox_args.comet_experiment,\n                )\n        else:\n            string += f\"{k} value: {v:.6E} | \"\n            tb_wandb_log(\n                f\"{chart_name}/{k}\",\n                v,\n                iteration,\n                use_wandb=neox_args.use_wandb,\n                tensorboard_writer=neox_args.tensorboard_writer,\n                comet_experiment=neox_args.comet_experiment,\n            )\n\n    length = len(string) + 1\n    print_rank_0(\"-\" * length)\n    print_rank_0(string)\n    print_rank_0(\"-\" * length)\n\n\ndef save_snapshot(neox_args):\n    assert (\n        neox_args.memory_profiling_path is not None\n    ), \"Must pass memory_profiling_path config arg to use profiling\"\n    snapshot = torch.cuda.memory._snapshot()\n    snapshot_path = os.path.join(neox_args.memory_profiling_path)\n    if not os.path.exists(snapshot_path):\n        os.makedirs(snapshot_path)\n    with open(os.path.join(snapshot_path, \"mem_snapshot.pickle\"), \"wb\") as f:\n        dump(snapshot, f)\n",
        "megatron/utils.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"General utilities.\"\"\"\nimport os\nimport sys\nimport re\nimport time\nimport socket\nfrom typing import Dict, List\n\nimport requests\n\ntry:\n    import wandb\nexcept ModuleNotFoundError:\n    pass\n\nimport torch\n\nfrom deepspeed.launcher.runner import fetch_hostfile, parse_inclusion_exclusion\nfrom deepspeed.runtime.bf16_optimizer import BF16_Optimizer\n\nfrom megatron import print_rank_0\nfrom megatron import mpu\n\nfrom collections import deque\n\n\ndef reduce_losses(losses):\n    \"\"\"Reduce a tensor of losses across all GPUs.\"\"\"\n    reduced_losses = torch.cat([loss.clone().detach().view(1) for loss in losses])\n    torch.distributed.all_reduce(reduced_losses)\n    reduced_losses = reduced_losses / torch.distributed.get_world_size()\n    return reduced_losses\n\n\ndef report_memory(name):\n    \"\"\"Simple GPU memory report.\"\"\"\n    mega_bytes = 1024.0 * 1024.0\n    string = name + \" memory (MB)\"\n    string += \" | allocated: {}\".format(torch.cuda.memory_allocated() / mega_bytes)\n    string += \" | max allocated: {}\".format(\n        torch.cuda.max_memory_allocated() / mega_bytes\n    )\n    string += \" | reserved: {}\".format(torch.cuda.memory_reserved() / mega_bytes)\n    string += \" | max reserved: {}\".format(\n        torch.cuda.max_memory_reserved() / mega_bytes\n    )\n    print_rank_0(string)\n\n\ndef get_attn_mask(seq_length, device, sliding_window_width):\n    \"\"\"\n    Get triangular attention mask for a given sequence length / device.\n    \"\"\"\n    # lower triangular attention mask\n    mask = torch.tril(torch.ones((1, seq_length, seq_length), device=device)).view(\n        1, 1, seq_length, seq_length\n    )\n    # get rid of lower diagonals than the sliding window width, if a value was provided\n    if sliding_window_width is not None:\n        mask = torch.triu(mask, diagonal=-sliding_window_width)\n\n    # convert to binary\n    return mask < 0.5\n\n\ndef get_ltor_masks_and_position_ids(\n    data,\n    eod_token,\n    eod_mask_loss=False,\n    sliding_window_width=None,\n):\n    \"\"\"Build masks and position id for left to right model.\"\"\"\n\n    # Extract batch size and sequence length.\n    batch_size, seq_length = data.size()\n\n    # Attention mask (lower triangular).\n    attention_mask = get_attn_mask(\n        seq_length=seq_length,\n        device=data.device,\n        sliding_window_width=sliding_window_width,\n    )\n\n    # Loss mask.\n    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)\n    if eod_mask_loss:\n        loss_mask[data == eod_token] = 0.0\n\n    # Position ids.\n    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)\n    position_ids = position_ids.unsqueeze(0).expand_as(data)\n\n    return attention_mask, loss_mask, position_ids\n\n\ndef local_rank():\n    \"\"\"Local rank of process\"\"\"\n    local_rank = os.environ.get(\"LOCAL_RANK\")\n\n    if local_rank is None:\n        local_rank = os.environ.get(\"SLURM_LOCALID\")\n\n    if local_rank is None:\n        print(\n            \"utils.local_rank() environment variable LOCAL_RANK not set, defaulting to 0\",\n            flush=True,\n        )\n        local_rank = 0\n    return int(local_rank)\n\n\ndef is_bnb_available():\n    \"\"\"True if bitsandbytes optimizers are available\"\"\"\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n\n\ndef is_local_main():\n    \"\"\"True if is the local main process\"\"\"\n    return local_rank() == 0\n\n\ndef is_mp_rank_0():\n    \"\"\"True if mp rank == 0\"\"\"\n    return mpu.get_model_parallel_rank() == 0\n\n\ndef get_wandb_api_key(neox_args):\n    \"\"\"Get Weights and Biases API key from ENV or .netrc file. Otherwise return None\"\"\"\n    if \"WANDB_LOCAL\" in os.environ:\n        return \"LOCAL\"\n    if \"WANDB_API_KEY\" in os.environ:\n        return os.environ[\"WANDB_API_KEY\"]\n\n    wandb_token = requests.utils.get_netrc_auth(neox_args.wandb_host)\n\n    if wandb_token is not None:\n        return wandb_token[1]\n\n\ndef init_wandb(neox_args):\n    # Wandb. (one worker per machine)\n    if neox_args.use_wandb == False:\n        return\n\n    if not neox_args.wandb_init_all_ranks:\n        use_wandb = is_local_main() and (\n            get_wandb_api_key(neox_args=neox_args) is not None\n        )\n        neox_args.update_value(\"use_wandb\", use_wandb)\n    if neox_args.use_wandb:\n        group_name = neox_args.wandb_group\n        run_name = neox_args.wandb_run_name\n        try:\n            wandb.init(\n                project=neox_args.wandb_project,\n                group=group_name,\n                name=run_name,\n                save_code=False,\n                force=False,\n                entity=neox_args.wandb_team,\n            )\n        except wandb.UsageError as e:\n            neox_args.update_value(\"use_wandb\", False)\n            print(e)\n            print(\n                \"Skipping wandb. Execute `wandb login` on local or main node machine to enable.\",\n                flush=True,\n            )\n        wandb.config.update(neox_args.all_config)\n\n\ndef obtain_resource_pool(\n    hostfile_path, include_arg, exclude_arg\n) -> Dict[str, List[int]]:\n    \"\"\"\n    Get dict of `resource_pool[hostname] = [list of GPU ranks]` using hostfile, include and exclude args.\n    Modified from: `deepspeed.launcher.runner.main`\n    \"\"\"\n    resource_pool = fetch_hostfile(hostfile_path)\n    if not resource_pool:\n        resource_pool = {}\n        device_count = torch.cuda.device_count()\n        if device_count == 0:\n            raise RuntimeError(\"Unable to proceed, no GPU resources available\")\n        resource_pool[\"localhost\"] = device_count\n\n    active_resources = parse_inclusion_exclusion(\n        resource_pool, include_arg, exclude_arg\n    )\n    return active_resources\n\n\ndef natural_sort(l):\n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n    return sorted(l, key=alphanum_key)\n\n\ndef ddb(rank=0):\n    \"\"\"\n    Distributed Debugger that will insert a py debugger on rank `rank` and\n    pause all other distributed processes until debugging is complete.\n    :param rank:\n    \"\"\"\n    if torch.distributed.get_rank() == rank:\n        from pdb import Pdb\n\n        pdb = Pdb(skip=[\"torch.distributed.*\"])\n        pdb.set_trace(sys._getframe().f_back)\n    torch.distributed.barrier()\n\n\nclass Timer:\n    \"\"\"Timer.\"\"\"\n\n    def __init__(self, name):\n        self.name_ = name\n        self.elapsed_ = 0.0\n        self.started_ = False\n        self.start_time = time.time()\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        assert not self.started_, \"timer has already been started\"\n        torch.cuda.synchronize()\n        self.start_time = time.time()\n        self.started_ = True\n\n    def stop(self):\n        \"\"\"Stop the timer.\"\"\"\n        assert self.started_, \"timer is not started\"\n        torch.cuda.synchronize()\n        self.elapsed_ += time.time() - self.start_time\n        self.started_ = False\n\n    def reset(self):\n        \"\"\"Reset timer.\"\"\"\n        self.elapsed_ = 0.0\n        self.started_ = False\n\n    def elapsed(self, reset=True):\n        \"\"\"Calculate the elapsed time.\"\"\"\n        started_ = self.started_\n        # If the timing in progress, end it first.\n        if self.started_:\n            self.stop()\n        # Get the elapsed time.\n        elapsed_ = self.elapsed_\n        # Reset the elapsed time\n        if reset:\n            self.reset()\n        # If timing was in progress, set it back.\n        if started_:\n            self.start()\n        return elapsed_\n\n\nclass Timers:\n    \"\"\"Group of timers.\"\"\"\n\n    def __init__(self, use_wandb, tensorboard_writer, comet_experiment):\n        self.timers = {}\n        self.use_wandb = use_wandb\n        self.tensorboard_writer = tensorboard_writer\n        self.comet_experiment = comet_experiment\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = Timer(name)\n        return self.timers[name]\n\n    def write(self, names, iteration, normalizer=1.0, reset=False):\n        \"\"\"Write timers to a tensorboard writer\"\"\"\n        # currently when using add_scalars,\n        # torch.utils.add_scalars makes each timer its own run, which\n        # pollutes the runs list, so we just add each as a scalar\n        assert normalizer > 0.0\n        for name in names:\n            value = self.timers[name].elapsed(reset=reset) / normalizer\n\n            if self.tensorboard_writer:\n                self.tensorboard_writer.add_scalar(f\"timers/{name}\", value, iteration)\n\n            if self.use_wandb:\n                wandb.log({f\"timers/{name}\": value}, step=iteration)\n\n            if self.comet_experiment:\n                self.comet_experiment.__internal_api__log_metric__(\n                    f\"timers/{name}\",\n                    value,\n                    framework=\"gpt-neox\",\n                    step=iteration,\n                )\n\n    def log(self, names, normalizer=1.0, reset=True):\n        \"\"\"Log a group of timers.\"\"\"\n        assert normalizer > 0.0\n        string = \"time (ms)\"\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n            string += \" | {}: {:.2f}\".format(name, elapsed_time)\n        if torch.distributed.is_initialized():\n            if torch.distributed.get_rank() == 0:\n                print(string, flush=True)\n        else:\n            print(string, flush=True)\n\n\ndef expand_attention_types(attention_config, num_layers):\n    \"\"\"\n    Expands an `attention_config` list in the following format:\n\n        [\n        [['attention_type_1', ..., `attention_type_n`], 12]\n        ]\n\n    to a flattened list of length `num_layers`.\n\n    :param params_list:\n    :return:\n    \"\"\"\n    # if only strings are found in the config, we assume it's already expanded\n    if all([isinstance(i, str) for i in attention_config]):\n        return attention_config\n    newlist = []\n    for item in attention_config:\n        # instead of specifying a number - we can specify 'all' to extend this pattern across all layers\n        if item[1] == \"all\":\n            assert num_layers % len(item[0]) == 0, (\n                f\"Number of layers ({num_layers}) is not divisible by the length \"\n                f\"of pattern: {item[0]}\"\n            )\n            return item[0] * (num_layers // len(item[0]))\n        for _ in range(item[1]):\n            newlist.extend(item[0])\n    return newlist\n\n\nclass OverflowMonitor:\n\n    \"\"\"\n    Checks if the past n iterations have been skipped due to overflow, and exits\n    training if that happens.\n    \"\"\"\n\n    def __init__(self, optimizer, n=50):\n        self.optimizer = optimizer\n        self.n = n\n        self.history = deque(maxlen=n)\n        self.bf16 = isinstance(optimizer, BF16_Optimizer)\n\n    def check(self, skipped):\n        if self.bf16:\n            return\n        self.history.append(skipped)\n        if (\n            self.optimizer.overflow\n            and len(self.history) == self.n\n            and all(self.history)\n        ):\n            raise Exception(\n                f\"Skipped {self.n} iterations in a row due to Overflow - Exiting training.\"\n            )\n\n\ndef get_noise_scale_logger(neox_args):\n    if neox_args.log_gradient_noise_scale:\n        if neox_args.zero_stage >= 1:\n            raise NotImplementedError(\n                \"Gradient Noise Scale logging does not work with zero stage 2+, as the \"\n                \"gradients are distributed across ranks.\"\n            )\n        noise_scale_logger = GradientNoiseScale(\n            model=model,\n            batch_size_small=neox_args.train_batch_size,\n            n_batches=neox_args.gradient_noise_scale_n_batches,\n            cpu_offload=neox_args.gradient_noise_scale_cpu_offload,\n            neox_args=neox_args,\n            mpu=mpu,\n        )\n    else:\n        noise_scale_logger = None\n    return noise_scale_logger\n\n\ndef get_total_params(model):\n    # Print number of parameters.\n    if mpu.get_data_parallel_rank() == 0:\n        params = sum([p.nelement() for p in model.parameters()])\n        print(\n            \" > number of parameters on model parallel rank {}: {}\".format(\n                mpu.get_model_parallel_rank(), params\n            ),\n            flush=True,\n        )\n    else:\n        params = 0\n\n    total_n_parameters = torch.tensor([params]).cuda(torch.cuda.current_device())\n    torch.distributed.all_reduce(total_n_parameters)\n    total_n_parameters = total_n_parameters.item()\n    return total_n_parameters\n\n\ndef setup_for_inference_or_eval(use_cache=True, overwrite_values=None, input_args=None):\n    \"\"\"\n    Initializes the model for evaluation or inference (doesn't load optimizer states, etc.) from command line args.\n\n    use_cache: bool\n        Whether to use key value caching in inference.\n    overwrite_values: dict\n        Optional Values to overwrite in the model config.\n    \"\"\"\n\n    from megatron.neox_arguments import NeoXArgs\n    from megatron.initialize import initialize_megatron\n    from megatron.training import setup_model_and_optimizer\n\n    _overwrite_values = {\n        \"checkpoint_activations\": False,\n        \"partition_activations\": False,\n        \"no_load_optim\": True,\n        \"optimizer\": None,  # prevent loading optimizer (no_load_optim alone won't work)\n        \"zero_optimization\": None,  # disable zero optimization (won't be used in inference, and loading zero optimizer can cause errors)\n    }\n    if overwrite_values:\n        _overwrite_values.update(overwrite_values)\n    neox_args = NeoXArgs.consume_neox_args(\n        overwrite_values=_overwrite_values, input_args=input_args\n    )\n    neox_args.configure_distributed_args()\n    neox_args.build_tokenizer()\n\n    if neox_args.load is None:\n        raise ValueError(\"`load` parameter must be supplied to load a model`\")\n\n    # initialize wandb\n    init_wandb(neox_args=neox_args)\n\n    # initialize megatron\n    initialize_megatron(neox_args)\n\n    # set up model and load checkpoint.\n    model, _, _, _ = setup_model_and_optimizer(\n        neox_args=neox_args,\n        use_cache=use_cache,\n        iteration=neox_args.iteration,\n    )  # we use setup_model_and_optimizer instead of get_model in order to initialize deepspeed\n    print_rank_0(\"Finished loading model\")\n\n    model.module.inference_mode(use_cache=use_cache)\n    return model, neox_args\n\n\nclass CharCounter:\n    \"\"\"\n    Wraps the data_iterator to count the number of characters in a batch\n    \"\"\"\n\n    def __init__(self, data_iterator, tokenizer):\n        self.tokenizer = tokenizer\n        self.data_iterator = data_iterator\n        self.char_count = 0\n        self.batch_count = 0\n        self.token_count = 0\n        self.total_time = 0\n\n    def tokens_per_char(self):\n        return self.token_count / self.char_count\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        start = time.time()\n        batch = self.data_iterator.__next__()\n        for b in batch[\"text\"]:\n            self.token_count += len(b)\n            self.char_count += len(self.tokenizer.detokenize(b.tolist()))\n        self.batch_count += 1\n        end = time.time()\n        self.total_time += end - start\n        return batch\n\n\ndef _kernel_make_viewless_tensor(inp, requires_grad):\n    \"\"\"Make a viewless tensor.\n\n    View tensors have the undesirable side-affect of retaining a reference\n    to the originally-viewed tensor, even after manually setting the '.data'\n    field. This method creates a new tensor that links to the old tensor's\n    data, without linking the viewed tensor, referenced via the '._base'\n    field.\n    \"\"\"\n    out = torch.empty(\n        (1,),\n        dtype=inp.dtype,\n        device=inp.device,\n        requires_grad=requires_grad,\n    )\n    out.data = inp.data\n    return out\n\n\nclass MakeViewlessTensor(torch.autograd.Function):\n    \"\"\"\n    Autograd function to make a viewless tensor.\n\n    This function should be used in cases where the computation graph needs\n    to be propagated, but we only want a viewless tensor (e.g.,\n    ParallelTransformer's hidden_states). Call this function by passing\n    'keep_graph = True' to 'make_viewless_tensor()'.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inp, requires_grad):\n        return _kernel_make_viewless_tensor(inp, requires_grad)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output, None\n\n\ndef make_viewless_tensor(inp, requires_grad, keep_graph):\n    \"\"\"\n    Entry-point for creating viewless tensors.\n\n    This method should be used, rather than calling 'MakeViewlessTensor'\n    or '_kernel_make_viewless_tensor' directly. This method acts as a\n    switch for determining if an autograd function or a regular method\n    should be used to create the tensor.\n    \"\"\"\n\n    # return tensor as-is, if not a 'view'\n    if inp._base is None:\n        return inp\n\n    # create viewless tensor\n    if keep_graph:\n        return MakeViewlessTensor.apply(inp, requires_grad)\n    else:\n        return _kernel_make_viewless_tensor(inp, requires_grad)\n",
        "post-training/dpo_data.py": "\"\"\"\nhttps://github.com/huggingface/alignment-handbook/blob/main/scripts/run_dpo.py\nadapted to just grab the dataset\n\"\"\"\nimport os\nfrom alignment import (\n    DataArguments,\n    DPOConfig,\n    H4ArgumentParser,\n    ModelArguments,\n    apply_chat_template,\n    decontaminate_humaneval,\n    get_checkpoint,\n    get_datasets,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n    get_tokenizer,\n    is_adapter_model,\n)\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import AutoTokenizer\n\nimport jsonlines\n\n###############\n# Load datasets\n###############\nraw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")\nraw_datasets = DatasetDict(\n    {\n        \"train\": raw_datasets[\"train_prefs\"],\n        \"test\": raw_datasets[\"test_prefs\"],\n    }\n)\ncolumn_names = list(raw_datasets[\"train\"].features)\n\n#####################################\n# Load tokenizer and process datasets\n#####################################\ntruncation_side = (\n    \"left\"  # Truncate from left to ensure we don't lose labels in final turn\n)\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\n#####################\n# Apply chat template\n#####################\nraw_datasets = raw_datasets.map(\n    apply_chat_template,\n    fn_kwargs={\n        \"tokenizer\": tokenizer,\n        \"task\": \"dpo\",\n        \"auto_insert_empty_system_msg\": True,\n    },\n    desc=\"Formatting comparisons with prompt template\",\n)\n\n##########################\n# Decontaminate benchmarks\n##########################\nnum_raw_train_samples = len(raw_datasets[\"train\"])\nraw_datasets = raw_datasets.filter(\n    decontaminate_humaneval,\n    fn_kwargs={\"text_column\": \"text_chosen\"},\n    batched=True,\n    batch_size=10_000,\n    num_proc=1,\n    desc=\"Decontaminating HumanEval samples\",\n)\nnum_filtered_train_samples = num_raw_train_samples - len(raw_datasets[\"train\"])\nprint(\n    f\"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples / num_raw_train_samples * 100:.2f}%) samples from the training set.\"\n)\n###############\n# Length filter\n###############\n# Since the alignment handbook recipes call for a max token limit of 1024...\nnum_filtered_train_samples = len(raw_datasets[\"train\"])\n\n\ndef length_filter(example):\n    return (len(tokenizer.apply_chat_template(example[\"chosen\"])) < 1024) and (\n        len(tokenizer.apply_chat_template(example[\"rejected\"])) < 1024\n    )\n\n\nnum_length_filtered_train_samples = num_filtered_train_samples - len(\n    raw_datasets[\"train\"]\n)\nprint(\n    f\"Length Filtered {num_length_filtered_train_samples} ({num_length_filtered_train_samples / num_filtered_train_samples * 100:.2f}%) samples from the training set.\"\n)\n# get directory of the python script\ndir_path = os.path.dirname(os.path.realpath(__file__))\nfor split in [\"train\", \"test\"]:\n    with open(os.path.join(dir_path, f\"dpo_{split}_filtered.jsonl\"), \"w\") as f:\n        writer = jsonlines.Writer(f)\n        for item in raw_datasets[split]:\n            # add empty system messages\n            item[\"chosen\"] = [{\"role\": \"system\", \"content\": \"\"}] + item[\"chosen\"]\n            item[\"rejected\"] = [{\"role\": \"system\", \"content\": \"\"}] + item[\"rejected\"]\n            writer.write(item)\n",
        "post-training/llama_data.py": "import os\n\nfrom datasets import load_dataset, DatasetDict\n\nimport jsonlines\n\n###############\n# Load datasets\n###############\nraw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")\n# convert to just train and test, not necessary but it looks better\nraw_datasets = DatasetDict(\n    {\n        \"train\": raw_datasets[\"train_prefs\"],\n        \"test\": raw_datasets[\"test_prefs\"],\n    }\n)\nos.makedirs(os.path.join(\"data\", \"pairwise\"), exist_ok=True)\nfor split in [\"train\", \"test\"]:\n    with open(\n        os.path.join(\"data\", \"pairwise\", f\"llama3_dpo_{split}_filtered.jsonl\"), \"w\"\n    ) as f:\n        writer = jsonlines.Writer(f)\n        for item in raw_datasets[split]:\n            item[\"chosen\"] = item[\"chosen\"]\n            item[\"rejected\"] = item[\"rejected\"]\n            writer.write(item)\nos.makedirs(os.path.join(\"data\", \"sft\"), exist_ok=True)\nfor split in [\"train\", \"test\"]:\n    with open(\n        os.path.join(\"data\", \"sft\", f\"llama3_sft_{split}_filtered.jsonl\"), \"w\"\n    ) as f:\n        writer = jsonlines.Writer(f)\n        for item in raw_datasets[split]:\n            item[\"messages\"] = item[\"chosen\"]\n            writer.write(item)\nos.makedirs(os.path.join(\"data\", \"kto\"), exist_ok=True)\nfor split in [\"train\", \"test\"]:\n    with open(\n        os.path.join(\"data\", \"kto\", f\"llama3_kto_{split}_filtered.jsonl\"), \"w\"\n    ) as f:\n        writer = jsonlines.Writer(f)\n        for item in raw_datasets[split]:\n            item[\"messages\"] = item[\"chosen\"]\n            item[\"reward\"] = 1\n            writer.write(item)\n            item[\"messages\"] = item[\"rejected\"]\n            item[\"reward\"] = -1\n            writer.write(item)\n",
        "post-training/online_data_example_llama3.py": "import socket\nimport threading\nimport datasets\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport requests\nimport pickle\nfrom collections import defaultdict\nimport time\n\n\ndef get_positive_score(scores):\n    \"Extract value associated with a positive sentiment from pipeline's output\"\n    return dict(map(lambda x: tuple(x.values()), scores))[\"POSITIVE\"]\n\n\ndef http_bot(url, pload):\n    for i in range(10):\n        try:\n            headers = {\"User-Agent\": \"vLLM Client\"}\n            response = requests.post(url, headers=headers, json=pload, stream=True)\n            data = response.json()\n            return data\n        except Exception as e:\n            # give it a few seconds to recover\n            time.sleep(5)\n            print(e)\n            continue\n    raise Exception(\"Failed to connect to server\")\n\n\ndef threaded_data_gatherer(\n    prefix,\n    max_completion_len,\n    tokenizer,\n    model_name,\n    num_completions,\n    i,\n    dp_idx,\n    data_to_send,\n    rm_pipeline,\n):\n    pload = {\n        \"temperature\": 1.0,\n        \"max_tokens\": 0,\n        \"stop\": \"<|eot_id|>\",\n        \"stream\": False,\n        \"model\": model_name,\n        \"prompt\": \"\",\n        \"n\": num_completions,\n    }\n    # Grab tokens...\n    prefix_tokens = tokenizer.encode(prefix)\n    prompt = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"Please write a mildly negative movie review starting with \"\n                + prefix,\n            }\n        ],\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    prompt_tokens = tokenizer.encode(prompt)\n    pload[\"max_tokens\"] = max_completion_len - len(prefix_tokens)\n    pload[\"prompt\"] = prompt + prefix\n    completions = http_bot(f\"http://localhost:{8000+dp_idx}/v1/completions\", pload)\n    completions = [completion[\"text\"].strip() for completion in completions[\"choices\"]]\n\n    def reward_fn(samples, **kwargs):\n        sentiments = list(map(get_positive_score, rm_pipeline(samples)))\n        return sentiments\n\n    rewards = reward_fn([prefix + \" \" + completion for completion in completions])\n    if i == 0 and dp_idx == 0:\n        print(completions)\n    completions = [\n        tokenizer.encode(completion + \"<|eot_id|>\") for completion in completions\n    ]\n    data_to_send.append(\n        {\"prefix\": prompt_tokens, \"completions\": completions, \"rewards\": rewards}\n    )\n\n\ndef data_generator(\n    bs_per_dp,\n    dataset,\n    tokenizer,\n    model_name,\n    max_prefix_len,\n    max_completion_len,\n    num_completions,\n    dp_idx,\n    dp_size,\n    tp_size,\n    rm_pipeline,\n):\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.bind(\n        (\"localhost\", 10000 + dp_idx)\n    )  # only one data loader per data parallel group\n    split_counter = defaultdict(lambda: dp_idx)\n    while True:\n        server.listen(1)\n        conn, addr = server.accept()\n        split = conn.recv(4096).decode()\n        if split == \"valid\":\n            split = \"unsupervised\"\n        data_to_send = list()\n        threads = list()\n        for i in range(bs_per_dp):\n            prefix = \" \".join(\n                dataset[split][split_counter[split]][\"text\"].split()[:5]\n            )  # grab a few words to prompt it...\n            split_counter[split] = (split_counter[split] + dp_size) % len(\n                dataset[split]\n            )\n            threads.append(\n                threading.Thread(\n                    target=threaded_data_gatherer,\n                    args=(\n                        prefix,\n                        max_completion_len,\n                        tokenizer,\n                        model_name,\n                        num_completions,\n                        i,\n                        dp_idx,\n                        data_to_send,\n                        rm_pipeline,\n                    ),\n                )\n            )\n            threads[-1].start()\n        for thread in threads:\n            thread.join()\n        conn.send(pickle.dumps(data_to_send))\n        conn.close()\n        print(\n            f\"Sent data to {dp_idx} for {split} split at iter {split_counter[split]}...\"\n        )\n\n\nif __name__ == \"__main__\":\n    sentiment_fn = pipeline(\n        \"sentiment-analysis\",\n        \"lvwerra/distilbert-imdb\",\n        top_k=2,\n        truncation=True,\n        batch_size=256,\n        device=\"cpu\",\n    )\n    dataset = datasets.load_dataset(\"imdb\")\n    threads = list()\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n    for i in range(2):\n        threads.append(\n            threading.Thread(\n                target=data_generator,\n                args=(\n                    64,  # bs_per_dp\n                    dataset,  # dataset\n                    tokenizer,  # tokenizer\n                    \"meta-llama/Meta-Llama-3-8B-Instruct\",  # model_name\n                    128,  # max_prefix_len\n                    256,  # max_completion_len\n                    4,  # num_completions\n                    i,  # dp_idx\n                    2,  # dp_size\n                    4,  # tp_size\n                    sentiment_fn,  # rm_pipeline\n                ),\n            )\n        )\n        threads[-1].start()\n    for thread in threads:\n        thread.join()\n",
        "prepare_data.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tools.datasets.corpora import prepare_dataset, DATA_DOWNLOADERS\nimport argparse\n\nTOKENIZER_CHOICES = [\n    \"HFGPT2Tokenizer\",\n    \"HFTokenizer\",\n    \"GPT2BPETokenizer\",\n    \"CharLevelTokenizer\",\n    \"TiktokenTokenizer\",\n    \"SPMTokenizer\",\n]\nDATASET_CHOICES = [i for i in DATA_DOWNLOADERS.keys() if i != \"pass\"]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\"Download & preprocess neox datasets\")\n    parser.add_argument(\n        \"dataset\",\n        nargs=\"?\",\n        default=\"enwik8\",\n        help=\"name of dataset to download.\",\n        choices=DATASET_CHOICES,\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--tokenizer\",\n        default=\"GPT2BPETokenizer\",\n        choices=TOKENIZER_CHOICES,\n        help=f'Type of tokenizer to use - choose from {\", \".join(TOKENIZER_CHOICES)}',\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--data-dir\",\n        default=None,\n        help=f\"Directory to which to download datasets / tokenizer \"\n        f\"files - defaults to ./data\",\n    )\n    parser.add_argument(\n        \"-v\", \"--vocab-file\", default=None, help=f\"Tokenizer vocab file (if required)\"\n    )\n    parser.add_argument(\n        \"-m\", \"--merge-file\", default=None, help=f\"Tokenizer merge file (if required)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--force-redownload\",\n        dest=\"force_redownload\",\n        default=False,\n        action=\"store_true\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    prepare_dataset(\n        dataset_name=args.dataset,\n        tokenizer_type=args.tokenizer,\n        data_dir=args.data_dir,\n        vocab_file=args.vocab_file,\n        merge_file=args.merge_file,\n        force_redownload=args.force_redownload,\n    )\n",
        "tests/__init__.py": "",
        "tests/common.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport time\nimport shutil\nimport itertools\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\nfrom deepspeed.accelerator import get_accelerator\n\nimport pytest\nfrom _pytest.outcomes import Skipped\nfrom _pytest.fixtures import FixtureLookupError, FixtureFunctionMarker\nimport random\nimport train\n\nimport torch\n\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\nimport torch.multiprocessing as mp\nfrom yaml import load\n\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\nfrom copy import deepcopy\nimport deepspeed\n\nTEST_CHECKPOINT_DIR = \"test_checkpoint\"\nTEST_LOG_DIR = \"test_logs\"\nTEST_TENSORBOARD_DIR = \"test_tensorboard\"\n\n# Worker timeout *after* the first worker has completed.\nDEEPSPEED_UNIT_WORKER_TIMEOUT = 120\nDEEPSPEED_TEST_TIMEOUT = 600\n\n\ndef get_xdist_worker_id():\n    xdist_worker = os.environ.get(\"PYTEST_XDIST_WORKER\", None)\n    if xdist_worker is not None:\n        xdist_worker_id = xdist_worker.replace(\"gw\", \"\")\n        return int(xdist_worker_id)\n    return None\n\n\ndef get_master_port():\n    master_port = os.environ.get(\"DS_TEST_PORT\", \"29503\")\n    xdist_worker_id = get_xdist_worker_id()\n    if xdist_worker_id is not None:\n        master_port = str(int(master_port) + xdist_worker_id)\n    return master_port\n\n\n_num_gpus = None\n\n\ndef set_accelerator_visible():\n    cuda_visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n    xdist_worker_id = get_xdist_worker_id()\n    if xdist_worker_id is None:\n        xdist_worker_id = 0\n    if cuda_visible is None:\n        # CUDA_VISIBLE_DEVICES is not set, discover it using accelerator specific command instead\n        if get_accelerator().device_name() == \"cuda\":\n            if is_rocm_pytorch():\n                rocm_smi = subprocess.check_output([\"rocm-smi\", \"--showid\"])\n                gpu_ids = filter(\n                    lambda s: \"GPU\" in s, rocm_smi.decode(\"utf-8\").strip().split(\"\\n\")\n                )\n                num_accelerators = len(list(gpu_ids))\n            else:\n                nvidia_smi = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"])\n                num_accelerators = len(nvidia_smi.decode(\"utf-8\").strip().split(\"\\n\"))\n        elif get_accelerator().device_name() == \"xpu\":\n            clinfo = subprocess.check_output([\"clinfo\"])\n            lines = clinfo.decode(\"utf-8\").strip().split(\"\\n\")\n            num_accelerators = 0\n            for line in lines:\n                match = re.search(\"Device Type.*GPU\", line)\n                if match:\n                    num_accelerators += 1\n        elif get_accelerator().device_name() == \"npu\":\n            npu_smi = subprocess.check_output([\"npu-smi\", \"info\", \"-l\"])\n            num_accelerators = int(\n                npu_smi.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\":\")[1].strip()\n            )\n        else:\n            assert get_accelerator().device_name() == \"cpu\"\n            cpu_sockets = int(\n                subprocess.check_output(\n                    'cat /proc/cpuinfo | grep \"physical id\" | sort -u | wc -l',\n                    shell=True,\n                )\n            )\n            num_accelerators = cpu_sockets\n\n        cuda_visible = \",\".join(map(str, range(num_accelerators)))\n\n    # rotate list based on xdist worker id, example below\n    # wid=0 -> ['0', '1', '2', '3']\n    # wid=1 -> ['1', '2', '3', '0']\n    # wid=2 -> ['2', '3', '0', '1']\n    # wid=3 -> ['3', '0', '1', '2']\n    dev_id_list = cuda_visible.split(\",\")\n    dev_id_list = dev_id_list[xdist_worker_id:] + dev_id_list[:xdist_worker_id]\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(dev_id_list)\n\n\ndef count_gpus():\n    global _num_gpus\n    if _num_gpus is None:\n        import subprocess\n\n        nvidia_smi = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"])\n        _num_gpus = len(nvidia_smi.decode(\"utf-8\").strip().split(\"\\n\"))\n    return _num_gpus\n\n\ndef set_cuda_visibile():\n    cuda_visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n    xdist_worker_id = get_xdist_worker_id()\n    if xdist_worker_id is None:\n        xdist_worker_id = 0\n    if cuda_visible is None:\n        # CUDA_VISIBLE_DEVICES is not set, discover it from nvidia-smi instead\n        import subprocess\n\n        nvidia_smi = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"])\n        num_gpus = len(nvidia_smi.decode(\"utf-8\").strip().split(\"\\n\"))\n        cuda_visible = \",\".join(map(str, range(num_gpus)))\n\n    # rotate list based on xdist worker id, example below\n    # wid=0 -> ['0', '1', '2', '3']\n    # wid=1 -> ['1', '2', '3', '0']\n    # wid=2 -> ['2', '3', '0', '1']\n    # wid=3 -> ['3', '0', '1', '2']\n    dev_id_list = cuda_visible.split(\",\")\n    dev_id_list = dev_id_list[xdist_worker_id:] + dev_id_list[:xdist_worker_id]\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(dev_id_list)\n\n\ndef get_root_directory():\n    return Path(__file__).parents[1]\n\n\ndef get_config_directory():\n    return get_root_directory() / \"configs\"\n\n\ndef get_configs_with_path(configs):\n    return [str(get_config_directory() / cfg) for cfg in configs]\n\n\ndef clear_test_dirs():\n    log_dir = os.path.join(get_root_directory(), TEST_LOG_DIR)\n    if os.path.isdir(log_dir):\n        shutil.rmtree(log_dir)\n\n    checkpoint_dir = os.path.join(get_root_directory(), TEST_CHECKPOINT_DIR)\n    if os.path.isdir(checkpoint_dir):\n        shutil.rmtree(checkpoint_dir)\n\n    tensorboard_dir = os.path.join(get_root_directory(), TEST_TENSORBOARD_DIR)\n    if os.path.isdir(tensorboard_dir):\n        shutil.rmtree(tensorboard_dir)\n\n\nclass DistributedExec(ABC):\n    \"\"\"\n    Base class for distributed execution of functions/methods. Contains common\n    methods needed for DistributedTest and DistributedFixture.\n    \"\"\"\n\n    world_size = 2\n    backend = get_accelerator().communication_backend_name()\n    init_distributed = True\n    set_dist_env = True\n    requires_cuda_env = True\n    reuse_dist_env = False\n    _pool_cache = {}\n    exec_timeout = DEEPSPEED_TEST_TIMEOUT\n\n    @abstractmethod\n    def run(self):\n        ...\n\n    def __call__(self, request=None):\n        self._fixture_kwargs = self._get_fixture_kwargs(request, self.run)\n        world_size = self.world_size\n        if self.requires_cuda_env and not get_accelerator().is_available():\n            pytest.skip(\"only supported in accelerator environments.\")\n\n        if isinstance(world_size, int):\n            world_size = [world_size]\n        for procs in world_size:\n            self._launch_procs(procs)\n\n    def _get_fixture_kwargs(self, request, func):\n        if not request:\n            return {}\n        # Grab fixture / parametrize kwargs from pytest request object\n        fixture_kwargs = {}\n        params = inspect.getfullargspec(func).args\n        params.remove(\"self\")\n        for p in params:\n            try:\n                fixture_kwargs[p] = request.getfixturevalue(p)\n            except FixtureLookupError:\n                pass  # test methods can have kwargs that are not fixtures\n        return fixture_kwargs\n\n    def _launch_procs(self, num_procs):\n        # Verify we have enough accelerator devices to run this test\n        if (\n            get_accelerator().is_available()\n            and get_accelerator().device_count() < num_procs\n        ):\n            pytest.skip(\n                f\"Skipping test because not enough GPUs are available: {num_procs} required, {get_accelerator().device_count()} available\"\n            )\n\n        mp.set_start_method(\"spawn\", force=True)\n\n        # Create process pool or use cached one\n        master_port = None\n        if self.reuse_dist_env:\n            if num_procs not in self._pool_cache:\n                self._pool_cache[num_procs] = mp.Pool(processes=num_procs)\n                master_port = get_master_port()\n            pool = self._pool_cache[num_procs]\n        else:\n            pool = mp.Pool(processes=num_procs)\n            master_port = get_master_port()\n\n        # Run the test\n        args = [(local_rank, num_procs, master_port) for local_rank in range(num_procs)]\n        skip_msgs_async = pool.starmap_async(self._dist_run, args)\n\n        try:\n            skip_msgs = skip_msgs_async.get(self.exec_timeout)\n        except mp.TimeoutError:\n            # Shortcut to exit pytest in the case of a hanged test. This\n            # usually means an environment error and the rest of tests will\n            # hang (causing super long unit test runtimes)\n            pytest.exit(\"Test hanged, exiting\", returncode=0)\n\n        # Tear down distributed environment and close process pools\n        self._close_pool(pool, num_procs)\n\n        # If we skipped a test, propagate that to this process\n        if any(skip_msgs):\n            assert len(set(skip_msgs)) == 1, \"Multiple different skip messages received\"\n            pytest.skip(skip_msgs[0])\n\n    def _dist_run(self, local_rank, num_procs, master_port):\n        skip_msg = \"\"\n        if not dist.is_initialized():\n            \"\"\"Initialize deepspeed.comm and execute the user function.\"\"\"\n            if self.set_dist_env:\n                os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n                os.environ[\"MASTER_PORT\"] = str(master_port)\n                os.environ[\"LOCAL_RANK\"] = str(local_rank)\n                # NOTE: unit tests don't support multi-node so local_rank == global rank\n                os.environ[\"RANK\"] = str(local_rank)\n                # In case of multiprocess launching LOCAL_SIZE should be same as WORLD_SIZE\n                # DeepSpeed single node launcher would also set LOCAL_SIZE accordingly\n                os.environ[\"LOCAL_SIZE\"] = str(num_procs)\n                os.environ[\"WORLD_SIZE\"] = str(num_procs)\n\n            # turn off NCCL logging if set\n            os.environ.pop(\"NCCL_DEBUG\", None)\n\n            if get_accelerator().is_available():\n                set_accelerator_visible()\n\n            if get_accelerator().is_available():\n                get_accelerator().set_device(local_rank)\n\n            if self.init_distributed:\n                deepspeed.init_distributed(dist_backend=self.backend)\n                dist.barrier()\n\n        try:\n            self.run(**self._fixture_kwargs)\n        except BaseException as e:\n            if isinstance(e, Skipped):\n                skip_msg = e.msg\n            else:\n                raise e\n\n        return skip_msg\n\n    def _dist_destroy(self):\n        if (dist is not None) and dist.is_initialized():\n            dist.barrier()\n            dist.destroy_process_group()\n\n    def _close_pool(self, pool, num_procs, force=False):\n        if force or not self.reuse_dist_env:\n            msg = pool.starmap(self._dist_destroy, [() for _ in range(num_procs)])\n            pool.close()\n            pool.join()\n\n\nclass DistributedFixture(DistributedExec):\n    \"\"\"\n    Implementation that extends @pytest.fixture to allow for distributed execution.\n    This is primarily meant to be used when a test requires executing two pieces of\n    code with different world sizes.\n\n    There are 2 parameters that can be modified:\n        - world_size: int = 2 -- the number of processes to launch\n        - backend: Literal['nccl','mpi','gloo'] = 'nccl' -- which backend to use\n\n    Features:\n        - able to call pytest.skip() inside fixture\n        - can be reused by multiple tests\n        - can accept other fixtures as input\n\n    Limitations:\n        - cannot use @pytest.mark.parametrize\n        - world_size cannot be modified after definition and only one world_size value is accepted\n        - any fixtures used must also be used in the test that uses this fixture (see example below)\n        - return values cannot be returned. Passing values to a DistributedTest\n          object can be achieved using class_tmpdir and writing to file (see example below)\n\n    Usage:\n        - must implement a run(self, ...) method\n        - fixture can be used by making the class name input to a test function\n\n    Example:\n        @pytest.fixture(params=[10,20])\n        def regular_pytest_fixture(request):\n            return request.param\n\n        class distributed_fixture_example(DistributedFixture):\n            world_size = 4\n\n            def run(self, regular_pytest_fixture, class_tmpdir):\n                assert int(os.environ[\"WORLD_SIZE\"]) == self.world_size\n                local_rank = os.environ[\"LOCAL_RANK\"]\n                print(f\"Rank {local_rank} with value {regular_pytest_fixture}\")\n                with open(os.path.join(class_tmpdir, f\"{local_rank}.txt\"), \"w\") as f:\n                    f.write(f\"{local_rank},{regular_pytest_fixture}\")\n\n        class TestExample(DistributedTest):\n            world_size = 1\n\n            def test(self, distributed_fixture_example, regular_pytest_fixture, class_tmpdir):\n                assert int(os.environ[\"WORLD_SIZE\"]) == self.world_size\n                for rank in range(4):\n                    with open(os.path.join(class_tmpdir, f\"{rank}.txt\"), \"r\") as f:\n                        assert f.read() == f\"{rank},{regular_pytest_fixture}\"\n    \"\"\"\n\n    is_dist_fixture = True\n\n    # These values are just placeholders so that pytest recognizes this as a fixture\n    _pytestfixturefunction = FixtureFunctionMarker(scope=\"function\", params=None)\n    __name__ = \"\"\n\n    def __init__(self):\n        assert isinstance(\n            self.world_size, int\n        ), \"Only one world size is allowed for distributed fixtures\"\n        self.__name__ = type(self).__name__\n        _pytestfixturefunction = FixtureFunctionMarker(\n            scope=\"function\", params=None, name=self.__name__\n        )\n\n\nclass DistributedTest(DistributedExec):\n    \"\"\"\n    Implementation for running pytest with distributed execution.\n\n    There are 2 parameters that can be modified:\n        - world_size: Union[int,List[int]] = 2 -- the number of processes to launch\n        - backend: Literal['nccl','mpi','gloo'] = 'nccl' -- which backend to use\n\n    Features:\n        - able to call pytest.skip() inside tests\n        - works with pytest fixtures, parametrize, mark, etc.\n        - can contain multiple tests (each of which can be parametrized separately)\n        - class methods can be fixtures (usable by tests in this class only)\n        - world_size can be changed for individual tests using @pytest.mark.world_size(world_size)\n        - class_tmpdir is a fixture that can be used to get a tmpdir shared among\n          all tests (including DistributedFixture)\n\n    Usage:\n        - class name must start with \"Test\"\n        - must implement one or more test*(self, ...) methods\n\n    Example:\n        @pytest.fixture(params=[10,20])\n        def val1(request):\n            return request.param\n\n        @pytest.mark.fast\n        @pytest.mark.parametrize(\"val2\", [30,40])\n        class TestExample(DistributedTest):\n            world_size = 2\n\n            @pytest.fixture(params=[50,60])\n            def val3(self, request):\n                return request.param\n\n            def test_1(self, val1, val2, str1=\"hello world\"):\n                assert int(os.environ[\"WORLD_SIZE\"]) == self.world_size\n                assert all(val1, val2, str1)\n\n            @pytest.mark.world_size(1)\n            @pytest.mark.parametrize(\"val4\", [70,80])\n            def test_2(self, val1, val2, val3, val4):\n                assert int(os.environ[\"WORLD_SIZE\"]) == 1\n                assert all(val1, val2, val3, val4)\n    \"\"\"\n\n    def __init__(self):\n        self.is_dist_test = True\n\n    # Temporary directory that is shared among test methods in a class\n    @pytest.fixture(autouse=True, scope=\"class\")\n    def class_tmpdir(self, tmpdir_factory):\n        fn = tmpdir_factory.mktemp(self.__class__.__name__)\n        return fn\n\n    def run(self, **fixture_kwargs):\n        self._current_test(**fixture_kwargs)\n\n    def __call__(self, request):\n        self._current_test = self._get_current_test_func(request)\n        self._fixture_kwargs = self._get_fixture_kwargs(request, self._current_test)\n\n        if self.requires_cuda_env and not get_accelerator().is_available():\n            pytest.skip(\"only supported in accelerator environments.\")\n\n        # Catch world_size override pytest mark\n        for mark in getattr(request.function, \"pytestmark\", []):\n            if mark.name == \"world_size\":\n                world_size = mark.args[0]\n                break\n        else:\n            world_size = self.world_size\n\n        if isinstance(world_size, int):\n            world_size = [world_size]\n        for procs in world_size:\n            self._launch_procs(procs)\n            time.sleep(0.5)\n\n    def _get_current_test_func(self, request):\n        # DistributedTest subclasses may have multiple test methods\n        func_name = request.function.__name__\n        return getattr(self, func_name)\n\n\ndef get_test_path(filename):\n    curr_path = Path(__file__).parent\n    return str(curr_path.joinpath(filename))\n\n\ndef model_setup(yaml_list=None, param_dict=None, clear_data=True):\n    from megatron.neox_arguments import NeoXArgs\n    from megatron.mpu import destroy_model_parallel\n    from megatron import initialize_megatron\n    from megatron.training import setup_model_and_optimizer\n\n    destroy_model_parallel()  # mpu model parallel contains remaining global vars\n    if clear_data and (\n        not torch.distributed.is_initialized()\n        or torch.distributed.get_world_size() == 1\n        or torch.distributed.get_rank() == 0\n    ):\n        clear_test_dirs()\n\n    overwrite_values = {\n        \"user_script\": str(get_root_directory() / \"train.py\"),\n        \"save\": TEST_CHECKPOINT_DIR,\n        \"load\": TEST_CHECKPOINT_DIR,\n        \"log_dir\": TEST_LOG_DIR,\n        \"tensorboard_dir\": TEST_TENSORBOARD_DIR,\n    }\n\n    # should not both be none\n    assert yaml_list is not None or param_dict is not None\n\n    # initially load config from files as would be the case in deepy.py\n    if yaml_list is not None:\n        args_loaded = NeoXArgs.from_ymls(yaml_list, overwrite_values=overwrite_values)\n    else:\n        p_dict = param_dict.copy()\n        p_dict.update(overwrite_values)\n        args_loaded = NeoXArgs.from_dict(p_dict)\n\n    args_loaded.build_tokenizer()\n\n    initialize_megatron(neox_args=args_loaded)\n    model, optimizer, lr_scheduler = setup_model_and_optimizer(\n        neox_args=args_loaded, use_cache=True\n    )\n    return model, optimizer, lr_scheduler, args_loaded\n\n\ndef simulate_deepy_env(monkeypatch, input_args):\n    from megatron.neox_arguments import NeoXArgs\n\n    monkeypatch.setenv(\"WORLD_SIZE\", \"1\")\n    monkeypatch.setenv(\"RANK\", \"0\")\n    neox_args = NeoXArgs.consume_deepy_args(input_args)\n    deepspeed_main_args = neox_args.get_deepspeed_main_args()\n    return deepspeed_main_args\n\n\ndef save_random_model(input_args, model_dir, train_iters=0):\n    # Save randomly initialised model\n    train_args = {\n        \"do_train\": False,\n        \"train_iters\": train_iters,\n        \"save\": model_dir,\n        \"extra_save_iters\": [train_iters],\n    }\n    train.main(input_args=input_args, overwrite_values=train_args)\n\n\ndef bounded_product(sequence, n=None, seed=None):\n    \"\"\"\n    Returns a shuffled, bounded cartesian product of the input sequence.\n    Designed to cover as wide a range of permutations as possible with a limited number of iterations.\n    Will manifest the whole list in memory, so not suitable for super large sequences.\n\n    :param sequence: iterable\n    :param n: length of returned list\n    :param seed: random seed for reproducibility\n    :return: list\n    \"\"\"\n    p = list(itertools.product(*sequence))\n    if seed is not None:\n        random.seed(seed)\n    random.shuffle(p)\n    return p if n is None else p[:n]\n\n\ndef model_setup_simple(deepspeed_main_args, overwrite_values, iteration=None):\n    from megatron.neox_arguments import NeoXArgs\n    from megatron import initialize_megatron\n    from megatron.training import setup_model_and_optimizer\n\n    neox_args = NeoXArgs.consume_neox_args(\n        input_args=deepspeed_main_args, overwrite_values=overwrite_values\n    )\n    neox_args.configure_distributed_args()\n    neox_args.build_tokenizer()\n    initialize_megatron(neox_args=neox_args)\n    model, optimizer, lr_scheduler = setup_model_and_optimizer(\n        neox_args=neox_args, use_cache=False\n    )\n    return model, optimizer, lr_scheduler, neox_args\n\n\ndef parametrize(\n    params_to_test: dict, max_tests: int = 50, seed: int = None, with_names=True\n):\n    \"\"\"\n    Generates a random sample of max_tests length of all possible combinations of values in\n    `params_to_test`.\n\n    In `params_to_test` you can either specify one value, and all possible settings of that value,\n    or two values separated by a comma, and all possible combinations of those two values in tandem.\n        i.e \"hidden_size,num_heads\": [[768,12], [1024,32], [2048, 64]]\n    so the first item in each list is a value of `hidden_size` and the second a value of `num_heads`\n    this is useful for reducing the size of possible tests for values we know are unlikely to interact beforehand,\n    since the cartesian product can grow very large.\n\n    :param params_to_test: dict of neox params\n    :param max_tests: maximum number of tests to run\n    :param seed: random seed\n    :return: a list of neox param dicts to pass to a parametrized unit test\n    \"\"\"\n    keys, values = zip(*params_to_test.items())\n    ret = []\n    if with_names:\n        experiments = []\n    for p in bounded_product(values, n=max_tests, seed=seed):\n        experiment = dict(zip(keys, p))\n        to_pop = []\n        to_add = {}\n        for k, v in experiment.items():\n            if \",\" in k:\n                keys_split = [i.strip() for i in k.split(\",\")]\n                values_separated = experiment[k]\n                to_pop.append(k)\n                assert len(values_separated) == len(keys_split)\n                new_dict = dict(zip(keys_split, values_separated))\n                to_add.update(new_dict)\n        experiment.update(to_add)\n        for k in to_pop:\n            experiment.pop(k)\n        base = deepcopy(BASE_CONFIG)\n        base.update(experiment)\n        ret.append(base)\n        if with_names:\n            experiments.append(experiment)\n    if with_names:\n        return ret, [dict_repr(d) for d in experiments]\n    return ret\n\n\ndef dict_repr(d):\n    return \" \".join([f\"{str(k)} : {str(v)}\" for k, v in d.items()])\n\n\nbinary = [True, False]\n\nwith open(\"tests/config/test_setup.yml\", \"r\") as f:\n    BASE_CONFIG = load(f, Loader=Loader)\n    print(f\"Base Config:\\n{BASE_CONFIG}\")\n",
        "tests/conftest.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# tests directory-specific settings - this file is run automatically by pytest before any tests are run\n\nimport sys\nimport pytest\nimport os\nfrom os.path import abspath, dirname, join\nimport torch\nimport warnings\n\n# Set this environment variable for the T5 inference unittest(s) (e.g. google/t5-v1_1-small)\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\n# allow having multiple repository checkouts and not needing to remember to rerun\n# 'pip install -e .[dev]' when switching between checkouts and running tests.\ngit_repo_path = abspath(join(dirname(dirname(__file__)), \"src\"))\nsys.path.insert(1, git_repo_path)\n\n\ndef pytest_configure(config):\n    #    config.option.color = \"yes\"\n    #    config.option.durations = 0\n    #    config.option.durations_min = 1\n    config.option.verbose = True\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\"--torch_ver\", default=None, type=str)\n    parser.addoption(\"--cuda_ver\", default=None, type=str)\n\n\ndef validate_version(expected, found):\n    version_depth = expected.count(\".\") + 1\n    found = \".\".join(found.split(\".\")[:version_depth])\n    return found == expected\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef check_environment(pytestconfig):\n    expected_torch_version = pytestconfig.getoption(\"torch_ver\")\n    expected_cuda_version = pytestconfig.getoption(\"cuda_ver\")\n    if expected_torch_version is None:\n        warnings.warn(\n            \"Running test without verifying torch version, please provide an expected torch version with --torch_ver\"\n        )\n    elif not validate_version(expected_torch_version, torch.__version__):\n        pytest.exit(\n            f\"expected torch version {expected_torch_version} did not match found torch version {torch.__version__}\",\n            returncode=2,\n        )\n    if expected_cuda_version is None:\n        warnings.warn(\n            \"Running test without verifying cuda version, please provide an expected cuda version with --cuda_ver\"\n        )\n    elif not validate_version(expected_cuda_version, torch.version.cuda):\n        pytest.exit(\n            f\"expected cuda version {expected_cuda_version} did not match found cuda version {torch.version.cuda}\",\n            returncode=2,\n        )\n\n\n# Override of pytest \"runtest\" for DistributedTest class\n# This hook is run before the default pytest_runtest_call\n@pytest.hookimpl(tryfirst=True)\ndef pytest_runtest_call(item):\n    # We want to use our own launching function for distributed tests\n    if getattr(item.cls, \"is_dist_test\", False):\n        dist_test_class = item.cls()\n        dist_test_class(item._request)\n        item.runtest = lambda: True  # Dummy function so test is not run twice\n\n\n# We allow DistributedTest to reuse distributed environments. When the last\n# test for a class is run, we want to make sure those distributed environments\n# are destroyed.\ndef pytest_runtest_teardown(item, nextitem):\n    if getattr(item.cls, \"reuse_dist_env\", False) and not nextitem:\n        dist_test_class = item.cls()\n        for num_procs, pool in dist_test_class._pool_cache.items():\n            dist_test_class._close_pool(pool, num_procs, force=True)\n\n\n@pytest.hookimpl(tryfirst=True)\ndef pytest_fixture_setup(fixturedef, request):\n    if getattr(fixturedef.func, \"is_dist_fixture\", False):\n        dist_fixture_class = fixturedef.func()\n        dist_fixture_class(request)\n",
        "tests/model/__init__.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "tests/model/test_fused_kernels.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport pytest\nimport torch\n\nfrom transformers import BertTokenizer\nfrom transformers.models.bert.modeling_bert import BertModel\n\nfrom transformers import BertTokenizer, GPT2Tokenizer\nfrom transformers.models.bert.modeling_bert import BertModel\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Model\nfrom megatron.fused_kernels import load\nimport transformers\n\ntransformers.logging.set_verbosity(\n    transformers.logging.FATAL,\n)\n\n\n@pytest.mark.xfail(reason=\"SystemExit: None\")\ndef test_load_fused_kernels():\n    load()\n    try:\n        import scaled_masked_softmax_cuda\n        import scaled_upper_triang_masked_softmax_cuda\n        import fused_rotary_positional_embedding\n        import torch\n\n        print(\"[Success] load_fused_kernels\")\n    except ImportError as e:\n        print(\"[Fail] load_fused_kernels\")\n        raise e\n\n\n@pytest.mark.xfail(reason=\"SystemExit: None\")\ndef test_fused_softmax():\n    load()\n    from megatron.model.fused_softmax import FusedScaleMaskSoftmax, SoftmaxFusionTypes\n    from megatron.model.gpt2_model import (\n        gpt2_attention_mask_func as attention_mask_func,\n    )\n\n    bert = BertModel.from_pretrained(\"bert-base-cased\").cuda().half()\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n    test_text = (\n        \"Hello. How are you? I am fine thank you and you? yes Good. \"\n        \"hi hi hi hi hi hi hi hi hi hi hi hi hi\"  # 32\n    )\n\n    tokens = tokenizer(\n        [test_text] * 4,\n        return_tensors=\"pt\",\n    )\n\n    embedding_output = bert.embeddings(\n        input_ids=tokens[\"input_ids\"].cuda(),\n        position_ids=None,\n        token_type_ids=tokens[\"token_type_ids\"].cuda(),\n        inputs_embeds=None,\n        past_key_values_length=0,\n    )\n\n    # (bsz, 1, 1, seq_len)\n    mask = bert.get_extended_attention_mask(\n        attention_mask=tokens[\"attention_mask\"].cuda(),\n        input_shape=tokens[\"input_ids\"].shape,\n        device=bert.device,\n    )\n    # (bsz, 1, seq_len, seq_len)\n    mask = mask.repeat(1, 1, mask.size()[-1], 1)\n\n    attention = bert.encoder.layer[0].attention.self\n    key_layer = attention.transpose_for_scores(attention.key(embedding_output))\n    query_layer = attention.transpose_for_scores(attention.query(embedding_output))\n\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n    attention_scores /= math.sqrt(key_layer.size()[-1])\n\n    fused_softmax = (\n        FusedScaleMaskSoftmax(\n            input_in_fp16=True,\n            input_in_bf16=False,\n            fusion_type=SoftmaxFusionTypes.general,\n            mask_func=attention_mask_func,\n            scale=None,\n            softmax_in_fp32=False,\n        )\n        .cuda()\n        .half()\n    )\n\n    fused_softmax_output = fused_softmax(\n        attention_scores,\n        (mask != 0),\n    )\n\n    torch_softmax = (\n        FusedScaleMaskSoftmax(\n            input_in_fp16=True,\n            input_in_bf16=False,\n            mask_func=attention_mask_func,\n            fusion_type=SoftmaxFusionTypes.none,\n            scale=None,\n            softmax_in_fp32=False,\n        )\n        .cuda()\n        .half()\n    )\n\n    torch_softmax_output = torch_softmax(\n        attention_scores,\n        (mask != 0),\n    )\n\n    test_result = (fused_softmax_output - torch_softmax_output).abs()\n\n    while test_result.dim() != 1:\n        test_result = test_result.mean(dim=-1)\n\n    diff = test_result.mean(dim=-1)\n\n    if diff <= 1e-3:\n        print(\n            f\"\\n[Success] test_fused_softmax\"\n            f\"\\n > mean_difference={diff}\"\n            f\"\\n > fused_values={fused_softmax_output[-1][-1][-1][:5].tolist()}\"\n            f\"\\n > torch_values={torch_softmax_output[-1][-1][-1][:5].tolist()}\"\n        )\n    else:\n        print(\n            f\"\\n[Fail] test_fused_softmax\"\n            f\"\\n > mean_difference={diff}, \"\n            f\"\\n > fused_values={fused_softmax_output[-1][-1][-1][:5].tolist()}, \"\n            f\"\\n > torch_values={torch_softmax_output[-1][-1][-1][:5].tolist()}\"\n        )\n\n\n@pytest.mark.xfail(reason=\"SystemExit: None\")\ndef test_fused_upper_triangle_mask_softmax():\n    load()\n    from megatron.model.gpt2_model import (\n        gpt2_attention_mask_func as attention_mask_func,\n    )\n    from megatron.model.fused_softmax import FusedScaleMaskSoftmax, SoftmaxFusionTypes\n\n    gpt = GPT2Model.from_pretrained(\"gpt2\").cuda().half()\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    test_text = (\n        \"Hello. How are you? I am fine thank you and you? yes Good. \"\n        \"hi hi hi hi hi hi hi\"  # 24\n    )\n\n    tokens = tokenizer(\n        [test_text] * 4,\n        return_tensors=\"pt\",\n    )\n\n    attention_mask = tokens[\"attention_mask\"].cuda()\n    attention_mask = attention_mask.view(attention_mask.size(0), -1)\n    attention_mask = attention_mask[:, None, None, :]\n    attention_mask = (1.0 - attention_mask) * -10000.0\n    attention_mask = attention_mask.repeat(1, 1, attention_mask.size()[-1], 1)\n    attn = gpt.h[0]\n\n    hidden_states = gpt.wte(tokens[\"input_ids\"].cuda())\n    q, k, v = attn.attn.c_attn(hidden_states).split(768, dim=-1)\n    q = attn.attn._split_heads(q, attn.attn.num_heads, attn.attn.head_dim)\n    k = attn.attn._split_heads(k, attn.attn.num_heads, attn.attn.head_dim)\n    attn_weights = torch.matmul(q, k.transpose(-1, -2))\n\n    sq, sk = q.size(-2), k.size(-2)\n    causal_mask = attn.attn.bias[:, :, sk - sq : sk, :sk].bool()\n    total_mask = ~(causal_mask & (attention_mask == 0))\n    \"\"\"\n    tensor([[[[False,  True,  True,  ...,  True,  True,  True],\n              [False, False,  True,  ...,  True,  True,  True],\n              [False, False, False,  ...,  True,  True,  True],\n              ...,\n              [False, False, False,  ..., False,  True,  True],\n              [False, False, False,  ..., False, False,  True],\n              [False, False, False,  ..., False, False, False]]]\n    \"\"\"\n\n    fused_softmax = (\n        FusedScaleMaskSoftmax(\n            input_in_fp16=True,\n            input_in_bf16=False,\n            mask_func=attention_mask_func,\n            fusion_type=SoftmaxFusionTypes.upper_triang,\n            scale=None,\n            softmax_in_fp32=False,\n        )\n        .cuda()\n        .half()\n    )\n\n    fused_softmax_output = fused_softmax(\n        attn_weights,\n        total_mask,\n    )\n\n    torch_softmax = (\n        FusedScaleMaskSoftmax(\n            input_in_fp16=True,\n            input_in_bf16=False,\n            fusion_type=SoftmaxFusionTypes.none,\n            mask_func=attention_mask_func,\n            scale=None,\n            softmax_in_fp32=False,\n        )\n        .cuda()\n        .half()\n    )\n\n    torch_softmax_output = torch_softmax(\n        attn_weights,\n        total_mask,\n    )\n\n    test_result = (fused_softmax_output - torch_softmax_output).abs()\n\n    while test_result.dim() != 1:\n        test_result = test_result.mean(dim=-1)\n\n    diff = test_result.mean(dim=-1)\n\n    if diff <= 1e-3:\n        print(\n            f\"\\n[Success] test_fused_upper_triangle_mask_softmax\"\n            f\"\\n > mean_difference={diff}\"\n            f\"\\n > fused_values={fused_softmax_output[-1][-1][-1][:5].tolist()}\"\n            f\"\\n > torch_values={torch_softmax_output[-1][-1][-1][:5].tolist()}\"\n        )\n    else:\n        print(\n            f\"\\n[Fail] test_fused_upper_triangle_mask_softmax\"\n            f\"\\n > mean_difference={diff}, \"\n            f\"\\n > fused_values={fused_softmax_output[-1][-1][-1][:5].tolist()}, \"\n            f\"\\n > torch_values={torch_softmax_output[-1][-1][-1][:5].tolist()}\"\n        )\n",
        "tests/model/test_model_checkpoint.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\ninstantiate models, save checkpoints, load checkpoints, compare loaded parameters to saved parameters and compare forward pass outputs\n\nThis tests contain a relatively large number of functions. They are not split into separate tests because a lot of boilerplate (e.g. instantiate model) needs\nto run in order to perform follow up tests. Joining in one test reduces runtime at the expense of decreased transparency of test results in case of failures.\n\"\"\"\nimport os\nimport shutil\nimport torch\n\nimport pytest\nfrom tests.common import (\n    DistributedTest,\n    clear_test_dirs,\n    model_setup,\n    binary,\n    parametrize,\n)\nimport torch\n\nPARAMS_TO_TEST = {\n    \"pipe_parallel_size,model_parallel_size\": [[0, 1], [1, 2], [0, 2], [2, 1]],\n    \"checkpoint_validation_with_forward_pass\": [True],\n    \"fp16,fp32_allreduce\": [\n        [\n            {\n                \"enabled\": True,\n                \"type\": \"bfloat16\",\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            True,\n        ],\n        [\n            {\n                \"enabled\": True,\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            False,\n        ],\n    ],\n}\n\nparameters, names = parametrize(\n    PARAMS_TO_TEST, max_tests=int(os.getenv(\"MAX_TESTCASES\", 50)), seed=None\n)\n\n\n@pytest.mark.skip\n@pytest.mark.parametrize(\"param_dict\", parameters, ids=names)\ndef test_train(param_dict):\n    import tempfile\n\n    d = tempfile.mkdtemp()\n    param_dict[\"save\"] = d\n\n    t1 = test_run_checkpoint_test_class()\n    t1.run_checkpoint_test(param_dict=param_dict)\n\n\nclass test_run_checkpoint_test_class(DistributedTest):\n    def run_checkpoint_test(yaml_list=None, param_dict=None):\n\n        from megatron.checkpointing import load_checkpoint\n        from megatron.checkpointing import save_checkpoint\n\n        model, optimizer, lr_scheduler, args_loaded = model_setup(\n            yaml_list, param_dict, clear_data=True\n        )\n\n        # save model checkpoint\n        save_checkpoint(\n            neox_args=args_loaded,\n            iteration=42,\n            model=model,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n        )\n\n        # reload model from checkpoint\n        (\n            reloaded_model,\n            reloaded_optimizer,\n            reloaded_lr_scheduler,\n            args_reloaded,\n        ) = model_setup(yaml_list, param_dict, clear_data=False)\n        iteration = load_checkpoint(\n            neox_args=args_reloaded,\n            model=reloaded_model,\n            optimizer=reloaded_optimizer,\n            lr_scheduler=reloaded_lr_scheduler,\n        )\n\n        # ensure same checkpoint is loaded\n        assert (\n            iteration == 42\n        ), \"run_checkpoint_test() iteration loaded from checkpoint correct\"\n\n        # check all weight groups are the same\n        for idx, ((n1, p1), (n2, p2)) in enumerate(\n            zip(\n                list(model.module.named_parameters()),\n                list(reloaded_model.module.named_parameters()),\n            )\n        ):\n            assert n1 == n2\n            params_equal = (p1 == p2).all().item()\n            assert params_equal, \"run_checkpoint_test() params equal: \" + str(n1)\n\n\nif __name__ == \"__main__\":\n    params = list(\n        parametrize(\n            PARAMS_TO_TEST, max_tests=int(os.getenv(\"MAX_TESTCASES\", 50)), seed=None\n        )\n    )\n    test_train(params[0])\n",
        "tests/model/test_model_generation.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\ninstantiate models, save checkpoints, load checkpoints, compare loaded parameters to saved parameters and compare forward pass outputs\n\nThis tests contain a relatively large number of functions. They are not split into separate tests because a lot of boilerplate (e.g. instantiate model) needs\nto run in order to perform follow up tests. Joining in one test reduces runtime at the expense of decreased transparency of test results in case of failures.\n\"\"\"\n\n\nimport os\nimport pytest\nfrom tests.common import DistributedTest, model_setup, parametrize\n\nPARAMS_TO_TEST = {\n    \"pipe_parallel_size,model_parallel_size,world_size\": [\n        [0, 1, 1],\n        [0, 1, 2],\n        [1, 2, 2],\n        [0, 2, 2],\n        [2, 1, 2],\n    ],\n    \"top_p,temperature,top_k\": [[0.0, 0.5, 0], [0.5, 0.0, 100], [0.5, 0.5, 0]],\n    \"prompt\": [\"\", \"hello world\"],\n    \"fp16,fp32_allreduce\": [\n        [\n            {\n                \"enabled\": True,\n                \"type\": \"bfloat16\",\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            True,\n        ],\n        [\n            {\n                \"enabled\": True,\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            False,\n        ],\n    ],\n}\n\nparameters, names = parametrize(\n    PARAMS_TO_TEST, max_tests=int(os.getenv(\"MAX_TESTCASES\", 50)), seed=None\n)\n\n\n@pytest.mark.skip\n@pytest.mark.parametrize(\"param_dict\", parameters, ids=names)\ndef test_train(param_dict):\n    t1 = run_generate_test_class()\n    t1.run_generate_test(param_dict, param_dict.pop(\"prompt\"))\n\n\nclass run_generate_test_class(DistributedTest):\n    world_size = 2\n\n    def run_generate_test(param_dict, prompt):\n        from megatron.text_generation_utils import generate_samples_from_prompt\n        from megatron.utils import is_mp_rank_0\n\n        fixed_params = {\n            \"num_samples\": 3,\n            \"maximum_tokens\": 50,\n            \"make_vocab_size_divisible_by\": 2,\n            \"sample_output_file\": \"test_sample_output.txt\",\n            \"checkpoint_activations\": False,\n            \"partition_activations\": False,\n            \"no_load_optim\": True,\n        }\n\n        param_dict.update(fixed_params)\n        # TODO: we don't need to reinstantiate the model every time if we're only changing sampling settings - should be a workaround for this\n        model, _, _, args_loaded = model_setup(None, param_dict, clear_data=True)\n        model.eval()\n\n        prompts = [prompt for _ in range(args_loaded.num_samples)]\n        output = generate_samples_from_prompt(\n            neox_args=args_loaded,\n            model=model,\n            text=prompts,\n            maximum_tokens=args_loaded.maximum_tokens,\n            recompute=False,\n            temperature=args_loaded.temperature,\n            top_k=args_loaded.top_k,\n            top_p=args_loaded.top_p,\n        )\n\n        # outputs only get generated on mp rank 0\n        if is_mp_rank_0():\n            assert len(output) == len(prompts)\n            for prompt, out in zip(prompts, output):\n                assert prompt == out[\"context\"]\n                assert len(out[\"text\"]) > 0\n",
        "tests/model/test_model_instantiation.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\ninstantiate models with different configurations as a first possible point of failure\n\"\"\"\n\nimport pytest\n\nimport torch\nimport os\nfrom tests.common import (\n    DistributedTest,\n    model_setup,\n    clear_test_dirs,\n    parametrize,\n    binary,\n)\n\nPARAMS_TO_TEST = {\n    \"pipe_parallel_size,model_parallel_size,world_size\": [\n        [0, 1, 1],\n        [1, 2, 2],\n        [0, 2, 2],\n    ],\n    \"no_weight_tying\": binary,\n    \"attention_config\": [\n        [[[\"global\"], \"all\"]],\n        [[[\"local\"], \"all\"]],\n        [[[\"sparse_variable\"], \"all\"]],\n        [[[\"sparse_fixed\"], \"all\"]],\n    ],\n    \"scaled_upper_triang_masked_softmax_fusion,bias_gelu_fusion\": [\n        [True, False],\n        [False, True],\n    ],\n    \"fp16,fp32_allreduce\": [\n        [\n            {\n                \"enabled\": True,\n                \"type\": \"bfloat16\",\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            True,\n        ],\n        [\n            {\n                \"enabled\": True,\n                \"loss_scale\": 0,\n                \"loss_scale_window\": 1000,\n                \"hysteresis\": 2,\n                \"min_loss_scale\": 1,\n            },\n            False,\n        ],\n    ],\n}\n\nparameters, names = parametrize(\n    PARAMS_TO_TEST, max_tests=int(os.getenv(\"MAX_TESTCASES\", 50)), seed=None\n)\n\n\n@pytest.mark.xfail(\n    reason=\"Either fused kernels are not installed, or Cannot re-initialize CUDA in forked subprocess'\"\n)\n@pytest.mark.parametrize(\"param_dict\", parameters, ids=names)\ndef test_instantiate(param_dict):\n    t1 = test_instantiate_optimizers_class()\n    t1.run_test_model_instantiation(param_dict)\n\n\nOPTIMIZER_PARAMS = {\n    \"optimizer\": [\n        {\"type\": \"adam\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"onebitadam\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"cpu_adam\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"cpu_torch_adam\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"sm3\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"lion\", \"params\": {\"lr\": 0.0006}},\n        {\"type\": \"madgrad_wd\", \"params\": {\"lr\": 0.0006}},\n    ]\n}\nopt_params, opt_name = parametrize(\n    OPTIMIZER_PARAMS, max_tests=int(os.getenv(\"MAX_TESTCASES\", 50)), seed=None\n)\n\n\n@pytest.mark.xfail(\n    reason=\"Either fused kernels are not installed, or 'Cannot re-initialize CUDA in forked subprocess'\"\n)\n@pytest.mark.parametrize(\"param_dict\", opt_params, ids=opt_name)\ndef test_instantiate_optimizers(param_dict):\n    t1 = test_instantiate_optimizers_class()\n    t1.run_test_model_instantiation(param_dict)\n\n\nclass test_instantiate_optimizers_class(DistributedTest):\n    world_size = 2\n\n    def run_test_model_instantiation(yaml_list=None, param_dict=None):\n        from deepspeed.runtime.pipe.engine import PipelineEngine, DeepSpeedEngine\n\n        model, optimizer, lr_scheduler, args_loaded = model_setup(yaml_list, param_dict)\n        if args_loaded.pipe_parallel_size < 2:\n            assert isinstance(\n                model, DeepSpeedEngine\n            ), \"test model instantiation \" + str(yaml_list)\n        else:\n            assert isinstance(model, PipelineEngine), \"test model instantiation \" + str(\n                yaml_list\n            )\n        if torch.distributed.get_world_size() == 1 or torch.distributed.get_rank() == 0:\n            clear_test_dirs()\n",
        "tests/model/test_model_train.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nInstantiate models, run for a small number of iterations, and check that training loss improves.\n\nPerforms testing using a linear grid search over important parameter values, so that each setting that differs from the base is tested in isolation.\n\nPotentially use fuzzing to test parameters in combination.\n\"\"\"\nimport pytest\nimport train\nfrom copy import deepcopy\nfrom unittest.mock import patch\nfrom megatron.neox_arguments import NeoXArgs\nfrom tests.common import simulate_deepy_env, BASE_CONFIG\n\nPARAMS_TO_TEST = {\n    \"gpt_j_residual\": [True, False],\n    \"pos_emb\": [\"learned\", \"rotary\", \"sinusoidal\", \"rpe\", \"alibi\", \"none\"],\n    \"attention_config\": [\n        \"global\",\n        \"local\",\n        \"sparse_fixed\",\n        \"sparse_variable\",\n        \"bigbird\",\n        \"bslongformer\",\n        \"gmlp\",\n        \"flash\",\n    ],\n    \"hidden_dropout\": [0, 0.1],\n    \"weight_decay\": [0, 0.1],\n    \"use_bias_in_attn_linear\": [True, False],\n    \"use_bias_in_norms\": [True, False],\n    \"precision\": [\"fp16\", \"fp32\", \"bfloat16\"],\n}\n\nkeys_to_test = PARAMS_TO_TEST.keys()\n\n# TODO: fix model training tests\n@pytest.mark.skip(\n    reason=\"All model tests are skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\n@pytest.mark.parametrize(\n    \"key, value\",\n    [(key, value) for key in keys_to_test for value in PARAMS_TO_TEST[key]],\n)\ndef test_model_training_options(monkeypatch, key, value):\n    # TODO: Possibly add testing over world_size=2 back in\n    neox_args = NeoXArgs.from_dict(BASE_CONFIG)\n    if getattr(neox_args, key) == value:\n        pytest.skip(\"Skipping to avoid redundancy as no change in base config\")\n    if key == \"precision\" and value == \"bfloat16\":\n        pytest.xfail(\n            reason=\"Assumes that ZeRO optimization stage has been set in the YAML\"\n        )\n    param_dict = {key: value}\n    run_train_test(monkeypatch, overwrite_values=param_dict)\n\n\ndef run_train_test(monkeypatch, overwrite_values: dict):\n    max_train_iters = 32\n    checkpoint_args = {\"train_iters\": max_train_iters}\n    overwrite_values = checkpoint_args\n    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n\n    # Train model, whilst patching collect_loss_for_unit_test to track model loss at each step\n    loss_per_iteration = []\n    with patch(\n        \"megatron.training.collect_loss_for_unit_test\",\n        side_effect=lambda x: loss_per_iteration.append(x),\n    ):\n        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n        assert (\n            len(loss_per_iteration) == max_train_iters\n        ), \"patching should have collected loss values from each train step\"\n\n        # loss should have decreased by now (otherwise increasing the max_steps parameter could have the testcase pass)\n        assert min(loss_per_iteration) < loss_per_iteration[0], (\n            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n        )\n",
        "tests/neox_args/__init__.py": "\"\"\"\ntesting of implementation of command line arguments and configuration (NeoXArgs)\n\"\"\"\n",
        "tests/neox_args/test_neoxargs_commandline.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nverify parsing and handover of command line arguments\n\"\"\"\nimport pytest\nimport sys\nfrom unittest.mock import patch\n\nfrom ..common import get_root_directory, get_config_directory, get_configs_with_path\n\n\n@pytest.mark.cpu\ndef test_neoxargs_consume_deepy_args_without_config_dir():\n    \"\"\"\n    verify consume_deepy_args processes command line arguments without config dir\n    \"\"\"\n\n    from megatron.neox_arguments import NeoXArgs\n\n    # load neox args with command line\n    with patch(\n        \"sys.argv\",\n        [str(get_root_directory() / \"deepy.py\"), \"train.py\"]\n        + get_configs_with_path([\"125M.yml\", \"local_setup.yml\"]),\n    ):\n        args_loaded_consume = NeoXArgs.consume_deepy_args()\n\n    # load neox args directly from yaml files\n    args_loaded_yamls = NeoXArgs.from_ymls(\n        get_configs_with_path([\"125M.yml\", \"local_setup.yml\"])\n    )\n\n    # update values from yaml files that cannot otherwise be matched\n    args_loaded_yamls.update_value(\"user_script\", \"train.py\")\n    args_loaded_yamls.wandb_group = args_loaded_consume.wandb_group\n\n    assert args_loaded_yamls == args_loaded_consume\n\n\n@pytest.mark.cpu\ndef test_neoxargs_consume_deepy_args_without_yml_suffix():\n    \"\"\"\n    verify consume_deepy_args processes command line arguments without yaml suffix\n    \"\"\"\n\n    from megatron.neox_arguments import NeoXArgs\n\n    # load neox args with command line\n    with patch(\n        \"sys.argv\",\n        [str(get_root_directory() / \"deepy.py\"), \"train.py\"]\n        + get_configs_with_path([\"125M\", \"local_setup\", \"cpu_mock_config.yml\"]),\n    ):\n        args_loaded_consume = NeoXArgs.consume_deepy_args()\n\n    # load neox args directly from yaml files\n    args_loaded_yamls = NeoXArgs.from_ymls(\n        get_configs_with_path([\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n    )\n\n    # update values from yaml files that cannot otherwise be matched\n    args_loaded_yamls.update_value(\"user_script\", \"train.py\")\n    args_loaded_yamls.wandb_group = args_loaded_consume.wandb_group\n\n    assert args_loaded_yamls == args_loaded_consume\n\n\n@pytest.mark.cpu\ndef test_neoxargs_consume_deepy_args_with_hostfile_param():\n    \"\"\"\n    Verify consume_deepy_args processes command line arguments without yaml suffix.\n    Also test the hostfile CLI arg\n    \"\"\"\n\n    from megatron.neox_arguments import NeoXArgs\n\n    # load neox args with command line\n    with patch(\n        \"sys.argv\",\n        [str(get_root_directory() / \"deepy.py\"), \"train.py\"]\n        + get_configs_with_path([\"125M\", \"local_setup\", \"cpu_mock_config.yml\"])\n        + [\"--hostfile=/mock_path\"],\n    ):\n        args_loaded_consume = NeoXArgs.consume_deepy_args()\n\n    # load neox args directly from yaml files\n    args_loaded_yamls = NeoXArgs.from_ymls(\n        get_configs_with_path([\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n    )\n\n    # update values from yaml files that cannot otherwise be matched\n    args_loaded_yamls.update_value(\"user_script\", \"train.py\")\n    args_loaded_yamls.wandb_group = args_loaded_consume.wandb_group\n\n    assert args_loaded_yamls == args_loaded_consume\n\n\n@pytest.mark.cpu\ndef test_neoxargs_consume_deepy_args_with_config_dir():\n    \"\"\"\n    verify consume_deepy_args processes command line arguments including config dir\n    \"\"\"\n\n    from megatron.neox_arguments import NeoXArgs\n\n    # load neox args with command line\n    with patch(\n        \"sys.argv\",\n        [\n            str(get_root_directory() / \"deepy.py\"),\n            \"train.py\",\n            \"-d\",\n            str(get_config_directory()),\n        ]\n        + [\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"],\n    ):\n        args_loaded_consume = NeoXArgs.consume_deepy_args()\n\n    # load neox args directly from yaml files\n    args_loaded_yamls = NeoXArgs.from_ymls(\n        get_configs_with_path([\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n    )\n\n    # update values from yaml files that cannot otherwise be matched\n    args_loaded_yamls.update_value(\"user_script\", \"train.py\")\n    args_loaded_yamls.wandb_group = args_loaded_consume.wandb_group\n\n    assert args_loaded_yamls == args_loaded_consume\n\n\n@pytest.mark.cpu\ndef test_neoxargs_consume_neox_args():\n    \"\"\"\n    verify megatron args are correctly consumed after sending via deepspeed\n    \"\"\"\n    from megatron.neox_arguments import NeoXArgs\n\n    # intitially load config from files as would be the case in deepy.py\n    yaml_list = get_configs_with_path(\n        [\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"]\n    )\n    args_baseline = NeoXArgs.from_ymls(yaml_list)\n    args_baseline.update_value(\"user_script\", str(get_root_directory() / \"train.py\"))\n    deepspeed_main_args = args_baseline.get_deepspeed_main_args()\n\n    # patch sys.argv so that args can be access by set_global_variables within initialize_megatron\n    with patch(\"sys.argv\", deepspeed_main_args):\n        args_loaded = NeoXArgs.consume_neox_args()\n\n    # TODO is the wandb group really to be changed?\n    args_loaded.wandb_group = args_baseline.wandb_group\n    assert args_baseline.megatron_config == args_loaded.megatron_config\n",
        "tests/neox_args/test_neoxargs_implementation.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\ncheck implementation of NeoXArgs for duplication errors (would overwrite)\n\"\"\"\nimport pytest\n\n\n@pytest.mark.cpu\ndef test_neoxargs_duplicates():\n    \"\"\"\n    tests that there are no duplicates among parent classes of NeoXArgs\n    \"\"\"\n    from megatron import NeoXArgs\n\n    assert NeoXArgs.validate_keys(), \"test_neoxargs_duplicates\"\n",
        "tests/neox_args/test_neoxargs_load.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nload all confings in neox/configs in order to perform validations implemented in NeoXArgs\n\"\"\"\nimport pytest\nimport yaml\nfrom ..common import get_configs_with_path\n\n\ndef run_neox_args_load_test(yaml_files):\n    from megatron.neox_arguments import NeoXArgs\n\n    yaml_list = get_configs_with_path(yaml_files)\n    args_loaded = NeoXArgs.from_ymls(yaml_list)\n    assert isinstance(args_loaded, NeoXArgs)\n\n    # initialize an empty config dictionary to be filled by yamls\n    config = dict()\n\n    # iterate of all to be loaded yaml files\n    for conf_file_name in yaml_list:\n\n        # load file\n        with open(conf_file_name) as conf_file:\n            conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n\n        # check for key duplicates and load values\n        for conf_key, conf_value in conf.items():\n            if conf_key in config:\n                raise ValueError(\n                    f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n                )\n\n            conf_key_converted = conf_key.replace(\n                \"-\", \"_\"\n            )  # TODO remove replace and update configuration files?\n            config[conf_key_converted] = conf_value\n\n    # validate that neox args has the same value as specified in the config (if specified in the config)\n    for k, v in config.items():\n        neox_args_value = getattr(args_loaded, k)\n        assert v == neox_args_value, (\n            \"loaded neox args value \"\n            + str(k)\n            + \" == \"\n            + str(neox_args_value)\n            + \" different from config file \"\n            + str(v)\n        )\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_125M_local_setup():\n    \"\"\"\n    verify 125M.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"125M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_125M_local_setup_text_generation():\n    \"\"\"\n    verify 125M.yml can be loaded together with text generation without raising validation errors\n    \"\"\"\n    run_neox_args_load_test(\n        [\"125M.yml\", \"local_setup.yml\", \"text_generation.yml\", \"cpu_mock_config.yml\"]\n    )\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_350M_local_setup():\n    \"\"\"\n    verify 350M.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"350M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_760M_local_setup():\n    \"\"\"\n    verify 760M.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"760M.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_2_7B_local_setup():\n    \"\"\"\n    verify 2-7B.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"2-7B.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_6_7B_local_setup():\n    \"\"\"\n    verify 6-7B.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"6-7B.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_13B_local_setup():\n    \"\"\"\n    verify 13B.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"13B.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_1_3B_local_setup():\n    \"\"\"\n    verify 1-3B.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"1-3B.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_load_arguments_175B_local_setup():\n    \"\"\"\n    verify 13B.yml can be loaded without raising validation errors\n    \"\"\"\n    run_neox_args_load_test([\"175B.yml\", \"local_setup.yml\", \"cpu_mock_config.yml\"])\n\n\n@pytest.mark.cpu\ndef test_neoxargs_fail_instantiate_without_required_params():\n    \"\"\"\n    verify assertion error if required arguments are not provided\n    \"\"\"\n\n    try:\n        run_neox_args_load_test([\"local_setup.yml\"])\n        assert False\n    except Exception as e:\n        assert True\n\n\n@pytest.mark.cpu\ndef test_neoxargs_fail_instantiate_without_any_params():\n    \"\"\"\n    verify assertion error if required arguments are not provided\n    \"\"\"\n    from megatron.neox_arguments import NeoXArgs\n\n    try:\n        args_loaded = NeoXArgs()\n        assert False\n    except Exception as e:\n        assert True\n",
        "tests/neox_args/test_neoxargs_usage.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nplausibility check for the usage of neox_args in the megatron codebase\n\"\"\"\nimport pytest\nimport re\nfrom ..common import get_root_directory\n\n\n@pytest.mark.cpu\ndef test_neoxargs_usage():\n    \"\"\" \"\n    checks for code pieces of the pattern \"args.*\" and verifies that such used arg is defined in NeoXArgs\n    \"\"\"\n    from megatron.neox_arguments import NeoXArgs\n\n    declared_all = True\n    neox_args_attributes = set(NeoXArgs.__dataclass_fields__.keys())\n\n    # we exclude a number of properties (implemented with the @property decorator) or functions that we know exists\n    exclude = set(\n        [\n            \"params_dtype\",\n            \"deepspeed_config\",\n            \"get\",\n            \"pop\",\n            \"get_deepspeed_main_args\",\n            'optimizer[\"params\"]',\n            \"attention_config[layer_number]\",\n            \"adlr_autoresume_object\",\n            \"update_value\",\n            \"all_config\",\n            \"tensorboard_writer\",\n            \"tokenizer\",\n            \"train_batch_size]\",\n            \"items\",\n            \"configure_distributed_args\",\n            \"build_tokenizer\",\n            \"attention_config[i]\",\n            \"print\",\n            \"update\",\n        ]\n    )\n\n    # test file by file\n    for filename in (get_root_directory() / \"megatron\").glob(\"**/*.py\"):\n        if filename.name in [\"text_generation_utils.py\", \"train_tokenizer.py\"]:\n            continue\n\n        # load file\n        with open(filename, \"r\") as f:\n            file_contents = f.read()\n\n        # find args matches\n        matches = list(\n            re.findall(\n                r\"(?<=neox_args\\.).{2,}?(?=[\\s\\n(){}+-/*;:,=,[,\\]])\", file_contents\n            )\n        )\n        if len(matches) == 0:\n            continue\n\n        # compare\n        for match in matches:\n            if match not in neox_args_attributes and match not in exclude:\n                print(\n                    f\"(arguments used not found in neox args): {filename.name}: {match}\",\n                    flush=True,\n                )\n                declared_all = False\n\n    assert declared_all, \"all arguments used in code defined in NeoXArgs\"\n",
        "tests/requirements/test_requirements.py": "import pytest\nimport toml\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom packaging.version import parse as parse_version, Version\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Dependency:\n    name: str\n    version: Optional[str] = None\n\n    @classmethod\n    def from_requirement(cls, requirement: str) -> \"Dependency\":\n        \"\"\"Parse a requirement string into a Dependency object.\"\"\"\n        # Common version specifiers\n        specifiers = [\"==\", \">=\", \">\", \"<=\", \"<\"]\n        name = requirement\n        version = None\n\n        for spec in specifiers:\n            if spec in requirement:\n                name, version = requirement.split(spec, 1)\n                version = version.strip()\n                break\n\n        return cls(name.lower().strip(), version)\n\n    def matches_version(self, other_version: str) -> bool:\n        \"\"\"Check if this dependency's version matches another version string.\"\"\"\n        if not self.version or not other_version:\n            return True\n\n        try:\n            # Convert versions to comparable objects\n            our_version = parse_version(self.version)\n            their_version = parse_version(other_version.replace(\"*\", \"0\"))\n            return our_version == their_version\n        except ValueError:\n            # If versions can't be parsed, fall back to string comparison\n            return self.version.replace(\"*\", \"0\") == other_version.replace(\"*\", \"0\")\n\n\nclass DependencyValidator:\n    def __init__(self, requirements_dir: Path):\n        self.requirements_dir = requirements_dir\n\n    def parse_requirements(self, file_path: Path) -> List[Dependency]:\n        \"\"\"Parse requirements.txt file into a list of Dependencies.\"\"\"\n        try:\n            with open(file_path, \"r\") as f:\n                lines = [\n                    line.strip()\n                    for line in f\n                    if line.strip() and not line.startswith(\"#\")\n                ]\n                return [Dependency.from_requirement(line) for line in lines]\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Requirements file not found: {file_path}\")\n        except Exception as e:\n            raise ValueError(f\"Error parsing requirements file {file_path}: {str(e)}\")\n\n    def parse_pyproject(self, file_path: Path) -> Dict[str, str]:\n        \"\"\"Parse pyproject.toml file and extract dependencies.\"\"\"\n        try:\n            with open(file_path, \"r\") as f:\n                pyproject_data = toml.load(f)\n                return {\n                    name.lower(): str(version)\n                    for name, version in pyproject_data[\"tool\"][\"poetry\"][\n                        \"dependencies\"\n                    ].items()\n                    if name.lower() != \"python\"  # Exclude Python version\n                }\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"pyproject.toml file not found: {file_path}\")\n        except Exception as e:\n            raise ValueError(f\"Error parsing pyproject.toml {file_path}: {str(e)}\")\n\n    def compare_dependencies(\n        self, req_deps: List[Dependency], pyproject_deps: Dict[str, str]\n    ) -> tuple[bool, List[str]]:\n        \"\"\"Compare dependencies between requirements.txt and pyproject.toml.\"\"\"\n        mismatches = []\n\n        for req in req_deps:\n            if req.name not in pyproject_deps:\n                mismatches.append(\n                    f\"Dependency '{req.name}' not found in pyproject.toml\"\n                )\n                continue\n\n            if not req.matches_version(pyproject_deps[req.name]):\n                mismatches.append(\n                    f\"Version mismatch for '{req.name}': \"\n                    f\"requirements.txt={req.version}, \"\n                    f\"pyproject.toml={pyproject_deps[req.name]}\"\n                )\n\n        return len(mismatches) == 0, mismatches\n\n\ndef get_corresponding_pyproject(req_file: Path) -> Path:\n    \"\"\"Get the corresponding pyproject.toml file for a requirements file.\"\"\"\n    env_name = req_file.stem.split(\"-\")[1]\n    return req_file.parent / f\"pyproject-{env_name}.toml\"\n\n\n@pytest.mark.parametrize(\"req_file\", Path(\"requirements\").glob(\"requirements-*.txt\"))\ndef test_pyproject_matches_requirements(req_file: Path):\n    \"\"\"Test that requirements.txt dependencies match pyproject.toml dependencies.\"\"\"\n    validator = DependencyValidator(req_file.parent)\n    pyproject_file = get_corresponding_pyproject(req_file)\n\n    # Parse both dependency files\n    req_deps = validator.parse_requirements(req_file)\n    pyproject_deps = validator.parse_pyproject(pyproject_file)\n\n    # Compare dependencies and get detailed mismatches\n    is_match, mismatches = validator.compare_dependencies(req_deps, pyproject_deps)\n\n    # Create detailed error message if there are mismatches\n    if not is_match:\n        error_msg = \"\\n\".join(\n            [\n                f\"\\nDependency mismatches found between {req_file} and {pyproject_file}:\",\n                *[f\"- {msg}\" for msg in mismatches],\n            ]\n        )\n        pytest.fail(error_msg)\n",
        "tests/unit/__init__.py": "",
        "tests/unit/test_arguments.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.neox_arguments import NeoXArgs\nfrom tests.common import BASE_CONFIG, DistributedTest\n\n\ndef test_main_constructor():\n    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n    neox_args = NeoXArgs.consume_deepy_args(input_args)\n    deepspeed_main_args = neox_args.get_deepspeed_main_args()\n    neox_args = NeoXArgs.consume_neox_args(input_args=deepspeed_main_args)\n    neox_args.configure_distributed_args()\n\n\nclass test_constructor_from_ymls_class(DistributedTest):\n    world_size = 2\n\n    def test(self):\n        neox_args = NeoXArgs.from_ymls([\"tests/config/test_setup.yml\"])\n        neox_args.configure_distributed_args()\n\n\ndef test_constructor_from_ymls():\n    t1 = test_constructor_from_ymls_class()\n    t1.test()\n\n\nclass test_constructor_from_dict_class(DistributedTest):\n    world_size = 2\n\n    def test(self):\n        neox_args = NeoXArgs.from_dict(BASE_CONFIG)\n\n\ndef test_constructor_from_dict():\n    t1 = test_constructor_from_dict_class()\n    t1.test()\n",
        "tests/unit/test_dependencies.py": "import pytest\nfrom megatron import fused_kernels\n\n\ndef test_fused_kernels():\n    pytest.xfail(reason=\"Fused kernels require manual intervention to install\")\n    fused_kernels.load_fused_kernels()\n",
        "tests/unit/test_format_conversion_scripts.py": "import pytest\nfrom tools.ckpts import convert_neox_to_hf\nfrom tests.common import simulate_deepy_env, save_random_model\nfrom megatron.neox_arguments.neox_args import NeoXArgsTokenizer\n\n\n@pytest.mark.skip(\n    reason=\"Conversion test is skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\ndef test_gpt_neox_to_huggingface(monkeypatch, tmpdir, tmp_path):\n    # Generate random GPT-NEOX model, check we can convert to hf format\n\n    model_dir = str(tmpdir)\n    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n    save_random_model(deepspeed_main_args, model_dir, train_iters=1)\n\n    # Generate output\n    script_args = [\n        \"--config_file\",\n        \"tests/config/test_setup.yml\",\n        \"--input_dir\",\n        model_dir + \"/global_step1\",\n        \"--output_dir\",\n        model_dir,\n    ]\n    overwrite_values = {\"tokenizer_type\": NeoXArgsTokenizer.tokenizer_type}\n    convert_neox_to_hf.main(input_args=script_args, overwrite_values=overwrite_values)\n",
        "tests/unit/test_launcher_scripts.py": "# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n\nimport pytest\n\nimport eval\nimport generate\nimport train\nfrom megatron.neox_arguments import NeoXArgs\nfrom tests.common import save_random_model, simulate_deepy_env\nfrom tools.datasets import preprocess_data\n\n\n@pytest.fixture(\n    params=[\n        \"HFGPT2Tokenizer\",\n        \"HFTokenizer\",\n        \"GPT2BPETokenizer\",\n        \"CharLevelTokenizer\",\n        \"TiktokenTokenizer\",\n        \"SPMTokenizer\",\n    ]\n)\ndef tokenizer_type(request):\n    return request.param\n\n\n@pytest.fixture(params=[None, \"tests/data/sample_prompt.txt\"])\ndef sample_input_file(request):\n    return request.param\n\n\n@pytest.mark.cpu\ndef test_preprocess_data(tokenizer_type):\n    if tokenizer_type == \"SPMTokenizer\":\n        pytest.xfail(\n            reason=\"Expected easy resolution: Need to provide a valid model file from somewhere\"\n        )\n    vocab_file = {\n        \"HFTokenizer\": \"tests/data/hf_cache/tokenizer/gpt2.json\",\n        \"TiktokenTokenizer\": \"cl100k_base\",\n        \"HFGPT2Tokenizer\": \"gpt2\",\n    }\n    input_args = [\n        \"--input\",\n        \"./tests/data/enwik8_first100.txt\",\n        \"--output-prefix\",\n        \"./tests/data/enwik8_first100\",\n        \"--vocab\",\n        vocab_file.get(tokenizer_type, \"./data/gpt2-vocab.json\"),\n        \"--tokenizer-type\",\n        tokenizer_type,\n        \"--merge-file\",\n        \"./data/gpt2-merges.txt\",\n        \"--append-eod\",\n    ]\n    preprocess_data.main(input_args)\n\n\n@pytest.mark.skip(\n    reason=\"All model tests are skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\ndef test_generate(monkeypatch, tmpdir, tmp_path, sample_input_file):\n    model_dir = str(tmpdir)\n    sample_output_file = str(tmp_path) + \".txt\"\n    input_args = [\"generate.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n    save_random_model(deepspeed_main_args, model_dir)\n\n    # Generate output\n    generate_args = {\n        \"load\": model_dir,\n        \"sample_input_file\": sample_input_file,\n        \"sample_output_file\": sample_output_file,\n    }\n    generate.main(input_args=deepspeed_main_args, overwrite_values=generate_args)\n\n\n@pytest.mark.skip(\n    reason=\"All model tests are skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\ndef test_evaluate(monkeypatch, tmpdir, tmp_path):\n    model_dir = str(tmpdir)\n    sample_output_file = str(tmp_path)\n    input_args = [\"generate.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n    save_random_model(deepspeed_main_args, model_dir)\n\n    # Generate output\n    evaluate_args = {\n        \"load\": model_dir,\n        \"eval_tasks\": [\"lambada\"],  # [\"lambada\", \"hellaswag\", \"piqa\", \"sciq\"],\n        \"eval_results_prefix\": sample_output_file,\n    }\n    eval.main(input_args=deepspeed_main_args, overwrite_values=evaluate_args)\n\n\n@pytest.mark.skip(\n    reason=\"All model tests are skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\ndef test_finetuning(monkeypatch, tmpdir, tmp_path):\n    # Save random model, load random model, keep training\n    # TODO: add mocking to check that we're not ignoring the previously loaded model\n    model_dir = str(tmpdir)\n    sample_output_file = str(tmp_path)\n    input_args = [\"generate.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n    save_random_model(deepspeed_main_args, model_dir)\n\n    # Generate output\n    finetune_args = {\"load\": model_dir, \"finetune\": True}\n    train.main(input_args=deepspeed_main_args, overwrite_values=finetune_args)\n\n\n@pytest.mark.skip(\n    reason=\"All model tests are skipped until we fix the CUDA + torch multiprocessing issue.\"\n)\ndef test_train_launcher(monkeypatch):\n    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n    train.main(input_args=deepspeed_main_args)\n",
        "tests/unit/test_tokenizer.py": "import pytest\nfrom megatron.tokenizer import train_tokenizer\n\n\n@pytest.mark.cpu\ndef test_train_tokenizer():\n    input_args = [\n        \"--json_input_dir\",\n        \"./tests/data/enwik8_first100.txt\",\n        \"--tokenizer_output_path\",\n        \"\",\n    ]\n    args = train_tokenizer.parse_args(input_args)\n    train_tokenizer.main(args)\n",
        "tests/unit/test_url_accessibility.py": "import pytest\nimport requests\n\nfrom tools.datasets.corpora import DATA_DOWNLOADERS\n\n\ndef check_url_accessible(url):\n    try:\n        response = requests.head(url, timeout=5)\n        response.raise_for_status()\n        return True\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Unable to access URL - {e}\")\n        return False\n\n\n@pytest.mark.cpu\n@pytest.mark.parametrize(\"dataset_name\", list(DATA_DOWNLOADERS.keys()))\ndef test_url_accessibility(dataset_name):\n    if dataset_name == \"pass\":\n        return\n    elif not dataset_name == \"enwik8\":\n        pytest.xfail()\n    for url in DATA_DOWNLOADERS[dataset_name].urls:\n        assert check_url_accessible(url)\n",
        "tools/__init__.py": "",
        "tools/ckpts/convert_hf_llama_to_neox.py": "import torch\nimport argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport os\nimport tqdm\n\n\ndef convert_model(hf_state_dict, hf_config, tp_ranks):\n    conv_state_dicts = [{} for _ in range(tp_ranks)]\n    # get embeddings...\n    for i, chunk in enumerate(\n        torch.chunk(hf_state_dict[\"model.embed_tokens.weight\"], tp_ranks, dim=0)\n    ):\n        conv_state_dicts[i][\n            \"sequential.0.word_embeddings.weight\"\n        ] = chunk.clone().detach()\n    print(\n        \"model.embed_tokens.weight\",\n        hf_state_dict[\"model.embed_tokens.weight\"].shape,\n        \"sequential.0.word_embeddings.weight\",\n        conv_state_dicts[0][\"sequential.0.word_embeddings.weight\"].shape,\n    )\n    # Get config data...\n    num_kv_heads = hf_config.num_key_value_heads\n    num_q_heads = hf_config.num_attention_heads\n    head_dim = hf_config.hidden_size // num_q_heads\n    # do layers...\n    for layer_num in tqdm.tqdm(range(model.model.config.num_hidden_layers)):\n        # --- attention ---\n        # Output first since it's a simple row parallel...\n        for i, chunk in enumerate(\n            torch.chunk(\n                hf_state_dict[f\"model.layers.{layer_num}.self_attn.o_proj.weight\"],\n                tp_ranks,\n                dim=1,\n            )\n        ):\n            conv_state_dicts[i][\n                f\"sequential.{layer_num+2}.attention.dense.weight\"\n            ] = chunk.clone().detach()\n        print(\n            f\"model.layers.{layer_num}.self_attn.o_proj.weight\",\n            hf_state_dict[f\"model.layers.{layer_num}.self_attn.o_proj.weight\"].shape,\n            f\"sequential.{layer_num+2}.attention.dense.weight\",\n            conv_state_dicts[0][\n                f\"sequential.{layer_num+2}.attention.dense.weight\"\n            ].shape,\n        )\n        # Now for attention...\n        # Split into heads...\n        q = hf_state_dict[f\"model.layers.{layer_num}.self_attn.q_proj.weight\"]\n        k = hf_state_dict[f\"model.layers.{layer_num}.self_attn.k_proj.weight\"]\n        v = hf_state_dict[f\"model.layers.{layer_num}.self_attn.v_proj.weight\"]\n\n        # Chunk for tensor parallelism...\n        for i, q_chunk, k_chunk, v_chunk in zip(\n            range(tp_ranks),\n            torch.chunk(q, tp_ranks, dim=0),\n            torch.chunk(k, tp_ranks, dim=0),\n            torch.chunk(v, tp_ranks, dim=0),\n        ):\n            # The GQA code simply expects concatenated q,k,v weights for each tp partition\n            conv_state_dicts[i][\n                f\"sequential.{layer_num+2}.attention.query_key_value.weight\"\n            ] = (torch.cat([q_chunk, k_chunk, v_chunk], dim=0).clone().detach())\n        print(\n            f\"model.layers.{layer_num}.self_attn.(q/k/v)_proj.weight\",\n            hf_state_dict[f\"model.layers.{layer_num}.self_attn.q_proj.weight\"].shape,\n            hf_state_dict[f\"model.layers.{layer_num}.self_attn.k_proj.weight\"].shape,\n            hf_state_dict[f\"model.layers.{layer_num}.self_attn.v_proj.weight\"].shape,\n            f\"sequential.{layer_num+2}.attention.query_key_value.weight\",\n            conv_state_dicts[0][\n                f\"sequential.{layer_num+2}.attention.query_key_value.weight\"\n            ].shape,\n        )\n        # --- mlp ---\n        # Do SwiGLU weights...\n        # w1...\n        for i, (w1, w3) in enumerate(\n            zip(\n                torch.chunk(\n                    hf_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"],\n                    tp_ranks,\n                    dim=0,\n                ),\n                torch.chunk(\n                    hf_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"],\n                    tp_ranks,\n                    dim=0,\n                ),\n            )\n        ):\n            conv_state_dicts[i][\n                f\"sequential.{layer_num+2}.mlp.linear1.weight\"\n            ] = torch.cat([w3.clone().detach(), w1.clone().detach()], dim=0)\n        print(\n            f\"model.layers.{layer_num}.mlp.gate_proj.weight\",\n            hf_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"].shape,\n            f\"model.layers.{layer_num}.mlp.up_proj.weight\",\n            hf_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"].shape,\n            f\"sequential.{layer_num+2}.mlp.w3.weight\",\n            conv_state_dicts[0][f\"sequential.{layer_num+2}.mlp.linear1.weight\"].shape,\n        )\n        # w2 (output)...\n        for i, chunk in enumerate(\n            torch.chunk(\n                hf_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"],\n                tp_ranks,\n                dim=1,\n            )\n        ):\n            conv_state_dicts[i][\n                f\"sequential.{layer_num+2}.mlp.linear2.weight\"\n            ] = chunk.clone().detach()\n        print(\n            f\"model.layers.{layer_num}.mlp.down_proj.weight\",\n            hf_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"].shape,\n            f\"sequential.{layer_num+2}.mlp.linear2.weight\",\n            conv_state_dicts[0][f\"sequential.{layer_num+2}.mlp.linear2.weight\"].shape,\n        )\n        # --- norm ---\n        for i in range(tp_ranks):\n            conv_state_dicts[i][f\"sequential.{layer_num+2}.input_layernorm.scale\"] = (\n                hf_state_dict[f\"model.layers.{layer_num}.input_layernorm.weight\"]\n                .clone()\n                .detach()\n            )\n            conv_state_dicts[i][\n                f\"sequential.{layer_num+2}.post_attention_layernorm.scale\"\n            ] = (\n                hf_state_dict[\n                    f\"model.layers.{layer_num}.post_attention_layernorm.weight\"\n                ]\n                .clone()\n                .detach()\n            )\n\n    # Get final ln/linear....\n    index = model.model.config.num_hidden_layers + 3\n    for i in range(tp_ranks):\n        conv_state_dicts[i][f\"sequential.{index}.norm.scale\"] = (\n            hf_state_dict[\"model.norm.weight\"].clone().detach()\n        )\n    index += 1\n    # do output...\n    for i, chunk in enumerate(\n        torch.chunk(hf_state_dict[\"lm_head.weight\"], tp_ranks, dim=0)\n    ):\n        conv_state_dicts[i][\n            f\"sequential.{index}.final_linear.weight\"\n        ] = chunk.clone().detach()\n    print(\n        \"lm_head.weight\",\n        hf_state_dict[\"lm_head.weight\"].shape,\n        f\"sequential.{index}.final_linear.weight\",\n        conv_state_dicts[0][f\"sequential.{index}.final_linear.weight\"].shape,\n    )\n    return conv_state_dicts\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--tp\", type=int, default=1, help=\"Number of tensor parallelism ranks\"\n    )\n    parser.add_argument(\n        \"--pp\", type=int, default=0, help=\"Number of pipeline parallelism stages\"\n    )\n    parser.add_argument(\"--model\", type=str, default=\"gpt2\", help=\"HF model name\")\n    parser.add_argument(\n        \"--model_path\", type=str, default=None, help=\"Path to save model\"\n    )\n    args = parser.parse_args()\n    assert args.pp == 0, \"Pipeline parallelism not supported yet\"\n    tokenizer = AutoTokenizer.from_pretrained(args.model).save_pretrained(\n        args.model_path + \"/tokenizer\"\n    )\n    model = AutoModelForCausalLM.from_pretrained(args.model, torch_dtype=\"auto\")\n    state_dict = model.state_dict()\n    for key in state_dict.keys():\n        print(key, state_dict[key].shape)\n    os.makedirs(args.model_path, exist_ok=True)\n    # Setup model directory...\n    os.makedirs(f\"{args.model_path}/0\", exist_ok=True)\n    # Save the latest file so neox can figure out where to grab the weights...\n    with open(f\"{args.model_path}/latest\", \"w\") as f:\n        f.write(\"0\")\n    # Convert the model...\n    tp_state_dicts = convert_model(state_dict, model.model.config, args.tp)\n    for i in range(args.tp):\n        torch.save(\n            {\n                \"dp_world_size\": 1,\n                \"mp_world_size\": args.tp,\n                \"optimizer\": {},\n                \"global_steps\": 1,\n                \"skipped_steps\": 1,\n                \"iteration\": 1,\n                \"module\": tp_state_dicts[i],\n            },\n            f\"{args.model_path}/0/mp_rank_{i:02d}_model_states.pt\",\n        )\n",
        "tools/ckpts/convert_hf_to_sequential.py": "import sys\nimport os\nimport copy\nimport deepspeed\n\n# import time\n\nimport argparse\nimport torch\n\nimport numpy as np\n\nfrom functools import reduce\nfrom transformers import GPTNeoXForCausalLM, GPTNeoXConfig\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\nfrom megatron.neox_arguments import NeoXArgs\nfrom megatron.training import get_model, get_optimizer, get_learning_rate_scheduler\nfrom megatron.initialize import initialize_megatron\nfrom megatron import mpu\nfrom megatron.checkpointing import load_checkpoint, save_checkpoint\n\n# from megatron.utils import (\n#     Timers,\n#     init_wandb,\n# )\n\n\"\"\"\nA script for converting publicly available Huggingface (HF) checkpoints NeoX format.\n\nNote that this script requires access to corresponding config files for equivalent NeoX models to those found in Hugging face.\n\nExample usage: (Converts the 70M Pythia model to NeoX format)\n================================================================\nOMPI_COMM_WORLD_RANK=0 CUDA_VISIBLE_DEVICES=0 python tools/ckpts/convert_hf_to_sequential.py \\\n    --hf-model-name pythia-70m-v0 \\\n    --revision 143000 \\\n    --output-dir checkpoints/neox_converted/pythia/70m \\\n    --cache-dir checkpoints/HF \\\n    --config configs/pythia/70M.yml configs/local_setup.yml \\\n    --test\n\n\nFor multi-gpu support we must initialize deepspeed:\nNOTE: This requires manually changing the arguments below.\n================================================================\nCUDA_VISIBLE_DEVICES=0,1,2,3 python ./deepy.py tools/ckpts/convert_hf_to_sequential.py \\\n    -d configs pythia/70M.yml local_setup.yml\n\"\"\"\n\nMULTI_GPU_ARGS = \" \".join(\n    [\n        \"--hf-model-name pythia-70m-v0\",\n        \"--revision 143000\",\n        \"--output-dir checkpoints/neox_converted/pythia/70m\",\n        \"--cache-dir checkpoints/HF\",\n        \"--config configs/pythia/70M.yml configs/local_setup.yml\",\n        \"--test\",\n    ]\n)\n\n\ndef convert_hf_to_sequential(hf_model, seq_state_dict):\n    \"\"\"Converts the weights of a HuggingFace model to neox 2.0 format.\n\n    :param hf_model: the huggingface model\n    :param seq_state_dict: the state dict of the equivalent neox model\n\n    returns the updated sequential state dict\n    \"\"\"\n    num_layers = hf_model.config.num_hidden_layers\n    # Embedding is layer idx 0\n    seq_state_dict[\n        \"sequential.0.word_embeddings.weight\"\n    ] = hf_model.gpt_neox.embed_in.state_dict()[\"weight\"]\n\n    for layer_hf in range(num_layers):\n        # offset by 2\n        layer_seq = layer_hf + 2\n\n        # get layer from hf model\n        hf_layer = hf_model.gpt_neox.layers[layer_hf]\n        hf_layer_sd = hf_layer.state_dict()\n\n        for key in hf_model.gpt_neox.layers[0].state_dict().keys():\n\n            if key in [\"attention.bias\", \"attention.masked_bias\"]:\n                continue\n            seq_state_dict[f\"sequential.{layer_seq}.{key}\"] = hf_layer_sd[key]\n\n    # Load final layer norm\n    layer_seq = num_layers + 3\n    seq_state_dict[\n        f\"sequential.{layer_seq}.norm.weight\"\n    ] = hf_model.gpt_neox.final_layer_norm.state_dict()[\"weight\"]\n    seq_state_dict[\n        f\"sequential.{layer_seq}.norm.bias\"\n    ] = hf_model.gpt_neox.final_layer_norm.state_dict()[\"bias\"]\n\n    # output embedding / LM head\n    layer_seq += 1\n    seq_state_dict[\n        f\"sequential.{layer_seq}.final_linear.weight\"\n    ] = hf_model.embed_out.state_dict()[\"weight\"]\n\n\ndef shard_sequential_mp(num_mp_ranks, sequential):\n    \"\"\"Shards the sequential model into model parallel ranks.\n\n    :param num_mp_ranks: the number of model parallel ranks\n    :param sequential: the state dict of the sequential model at mp=1\n\n    returns a dict of state dicts for each mp rank\n    \"\"\"\n    ranks = {x: dict() for x in range(num_mp_ranks)}\n    for k, v in sequential.items():\n        if reduce(\n            np.logical_or,\n            [\n                x in k\n                for x in [\n                    \"dense_4h_to_h.bias\",\n                    \"attention.dense.bias\",\n                ]\n            ],\n        ):\n            # Divide by tp_size since they get added together\n            for x in range(num_mp_ranks):\n                ranks[x][k] = v / num_mp_ranks\n        elif reduce(\n            np.logical_or,\n            [\n                x in k\n                for x in [\n                    \"layernorm\",\n                    \"rotary_emb\",\n                    \"norm.weight\",\n                    \"norm.bias\",\n                ]\n            ],\n        ):\n            # no splitting\n            for x in range(num_mp_ranks):\n                ranks[x][k] = v\n        else:\n            if len(v.shape) == 1:\n                size_per_rank = v.shape[0] / num_mp_ranks\n                if size_per_rank % 128 != 0.0:\n                    padded_size = (128 - (size_per_rank % 128)) + size_per_rank\n                    size_diff = int((padded_size * 4) - v.shape[max_])\n                    zero_pad = torch.zeros((size_diff))\n                    v = torch.cat([v, zero_pad], dim=max_)\n                else:\n                    padded_size = size_per_rank\n\n                assert size_per_rank % 1.0 == 0.0\n                assert padded_size % 1.0 == 0.0\n\n                padded_size = int(padded_size)\n                size_per_rank = int(size_per_rank)\n\n                for x in range(num_mp_ranks):\n                    if size_per_rank != padded_size:\n                        # need to pad\n                        ranks[x][k] = v[padded_size * x : padded_size * (x + 1)]\n                    else:\n                        ranks[x][k] = v[size_per_rank * x : size_per_rank * (x + 1)]\n\n            elif len(v.shape) == 2:\n\n                if reduce(\n                    np.logical_or,\n                    [\n                        x in k\n                        for x in [\n                            \"attention.dense.weight\",\n                            \"mlp.dense_4h_to_h.weight\",\n                        ]\n                    ],\n                ):  # column parallel\n                    max_, min_ = 1, 0\n                elif reduce(\n                    np.logical_or,\n                    [\n                        x in k\n                        for x in [\n                            \"mlp.dense_h_to_4h.weight\",\n                            \"mlp.dense_h_to_4h.bias\",\n                            \"attention.query_key_value.weight\",\n                            \"attention.query_key_value.bias\",\n                            \"word_embeddings.weight\",\n                            \"final_linear.weight\",\n                        ]\n                    ],\n                ):\n                    # row parallel\n                    max_, min_ = 0, 1\n                else:\n                    raise Exception(\"Unknown weight to shard: {}\".format(k))\n\n                size_per_rank = v.shape[max_] / num_mp_ranks\n                if size_per_rank % 128 != 0.0:\n                    padded_size = (128 - (size_per_rank % 128)) + size_per_rank\n                    size_diff = int((padded_size * num_mp_ranks) - v.shape[max_])\n\n                    assert (\n                        size_diff > 0\n                    ), \"[ERROR] size diff is negative: {} for size_per_rank: {}, k:{}, shape:{}, padded_size:{}\".format(\n                        size_diff, size_per_rank, k, v.shape, padded_size\n                    )\n\n                    zero_pad = (\n                        torch.zeros((size_diff, v.shape[min_]))\n                        if max_ == 0\n                        else torch.zeros((v.shape[min_], size_diff))\n                    )\n\n                    v = torch.cat([v, zero_pad], dim=max_)\n                else:\n                    padded_size = size_per_rank\n\n                assert size_per_rank % 1.0 == 0.0\n                assert padded_size % 1.0 == 0.0\n\n                padded_size = int(padded_size)\n                size_per_rank = int(size_per_rank)\n\n                for x in range(num_mp_ranks):\n                    if size_per_rank != padded_size:\n                        # need to pad\n                        ranks[x][k] = (\n                            v[padded_size * x : padded_size * (x + 1), :]\n                            if max_ == 0\n                            else v[:, padded_size * x : padded_size * (x + 1)]\n                        )\n                    else:\n                        ranks[x][k] = (\n                            v[size_per_rank * x : size_per_rank * (x + 1), ...]\n                            if max_ == 0\n                            else v[:, size_per_rank * x : size_per_rank * (x + 1)]\n                        )\n\n            else:\n                raise NotImplementedError()\n\n    return ranks\n\n\ndef replace_sharded_seq(mp_checkpoints, mp_sharded_seq):\n    \"\"\"replaces the values within checkpointed configs with those\n    from the sharded sequential object.\"\"\"\n\n    for mp_idx, shard in mp_sharded_seq.items():\n        mp_key = f\"mp_rank_{mp_idx:02}_model_states.pt\"\n\n        # use for loop instead of direct assignment\n        # to check for compatibility\n        for k, v in mp_checkpoints[mp_key][\"module\"].items():\n            try:\n                mp_checkpoints[mp_key][\"module\"][k] = shard[k]\n            except KeyError:\n                print(\"ERROR key:{} not found in shard.\".format(k))\n\n\ndef shard_pp(sequential, mp_rank, num_layers):\n    \"\"\"Shards the model into layers.\n\n    :param sequential: the state dict of the sequential model at mp=1\n    :param mp_rank: the model parallel rank of the layers\n\n    returns a dict of state dicts for each layer\n    \"\"\"\n    suffix = f\"-model_{mp_rank:02}-model_states.pt\"\n\n    layers_seq = dict()\n    layers_seq[f\"layer_00\" + suffix] = {\n        \"word_embeddings.weight\": sequential[f\"sequential.0.word_embeddings.weight\"]\n    }\n    layers_seq[f\"layer_{num_layers+3:02}\" + suffix] = {\n        \"norm.weight\": sequential[f\"sequential.{num_layers+3}.norm.weight\"],\n        \"norm.bias\": sequential[f\"sequential.{num_layers+3}.norm.bias\"],\n    }\n\n    layers_seq[f\"layer_{num_layers+4:02}\" + suffix] = {\n        \"final_linear.weight\": sequential[\n            f\"sequential.{num_layers+4}.final_linear.weight\"\n        ]\n    }\n\n    for layer in range(2, num_layers + 2):\n        layer_keys = [x for x in sequential if \".{}.\".format(layer) in x]\n        layers_seq[f\"layer_{layer:02}\" + suffix] = {\n            k.split(\".{}.\".format(layer))[1]: sequential[k] for k in layer_keys\n        }\n\n    return layers_seq\n\n\ndef shard_pp_mp(num_mp_ranks, sequential, num_layers):\n    \"\"\"Shards the model into layers and model parallel ranks.\n\n    :param num_mp_ranks: the number of model parallel ranks\n    :param sequential: the state dict of the sequential model at mp=1\n    :param num_layers: the number of layers in the model\n\n    returns a dict of state dicts for each layer for each model parallel rank\n    \"\"\"\n    mp_sharded = shard_sequential_mp(num_mp_ranks=num_mp_ranks, sequential=sequential)\n\n    layers_pp_mp = {}\n    for mp_rank, d in mp_sharded.items():\n        layers_pp_mp.update(\n            shard_pp(sequential=d, mp_rank=mp_rank, num_layers=num_layers)\n        )\n    return layers_pp_mp\n\n\ndef convert(hf_model, ckpt_dir, output_dir):\n    \"\"\"Converts a huggingface model to a NeoX checkpoint for different\n        model parallel and pipeline parallel settings degrees.\n\n    :param hf_model: the huggingface model\n    :param ckpt_dir: the directory containing the NeoX checkpoint\n    :param output_dir: the directory to save the converted checkpoint\n    returns None\n    \"\"\"\n\n    os.listdir(ckpt_dir)\n\n    ckpts, layers = {}, {}\n    for x in os.listdir(ckpt_dir):\n        if x.startswith(\"mp_rank\"):\n            ckpts[x] = torch.load(os.path.join(ckpt_dir, x))\n        elif x.startswith(\"layer\"):\n            layers[x] = torch.load(os.path.join(ckpt_dir, x))\n\n    assert len(layers) + len(ckpts) > 0, \"No checkpoints found in {}\".format(ckpt_dir)\n\n    os.makedirs(output_dir, exist_ok=True)\n    seq_state_dict = dict()\n    convert_hf_to_sequential(hf_model, seq_state_dict)\n\n    if len(ckpts) == 1 and len(layers) == 0:\n        # pp=0, mp=1\n        key = list(ckpts.keys())[0]\n        ckpts[key][\"module\"] = seq_state_dict\n        to_save = ckpts\n\n    elif len(ckpts) > 1 and len(layers) == 0:\n        # pp=0, mp>1\n        sharded_seq = shard_sequential_mp(\n            num_mp_ranks=len(ckpts), sequential=seq_state_dict\n        )\n        replace_sharded_seq(mp_checkpoints=ckpts, mp_sharded_seq=sharded_seq)\n        to_save = ckpts\n\n    elif len(ckpts) == 1 and len(layers) > 1:\n        # pp>0, mp==1\n        to_save = shard_pp(\n            sequential=seq_state_dict,\n            mp_rank=0,\n            num_layers=hf_model.config.num_hidden_layers,\n        )\n\n    elif len(ckpts) > 1 and len(layers) > 1:\n        # pp>0, mp>1\n        to_save = shard_pp_mp(\n            num_mp_ranks=len(ckpts),\n            sequential=seq_state_dict,\n            num_layers=hf_model.config.num_hidden_layers,\n        )\n\n    else:\n        raise NotImplementedError(\n            \"Not implemented for len(ckpts)={} and len(layers)={}\".format(\n                len(ckpts), len(layers)\n            )\n        )\n\n    for k, v in to_save.items():\n        print(\"saving {}...\".format(os.path.join(output_dir, k)))\n        torch.save(v, os.path.join(ckpt_dir, k))\n\n    # copy the checkpoint to the output_dir\n    print(\"rm {}/*\".format(output_dir))\n    os.system(\"rm {}/*\".format(output_dir))\n    os.makedirs(output_dir, exist_ok=True)\n    print(\"cp {} {}\".format(os.path.join(ckpt_dir, \"*\"), output_dir))\n    os.system(\"cp {} {}\".format(os.path.join(ckpt_dir, \"*\"), output_dir))\n\n    # set latest file within the output_dir\n    latest_file = os.path.join(\"/\".join(output_dir.split(\"/\")[:-1]), \"latest\")\n    os.system(\"rm \" + latest_file)\n    with open(latest_file, \"w\") as f:\n        f.write(output_dir.split(\"/\")[-1])\n\n\ndef consume_neox_args2(args_parsed, overwrite_values=None):\n    \"\"\"\n    Deepspeed launcher needs to pass the arguments for `pretrain_gpt2.py` across to all machines.\n\n    In order not to have any problems with different configs being mismatched across machines, we instead read the .yaml configuration file from the main rank,\n    then serialize the arguments to a dictionary, which the deepspeed launcher broadcasts to all machines (`--megatron_config`).\n\n    We then instantiate a new NeoXArgs from the dictionary (`.from_dict`). This should ensure args are never inconsistent across machines.\n    \"\"\"\n\n    with open(args_parsed.megatron_config) as jsonfile:\n        megatron_config = json.load(jsonfile)\n    if args_parsed.deepspeed_config is not None:\n        overwrite_values = NeoXArgs.set_up_autotuning(\n            args_parsed.deepspeed_config, overwrite_values\n        )\n    if overwrite_values is not None:\n        megatron_config.update(overwrite_values)\n    return NeoXArgs.from_dict(args_dict=megatron_config)\n\n\ndef get_non_existing_dir(tmp_dir):\n    while os.path.exists(tmp_dir):\n        tmp_dir = os.path.join(tmp_dir, \"tmp_dir\")\n    return tmp_dir\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Convert a Hugging Face GPT-NeoX model back to a sequential model compatible with GPT-NeoX training.\"\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=int,\n        default=143000,\n        help=\"Revision or step of the Pythia model to convert.\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        help=\"Path to save the converted GPT-NeoX model checkpoint.\",\n    )\n    parser.add_argument(\n        \"--config\",\n        nargs=\"*\",\n        default=[],\n        help=\"Path to the config file for the equivalent NeoX model.\",\n    )\n    parser.add_argument(\n        \"--test\",\n        action=\"store_true\",\n        help=\"If set, will run a test to ensure the conversion was successful.\",\n    )\n    parser.add_argument(\n        \"--download-only\",\n        action=\"store_true\",\n        help=\"If set, script will only download the model and not convert it.\",\n    )\n\n    parser.add_argument(\n        \"--ckpt-tmp-dir\",\n        default=\"/tmp/ckpt_tmp_dir\",\n        help=\"Directory to store cached hugging face checkpoints. [WARNING: MUST BE VISIBLE TO ALL RANKS]\",\n    )\n    parser.add_argument(\n        \"--hf-model-name\",\n        type=str,\n        help=\"Name of the hugging face model to download from EleutherAI/{hf-model-name}.}\",\n    )\n\n    parser.add_argument(\n        \"--cache-dir\",\n        default=\"/gpfs/alpine/csc499/proj-shared/hf_checkpoints\",\n        help=\"Directory to store cached hugging face checkpoints.\",\n    )\n    try:\n        if int(os.environ[\"WORLD_SIZE\"]) > 1:\n            args = parser.parse_args(MULTI_GPU_ARGS.split(\" \"))\n        else:\n            args = parser.parse_args()\n    except KeyError:\n        args = parser.parse_args()\n\n    tmp_cache_dir = get_non_existing_dir(args.ckpt_tmp_dir)\n\n    if args.download_only:\n        hf_model = GPTNeoXForCausalLM.from_pretrained(\n            f\"EleutherAI/{args.hf_model_name}\",\n            revision=f\"step{args.revision}\",\n            cache_dir=os.path.join(\n                args.cache_dir, f\"{args.hf_model_name}/step{args.revision}\"\n            ),\n        ).half()\n        exit(0)\n    else:\n        print(\"======================================================================\")\n        print(\n            \"Warning the following script will delete files within {}\".format(\n                args.output_dir\n            )\n        )\n        print(\n            \"Warning the following script will delete this directory {}\".format(\n                tmp_cache_dir\n            )\n        )\n        print(\"======================================================================\")\n        # time.sleep(5)\n\n    if int(os.environ.get(\"OMPI_COMM_WORLD_SIZE\", 1)) > 1:\n        neox_args = consume_neox_args2(args2)\n    else:\n        neox_args = NeoXArgs.from_ymls(args.config)\n    neox_args.configure_distributed_args()\n    neox_args.build_tokenizer()\n    neox_args.initialize_tensorboard_writer()\n    neox_args.comet()\n\n    # setup logging and timers\n    # init_wandb(neox_args=neox_args)\n    # timers = Timers(\n    #     use_wandb=neox_args.use_wandb, tensorboard_writer=neox_args.tensorboard_writer\n    # )\n    initialize_megatron(neox_args=neox_args)\n\n    torch.distributed.barrier()\n\n    model = get_model(neox_args=neox_args, use_cache=True)\n    optimizer, param_groups = get_optimizer(model=model, neox_args=neox_args)\n    lr_scheduler = get_learning_rate_scheduler(optimizer=optimizer, neox_args=neox_args)\n\n    model, optimizer, _, lr_scheduler = deepspeed.initialize(\n        model=model,\n        optimizer=optimizer,\n        # args=neox_args,\n        lr_scheduler=lr_scheduler,\n        dist_init_required=False,\n        model_parameters=None,\n        config_params=neox_args.deepspeed_config,\n        mpu=mpu,\n    )\n\n    if os.environ.get(\"OMPI_COMM_WORLD_RANK\", \"1\") == \"0\":\n        os.makedirs(f\"{tmp_cache_dir}\", exist_ok=True)\n\n    torch.distributed.barrier()\n    neox_args.save = tmp_cache_dir\n\n    save_checkpoint(\n        neox_args=neox_args,\n        iteration=0,\n        model=model,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n    )\n    print(os.listdir(f\"{tmp_cache_dir}\"))\n    ckpt_dir = os.path.join(tmp_cache_dir, \"global_step0\")\n\n    if torch.distributed.get_rank() == 0:\n        config = GPTNeoXConfig.from_pretrained(\n            f\"EleutherAI/{args.hf_model_name}\",\n            revision=f\"step{args.revision}\",\n            cache_dir=os.path.join(\n                args.cache_dir, f\"{args.hf_model_name}/step{args.revision}\"\n            ),\n        )\n        # does not change the weights, but is needed to align logits\n        config.update({\"hidden_act\": \"gelu_fast\"})\n        hf_model = GPTNeoXForCausalLM.from_pretrained(\n            f\"EleutherAI/{args.hf_model_name}\",\n            revision=f\"step{args.revision}\",\n            config=config,\n            cache_dir=os.path.join(\n                args.cache_dir, f\"{args.hf_model_name}/step{args.revision}\"\n            ),\n        ).half()\n        print(\"==========================================\")\n        print(\"Loaded Hugging Face model successfully!\")\n        print(\"==========================================\")\n        convert(hf_model, ckpt_dir=ckpt_dir, output_dir=args.output_dir)\n\n        if os.environ.get(\"OMPI_COMM_WORLD_RANK\", \"1\") == \"0\":\n            # cleanup temp dir\n            os.system(f\"rm -r {tmp_cache_dir}\")\n\n    torch.distributed.barrier()\n\n    # verify the conversion can be loaded\n    neox_args.load = \"/\".join(args.output_dir.split(\"/\")[:-1])\n    print(neox_args.load)\n    neox_args.finetune = True\n    load_checkpoint(\n        neox_args=neox_args,\n        model=model,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        iteration=None,\n    )\n    print(\"==========================================\")\n    print(\"Converted checkpoint successfully loaded!\")\n    print(\"==========================================\")\n\n    if args.test and torch.distributed.get_world_size() == 1:\n        # only implemented for world size 1\n\n        with torch.no_grad():\n            # torch.backends.cudnn.benchmark = False\n            # torch.use_deterministic_algorithms(True) #setting the CUBLAS_WORKSPACE_CONFIG=:4096:8 environment variable is required for this to work (tested for A6000)\n            model.eval()\n            hf_model.eval()\n\n            b = 10\n            seq_len = 32\n            inputs = torch.randint(0, 50304, (b, seq_len), dtype=torch.long).cuda()\n            mask = (\n                (torch.triu(torch.ones(seq_len, seq_len)) != 1).transpose(0, 1).cuda()\n            )\n            pos_ids = torch.arange(0, seq_len).unsqueeze(0).cuda()\n\n            torch.manual_seed(0)\n            outputs_neox = model.cuda()(\n                (inputs, pos_ids, mask.unsqueeze(0).unsqueeze(0)), neox_args=neox_args\n            )\n\n            torch.manual_seed(0)\n            outputs = hf_model.cuda()(input_ids=inputs)\n\n            print(\"HF logits   .sum(): \", outputs.logits.to(torch.float32).sum())\n            print(\"NeoX logits .sum(): \", outputs_neox.to(torch.float32).sum())\n\n            print(\n                \"\\nLogit comparison summary for {} sequences of length {}:\".format(\n                    b, seq_len\n                )\n            )\n            print(\"=============================================================\")\n            for i in range(b):\n                abs_diff = (\n                    outputs.logits[i, ...].to(torch.float32)\n                    - outputs_neox[i, ...].to(torch.float32)\n                ).abs()\n                print(\n                    \"[Random sequence {}] (hflogits - neoxlogits).abs() -- mean: {:.5f}\\tmax: {:.5f}\\tmin: {:.5f}\\tmedian: {:.5f}\".format(\n                        i,\n                        abs_diff.mean(),\n                        abs_diff.max(),\n                        abs_diff.min(),\n                        abs_diff.median(),\n                    )\n                )\n\n    elif args.test:\n        print(\n            \"[INFO] Checkpoint conversion logit test not implemented for distributed world_size > 1. Current world_size: {}\".format(\n                torch.distributed.get_world_size()\n            )\n        )\n",
        "tools/ckpts/convert_neox_to_hf.py": "# Copyright (c) 2023, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\nimport yaml\nimport argparse\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import (\n    MistralConfig,\n    LlamaConfig,\n    GPTNeoXConfig,\n    AutoModelForCausalLM,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n)\n\nfrom typing import List, Literal\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\nfrom megatron.tokenizer import build_tokenizer\n\n\n\"\"\"\nA script for converting saved NeoX Checkpoints to Huggingface (HF) compatible GPT-NeoX type models.\n\nNote that this script does not support all NeoX features.\nPlease investigate carefully whether your model is compatible with all architectures supported by the GPTNeoXForCausalLM class in HF.\n\n(e.g. position embeddings such as AliBi may not be supported by Huggingface's GPT-NeoX architecture).\n\"\"\"\n\n\n# Model definitions: a list of keys, and where they fall in terms of handling them in the presence of TP.\n# in format : {model arch: {param type: {param in neox: param in HF}}}\nMODEL_KEYS = {\n    \"neox\": {\n        \"new\": {\n            \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n                \"mlp.linear1.weight\": \"mlp.dense_h_to_4h.weight\",\n                \"mlp.linear1.bias\": \"mlp.dense_h_to_4h.bias\",\n                \"attention.query_key_value.weight\": \"attention.query_key_value.weight\",\n                \"attention.query_key_value.bias\": \"attention.query_key_value.bias\",  # TODO: handle GQA separately?\n            },\n            \"ROW_PARALLEL_LINEAR_KEYS\": {\n                \"attention.dense.weight\": \"attention.dense.weight\",\n                \"mlp.linear2.weight\": \"mlp.dense_4h_to_h.weight\",\n            },\n            \"ROW_PARALLEL_BIAS_KEYS\": {\n                \"mlp.linear2.bias\": \"mlp.dense_4h_to_h.bias\",\n                \"attention.dense.bias\": \"attention.dense.bias\",\n            },\n            \"NORM_KEYS\": {\n                \"input_layernorm.weight\": \"input_layernorm.weight\",\n                \"input_layernorm.bias\": \"input_layernorm.bias\",\n                \"post_attention_layernorm.weight\": \"post_attention_layernorm.weight\",\n                \"post_attention_layernorm.bias\": \"post_attention_layernorm.bias\",\n            },\n            \"FINAL_NORM_KEYS\": {\n                \"norm.weight\": \"weight\",\n                \"norm.bias\": \"bias\",\n            },\n        },\n        \"legacy\": {\n            \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n                \"mlp.dense_h_to_4h.weight\": \"mlp.dense_h_to_4h.weight\",\n                \"mlp.dense_h_to_4h.bias\": \"mlp.dense_h_to_4h.bias\",\n                \"attention.query_key_value.weight\": \"attention.query_key_value.weight\",\n                \"attention.query_key_value.bias\": \"attention.query_key_value.bias\",  # TODO: handle GQA separately?\n            },\n            \"ROW_PARALLEL_LINEAR_KEYS\": {\n                \"attention.dense.weight\": \"attention.dense.weight\",\n                \"mlp.dense_4h_to_h.weight\": \"mlp.dense_4h_to_h.weight\",\n            },\n            \"ROW_PARALLEL_BIAS_KEYS\": {\n                \"mlp.dense_4h_to_h.bias\": \"mlp.dense_4h_to_h.bias\",\n                \"attention.dense.bias\": \"attention.dense.bias\",\n            },\n            \"NORM_KEYS\": {\n                \"input_layernorm.weight\": \"input_layernorm.weight\",\n                \"input_layernorm.bias\": \"input_layernorm.bias\",\n                \"post_attention_layernorm.weight\": \"post_attention_layernorm.weight\",\n                \"post_attention_layernorm.bias\": \"post_attention_layernorm.bias\",\n            },\n            \"FINAL_NORM_KEYS\": {\n                \"norm.weight\": \"weight\",\n                \"norm.bias\": \"bias\",\n            },\n        },\n        ## TODO: Specify mapping dynamically based on TE modules enabled.\n        \"transformer_engine\": {\n            \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n                \"mlp.fc1_weight\": \"mlp.dense_h_to_4h.weight\",\n                \"mlp.fc1_bias\": \"mlp.dense_h_to_4h.bias\",\n                \"attention.qkv.weight\": \"attention.query_key_value.weight\",\n                \"attention.qkv.bias\": \"attention.query_key_value.bias\",\n            },\n            \"ROW_PARALLEL_LINEAR_KEYS\": {\n                \"attention.proj.weight\": \"attention.dense.weight\",\n                \"mlp.fc2_weight\": \"mlp.dense_4h_to_h.weight\",\n            },\n            \"ROW_PARALLEL_BIAS_KEYS\": {\n                \"mlp.fc2_bias\": \"mlp.dense_4h_to_h.bias\",\n                \"attention.proj.bias\": \"attention.dense.bias\",\n            },\n            \"NORM_KEYS\": {\n                \"input_layernorm.weight\": \"input_layernorm.weight\",\n                \"input_layernorm.bias\": \"input_layernorm.bias\",\n                \"mlp.layer_norm_weight\": \"post_attention_layernorm.weight\",\n                \"mlp.layer_norm_bias\": \"post_attention_layernorm.bias\",\n            },\n            \"FINAL_NORM_KEYS\": {\n                \"norm.weight\": \"weight\",\n                \"norm.bias\": \"bias\",\n            },\n            # These keys are Transformer Engine specific and can be ignored\n            \"IGNORE_KEYS\": [\n                \"attention.qkv._extra_state\",\n                \"attention.core_attention._extra_state\",\n                \"attention.proj._extra_state\",\n                \"mlp._extra_state\",\n            ],\n        },\n    },\n    \"llama\": {\n        \"new\": {\n            \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n                \"mlp.linear1.weight\": [\"mlp.up_proj.weight\", \"mlp.gate_proj.weight\"]\n            },\n            \"ROW_PARALLEL_LINEAR_KEYS\": {\n                \"attention.dense.weight\": \"self_attn.o_proj.weight\",\n                \"mlp.linear2.weight\": \"mlp.down_proj.weight\",\n            },\n            \"ROW_PARALLEL_BIAS_KEYS\": {},  # No biases in RowParallelLinear layers\n            \"NORM_KEYS\": {\n                \"input_layernorm.scale\": \"input_layernorm.weight\",\n                \"post_attention_layernorm.scale\": \"post_attention_layernorm.weight\",\n            },\n            \"FINAL_NORM_KEYS\": {\n                \"norm.scale\": \"weight\",\n            },\n            \"GQA_QKV_KEYS\": {  # because Llama can have Grouped Query Attention and has separate Q, K, and V linear proj params, handle them separately.\n                \"attention.query_key_value.weight\": [\n                    \"self_attn.q_proj.weight\",\n                    \"self_attn.k_proj.weight\",\n                    \"self_attn.v_proj.weight\",\n                ],\n            },\n        },\n        \"legacy\": {\n            \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n                \"mlp.w1.weight\": \"mlp.gate_proj.weight\",\n                \"mlp.w3.weight\": \"mlp.up_proj.weight\",\n            },\n            \"ROW_PARALLEL_LINEAR_KEYS\": {\n                \"attention.dense.weight\": \"self_attn.o_proj.weight\",\n                \"mlp.w2.weight\": \"mlp.down_proj.weight\",\n            },\n            \"ROW_PARALLEL_BIAS_KEYS\": {},  # No biases in RowParallelLinear layers\n            \"NORM_KEYS\": {\n                \"input_layernorm.scale\": \"input_layernorm.weight\",\n                \"post_attention_layernorm.scale\": \"post_attention_layernorm.weight\",\n            },\n            \"FINAL_NORM_KEYS\": {\n                \"norm.scale\": \"weight\",\n            },\n            \"GQA_QKV_KEYS\": {  # because Llama can have Grouped Query Attention and has separate Q, K, and V linear proj params, handle them separately.\n                \"attention.query_key_value.weight\": [\n                    \"self_attn.q_proj.weight\",\n                    \"self_attn.k_proj.weight\",\n                    \"self_attn.v_proj.weight\",\n                ],\n            },\n        },\n    },\n}\n\nMODEL_KEYS[\"mistral\"] = MODEL_KEYS[\"llama\"]\n\n\ndef load_partitions(\n    input_checkpoint_path: str, mp_partitions: int, layer_idx: int, sequential: bool\n) -> List[torch.Tensor]:\n    \"\"\"Returns a list containing all states from a model (across MP partitions)\"\"\"\n\n    if sequential:\n        filename_format = f\"mp_rank_{{i:02}}_model_states.pt\"\n    else:\n        filename_format = f\"layer_{layer_idx:02}-model_{{i:02}}-model_states.pt\"\n\n    loaded_tp_ranks = [\n        torch.load(\n            os.path.join(\n                input_checkpoint_path,\n                filename_format.format(i=i),\n            ),\n            map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n        )\n        for i in range(mp_partitions)\n    ]\n\n    return loaded_tp_ranks\n\n\ndef get_state(\n    state_dicts: List[torch.Tensor], key: str, layer_idx: int, sequential: bool\n) -> torch.Tensor:\n    \"\"\"Helper that returns a list containing a given weight's state from each MP partition, for a given layer in the model.\"\"\"\n\n    if sequential:\n        # use the correct key into the sequential dict for given weight/provided key\n        key = f\"sequential.{layer_idx}.{key}\"\n\n        return [state_dict[\"module\"][key] for state_dict in state_dicts]\n    else:\n        # For the PipelineModule case, we don't need any key / module prefix. just grab this weight value.\n        # layer_idx is also ignored because we've loaded only this layer's weights, ahead of time.\n        key = key\n\n        return [state_dict[key] for state_dict in state_dicts]\n\n\ndef get_key(loaded_config, key, default=None):\n    \"\"\"\n    Search for a given key in a NeoX yaml. normalizes underscores -> hyphens\n    \"\"\"\n    key = key.replace(\"_\", \"-\")\n    try:\n        return loaded_config[key]\n    except KeyError:\n        key = key.replace(\"-\", \"_\")\n        try:\n            return loaded_config[key]\n        except KeyError:\n            return default\n\n\ndef create_config(neox_config, architecture=\"neox\", is_rm=False, pad_token_id=-1):\n    \"\"\"take in a loaded yaml from NeoX and assign relevant values to HF config.\n    Returns: GPTNeoXConfig() object\n    \"\"\"\n\n    def gated_size(hidden_dim):\n        # takes in a hidden dim and calculates intermediate dim of a LLaMAParallelMLP.\n        # (only used if intermediate_size not specified in config)\n        # hidden-size * 8 / 3 , rounded up to nearest multiple of 256\n        ff_dim = int(2 * hidden_dim * 4 / 3)\n        ff_dim = 256 * ((ff_dim + 256 - 1) // 256)\n        return ff_dim\n\n    class TokenizerArgs:\n        # kinda hacky.\n        # this is to get something with the same interface as is used in build_tokenizer()\n        # without diving into loading a neox_args object or using argparse etc.\n        def __init__(self, neox_config):\n            self.make_vocab_size_divisible_by = get_key(\n                neox_config, \"make-vocab-size-divisible-by\", default=128\n            )\n            self.model_parallel_size = get_key(neox_config, \"model-parallel-size\")\n            self.vocab_file = get_key(neox_config, \"vocab-file\")\n            self.merge_file = get_key(neox_config, \"merge-file\")\n            self.tokenizer_type = get_key(neox_config, \"tokenizer-type\")\n\n            self.rank = 0\n\n    args = TokenizerArgs(neox_config)\n    tokenizer = build_tokenizer(args)\n    try:  # GPT2TokenizerFast raises NotImplementedError\n        pad_token = tokenizer.pad\n    except:\n        pad_token = (\n            1  # pad defaulting to 1. follows convention from GPT-NeoX-20b tokenizer\n        )\n\n    # TODO: change the default value here based on discussion regarding `gpt_j_tied` config parameter's default\n    use_tied_lns = get_key(neox_config, \"gpt-j-tied\", False)\n\n    if use_tied_lns:\n        raise NotImplementedError(\n            \"\"\"ERROR: Huggingface Transformers does not yet support a single shared layernorm\n                per transformer block for GPT-NeoX models trained  w/ GPT-J parallel residuals.\n                See https://github.com/EleutherAI/gpt-neox/pull/481 for further details.\"\"\"\n        )\n\n    # set all config values.\n\n    # shared config parameters.\n    args = {\n        \"vocab_size\": args.padded_vocab_size,\n        \"hidden_size\": get_key(neox_config, \"hidden-size\"),\n        \"num_hidden_layers\": get_key(neox_config, \"num-layers\"),\n        \"num_attention_heads\": get_key(neox_config, \"num-attention-heads\"),\n        \"max_position_embeddings\": get_key(neox_config, \"max-position-embeddings\"),\n        \"initializer_range\": get_key(neox_config, \"init-method-std\", 0.02),\n        \"tie_word_embeddings\": (not get_key(neox_config, \"no-weight-tying\", False)),\n        \"use_cache\": True,\n    }\n    if architecture == \"mistral\" or architecture == \"llama\":\n        args.update(\n            {\n                \"intermediate_size\": get_key(\n                    neox_config,\n                    \"intermediate-size\",\n                    gated_size(get_key(neox_config, \"hidden-size\")),\n                ),\n                \"num_key_value_heads\": get_key(\n                    neox_config,\n                    \"num-kv-heads\",\n                    get_key(neox_config, \"num-attention-heads\"),\n                ),\n                \"hidden_act\": get_key(\n                    neox_config, \"activation\", default=\"silu\"\n                ).replace(\"swiglu\", \"silu\"),\n                \"rms_norm_eps\": get_key(neox_config, \"rms-norm-epsilon\", 1.0e-6),\n                \"bos_token_id\": tokenizer.eod,\n                \"eos_token_id\": tokenizer.eod,\n                \"rope_theta\": get_key(neox_config, \"rotary-emb-base\", 10000.0),\n            }\n        )\n\n        if architecture == \"mistral\":\n            # mistral-specific options\n            args.update(\n                {\n                    \"sliding_window\": get_key(\n                        neox_config, \"sliding-window-width\", 4096\n                    ),\n                }\n            )\n            hf_config = MistralConfig(**args)\n        elif architecture == \"llama\":\n            # llama-specific options\n            args.update(\n                {\n                    # NeoX library defaults to using bias in attention\n                    \"attention_bias\": get_key(\n                        neox_config, \"use_bias_in_attn_linear\", True\n                    ),\n                }\n            )\n            hf_config = LlamaConfig(**args)\n    else:\n        # GPT-NeoX HF model class-specific options\n        args.update(\n            {\n                \"rotary_pct\": get_key(neox_config, \"rotary-pct\", default=1.0),\n                \"rotary_emb_base\": get_key(\n                    neox_config, \"rotary-emb-base\", default=10000.0\n                ),\n                \"use_parallel_residual\": get_key(neox_config, \"gpt-j-residual\", False),\n                \"layer_norm_eps\": get_key(neox_config, \"layernorm-epsilon\", 1e-5),\n                \"intermediate_size\": get_key(\n                    neox_config,\n                    \"intermediate-size\",\n                    4 * get_key(neox_config, \"hidden-size\"),\n                ),\n            }\n        )\n        hf_config = GPTNeoXConfig(**args)\n    if is_rm:\n        hf_config.num_labels = 1\n        hf_config.pad_token_id = pad_token_id\n\n    return hf_config\n\n\ndef reshard_and_split_qkv(\n    param_mapping: dict,  # a dictionary mapping the QKV weight keys in GPT-NeoX -> a list of keys representing the Q, K, and V weight keys the HF model will use\n    hf_config: AutoConfig,  # a HF model config for the model\n    loaded_tp_ranks: List[torch.Tensor],\n    layer_idx: int,\n    sequential: bool,\n):\n    \"\"\"\n    A helper function which performs reshaping and sharding to make the QKV projection from NeoX compatible with HF Llama models,\n    even when grouped-query attention is required.\n    \"\"\"\n    for key, hf_keys in param_mapping.items():\n        assert (\n            isinstance(hf_keys, list) and len(hf_keys) == 3\n        ), \"Must map QKV to precisely 3 resulting weight matrices.\"\n\n    for key, hf_keys in param_mapping.items():\n        # We first merge the QKV proj. across TP ranks\n        tp_sharded_qkv = torch.stack(\n            get_state(loaded_tp_ranks, key, layer_idx, sequential), dim=0\n        )\n        # We should now have shape [TP_SIZE, (hidden_size + 2 * kv_hidden_size) / TP_SIZE, hidden_size].\n        # At this point, for each TP rank, q, k, and v are concatenated\n\n        # Next, we split tp_harded_qkv into q, k, v along dim 1\n        hidden_size_per_attention_head = (\n            hf_config.hidden_size // hf_config.num_attention_heads\n        )\n        kv_hidden_size = int(\n            hidden_size_per_attention_head * hf_config.num_key_value_heads\n        )\n        tensor_parallel_size = len(loaded_tp_ranks)\n\n        q, k, v = torch.split(\n            tp_sharded_qkv,\n            [\n                hf_config.hidden_size // tensor_parallel_size,\n                kv_hidden_size // tensor_parallel_size,\n                kv_hidden_size // tensor_parallel_size,\n            ],\n            dim=1,\n        )  # New shapes:\n        # q-->[TP_SIZE, hidden_size/TP_SIZE, hidden_size]\n        # k-->[TP_SIZE, kv_hidden_size/TP_SIZE, hidden_size]\n        # v-->[TP_SIZE, kv_hidden_size/TP_SIZE, hidden_size]\n\n        # Finally, we flatten the first two dimensions merging the TP partitions\n        q, k, v = (\n            q.reshape(-1, q.shape[2]),\n            k.reshape(-1, k.shape[2]),\n            v.reshape(-1, k.shape[2]),\n        )\n\n        # return these\n        state_dict = {}\n        for hf_key, proj in zip(hf_keys, [q, k, v]):\n            state_dict[hf_key] = proj.clone()\n        return state_dict\n\n\ndef get_mlp_naming_convention(loaded_tp_ranks, layer_idx, sequential):\n    \"\"\"Determine whether the checkpoint uses the legacy, new, or Transformer Engine naming convention.\"\"\"\n    if sequential:\n        key_list = (\n            loaded_tp_ranks[0][\"module\"].keys()\n            if \"module\" in loaded_tp_ranks[0]\n            else loaded_tp_ranks[0].keys()\n        )\n    else:\n        key_list = loaded_tp_ranks[0].keys()\n\n    if any([\"mlp.fc1_weight\" in key for key in key_list]):\n        return \"transformer_engine\"\n    elif any([\"mlp.linear1.weight\" in key for key in key_list]):\n        return \"new\"\n    elif any([\"mlp.dense_h_to_4h.weight\" in key for key in key_list]):\n        return \"legacy\"\n    else:\n        raise ValueError(\"Unable to determine MLP naming convention in checkpoint\")\n\n\ndef convert(\n    input_checkpoint_path,\n    loaded_config,\n    output_checkpoint_path,\n    sequential: bool = True,\n    precision: Literal[\"auto\", \"fp16\", \"bf16\", \"fp32\"] = \"auto\",\n    architecture: Literal[\"neox\", \"llama\", \"mistral\"] = \"neox\",\n    is_rm: bool = False,\n    pad_token_id: int = -1,\n):\n    \"\"\"convert a NeoX checkpoint to a HF model format.\n    should perform model-parallel merging correctly\n    but only supports features allowed by HF GPT-NeoX implementation (e.g. rotary embeddings)\n    \"\"\"\n\n    ARCH = MODEL_KEYS[architecture]\n\n    hf_config = create_config(\n        loaded_config, architecture=architecture, is_rm=is_rm, pad_token_id=pad_token_id\n    )\n\n    if not is_rm:\n        hf_model = AutoModelForCausalLM.from_config(hf_config)\n    else:\n        hf_model = AutoModelForSequenceClassification.from_config(hf_config)\n\n    if architecture == \"neox\":\n        hf_transformer = hf_model.gpt_neox\n    else:\n        hf_transformer = hf_model.model\n\n    if precision == \"auto\":\n        print(\"Auto-detecting precision to save model into...\")\n        # save model in FP16 if Deepspeed fp16 was used in config, else 32 bit\n        fp16 = get_key(loaded_config, \"fp16\")\n\n        if fp16:\n            try:\n                # current behavior is to pass \"fp16\": {\"enabled\": true}, when using upstream Deepspeed\n                if fp16[\"enabled\"]:\n                    hf_model.half()\n                    print(\"Saving weights in fp16 precision...\")\n            except:\n                try:\n                    # attempt to access bf16 dict in yaml file, if fp16 not enabled\n                    bf16 = get_key(loaded_config, \"bf16\")\n                    if bf16:\n                        hf_model.to(dtype=torch.bfloat16)\n                        print(\"Saving weights in bf16 precision...\")\n                except:\n                    hf_model.to(dtype=torch.float)\n                    print(\n                        \"Model not trained in fp16 / bf16 mixed precision, saving weights in fp32...\"\n                    )\n    else:\n        name_to_dtype = {\n            \"bf16\": torch.bfloat16,\n            \"fp16\": torch.float16,\n            \"fp32\": torch.float,\n        }\n        print(f\"Saving model into specified {precision} precision...\")\n        hf_model.to(dtype=name_to_dtype[precision])\n\n    mp_partitions = get_key(loaded_config, \"model-parallel-size\")\n\n    # Sequential saves all model states from an MP rank in one file.\n    # so we only load the MP ranks only once and index into them with get_state().\n    # for the pipeline-parallel case (pipeline-parallel-size >= 1),\n    # we must load the correct layer's states at each step.\n    # (this does mean that less memory is required for PP conversion.)\n    loaded_tp_ranks = load_partitions(\n        input_checkpoint_path, mp_partitions, layer_idx=0, sequential=sequential\n    )\n\n    ### Embedding layer ###\n    # Embedding is layer idx 0\n    if architecture == \"neox\":\n        embed_in = hf_transformer.embed_in\n    else:\n        embed_in = hf_transformer.embed_tokens\n    embed_in.load_state_dict(  # TODO: embed_in is not always model's name for embedding\n        {\n            \"weight\": torch.cat(\n                get_state(\n                    loaded_tp_ranks,\n                    \"word_embeddings.weight\",\n                    layer_idx=0,\n                    sequential=sequential,\n                ),\n                dim=0,\n            )\n        }\n    )\n    assert (\n        hf_config.vocab_size == embed_in.weight.shape[0]\n    ), f\"ERROR: calculated vocab size {hf_config.vocab_size} != embed param size {embed_in.shape[0]}\"\n    ### End Embedding Layer ###\n\n    # grab from 3rd layer to pass embeddings\n    mlp_naming = get_mlp_naming_convention(\n        load_partitions(\n            input_checkpoint_path,\n            mp_partitions,\n            layer_idx=3,\n            sequential=sequential,\n        ),\n        0,\n        sequential,\n    )\n    print(f\"Detected MLP naming convention: {mlp_naming}\")\n    ARCH = ARCH[mlp_naming]\n\n    for layer_i in tqdm(range(get_key(loaded_config, \"num-layers\"))):\n\n        # get layer from hf model\n        hf_layer = hf_transformer.layers[layer_i]  # TODO: model module names\n\n        if not sequential:\n            # in the non-sequential case, must load from each layer individually.\n            # use layer index + 2 bc of embed layer and a dummy _pre_transformer_block, which are \"layers 0 and 1\"\n            loaded_tp_ranks = load_partitions(\n                input_checkpoint_path,\n                mp_partitions,\n                layer_idx=layer_i + 2,\n                sequential=sequential,\n            )\n\n        # Skip keys that should be ignored\n        if \"IGNORE_KEYS\" in ARCH:\n            # Just for logging purposes, check if the ignore keys exist\n            for key in ARCH[\"IGNORE_KEYS\"]:\n                try:\n                    _ = get_state(\n                        loaded_tp_ranks,\n                        key,\n                        layer_idx=layer_i + 2,\n                        sequential=sequential,\n                    )\n                except Exception:\n                    pass\n\n        # + 2 bc of embed layer and a dummy _pre_transformer_block\n        state_dict = {}\n        for key, hf_key in ARCH[\"ROW_PARALLEL_LINEAR_KEYS\"].items():\n            state_dict[hf_key] = torch.cat(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                ),\n                dim=1,\n            )\n\n        # average layernorm stats over mp ranks\n        for key, hf_key in ARCH[\"NORM_KEYS\"].items():\n            state_dict[hf_key] = sum(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                )\n            ) / len(loaded_tp_ranks)\n\n        # LinearWithTPMerge\n        for key, hf_key in ARCH[\"COLUMN_PARALLEL_LINEAR_KEYS\"].items():\n            if type(hf_key) == list:\n                # Llama magic - split the weight into two parts for the gate and up proj\n                states = [\n                    torch.chunk(state, chunks=2, dim=0)\n                    for state in get_state(\n                        loaded_tp_ranks,\n                        key,\n                        layer_idx=layer_i + 2,\n                        sequential=sequential,\n                    )\n                ]\n                # Set up proj...\n                state_dict[hf_key[0]] = torch.cat([state[0] for state in states], dim=0)\n                # Set gate proj...\n                state_dict[hf_key[1]] = torch.cat([state[1] for state in states], dim=0)\n            else:\n                state_dict[hf_key] = torch.cat(\n                    get_state(\n                        loaded_tp_ranks,\n                        key,\n                        layer_idx=layer_i + 2,\n                        sequential=sequential,\n                    ),\n                    dim=0,\n                )\n\n        # LinearWithTPSplitBias\n        for key, hf_key in ARCH[\"ROW_PARALLEL_BIAS_KEYS\"].items():\n            state_dict[hf_key] = sum(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                )\n            )\n\n        # Just take one\n        if \"attention.bias\" in hf_layer.state_dict():\n            state_dict[\"attention.bias\"] = hf_layer.state_dict()[\"attention.bias\"]\n        if \"attention.masked_bias\" in hf_layer.state_dict():\n            state_dict[\"attention.masked_bias\"] = hf_layer.state_dict()[\n                \"attention.masked_bias\"\n            ]\n\n        # some architectures, like Mistral and Llama, have the following which must be handled specially:\n        # - Q, K, V projections are performed separately, so we must split apart GPT-NeoX library's single QKV proj\n        # - Support for Grouped-Query Attention, meaning the Q and the K, V projections may not be the same size\n        if \"GQA_QKV_KEYS\" in ARCH:\n            state_dict.update(\n                reshard_and_split_qkv(\n                    param_mapping=ARCH[\"GQA_QKV_KEYS\"],\n                    hf_config=hf_config,\n                    loaded_tp_ranks=loaded_tp_ranks,\n                    layer_idx=layer_i + 2,\n                    sequential=sequential,\n                )\n            )\n        # load state_dict into layer\n        hf_layer.load_state_dict(state_dict)\n\n    if not sequential:\n        loaded_tp_ranks = load_partitions(\n            input_checkpoint_path,\n            mp_partitions,\n            get_key(loaded_config, \"num-layers\") + 3,\n            sequential=sequential,\n        )\n    # Load final layer norm\n    norm_state_dict = {}\n    for key, hf_key in ARCH[\"FINAL_NORM_KEYS\"].items():\n        norm_state_dict[hf_key] = sum(\n            get_state(\n                loaded_tp_ranks,\n                key,\n                layer_idx=get_key(loaded_config, \"num-layers\") + 3,\n                sequential=sequential,\n            )\n        ) / len(loaded_tp_ranks)\n\n    if architecture == \"neox\":\n        final_layer_norm = hf_transformer.final_layer_norm\n    else:\n        final_layer_norm = hf_transformer.norm\n\n    final_layer_norm.load_state_dict(norm_state_dict)\n\n    # Load output embedding\n    if not sequential:\n        if get_key(loaded_config, \"no-weight-tying\", False):\n            # if we have trained input + output embedding layers without tied weights\n            loaded_tp_ranks = load_partitions(\n                input_checkpoint_path,\n                mp_partitions,\n                get_key(loaded_config, \"num-layers\") + 4,\n                sequential=sequential,\n            )\n        else:\n            # in this case, output embedding layer and input embedding layer are tied.\n            # load + save the input embed weights into the output embedding layer's place.\n            loaded_tp_ranks = load_partitions(\n                input_checkpoint_path,\n                mp_partitions,\n                layer_idx=0,\n                sequential=sequential,\n            )\n    # output embedding / LM head\n    if not is_rm:\n        if architecture == \"neox\":  # name of lm head / final linear proj varies\n            lm_head = hf_model.embed_out\n        else:\n            lm_head = hf_model.lm_head\n    else:\n        lm_head = hf_model.score\n\n    if get_key(loaded_config, \"no-weight-tying\", False):\n        # save the (untied) final linear into LM head for HF\n        lm_head.load_state_dict(\n            {\n                \"weight\": torch.cat(\n                    get_state(\n                        loaded_tp_ranks,\n                        \"final_linear.weight\" if not is_rm else \"rm_linear.weight\",\n                        layer_idx=get_key(loaded_config, \"num-layers\") + 4,\n                        sequential=sequential,\n                    ),\n                    dim=0 if not is_rm else 1,\n                ),\n            }\n        )\n    else:\n        # don't need to worry about rm here since you can't really tie them...\n\n        # embedding layers are tied. transpose input layer and save\n        lm_head.load_state_dict(\n            {\n                \"weight\": torch.cat(\n                    get_state(\n                        loaded_tp_ranks,\n                        \"word_embeddings.weight\",\n                        layer_idx=0,\n                        sequential=sequential,\n                    ),\n                    dim=0,\n                ),\n            }\n        )\n\n    del loaded_tp_ranks\n\n    return hf_model\n\n\ndef main(input_args=None, overwrite_values=None):\n    from huggingface_hub import create_repo, HfApi\n\n    parser = argparse.ArgumentParser(\n        description=\"Merge MP partitions and convert to HF Model.\"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        help=\"Path to NeoX checkpoint, e.g. /path/to/model/global_step143000\",\n    )\n    parser.add_argument(\n        \"--config_file\",\n        type=str,\n        help=\"Path to config file for the input NeoX checkpoint.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        help=\"Output dir, where to save the HF Model, tokenizer, and configs\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        default=\"auto\",\n        help=\"What precision to save the model into. Defaults to auto, which auto-detects which 16-bit dtype to save into, or falls back to fp32.\",\n    )\n    parser.add_argument(\n        \"--no_save_tokenizer\",\n        action=\"store_true\",\n        help=\"Whether to skip saving the tokenizer alongside a model.\",\n    )\n    parser.add_argument(\n        \"--vocab-is-hf-tokenizer\",\n        action=\"store_true\",\n        help=\"Whether the vocab file is in a Huggingface tokenizer path.\",\n    )\n    parser.add_argument(\n        \"--pad-token-id\",\n        type=int,\n        default=-1,\n        help=\"Pad token id to set in tokenizer. Required for RM style models.\",\n    )\n    parser.add_argument(\n        \"--architecture\",\n        type=str,\n        default=\"neox\",\n        help=\"What HF model class type to export into.\",\n    )\n    args = parser.parse_args(input_args)\n\n    # validate arguments\n    assert args.precision in [\n        \"auto\",\n        \"fp16\",\n        \"bf16\",\n        \"fp32\",\n    ], f\"expected --precision to be one of 'auto', 'fp16', 'bf16', 'fp32' but got '{args.precision}' !\"\n    assert args.architecture in [\n        \"neox\",\n        \"llama\",\n        \"mistral\",\n    ], f\"expected --architecture to be one of 'neox', 'mistral', 'llama', but got '{args.architecture}' !\"\n\n    with open(args.config_file) as f:\n        loaded_config = yaml.full_load(f)\n        if overwrite_values:\n            loaded_config.update(overwrite_values)\n\n    # Determine the checkpoint format of the model.\n    # DeepSpeed saves models wrapped in a PipelineModule differently from those not.\n    # PipelineModule models are saved as per-layer state dicts per TP shard,\n    # while Sequential model state dicts are saved all together in one mp_rank_xx_model_states.pt\n    # file per tensor/model parallel shard.\n    pipeline_world_size = get_key(loaded_config, \"pipe-parallel-size\", 1)\n    is_rm = get_key(loaded_config, \"train_impl\", \"normal\") == \"rm\"\n    if is_rm and args.pad_token_id == -1:\n        raise ValueError(\"RM models require a pad token id to be set.\")\n    if pipeline_world_size == 0:\n        sequential = True\n        print(\n            f\"Detected 'pipe-parallel-size' of {pipeline_world_size}, assuming model is saved as Sequential...\"\n        )\n    else:\n        sequential = False\n        print(\n            f\"Detected 'pipe-parallel-size' of {pipeline_world_size}, assuming model is saved as PipelineModule...\"\n        )\n\n    # convert the model to HF.\n    hf_model = convert(\n        args.input_dir,\n        loaded_config,\n        args.output_dir,\n        sequential=sequential,\n        architecture=args.architecture,\n        is_rm=is_rm,\n        pad_token_id=args.pad_token_id,\n    )\n\n    # Save to disk.\n    hf_model.save_pretrained(args.output_dir)\n\n    if not args.no_save_tokenizer:\n        # save tokenizer to directory as well, for easy loading of model as a HF model.\n        tokenizer_type = get_key(loaded_config, \"tokenizer-type\")\n        if args.vocab_is_hf_tokenizer:\n            from transformers import AutoTokenizer\n\n            tokenizer = AutoTokenizer.from_pretrained(\n                os.path.dirname(get_key(loaded_config, \"vocab-file\"))\n            )\n            if args.pad_token_id != -1:\n                tokenizer.pad_token_id = args.pad_token_id\n            print(\"loaded tokenizer: \", tokenizer)\n            tokenizer.save_pretrained(args.output_dir)\n            print(\"tokenizer saved!\")\n        elif tokenizer_type == \"HFTokenizer\":  # TODO: handle sentencepiece tokenizers?\n            print(f\"saving tokenizer from file {get_key(loaded_config, 'vocab-file')}\")\n            print(\n                \"Warning: please check that your model config and tokenizer end with the correct special tokens (EOS, BOS).\"\n            )\n            from transformers import PreTrainedTokenizerFast\n\n            tokenizer = PreTrainedTokenizerFast(\n                tokenizer_file=get_key(loaded_config, \"vocab-file\")\n            )\n            if args.pad_token_id != -1:\n                tokenizer.pad_token_id = args.pad_token_id\n            print(\"loaded tokenizer: \", tokenizer)\n            tokenizer.save_pretrained(args.output_dir)\n            print(\"tokenizer saved!\")\n\n\nif __name__ == \"__main__\":\n\n    # before running script:\n    # `pip install --upgrade transformers`\n    # `huggingface-cli login`\n    #\n    main()\n",
        "tools/ckpts/convert_neox_to_mamba_ssm.py": "import torch\n\nfrom convert_neox_to_hf import load_partitions, get_key, get_state\n\nfrom mamba_ssm.models.config_mamba import MambaConfig\nfrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n\n\nimport argparse\nfrom typing import Literal\nimport yaml\nfrom tqdm import tqdm\n\nimport os\nimport sys\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\nfrom megatron.tokenizer import build_tokenizer\n\n\"\"\"\nConversion utility for converting a Mamba model\ntrained in GPT-NeoX into the mamba_ssm package ckpt format.\n\"\"\"\nARCH = {\n    \"COLUMN_PARALLEL_LINEAR_KEYS\": {\n        # these require concat across dim=0\n        \"mixer.in_proj.weight\": \"mixer.in_proj.weight\",\n        # \"mixer.in_proj.bias\": \"mixer.in_proj.bias\",\n        \"mixer.A_log\": \"mixer.A_log\",\n        \"mixer.D\": \"mixer.D\",\n        \"mixer.conv1d.weight\": \"mixer.conv1d.weight\",\n        \"mixer.conv1d.bias\": \"mixer.conv1d.bias\",\n        \"mixer.dt_proj.weight\": \"mixer.dt_proj.weight\",\n        \"mixer.dt_proj.bias\": \"mixer.dt_proj.bias\",\n    },\n    \"ROW_PARALLEL_LINEAR_KEYS\": {\n        # these require concat across dim=1\n        \"mixer.out_proj.weight\": \"mixer.out_proj.weight\",\n        \"mixer.x_proj.weight\": \"mixer.x_proj.weight\",\n    },\n    \"ROW_PARALLEL_BIAS_KEYS\": {\n        # these require summing across ranks\n        # \"mixer.x_proj.bias\": \"mixer.x_proj.bias\",\n        # \"mixer.out_proj.bias\": \"mixer.out_proj.bias\",\n    },\n    \"NORM_KEYS\": {\n        \"norm.scale\": \"norm.weight\",\n        # \"norm.bias\": \"norm.bias\",\n    },\n    \"FINAL_NORM_KEYS\": {\n        \"norm.scale\": \"weight\",\n        # \"norm.bias\": \"bias\",\n    },\n}\n\n\ndef create_config(neox_config):\n    class TokenizerArgs:\n        # kinda hacky.\n        # this is to get something with the same interface as is used in build_tokenizer()\n        # without diving into loading a neox_args object or using argparse etc.\n        def __init__(self, neox_config):\n            self.make_vocab_size_divisible_by = get_key(\n                neox_config, \"make-vocab-size-divisible-by\", default=128\n            )\n            self.model_parallel_size = get_key(neox_config, \"model-parallel-size\")\n            self.vocab_file = get_key(neox_config, \"vocab-file\")\n            self.merge_file = get_key(neox_config, \"merge-file\")\n            self.tokenizer_type = get_key(neox_config, \"tokenizer-type\")\n\n            self.rank = 0\n\n    args = TokenizerArgs(neox_config)\n    tokenizer = build_tokenizer(args)\n    try:  # GPT2TokenizerFast raises NotImplementedError\n        pad_token = tokenizer.pad\n    except:\n        pad_token = (\n            1  # pad defaulting to 1. follows convention from GPT-NeoX-20b tokenizer\n        )\n    norm_type = get_key(neox_config, \"norm\", \"layernorm\")\n    if norm_type == \"rmsnorm\":\n        use_rms_norm = True\n    else:\n        assert (\n            norm_type == \"layernorm\"\n        ), \"only layernorm or rmsnorm supported by mamba_ssm!\"\n        use_rms_norm = False\n    return MambaConfig(\n        d_model=get_key(neox_config, \"hidden_size\"),\n        n_layer=get_key(neox_config, \"num_layers\"),\n        vocab_size=args.padded_vocab_size,\n        rms_norm=use_rms_norm,\n        residual_in_fp32=False,\n        fused_add_norm=True,\n        # shouldn't really matter? we didn't train with it but should be equiv.\n        # it's faster though\n        # pad_vocab_size_multiple_of=get_key(neox_config, \"make_vocab_size_divisible_by\", 128),\n        tie_embeddings=not get_key(\n            neox_config, \"no_weight_tying\", False\n        ),  # requires newer mamba_ssm>=1.2.0.post1\n    )\n\n\ndef convert(\n    input_checkpoint_path,\n    loaded_config,\n    output_checkpoint_path,\n    sequential: bool = True,\n    precision: Literal[\"auto\", \"fp16\", \"bf16\", \"fp32\"] = \"auto\",\n):\n\n    mamba_config = create_config(loaded_config)\n\n    if precision == \"auto\":\n        print(\"Auto-detecting precision to save model into...\")\n        # save model in FP16 if Deepspeed fp16 was used in config, else 32 bit\n        fp16 = get_key(loaded_config, \"fp16\")\n\n        if fp16:\n            try:\n                # current behavior is to pass \"fp16\": {\"enabled\": true}, when using upstream Deepspeed\n                if fp16[\"enabled\"]:\n                    dtype = torch.float16\n                    print(\"Saving weights in fp16 precision...\")\n            except:\n                try:\n                    # attempt to access bf16 dict in yaml file, if fp16 not enabled\n                    bf16 = get_key(loaded_config, \"bf16\")\n                    if bf16:\n                        dtype = torch.bfloat16\n                        print(\"Saving weights in bf16 precision...\")\n                except:\n                    dtype = torch.float\n                    print(\n                        \"Model not trained in fp16 / bf16 mixed precision, saving weights in fp32...\"\n                    )\n    else:\n        name_to_dtype = {\n            \"bf16\": torch.bfloat16,\n            \"fp16\": torch.float16,\n            \"fp32\": torch.float,\n        }\n        print(f\"Saving model into specified {precision} precision...\")\n        dtype = name_to_dtype[precision]\n\n    mamba_model = MambaLMHeadModel(\n        config=mamba_config,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        dtype=torch.float,\n    )  # dtype)\n\n    mp_partitions = get_key(loaded_config, \"model-parallel-size\")\n\n    # Sequential saves all model states from an MP rank in one file.\n    # so we only load the MP ranks only once and index into them with get_state().\n    # for the pipeline-parallel case (pipeline-parallel-size >= 1),\n    # we must load the correct layer's states at each step.\n    # (this does mean that less memory is required for PP conversion.)\n    loaded_tp_ranks = load_partitions(\n        input_checkpoint_path, mp_partitions, layer_idx=0, sequential=sequential\n    )\n\n    mamba_model.backbone.embedding.load_state_dict(\n        {\n            \"weight\": torch.cat(\n                get_state(\n                    loaded_tp_ranks,\n                    \"word_embeddings.weight\",\n                    layer_idx=0,\n                    sequential=sequential,\n                ),\n                dim=0,\n            )\n        }\n    )\n\n    for layer_i in tqdm(range(get_key(loaded_config, \"num-layers\"))):\n\n        layer = mamba_model.backbone.layers[layer_i]\n\n        if not sequential:\n            # in the non-sequential case, must load from each layer individually.\n            # use layer index + 2 bc of embed layer and a dummy _pre_transformer_block, which are \"layers 0 and 1\"\n            loaded_tp_ranks = load_partitions(\n                input_checkpoint_path,\n                mp_partitions,\n                layer_idx=layer_i + 2,\n                sequential=sequential,\n            )\n\n        state_dict = {}\n\n        for key, hf_key in ARCH[\"ROW_PARALLEL_LINEAR_KEYS\"].items():  # ROW_PARALLEL\n            state_dict[hf_key] = torch.cat(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                ),\n                dim=1,\n            )\n\n        # average layernorm stats over mp ranks\n        for key, hf_key in ARCH[\"NORM_KEYS\"].items():\n            state_dict[hf_key] = sum(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                )\n            ) / len(loaded_tp_ranks)\n\n        # LinearWithTPMerge\n        for key, hf_key in ARCH[\"COLUMN_PARALLEL_LINEAR_KEYS\"].items():\n            state_dict[hf_key] = torch.cat(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                ),\n                dim=0,\n            )\n\n        # LinearWithTPSplitBias\n        for key, hf_key in ARCH[\"ROW_PARALLEL_BIAS_KEYS\"].items():\n            state_dict[hf_key] = sum(\n                get_state(\n                    loaded_tp_ranks, key, layer_idx=layer_i + 2, sequential=sequential\n                )\n            )\n\n        layer.load_state_dict(state_dict)\n\n    if not sequential:\n        loaded_tp_ranks = load_partitions(\n            input_checkpoint_path,\n            mp_partitions,\n            get_key(loaded_config, \"num-layers\") + 3,\n            sequential=sequential,\n        )\n\n    norm_state_dict = {}\n    for key, hf_key in ARCH[\"FINAL_NORM_KEYS\"].items():\n        norm_state_dict[hf_key] = sum(\n            get_state(\n                loaded_tp_ranks,\n                key,\n                layer_idx=get_key(loaded_config, \"num-layers\") + 3,\n                sequential=sequential,\n            )\n        ) / len(loaded_tp_ranks)\n\n    final_layer_norm = mamba_model.backbone.norm_f\n\n    final_layer_norm.load_state_dict(norm_state_dict)\n\n    if not sequential:\n        loaded_tp_ranks = load_partitions(\n            input_checkpoint_path,\n            mp_partitions,\n            get_key(loaded_config, \"num-layers\") + 4,\n            sequential=sequential,\n        )\n\n    lm_head = mamba_model.lm_head\n\n    lm_head.load_state_dict(\n        {\n            \"weight\": torch.cat(\n                get_state(\n                    loaded_tp_ranks,\n                    \"final_linear.weight\",\n                    layer_idx=get_key(loaded_config, \"num-layers\") + 4,\n                    sequential=sequential,\n                ),\n                dim=0,\n            ),\n        }\n    )\n\n    del loaded_tp_ranks\n\n    return mamba_model\n\n\ndef main(input_args=None, overwrite_values=None):\n\n    parser = argparse.ArgumentParser(\n        description=\"Merge MP partitions and convert to HF Model.\"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        help=\"Path to NeoX checkpoint, e.g. /path/to/model/global_step143000\",\n    )\n    parser.add_argument(\n        \"--config_file\",\n        type=str,\n        help=\"Path to config file for the input NeoX checkpoint.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        help=\"Output dir, where to save the HF Model, tokenizer, and configs\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        default=\"auto\",\n        help=\"What precision to save the model into. Defaults to auto, which auto-detects which 16-bit dtype to save into, or falls back to fp32.\",\n    )\n    parser.add_argument(\n        \"--no_save_tokenizer\",\n        action=\"store_true\",\n        help=\"Whether to skip saving the tokenizer alongside a model.\",\n    )\n    args = parser.parse_args(input_args)\n\n    # validate arguments\n    assert args.precision in [\n        \"auto\",\n        \"fp16\",\n        \"bf16\",\n        \"fp32\",\n    ], f\"expected --precision to be one of 'auto', 'fp16', 'bf16', 'fp32' but got '{args.precision}' !\"\n\n    with open(args.config_file) as f:\n        loaded_config = yaml.full_load(f)\n        if overwrite_values:\n            loaded_config.update(overwrite_values)\n\n    # Determine the checkpoint format of the model.\n    # DeepSpeed saves models wrapped in a PipelineModule differently from those not.\n    # PipelineModule models are saved as per-layer state dicts per TP shard,\n    # while Sequential model state dicts are saved all together in one mp_rank_xx_model_states.pt\n    # file per tensor/model parallel shard.\n    pipeline_world_size = get_key(loaded_config, \"pipe-parallel-size\", 1)\n    if pipeline_world_size == 0:\n        sequential = True\n        print(\n            f\"Detected 'pipe-parallel-size' of {pipeline_world_size}, assuming model is saved as Sequential...\"\n        )\n    else:\n        sequential = False\n        print(\n            f\"Detected 'pipe-parallel-size' of {pipeline_world_size}, assuming model is saved as PipelineModule...\"\n        )\n\n    model = convert(\n        args.input_dir,\n        loaded_config,\n        args.output_dir,\n        sequential=sequential,\n        precision=args.precision,\n    )\n\n    model.save_pretrained(args.output_dir)\n\n\nif __name__ == \"__main__\":\n\n    main()\n",
        "tools/ckpts/convert_raw_llama_weights_to_neox.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport json\nimport math\nimport tqdm.auto as tqdm\n\n\nINTERMEDIATE_SIZE_MAP = {\n    \"7B\": 11008,\n    \"13B\": 13824,\n    \"30B\": 17920,\n    \"34B\": 22016,\n    \"65B\": 22016,\n    \"70B\": 28672,\n    \"mistral-7B-v0.1\": 14336,\n}\nNUM_SHARDS = {\n    \"7B\": 1,\n    \"13B\": 2,\n    \"30B\": 4,\n    \"34B\": 4,\n    \"65B\": 8,\n    \"70B\": 8,\n    \"mistral-7B-v0.1\": 1,\n}\n\n\ndef compute_intermediate_size(n):\n    return int(math.ceil(n * 8 / 3) + 255) // 256 * 256\n\n\ndef read_json(path):\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n\ndef write_json(text, path):\n    with open(path, \"w\") as f:\n        json.dump(text, f)\n\n\ndef write_file(text, path):\n    with open(path, \"w\") as f:\n        f.write(text)\n\n\ndef convert_model_pipeline(\n    output_base_path, input_base_path, model_size: str, num_output_shards: int\n):\n    assert model_size in NUM_SHARDS\n\n    model_path = os.path.join(output_base_path, \"global_step0\")\n    os.makedirs(model_path, exist_ok=True)\n    write_file(\"global_step0\", os.path.join(output_base_path, \"latest\"))\n\n    params = read_json(os.path.join(input_base_path, \"params.json\"))\n    num_input_shards = NUM_SHARDS[model_size]\n    num_layers = params[\"n_layers\"]\n    num_heads = params[\"n_heads\"]\n    if \"n_kv_heads\" in params:\n        num_kv_heads = params[\"n_kv_heads\"]\n    else:\n        num_kv_heads = num_heads\n    num_kv_heads_per_input_shard = num_kv_heads // num_input_shards\n    num_heads_per_input_shard = num_heads // num_input_shards\n    num_heads_per_output_shard = num_heads // num_output_shards\n    num_kv_heads_per_output_shard = num_kv_heads // num_output_shards\n    hidden_size = params[\"dim\"]\n    dims_per_head = hidden_size // num_heads\n    # base = 10000.0\n    # inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n\n    def permute_rotary(w):\n        if w.shape == (num_heads, dims_per_head, hidden_size):\n            N_HEADS = num_heads\n        elif w.shape == (num_kv_heads, dims_per_head, hidden_size):\n            N_HEADS = num_kv_heads\n        else:\n            assert False\n        return (\n            w.view(N_HEADS, dims_per_head // 2, 2, hidden_size)\n            .transpose(1, 2)\n            .reshape(N_HEADS, dims_per_head, hidden_size)\n        )\n\n    pbar = tqdm.tqdm(total=num_input_shards + num_layers + 3)\n\n    pbar.set_description(f\"Loading shard\")\n    loaded = []\n    for i in range(num_input_shards):\n        loaded.append(\n            torch.load(\n                os.path.join(input_base_path, f\"consolidated.{i:02d}.pth\"),\n                map_location=\"cpu\",\n            )\n        )\n        pbar.set_description(f\"Loaded shard {i}/{num_input_shards}\")\n        pbar.update(1)\n    helper = Helper(\n        loaded=loaded,\n        model_path=model_path,\n        num_output_shards=num_output_shards,\n        model_size=model_size,\n        pipeline_parallel=False,\n    )\n\n    sequential_cache = [{} for _ in range(num_output_shards)]\n\n    # Embedding in\n    embeddings_in = torch.cat(\n        [\n            loaded[rank][\"tok_embeddings.weight\"].cpu()\n            for rank in range(num_input_shards)\n        ],\n        dim=1,\n    )\n    print(embeddings_in.shape)\n    helper.save_shards(\n        {\"word_embeddings.weight\": helper.shard(embeddings_in, dim=0)}, layer_i=0\n    )\n    helper.del_loaded(\"tok_embeddings.weight\")\n    pbar.set_description(f\"Saved embeddings\")\n    pbar.update(1)\n\n    # Norms\n    helper.save_duplicates(\n        {\"norm.scale\": loaded[0][\"norm.weight\"]}, layer_i=num_layers + 3\n    )\n    helper.del_loaded(\"norm.weight\")\n    pbar.set_description(f\"Saved final norm\")\n    pbar.update(1)\n\n    # Embedding out\n    embeddings_out = torch.cat(\n        [loaded[rank][\"output.weight\"].cpu() for rank in range(num_input_shards)], dim=0\n    )\n    helper.save_shards(\n        {\"final_linear.weight\": helper.shard(embeddings_out, dim=0)},\n        layer_i=num_layers + 4,\n    )\n    helper.del_loaded(\"output.weight\")\n    pbar.set_description(f\"Saved out embeddings\")\n    pbar.update(1)\n\n    # Layers\n    for layer_i in range(num_layers):\n\n        # Linear\n        attn_wo = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wo.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=1,\n            ),\n            dim=1,\n        )\n        mlp_w1 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w1.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            ),\n            dim=0,\n        )\n        mlp_w2 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w2.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=1,\n            ),\n            dim=1,\n        )\n        mlp_w3 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w3.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            ),\n            dim=0,\n        )\n        helper.del_loaded(f\"layers.{layer_i}.attention.wo.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w1.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w2.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w3.weight\")\n\n        # Attention\n        w_q = permute_rotary(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wq.weight\"].view(\n                        num_heads_per_input_shard, dims_per_head, hidden_size\n                    )\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            )\n        )\n        w_k = permute_rotary(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wk.weight\"].view(\n                        num_kv_heads_per_input_shard, dims_per_head, hidden_size\n                    )\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            )\n        ).view(num_heads, int(dims_per_head * (num_kv_heads / num_heads)), hidden_size)\n\n        w_v = torch.cat(\n            [\n                loaded[rank][f\"layers.{layer_i}.attention.wv.weight\"].view(\n                    num_kv_heads_per_input_shard, dims_per_head, hidden_size\n                )\n                for rank in range(num_input_shards)\n            ],\n            dim=0,\n        ).view(num_heads, int(dims_per_head * (num_kv_heads / num_heads)), hidden_size)\n\n        sharded_qkv = torch.cat(\n            [\n                helper.shard(\n                    w_q, dim=0\n                ),  # num_output_shards, num_heads_per_output_shard, dims_per_head, hidden_size\n                helper.shard(w_k, dim=0),\n                helper.shard(w_v, dim=0),\n            ],\n            dim=2,\n        )  # num_output_shards, num_heads_per_output_shard, QKV=3, dims_per_head, hidden_size\n\n        sharded_qkv = sharded_qkv.view(\n            num_output_shards,\n            num_heads_per_output_shard * dims_per_head\n            + 2 * num_kv_heads_per_output_shard * dims_per_head,\n            hidden_size,\n        )\n        helper.del_loaded(f\"layers.{layer_i}.attention.wq.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.attention.wk.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.attention.wv.weight\")\n\n        # Duplicated\n        input_layernorm = loaded[0][f\"layers.{layer_i}.attention_norm.weight\"]\n        post_attention_layernorm = loaded[0][f\"layers.{layer_i}.ffn_norm.weight\"]\n        helper.del_loaded(f\"layers.{layer_i}.attention_norm.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.ffn_norm.weight\")\n\n        for out_rank in range(num_output_shards):\n            helper.save(\n                {\n                    \"attention.query_key_value.weight\": sharded_qkv[out_rank],\n                    # Sharded layers\n                    \"attention.dense.weight\": attn_wo[out_rank].clone(),\n                    \"mlp.w1.weight\": mlp_w1[out_rank].clone(),\n                    \"mlp.w2.weight\": mlp_w2[out_rank].clone(),\n                    \"mlp.w3.weight\": mlp_w3[out_rank].clone(),\n                    # Duplicated layers\n                    \"input_layernorm.scale\": input_layernorm,\n                    \"post_attention_layernorm.scale\": post_attention_layernorm,\n                },\n                layer_i=layer_i + 2,\n                rank=out_rank,\n            )\n\n        pbar.set_description(f\"Saved layer {layer_i} / {num_layers}\")\n        pbar.update(1)\n\n    model_state = {\n        \"dp_world_size\": 1,\n        \"mp_world_size\": num_output_shards,\n        \"module\": {},\n        \"optimizer\": {},\n        \"global_steps\": 1,\n        \"skipped_steps\": 1,\n        \"iteration\": 1,\n    }\n    for rank in range(num_output_shards):\n        torch.save(\n            model_state, os.path.join(model_path, f\"mp_rank_{rank:02d}_model_states.pt\")\n        )\n    pbar.set_description(\"Done.\")\n\n\ndef convert_model_sequential(\n    output_base_path, input_base_path, model_size: str, num_output_shards: int\n):\n    assert model_size in NUM_SHARDS\n\n    model_path = os.path.join(output_base_path, \"global_step0\")\n    os.makedirs(model_path, exist_ok=True)\n    write_file(\"global_step0\", os.path.join(output_base_path, \"latest\"))\n\n    params = read_json(os.path.join(input_base_path, \"params.json\"))\n    num_input_shards = NUM_SHARDS[model_size]\n    num_layers = params[\"n_layers\"]\n    num_heads = params[\"n_heads\"]\n    if \"n_kv_heads\" in params:\n        num_kv_heads = params[\"n_kv_heads\"]\n    else:\n        num_kv_heads = num_heads\n    num_kv_heads_per_input_shard = num_kv_heads // num_input_shards\n    num_heads_per_input_shard = num_heads // num_input_shards\n    num_heads_per_output_shard = num_heads // num_output_shards\n    num_kv_heads_per_output_shard = num_kv_heads // num_output_shards\n    hidden_size = params[\"dim\"]\n    dims_per_head = hidden_size // num_heads\n    # base = 10000.0\n    # inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n\n    def permute_rotary(w):\n        if w.shape == (num_heads, dims_per_head, hidden_size):\n            N_HEADS = num_heads\n        elif w.shape == (num_kv_heads, dims_per_head, hidden_size):\n            N_HEADS = num_kv_heads\n        else:\n            assert False\n        return (\n            w.view(N_HEADS, dims_per_head // 2, 2, hidden_size)\n            .transpose(1, 2)\n            .reshape(N_HEADS, dims_per_head, hidden_size)\n        )\n\n    pbar = tqdm.tqdm(total=num_input_shards + num_output_shards)\n\n    pbar.set_description(f\"Loading shard\")\n    loaded = []\n    for i in range(num_input_shards):\n        loaded.append(\n            torch.load(\n                os.path.join(input_base_path, f\"consolidated.{i:02d}.pth\"),\n                map_location=\"cpu\",\n            )\n        )\n        pbar.set_description(f\"Loaded shard {i}/{num_input_shards}\")\n        pbar.update(1)\n    helper = Helper(\n        loaded=loaded,\n        model_path=model_path,\n        num_output_shards=num_output_shards,\n        model_size=model_size,\n        pipeline_parallel=False,\n    )\n\n    # Embedding in\n    embeddings_in = torch.cat(\n        [\n            loaded[rank][\"tok_embeddings.weight\"].cpu()\n            for rank in range(num_input_shards)\n        ],\n        dim=1,\n    )\n\n    helper.add_sequential_shard(\n        {\"word_embeddings.weight\": helper.shard(embeddings_in, dim=0)}, layer_i=0\n    )\n    helper.del_loaded(\"tok_embeddings.weight\")\n\n    # Norms\n    helper.add_sequential_duplicates(\n        {\"norm.scale\": loaded[0][\"norm.weight\"]}, layer_i=num_layers + 3\n    )\n    helper.del_loaded(\"norm.weight\")\n\n    # Embedding out\n    embeddings_out = torch.cat(\n        [loaded[rank][\"output.weight\"].cpu() for rank in range(num_input_shards)], dim=0\n    )\n    helper.add_sequential_shard(\n        {\"final_linear.weight\": helper.shard(embeddings_out, dim=0)},\n        layer_i=num_layers + 4,\n    )\n    helper.del_loaded(\"output.weight\")\n\n    # Layers\n    for layer_i in range(num_layers):\n\n        # Linear\n        attn_wo = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wo.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=1,\n            ),\n            dim=1,\n        )\n        mlp_w1 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w1.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            ),\n            dim=0,\n        )\n        mlp_w2 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w2.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=1,\n            ),\n            dim=1,\n        )\n        mlp_w3 = helper.shard(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.feed_forward.w3.weight\"]\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            ),\n            dim=0,\n        )\n        helper.del_loaded(f\"layers.{layer_i}.attention.wo.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w1.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w2.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.feed_forward.w3.weight\")\n\n        # Attention\n        w_q = permute_rotary(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wq.weight\"].view(\n                        num_heads_per_input_shard, dims_per_head, hidden_size\n                    )\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            )\n        )\n\n        w_k = permute_rotary(\n            torch.cat(\n                [\n                    loaded[rank][f\"layers.{layer_i}.attention.wk.weight\"].view(\n                        num_kv_heads_per_input_shard, dims_per_head, hidden_size\n                    )\n                    for rank in range(num_input_shards)\n                ],\n                dim=0,\n            )\n        ).view(num_heads, int(dims_per_head * (num_kv_heads / num_heads)), hidden_size)\n\n        w_v = torch.cat(\n            [\n                loaded[rank][f\"layers.{layer_i}.attention.wv.weight\"].view(\n                    num_kv_heads_per_input_shard, dims_per_head, hidden_size\n                )\n                for rank in range(num_input_shards)\n            ],\n            dim=0,\n        ).view(num_heads, int(dims_per_head * (num_kv_heads / num_heads)), hidden_size)\n\n        sharded_qkv = torch.cat(\n            [\n                helper.shard(\n                    w_q, dim=0\n                ),  # num_output_shards, num_heads_per_output_shard, dims_per_head, hidden_size\n                helper.shard(w_k, dim=0),\n                helper.shard(w_v, dim=0),\n            ],\n            dim=2,\n        )  # num_output_shards, num_heads_per_output_shard, QKV=3, dims_per_head, hidden_size\n\n        sharded_qkv = sharded_qkv.view(\n            num_output_shards,\n            num_heads_per_output_shard * dims_per_head\n            + 2 * num_kv_heads_per_output_shard * dims_per_head,\n            hidden_size,\n        )\n\n        helper.del_loaded(f\"layers.{layer_i}.attention.wq.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.attention.wk.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.attention.wv.weight\")\n\n        # Duplicated\n        input_layernorm = loaded[0][f\"layers.{layer_i}.attention_norm.weight\"]\n        post_attention_layernorm = loaded[0][f\"layers.{layer_i}.ffn_norm.weight\"]\n        helper.del_loaded(f\"layers.{layer_i}.attention_norm.weight\")\n        helper.del_loaded(f\"layers.{layer_i}.ffn_norm.weight\")\n\n        for out_rank in range(num_output_shards):\n            helper.add_sequential(\n                {\n                    \"attention.query_key_value.weight\": sharded_qkv[out_rank],\n                    # Sharded layers\n                    \"attention.dense.weight\": attn_wo[out_rank].clone(),\n                    \"mlp.w1.weight\": mlp_w1[out_rank].clone(),\n                    \"mlp.w2.weight\": mlp_w2[out_rank].clone(),\n                    \"mlp.w3.weight\": mlp_w3[out_rank].clone(),\n                    # Duplicated layers\n                    \"input_layernorm.scale\": input_layernorm,\n                    \"post_attention_layernorm.scale\": post_attention_layernorm,\n                },\n                layer_i=layer_i + 2,\n                rank=out_rank,\n            )\n\n    for rank in range(num_output_shards):\n        model_state = {\n            \"dp_world_size\": 1,\n            \"mp_world_size\": num_output_shards,\n            \"module\": helper.sequential_cache[rank],\n            \"optimizer\": {},\n            \"global_steps\": 1,\n            \"skipped_steps\": 1,\n            \"iteration\": 1,\n        }\n        torch.save(\n            model_state, os.path.join(model_path, f\"mp_rank_{rank:02d}_model_states.pt\")\n        )\n        pbar.set_description(f\"Saved shard {rank}\")\n        pbar.update(1)\n    pbar.set_description(\"Done.\")\n\n\nclass Helper:\n    def __init__(\n        self, loaded, model_size, num_output_shards, model_path, pipeline_parallel\n    ):\n        self.loaded = loaded\n        self.model_size = model_size\n        self.num_output_shards = num_output_shards\n        self.model_path = model_path\n\n        self.pipeline_parallel = pipeline_parallel\n        self.sequential_cache = [{} for _ in range(num_output_shards)]\n\n    def del_loaded(self, key: str):\n        # Remove from memory as we go along\n        for loaded_shared in self.loaded:\n            del loaded_shared[key]\n\n    def save_shards(self, dictionary, layer_i: int):\n        for k, v in dictionary.items():\n            assert v.shape[0] == self.num_output_shards\n        for rank in range(self.num_output_shards):\n            torch.save(\n                {k: v[rank].clone() for k, v in dictionary.items()},\n                self.save_path(layer_i=layer_i, rank=rank),\n            )\n\n    def save_duplicates(self, dictionary, layer_i: int):\n        for rank in range(self.num_output_shards):\n            torch.save(\n                {k: v.clone() for k, v in dictionary.items()},\n                self.save_path(layer_i=layer_i, rank=rank),\n            )\n\n    def save(self, obj, layer_i, rank):\n        torch.save(obj, self.save_path(layer_i=layer_i, rank=rank))\n\n    def shard(self, x, dim):\n        x_shape = list(x.shape)\n        assert x_shape[dim] % self.num_output_shards == 0\n        new_x_shape = (\n            x_shape[:dim]\n            + [self.num_output_shards, x_shape[dim] // self.num_output_shards]\n            + x_shape[dim + 1 :]\n        )\n        x = x.view(*new_x_shape)\n        return torch.movedim(x, 0, dim)\n\n    def save_path(self, layer_i, rank):\n        return os.path.join(\n            self.model_path, f\"layer_{layer_i:02d}-model_{rank:02d}-model_states.pt\"\n        )\n\n    def add_sequential_shard(self, dictionary, layer_i):\n        assert not self.pipeline_parallel\n        for k, v in dictionary.items():\n            for rank in range(self.num_output_shards):\n                self.sequential_cache[rank][f\"sequential.{layer_i}.{k}\"] = v[\n                    rank\n                ].clone()\n\n    def add_sequential_duplicates(self, dictionary, layer_i):\n        assert not self.pipeline_parallel\n        for k, v in dictionary.items():\n            for rank in range(self.num_output_shards):\n                self.sequential_cache[rank][f\"sequential.{layer_i}.{k}\"] = v.clone()\n\n    def add_sequential(self, dictionary, layer_i, rank):\n        assert not self.pipeline_parallel\n        for k, v in dictionary.items():\n            self.sequential_cache[rank][f\"sequential.{layer_i}.{k}\"] = v.clone()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Convert raw LLaMA or Mistral checkpoints to GPT-NeoX format.\"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        help=\"Location of parent directory, which contains tokenizer.model and model weights subfolders\",\n    )\n    parser.add_argument(\n        \"--model_size\",\n        choices=[\"7B\", \"mistral-7B-v0.1\", \"13B\", \"30B\", \"34B\", \"65B\", \"tokenizer_only\"],\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        help=\"Location to write GPT-NeoX model\",\n    )\n    parser.add_argument(\n        \"--num_output_shards\",\n        type=int,\n        default=1,\n    )\n    parser.add_argument(\n        \"--pipeline_parallel\",\n        action=\"store_true\",\n        help=\"Only use if PP>1\",\n    )\n    args = parser.parse_args()\n    if args.pipeline_parallel:\n        print(\"parallel\")\n        convert_model_pipeline(\n            output_base_path=args.output_dir,\n            input_base_path=os.path.join(args.input_dir, args.model_size),\n            model_size=args.model_size,\n            num_output_shards=args.num_output_shards,\n        )\n    else:\n        print(\"sequential\")\n        convert_model_sequential(\n            output_base_path=args.output_dir,\n            input_base_path=os.path.join(args.input_dir, args.model_size),\n            model_size=args.model_size,\n            num_output_shards=args.num_output_shards,\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "tools/ckpts/inspect_checkpoints.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Adapted from https://github.com/awaelchli/pytorch-lightning-snippets/blob/master/checkpoint/peek.py\n\nimport code\nimport os\nimport re\nfrom argparse import ArgumentParser, Namespace\nfrom collections.abc import Mapping, Sequence\nfrom pathlib import Path\n\nimport torch\n\n\nclass COLORS:\n    BLUE = \"\\033[94m\"\n    CYAN = \"\\033[96m\"\n    GREEN = \"\\033[92m\"\n    RED = \"\\033[31m\"\n    YELLOW = \"\\033[33m\"\n    MAGENTA = \"\\033[35m\"\n    WHITE = \"\\033[37m\"\n    UNDERLINE = \"\\033[4m\"\n    END = \"\\033[0m\"\n\n\nPRIMITIVE_TYPES = (int, float, bool, str, type)\n\n\ndef natural_sort(l):\n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", str(key))]\n    return sorted(l, key=alphanum_key)\n\n\ndef sizeof_fmt(num, suffix=\"B\"):\n    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, \"Yi\", suffix)\n\n\ndef pretty_print(contents: dict):\n    \"\"\"Prints a nice summary of the top-level contents in a checkpoint dictionary.\"\"\"\n    col_size = max(len(str(k)) for k in contents)\n    for k, v in sorted(contents.items()):\n        key_length = len(str(k))\n        line = \" \" * (col_size - key_length)\n        line += f\"{k}: {COLORS.BLUE}{type(v).__name__}{COLORS.END}\"\n        if isinstance(v, dict) and v:\n            pretty_print(v)\n        elif isinstance(v, PRIMITIVE_TYPES):\n            line += f\" = \"\n            line += f\"{COLORS.CYAN}{repr(v)}{COLORS.END}\"\n        elif isinstance(v, Sequence):\n            line += \", \"\n            line += f\"{COLORS.CYAN}len={len(v)}{COLORS.END}\"\n        elif isinstance(v, torch.Tensor):\n            if v.ndimension() in (0, 1) and v.numel() == 1:\n                line += f\" = \"\n                line += f\"{COLORS.CYAN}{v.item()}{COLORS.END}\"\n            else:\n                line += \", \"\n                line += f\"{COLORS.CYAN}shape={list(v.shape)}{COLORS.END}\"\n                line += \", \"\n                line += f\"{COLORS.CYAN}dtype={v.dtype}{COLORS.END}\"\n            line += (\n                \", \"\n                + f\"{COLORS.CYAN}size={sizeof_fmt(v.nelement() * v.element_size())}{COLORS.END}\"\n            )\n        print(line)\n\n\ndef common_entries(*dcts):\n    if not dcts:\n        return\n    for i in set(dcts[0]).intersection(*dcts[1:]):\n        yield (i,) + tuple(d[i] for d in dcts)\n\n\ndef pretty_print_double(contents1: dict, contents2: dict, args):\n    \"\"\"Prints a nice summary of the top-level contents in a checkpoint dictionary.\"\"\"\n    col_size = max(\n        max(len(str(k)) for k in contents1), max(len(str(k)) for k in contents2)\n    )\n    common_keys = list(contents1.keys() & contents2.keys())\n    uncommon_keys_1 = [i for i in contents2.keys() if i not in common_keys]\n    uncommon_keys_2 = [i for i in contents1.keys() if i not in common_keys]\n    diffs_found = False\n    if uncommon_keys_1 + uncommon_keys_2:\n        diffs_found = True\n        if uncommon_keys_1:\n            print(\n                f\"{COLORS.RED}{len(uncommon_keys_1)} key(s) found in ckpt 1 that isn't present in ckpt 2:{COLORS.END} \\n\\t{COLORS.BLUE}{' '.join(uncommon_keys_1)}{COLORS.END}\"\n            )\n        if uncommon_keys_2:\n            print(\n                f\"{COLORS.RED}{len(uncommon_keys_2)} key(s) found in ckpt 2 that isn't present in ckpt 1:{COLORS.END} \\n\\t{COLORS.BLUE}{' '.join(uncommon_keys_2)}{COLORS.END}\"\n            )\n    for k, v1, v2 in sorted(common_entries(contents1, contents2)):\n        key_length = len(str(k))\n        line = \" \" * (col_size - key_length)\n        if type(v1) != type(v2):\n            print(\n                f\"{COLORS.RED}{k} is a different type between ckpt1 and ckpt2: ({type(v1).__name__} vs. {type(v2).__name__}){COLORS.END}\"\n            )\n            continue\n        else:\n            prefix = f\"{k}: {COLORS.BLUE}{type(v1).__name__} | {type(v2).__name__}{COLORS.END}\"\n        if isinstance(v1, dict) and v1 and v2:\n            pretty_print_double(v1, v2, args)\n        elif isinstance(v1, PRIMITIVE_TYPES):\n            if repr(v1) != repr(v2):\n                c = COLORS.RED\n                line += f\" = \"\n                line += f\"{c}{repr(v1)} | {repr(v2)}{COLORS.END}\"\n            else:\n                c = COLORS.CYAN\n                if not args.diff:\n                    line += f\" = \"\n                    line += f\"{c}{repr(v1)} | {repr(v2)}{COLORS.END}\"\n        elif isinstance(v1, Sequence):\n            if len(v1) != len(v2):\n                c = COLORS.RED\n                line += \", \"\n                line += f\"{c}len={len(v1)} | len={len(v2)}{COLORS.END}\"\n            else:\n                c = COLORS.CYAN\n                if not args.diff:\n                    line += \", \"\n                    line += f\"{c}len={len(v1)} | len={len(v2)}{COLORS.END}\"\n        elif isinstance(v1, torch.Tensor):\n            if v1.ndimension() != v2.ndimension():\n                c = COLORS.RED\n            else:\n                c = COLORS.CYAN\n\n            if (v1.ndimension() in (0, 1) and v1.numel() == 1) and (\n                v2.ndimension() in (0, 1) and v2.numel() == 1\n            ):\n                if not args.diff:\n                    line += f\" = \"\n                    line += f\"{c}{v1.item()} | {c}{v2.item()}{COLORS.END}\"\n            else:\n                if list(v1.shape) != list(v2.shape):\n                    c = COLORS.RED\n                    line += \", \"\n                    line += f\"{c}shape={list(v1.shape)} | shape={list(v2.shape)}{COLORS.END}\"\n                else:\n                    c = COLORS.CYAN\n                    if not args.diff:\n                        line += \", \"\n                        line += f\"{c}shape={list(v1.shape)} | shape={list(v2.shape)}{COLORS.END}\"\n                if v1.dtype != v2.dtype:\n                    c = COLORS.RED\n                    line += f\"{c}dtype={v1.dtype} | dtype={v2.dtype}{COLORS.END}\"\n\n                else:\n                    c = COLORS.CYAN\n                    if not args.diff:\n                        line += \", \"\n                        line += f\"{c}dtype={v1.dtype} | dtype={v2.dtype}{COLORS.END}\"\n                if list(v1.shape) == list(v2.shape):\n                    if torch.allclose(v1, v2):\n                        if not args.diff:\n                            line += f\", {COLORS.CYAN}VALUES EQUAL{COLORS.END}\"\n                    else:\n                        line += f\", {COLORS.RED}VALUES DIFFER{COLORS.END}\"\n\n        if line.replace(\" \", \"\") != \"\":\n            line = prefix + line\n            print(line)\n            diffs_found = True\n    if args.diff and not diffs_found:\n        pass\n    else:\n        if not args.diff:\n            print(\"\\n\")\n\n    return diffs_found\n\n\ndef get_attribute(obj: object, name: str) -> object:\n    if isinstance(obj, Mapping):\n        return obj[name]\n    if isinstance(obj, Namespace):\n        return obj.name\n    return getattr(object, name)\n\n\ndef get_files(pth):\n    if os.path.isdir(pth):\n        files = list(Path(pth).glob(\"*.pt\")) + list(Path(pth).glob(\"*.ckpt\"))\n    elif os.path.isfile(pth):\n        assert pth.endswith(\".pt\") or pth.endswith(\".ckpt\")\n        files = [Path(pth)]\n    else:\n        raise ValueError(\"Dir / File not found.\")\n    return natural_sort(files)\n\n\ndef peek(args: Namespace):\n\n    files = get_files(args.dir)\n\n    for file in files:\n        file = Path(file).absolute()\n        print(f\"{COLORS.GREEN}{file.name}:{COLORS.END}\")\n        ckpt = torch.load(file, map_location=torch.device(\"cpu\"))\n        selection = dict()\n        attribute_names = args.attributes or list(ckpt.keys())\n        for name in attribute_names:\n            parts = name.split(\"/\")\n            current = ckpt\n            for part in parts:\n                current = get_attribute(current, part)\n            selection.update({name: current})\n        pretty_print(selection)\n        print(\"\\n\")\n\n        if args.interactive:\n            code.interact(\n                banner=\"Entering interactive shell. You can access the checkpoint contents through the local variable 'checkpoint'.\",\n                local={\"checkpoint\": ckpt, \"torch\": torch},\n            )\n\n\ndef get_shared_fnames(files_1, files_2):\n    names_1 = [Path(i).name for i in files_1]\n    names_1_parent = Path(files_1[0]).parent\n    names_2 = [Path(i).name for i in files_2]\n    names_2_parent = Path(files_2[0]).parent\n    shared_names = list(set.intersection(*map(set, [names_1, names_2])))\n    return [names_1_parent / i for i in shared_names], [\n        names_2_parent / i for i in shared_names\n    ]\n\n\ndef get_selection(filename, args):\n    ckpt = torch.load(filename, map_location=torch.device(\"cpu\"))\n    selection = dict()\n    attribute_names = args.attributes or list(ckpt.keys())\n    for name in attribute_names:\n        parts = name.split(\"/\")\n        current = ckpt\n        for part in parts:\n            current = get_attribute(current, part)\n        selection.update({name: current})\n    return selection\n\n\ndef compare(args: Namespace):\n    dirs = [i.strip() for i in args.dir.split(\",\")]\n    assert len(dirs) == 2, \"Only works with 2 directories / files\"\n    files_1 = get_files(dirs[0])\n    files_2 = get_files(dirs[1])\n    files_1, files_2 = get_shared_fnames(files_1, files_2)\n\n    for file1, file2 in zip(files_1, files_2):\n        file1 = Path(file1).absolute()\n        file2 = Path(file2).absolute()\n        print(f\"COMPARING {COLORS.GREEN}{file1.name} & {file2.name}:{COLORS.END}\")\n        selection_1 = get_selection(file1, args)\n        selection_2 = get_selection(file2, args)\n        diffs_found = pretty_print_double(selection_1, selection_2, args)\n        if args.diff and diffs_found:\n            print(\n                f\"{COLORS.RED}THE ABOVE DIFFS WERE FOUND IN {file1.name} & {file2.name} ^{COLORS.END}\\n\"\n            )\n\n        if args.interactive:\n            code.interact(\n                banner=\"Entering interactive shell. You can access the checkpoint contents through the local variable 'selection_1' / 'selection_2'.\\nPress Ctrl-D to exit.\",\n                local={\n                    \"selection_1\": selection_1,\n                    \"selection_2\": selection_2,\n                    \"torch\": torch,\n                },\n            )\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\n        \"dir\",\n        type=str,\n        help=\"The checkpoint dir to inspect. Must be either: \\\n         - a directory containing pickle binaries saved with 'torch.save' ending in .pt or .ckpt \\\n         - a single path to a .pt or .ckpt file \\\n         - two comma separated directories - in which case the script will *compare* the two checkpoints\",\n    )\n    parser.add_argument(\n        \"--attributes\",\n        nargs=\"*\",\n        help=\"Name of one or several attributes to query. To access an attribute within a nested structure, use '/' as separator.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--interactive\",\n        \"-i\",\n        action=\"store_true\",\n        help=\"Drops into interactive shell after printing the summary.\",\n    )\n    parser.add_argument(\n        \"--compare\",\n        \"-c\",\n        action=\"store_true\",\n        help=\"If true, script will compare two directories separated by commas\",\n    )\n    parser.add_argument(\n        \"--diff\", \"-d\", action=\"store_true\", help=\"In compare mode, only print diffs\"\n    )\n\n    args = parser.parse_args()\n    if args.compare:\n        compare(args)\n    else:\n        peek(args)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "tools/ckpts/merge20b.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport yaml\nimport shutil\nfrom tqdm import auto as tqdm_lib\n\n\nVOCAB_SIZE = 50432\nIGNORED_MODEL_STATE_KEYS = [\n    \"optimizer\",\n    \"random_rng_state\",\n    \"np_rng_state\",\n    \"torch_rng_state\",\n    \"cuda_rng_state\",\n    \"rng_tracker_states\",\n]\n\n\ndef modify_config(input_config_path, output_config_path, output_dir):\n    with open(input_config_path) as f:\n        loaded_config = yaml.full_load(f)\n\n    # replace model/pipeline parallel\n    loaded_config[\"model_parallel_size\"] = 1\n    loaded_config[\"pipe_parallel_size\"] = 1\n\n    # replace load / save directories:\n    loaded_config[\"load\"] = output_dir\n    loaded_config[\"save\"] = output_dir\n\n    # replace some other paths\n    loaded_config[\"vocab_file\"] = os.path.join(output_dir, \"20B_tokenizer.json\")\n    loaded_config[\"log_dir\"] = \"./logs\"\n\n    # we need to make sure the resulting vocab size is correct\n    # do this by modifying the 'make_vocab_size_divisible_by' argument to be\n    # orig * (orig_mp / mp_out)\n    loaded_config[\"make_vocab_size_divisible_by\"] = VOCAB_SIZE\n\n    # remove zero optimizer\n    loaded_config[\"zero_optimization\"][\"stage\"] = 0\n\n    with open(output_config_path, \"w\") as f:\n        yaml.dump(loaded_config, f)\n\n\ndef modify_model_states(input_model_state_path, output_model_state_path):\n    model_state = torch.load(input_model_state_path)\n    for key in IGNORED_MODEL_STATE_KEYS:\n        del model_state[key]\n    model_state[\"mp_world_size\"] = 1\n    model_state[\"dp_world_size\"] = 1  # could make this configurable?\n    model_state[\"args\"][\"model_parallel_size\"] = 1\n    model_state[\"args\"][\"make_vocab_size_divisible_by\"] = VOCAB_SIZE\n    torch.save(model_state, output_model_state_path)\n\n\ndef merge_model_weights(input_checkpoint_path, output_checkpoint_path):\n    pbar = tqdm_lib.tqdm(total=47)\n\n    # Load transformer layers\n    for layer_i in range(44):\n        pbar.set_description(f\"Merging layer {layer_i}\")\n        filename_tp1 = f\"layer_{layer_i + 2:02d}-model_00-model_states.pt\"\n        filename_tp2 = f\"layer_{layer_i + 2:02d}-model_01-model_states.pt\"\n        loaded_tp1 = torch.load(os.path.join(input_checkpoint_path, filename_tp1))\n        loaded_tp2 = torch.load(os.path.join(input_checkpoint_path, filename_tp2))\n        # noinspection PyDictCreation\n        merged = {}\n\n        # RowParallelLinear\n        merged[\"mlp.dense_4h_to_h.weight\"] = torch.cat(\n            [\n                loaded_tp1[\"mlp.dense_4h_to_h.weight\"],\n                loaded_tp2[\"mlp.dense_4h_to_h.weight\"],\n            ],\n            dim=1,\n        )\n        merged[\"attention.dense.weight\"] = torch.cat(\n            [\n                loaded_tp1[\"attention.dense.weight\"],\n                loaded_tp2[\"attention.dense.weight\"],\n            ],\n            dim=1,\n        )\n        merged[\"mlp.dense_4h_to_h.bias\"] = (\n            loaded_tp1[\"mlp.dense_4h_to_h.bias\"] + loaded_tp2[\"mlp.dense_4h_to_h.bias\"]\n        )\n        merged[\"attention.dense.bias\"] = (\n            loaded_tp1[\"attention.dense.bias\"] + loaded_tp2[\"attention.dense.bias\"]\n        )\n\n        # Layer Norms\n        merged[\"input_layernorm.weight\"] = (\n            loaded_tp1[\"input_layernorm.weight\"] + loaded_tp2[\"input_layernorm.weight\"]\n        ) / 2\n        merged[\"input_layernorm.bias\"] = (\n            loaded_tp1[\"input_layernorm.bias\"] + loaded_tp2[\"input_layernorm.bias\"]\n        ) / 2\n        merged[\"post_attention_layernorm.weight\"] = (\n            loaded_tp1[\"post_attention_layernorm.weight\"]\n            + loaded_tp2[\"post_attention_layernorm.weight\"]\n        ) / 2\n        merged[\"post_attention_layernorm.bias\"] = (\n            loaded_tp1[\"post_attention_layernorm.bias\"]\n            + loaded_tp2[\"post_attention_layernorm.bias\"]\n        ) / 2\n\n        # ColumnParallelLinear\n        merged[\"mlp.dense_h_to_4h.weight\"] = torch.cat(\n            [\n                loaded_tp1[\"mlp.dense_h_to_4h.weight\"],\n                loaded_tp2[\"mlp.dense_h_to_4h.weight\"],\n            ],\n            dim=0,\n        )\n        merged[\"mlp.dense_h_to_4h.bias\"] = torch.cat(\n            [\n                loaded_tp1[\"mlp.dense_h_to_4h.bias\"],\n                loaded_tp2[\"mlp.dense_h_to_4h.bias\"],\n            ],\n            dim=0,\n        )\n        merged[\"attention.query_key_value.weight\"] = torch.cat(\n            [\n                loaded_tp1[\"attention.query_key_value.weight\"],\n                loaded_tp2[\"attention.query_key_value.weight\"],\n            ],\n            dim=0,\n        )\n        merged[\"attention.query_key_value.bias\"] = torch.cat(\n            [\n                loaded_tp1[\"attention.query_key_value.bias\"],\n                loaded_tp2[\"attention.query_key_value.bias\"],\n            ],\n            dim=0,\n        )\n\n        # Just take one\n        merged[\"attention.rotary_emb.inv_freq\"] = loaded_tp1[\n            \"attention.rotary_emb.inv_freq\"\n        ]\n\n        torch.save(merged, os.path.join(output_checkpoint_path, filename_tp1))\n        del loaded_tp1\n        del loaded_tp2\n        pbar.update(1)\n\n    # Load input embedding\n    pbar.set_description(f\"Merging input embedding\")\n    loaded_tp1 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_00-model_00-model_states.pt\")\n    )\n    loaded_tp2 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_00-model_01-model_states.pt\")\n    )\n    merged = {\n        \"word_embeddings.weight\": torch.cat(\n            [\n                loaded_tp1[\"word_embeddings.weight\"],\n                loaded_tp2[\"word_embeddings.weight\"],\n            ],\n            dim=0,\n        )\n    }\n    torch.save(\n        merged,\n        os.path.join(output_checkpoint_path, \"layer_00-model_00-model_states.pt\"),\n    )\n    del loaded_tp1\n    del loaded_tp2\n    pbar.update(1)\n\n    # Load final layer norm\n    pbar.set_description(f\"Merging final layer norm\")\n    loaded_tp1 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_47-model_00-model_states.pt\")\n    )\n    loaded_tp2 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_47-model_01-model_states.pt\")\n    )\n    merged = {\n        \"norm.weight\": (loaded_tp1[\"norm.weight\"] + loaded_tp2[\"norm.weight\"]) / 2,\n        \"norm.bias\": (loaded_tp1[\"norm.bias\"] + loaded_tp2[\"norm.bias\"]) / 2,\n    }\n    torch.save(\n        merged,\n        os.path.join(output_checkpoint_path, \"layer_47-model_00-model_states.pt\"),\n    )\n    del loaded_tp1\n    del loaded_tp2\n    pbar.update(1)\n\n    # Load output embedding\n    pbar.set_description(f\"Merging output embedding\")\n    loaded_tp1 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_48-model_00-model_states.pt\")\n    )\n    loaded_tp2 = torch.load(\n        os.path.join(input_checkpoint_path, \"layer_48-model_01-model_states.pt\")\n    )\n    merged = {\n        \"final_linear.weight\": torch.cat(\n            [\n                loaded_tp1[\"final_linear.weight\"],\n                loaded_tp2[\"final_linear.weight\"],\n            ],\n            dim=0,\n        ),\n    }\n    torch.save(\n        merged,\n        os.path.join(output_checkpoint_path, \"layer_48-model_00-model_states.pt\"),\n    )\n    del loaded_tp1\n    del loaded_tp2\n    pbar.update(1)\n    pbar.set_description(\"Done.\")\n\n\ndef merge(input_dir, output_dir):\n    input_checkpoint_path = os.path.join(input_dir, \"global_step150000\")\n    output_checkpoint_path = os.path.join(output_dir, \"global_step150000\")\n    os.makedirs(output_checkpoint_path, exist_ok=True)\n    os.makedirs(os.path.join(output_dir, \"configs\"), exist_ok=True)\n    for i in range(8):\n        modify_model_states(\n            input_model_state_path=os.path.join(\n                input_checkpoint_path, f\"mp_rank_{i:02d}_model_states.pt\"\n            ),\n            output_model_state_path=os.path.join(\n                output_checkpoint_path, f\"mp_rank_{i:02d}_model_states.pt\"\n            ),\n        )\n    modify_config(\n        input_config_path=os.path.join(input_dir, \"configs\", \"20B.yml\"),\n        output_config_path=os.path.join(output_dir, \"configs\", \"20B.yml\"),\n        output_dir=output_dir,\n    )\n    merge_model_weights(\n        input_checkpoint_path=input_checkpoint_path,\n        output_checkpoint_path=output_checkpoint_path,\n    )\n    shutil.copyfile(\n        os.path.join(input_dir, \"20B_tokenizer.json\"),\n        os.path.join(output_dir, \"20B_tokenizer.json\"),\n    )\n    with open(os.path.join(output_dir, \"latest\"), \"w\") as f:\n        f.write(\"global_step150000\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Merge 20B checkpoint.\")\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        help='Checkpoint dir, which should contain (e.g. a folder named \"global_step150000\")',\n    )\n    parser.add_argument(\n        \"--output_dir\", type=str, help=\"Output dir, to save the 1-GPU weights configs\"\n    )\n    args = parser.parse_args()\n    merge(args.input_dir, args.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "tools/ckpts/upload.py": "# Copyright (c) 2025, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\nfrom huggingface_hub import HfApi, create_repo\n\nconverted_ckpt = sys.argv[1]\nrepo_name = sys.argv[2]\nbranch_name = sys.argv[3]\ntry:\n    create_repo(repo_name, repo_type=\"model\", private=False)\nexcept:\n    print(\"repo {repo_name} already exists!\")\n    pass\n\nfiles = os.listdir(converted_ckpt)\n\napi = HfApi()\nif branch_name != \"main\":\n    try:\n        api.create_branch(\n            repo_id=repo_name,\n            repo_type=\"model\",\n            branch=branch_name,\n        )\n    except:\n        print(f\"branch {branch_name} already exists, try again...\")\nprint(f\"to upload: {files}\")\nfor file in files:\n    print(f\"Uploading {file} to branch {branch_name}...\")\n    api.upload_file(\n        path_or_fileobj=os.path.join(converted_ckpt, file),\n        path_in_repo=file,\n        repo_id=repo_name,\n        repo_type=\"model\",\n        commit_message=f\"Upload {file}\",\n        revision=branch_name,\n    )\n    print(f\"Successfully uploaded {file} !\")\n",
        "tools/datasets/corpora.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom abc import ABC, abstractmethod\nfrom multiprocessing import cpu_count\n\n\"\"\"\nThis registry is for automatically downloading and extracting datasets.\n\nTo register a class you need to inherit the DataDownloader class, and provide name and url attributes, and (optionally)\nthe number of documents.\n\nWhen done, add it to the DATA_DOWNLOADERS dict. The function process_data runs the pre-processing for the selected\ndataset.\n\"\"\"\n\nGPT2_VOCAB_URL = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\"\nGPT2_MERGE_URL = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\"\n\n\nclass DataDownloader(ABC):\n    \"\"\"Dataset registry class to automatically download / extract datasets\"\"\"\n\n    def __init__(\n        self,\n        tokenizer_type=None,\n        merge_file=None,\n        vocab_file=None,\n        data_dir=None,\n        force_redownload=None,\n        num_workers=None,\n    ):\n        if tokenizer_type is None:\n            tokenizer_type = \"GPT2BPETokenizer\"\n        if data_dir is None:\n            data_dir = os.environ.get(\"DATA_DIR\", \"./data\")\n        if merge_file is None:\n            merge_file = f\"{data_dir}/gpt2-merges.txt\"\n        if force_redownload is None:\n            force_redownload = False\n        if vocab_file is None:\n            if tokenizer_type == \"GPT2BPETokenizer\":\n                vocab_file = f\"{data_dir}/gpt2-vocab.json\"\n            elif tokenizer_type == \"HFGPT2Tokenizer\":\n                vocab_file = \"gpt2\"\n            elif tokenizer_type == \"CharLevelTokenizer\":\n                pass\n            else:\n                assert vocab_file is not None, \"No vocab file provided\"\n        if num_workers is None:\n            num_workers = cpu_count()\n        self._tokenizer_type = tokenizer_type\n        self._merge_file = merge_file\n        self._vocab_file = vocab_file\n        self._data_dir = data_dir\n        self._force_redownload = force_redownload\n        self._num_workers = num_workers\n\n    @property\n    def base_dir(self):\n        \"\"\"base data directory\"\"\"\n        return self._data_dir\n\n    @property\n    @abstractmethod\n    def name(self):\n        \"\"\"name of dataset\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def urls(self):\n        \"\"\"URLs from which to download dataset\"\"\"\n        pass\n\n    @property\n    def tokenizer_type(self):\n        \"\"\"tokenizer type to use when tokenizing data\"\"\"\n        return self._tokenizer_type\n\n    @property\n    def merge_file(self):\n        \"\"\"Merge file for tokenizer\"\"\"\n        return self._merge_file\n\n    @property\n    def vocab_file(self):\n        \"\"\"Vocab file for tokenizer\"\"\"\n        return self._vocab_file\n\n    @property\n    def num_workers(self):\n        \"\"\"Number of workers to use in preprocessing\"\"\"\n        return self._num_workers\n\n    @property\n    def num_docs(self):\n        \"\"\"Number of documents in the dataset (if known)\"\"\"\n        return None\n\n    @property\n    def ftfy(self):\n        \"\"\"Use ftfy (https://github.com/LuminosoInsight/python-ftfy) to fix text encodings\"\"\"\n        return False\n\n    def exists(self):\n        \"\"\"Checks if the dataset is present\"\"\"\n        return os.path.isdir(f\"{self.base_dir}/{self.name}\")\n\n    def download(self):\n        \"\"\"downloads dataset\"\"\"\n        os.makedirs(os.path.join(self.base_dir, self.name), exist_ok=True)\n        for url in self.urls:\n            try:\n                os_cmd = f\"wget {url} -O {os.path.join(self.base_dir, self.name, os.path.basename(url))}\"\n                if os.system(os_cmd) != 0:\n                    raise Exception(\n                        f\"Cannot download file at URL {url}: server may be down\"\n                    )\n            except Exception as e:\n                raise Exception(f\"Download error: {e}\")\n\n    def tokenize(self):\n        \"\"\"tokenizes dataset\"\"\"\n        parent_folder = os.path.join(self.base_dir, self.name)\n        jsonl_filepath = \",\".join(\n            [os.path.join(parent_folder, os.path.basename(url)) for url in self.urls]\n        )\n\n        cmd = f\"python tools/datasets/preprocess_data.py \\\n            --input {jsonl_filepath} \\\n            --output-prefix {parent_folder}/{self.name} \\\n            --vocab {self.vocab_file} \\\n            --dataset-impl mmap \\\n            --tokenizer-type {self.tokenizer_type} \\\n            --merge-file {self.merge_file} \\\n            --append-eod \\\n            --workers {self.num_workers} \"\n\n        if self.num_docs is not None:\n            cmd += f\"--num-docs {self.num_docs} \"\n\n        if self.ftfy:\n            cmd += f\"--ftfy \"\n\n        os.system(cmd)\n\n    def prepare(self):\n        if self._force_redownload:\n            self.download()\n        else:\n            if not self.exists():\n                self.download()\n\n        self.tokenize()\n\n\nclass Enron(DataDownloader):\n    name = \"enron\"\n    urls = [\"http://eaidata.bmk.sh/data/enron_emails.jsonl.zst\"]\n    num_docs = 517401\n\n\nclass PileSubset(DataDownloader):\n    name = \"pile_00\"\n    urls = [\"https://the-eye.eu/public/AI/pile/train/00.jsonl.zst\"]\n\n\nclass Pile(DataDownloader):\n    name = \"pile\"\n    urls = [\n        f\"https://the-eye.eu/public/AI/pile/train/{i:02}.jsonl.zst\" for i in range(30)\n    ]\n\n\nclass Github(DataDownloader):\n    name = \"github\"\n    urls = [\"http://eaidata.bmk.sh/data/github_small.jsonl.zst\"]\n\n\nclass ArXiv(DataDownloader):\n    name = \"arxiv\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/2020-09-08-arxiv-extracts-nofallback-until-2007-068.tar.gz\"\n    ]\n\n\nclass EuroParl(DataDownloader):\n    name = \"europarl\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/EuroParliamentProceedings_1996_2011.jsonl.zst\"\n    ]\n\n\nclass FreeLaw(DataDownloader):\n    name = \"freelaw\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\"\n    ]\n\n\nclass NiH(DataDownloader):\n    name = \"nih\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst\"\n    ]\n\n\nclass PubMed(DataDownloader):\n    name = \"pubmed\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/PMC_extracts.tar.gz\"\n    ]\n\n\nclass Books1(DataDownloader):\n    name = \"books1\"\n    urls = [\"https://the-eye.eu/public/AI/pile_preliminary_components/books1.tar.gz\"]\n\n\nclass Books3(DataDownloader):\n    name = \"books3\"\n    urls = [\"https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz\"]\n\n\nclass HackerNews(DataDownloader):\n    name = \"hackernews\"\n    urls = [\"https://the-eye.eu/public/AI/pile_preliminary_components/hn.tar.gz\"]\n    num_docs = 373000\n\n\nclass OpenWebText2(DataDownloader):\n    name = \"openwebtext2\"\n    urls = [\n        \"https://huggingface.co/datasets/segyges/OpenWebText2/resolve/main/openwebtext2.jsonl.zst.tar\"\n    ]\n    num_docs = 17103000\n\n\nclass StackExchange(DataDownloader):\n    name = \"stackexchange\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/stackexchange_dataset.tar\"\n    ]\n\n\nclass UbuntuIRC(DataDownloader):\n    name = \"ubuntu_irc\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/ubuntu_irc_until_2020_9_1.jsonl.zst\"\n    ]\n\n\nclass YoutubeSubtitles(DataDownloader):\n    name = \"youtube_subtitles\"\n    urls = [\n        \"https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst\"\n    ]\n\n\nclass C4(DataDownloader):\n    name = \"c4\"\n    urls = [\n        f\"https://the-eye.eu/eleuther_staging/c4/en/c4-train.{i:05}-of-01024.json.gz\"\n        for i in range(1024)\n    ]\n\n\nclass C4OpenWebText(DataDownloader):\n    name = \"c4_openwebtext\"\n    urls = [\n        f\"https://the-eye.eu/eleuther_staging/c4/realnewslike/c4-train.{i:05}-of-00512.json.gz\"\n        for i in range(512)\n    ]\n\n\nclass Enwik8(DataDownloader):\n    name = \"enwik8\"\n    urls = [\"http://mattmahoney.net/dc/enwik8.zip\"]\n\n\ndef maybe_download_gpt2_tokenizer_data(tokenizer_type, data_dir):\n    if tokenizer_type is None or tokenizer_type == \"GPT2BPETokenizer\":\n        GPT2_VOCAB_FP = f\"{data_dir}//gpt2-vocab.json\"\n        GPT2_MERGE_FP = f\"{data_dir}/gpt2-merges.txt\"\n        if not os.path.isfile(GPT2_VOCAB_FP):\n            os.system(f\"wget {GPT2_VOCAB_URL} -O {GPT2_VOCAB_FP}\")\n        if not os.path.isfile(GPT2_MERGE_FP):\n            os.system(f\"wget {GPT2_MERGE_URL} -O {GPT2_MERGE_FP}\")\n\n\nDATA_DOWNLOADERS = {\n    \"pass\": \"pass\",\n    \"enron\": Enron,\n    \"pile_subset\": PileSubset,\n    \"pile\": Pile,\n    \"github\": Github,\n    \"arxiv\": ArXiv,\n    \"europarl\": EuroParl,\n    \"freelaw\": FreeLaw,\n    \"nih\": NiH,\n    \"pubmed\": PubMed,\n    \"books1\": Books1,\n    \"books3\": Books3,\n    \"hackernews\": HackerNews,\n    \"openwebtext2\": OpenWebText2,\n    \"stackexchange\": StackExchange,\n    \"ubuntu_irc\": UbuntuIRC,\n    \"youtube_subtitles\": YoutubeSubtitles,\n    \"c4\": C4,\n    \"c4_openwebtext\": C4OpenWebText,\n    \"enwik8\": Enwik8,\n}\n\n\ndef prepare_dataset(\n    dataset_name: str,\n    tokenizer_type: str = None,\n    data_dir: str = None,\n    vocab_file: str = None,\n    merge_file: str = None,\n    force_redownload: bool = None,\n    num_workers: int = None,\n):\n    \"\"\"\n    Downloads + tokenizes a dataset in the registry (dataset_name) and saves output .npy files to data_dir.\n    \"\"\"\n    if data_dir is None:\n        data_dir = os.environ.get(\"DATA_DIR\", \"./data\")\n    os.makedirs(data_dir, exist_ok=True)\n    maybe_download_gpt2_tokenizer_data(tokenizer_type, data_dir)\n    DownloaderClass = DATA_DOWNLOADERS.get(dataset_name.lower(), None)\n    if DownloaderClass is None:\n        raise NotImplementedError(\n            f'Dataset \"{dataset_name}\" not recognized - please choose from {list(DATA_DOWNLOADERS.keys())}'\n        )\n    elif DownloaderClass == \"pass\":\n        # pass on building dataset (for unit tests)\n        pass\n    else:\n        num_workers = 1 if dataset_name == \"enwik8\" else num_workers\n        d = DownloaderClass(\n            tokenizer_type=tokenizer_type,\n            vocab_file=vocab_file,\n            merge_file=merge_file,\n            data_dir=data_dir,\n            force_redownload=force_redownload,\n            num_workers=num_workers,\n        )\n        d.prepare()\n",
        "tools/datasets/dataset_token_count.py": "# Script counts tokens in a pretokenized dataset from preprocess_data.py\n# Necessary for setting batch size, train_iters, etc\n\nimport sys\nimport os\n\n## Necessary for the import\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\nsys.path.insert(0, project_root)\n\nfrom megatron.data import indexed_dataset\nimport numpy as np\n\nif len(sys.argv) < 2:\n    print(\n        \"Usage: python dataset_token_count.py /absolute/file/path/to/dataset1 /absolute/file/path/to/dataset2 ...\"\n    )\n    sys.exit(1)\n\n# Access the command-line arguments\narguments = sys.argv[1:]\n\nfor arg in arguments:\n    print(\"Checking file\", arg)\n    try:\n        dataset = indexed_dataset.make_dataset(arg, \"mmap\")\n        size = np.sum(dataset.sizes)\n        print(\"Dataset size in tokens is\", size)\n    except AttributeError:\n        print(\"Dataset could not be loaded\", arg)\n",
        "tools/datasets/merge_datasets.py": "import os\nimport sys\nimport json\nimport argparse\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\n\nfrom megatron.data import indexed_dataset\n\n\ndef main(args):\n\n    prefixes = set()\n    for basename in os.listdir(args.input):\n        prefix, ext = os.path.splitext(basename)\n\n        if prefix in prefixes:\n            continue\n\n        if not os.path.isfile(os.path.join(args.input, basename)):\n            continue\n\n        ext_pair = \".bin\" if ext == \".idx\" else \".idx\"\n        assert os.path.isfile(\n            os.path.join(args.input, prefix) + ext_pair\n        ), f\"ERROR: {ext_pair} file not provided for {os.path.join(args.input, prefix)}\"\n\n        prefixes.add(prefix)\n\n    builder = None\n    for prefix in sorted(prefixes):\n        if builder is None:\n            dataset = indexed_dataset.make_dataset(\n                os.path.join(args.input, prefix), \"infer\"\n            )\n\n            if isinstance(dataset, indexed_dataset.MMapIndexedDataset):\n                builder = indexed_dataset.MMapIndexedDatasetBuilder(\n                    args.output_prefix + \".bin\", dtype=dataset._index.dtype\n                )\n            else:\n                builder = indexed_dataset.IndexedDatasetBuilder(\n                    args.output_prefix + \".bin\"\n                )\n\n            del dataset\n\n        builder.merge_file_(os.path.join(args.input, prefix))\n\n    builder.finalize(args.output_prefix + \".idx\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    group = parser.add_argument_group(title=\"input data\")\n    group.add_argument(\n        \"--input\",\n        type=str,\n        required=True,\n        help=\"Path to directory containing all document files to merge\",\n    )\n\n    group = parser.add_argument_group(title=\"output data\")\n    group.add_argument(\n        \"--output-prefix\",\n        type=str,\n        required=True,\n        help=\"Path to binary output file without suffix\",\n    )\n\n    args = parser.parse_args()\n\n    assert os.path.isdir(\n        args.input\n    ), f\"ERROR: {args.input} is not a directory or does not exist\"\n\n    assert os.path.isdir(\n        os.path.dirname(args.output_prefix)\n    ), f\"ERROR: {os.path.dirname(args.output_prefix)} is not a directory or does not exist\"\n\n    main(args)\n",
        "tools/datasets/preprocess_data.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Processing data for pretraining.\"\"\"\n\nimport argparse\nimport multiprocessing\nimport os\nimport sys\n\nimport lm_dataformat as lmd\nimport numpy as np\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\nimport time\nimport tqdm\nimport torch\nimport ftfy\n\nfrom megatron.tokenizer import build_tokenizer\nfrom megatron.data import indexed_dataset\nfrom threading import Semaphore\n\n\nclass Encoder(object):\n    def __init__(self, args):\n        self.args = args\n\n    def initializer(self):\n        # Use Encoder class as a container for global data\n        Encoder.tokenizer = build_tokenizer(self.args)\n\n    def encode(self, text):\n        if self.args.ftfy:\n            text = ftfy.fix_text(text)\n        ids = {}\n        for key in self.args.jsonl_keys:\n            doc_ids = []\n            text_ids = Encoder.tokenizer.tokenize(text)\n            if len(text_ids) > 0:\n                doc_ids.append(text_ids)\n            if self.args.append_eod:\n                doc_ids[-1].append(Encoder.tokenizer.eod)\n            ids[key] = doc_ids\n        return ids, len(text)\n\n\ndef get_args(input_args=None):\n    parser = argparse.ArgumentParser()\n    group = parser.add_argument_group(title=\"input data\")\n    group.add_argument(\n        \"--input\",\n        type=str,\n        required=True,\n        help=\"Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated \"\n        \"list\",\n    )\n    group.add_argument(\n        \"--jsonl-keys\",\n        nargs=\"+\",\n        default=[\"text\"],\n        help=\"space separate listed of keys to extract from jsonl. Default: text\",\n    )\n    group.add_argument(\n        \"--num-docs\",\n        default=None,\n        help=\"Optional: Number of documents in the input data (if known) for an accurate progress bar.\",\n        type=int,\n    )\n    group = parser.add_argument_group(title=\"tokenizer\")\n    group.add_argument(\n        \"--tokenizer-type\",\n        type=str,\n        required=True,\n        choices=[\n            \"HFGPT2Tokenizer\",\n            \"HFTokenizer\",\n            \"GPT2BPETokenizer\",\n            \"CharLevelTokenizer\",\n            \"TiktokenTokenizer\",\n            \"SPMTokenizer\",\n        ],\n        help=\"What type of tokenizer to use.\",\n    )\n    group.add_argument(\n        \"--vocab-file\", type=str, default=None, help=\"Path to the vocab file\"\n    )\n    group.add_argument(\n        \"--merge-file\",\n        type=str,\n        default=None,\n        help=\"Path to the BPE merge file (if necessary).\",\n    )\n    group.add_argument(\n        \"--append-eod\",\n        action=\"store_true\",\n        help=\"Append an <eod> token to the end of a document.\",\n    )\n    group.add_argument(\"--ftfy\", action=\"store_true\", help=\"Use ftfy to clean text\")\n    group = parser.add_argument_group(title=\"output data\")\n    group.add_argument(\n        \"--output-prefix\",\n        type=str,\n        required=True,\n        help=\"Path to binary output file without suffix\",\n    )\n    group.add_argument(\n        \"--dataset-impl\",\n        type=str,\n        default=\"mmap\",\n        choices=[\"lazy\", \"cached\", \"mmap\"],\n        help=\"Dataset implementation to use. Default: mmap\",\n    )\n\n    group = parser.add_argument_group(title=\"runtime\")\n    group.add_argument(\n        \"--workers\", type=int, default=1, help=\"Number of worker processes to launch\"\n    )\n    group.add_argument(\n        \"--log-interval\",\n        type=int,\n        default=100,\n        help=\"Interval between progress updates\",\n    )\n    args = parser.parse_args(input_args)\n    args.keep_empty = False\n\n    # some default/dummy values for the tokenizer\n    args.rank = 0\n    args.make_vocab_size_divisible_by = 128\n    args.model_parallel_size = 1\n\n    return args\n\n\ndef yield_from_files(fnames: list, semaphore):\n    \"\"\"\n    Iterator over input documents using lm_dataformat. Should be able to handle jsons / texts /\n    other compressed formats. Also filters out empty documents.\n\n    :param fnames: list of filenames\n    \"\"\"\n\n    def yielder(fname, semaphore):\n        for f in filter(lambda x: x, lmd.Reader(fname).stream_data()):\n            semaphore.acquire()\n            yield f\n\n    for fname in fnames:\n        semaphore.acquire()\n\n        yield from yielder(fname, semaphore)\n\n\ndef main(input_args=None):\n    args = get_args(input_args)\n    encoder = Encoder(args)\n    tokenizer = build_tokenizer(args)\n    print(f\"Vocab size: {tokenizer.vocab_size}\")\n    print(f\"Output prefix: {args.output_prefix}\")\n\n    # build a semaphore object to stop `yield_from_files` from getting ahead of encoder.encode and\n    # hence building up memory\n    semaphore = Semaphore(10000 + args.workers)\n\n    # use multiprocessing to iterate over input documents\n    fin = yield_from_files(args.input.split(\",\"), semaphore)\n\n    if args.workers > 1:\n        pool = multiprocessing.Pool(args.workers, initializer=encoder.initializer)\n        encoded_docs = pool.imap(encoder.encode, fin, chunksize=25)\n    else:\n        encoder.initializer()\n        encoded_docs = (encoder.encode(doc) for doc in fin)\n\n    # make a dataset builder for each key in args.jsonl_keys\n    # each key will output to a different file beginning with args.output_prefix\n    output_bin_files = {}\n    output_idx_files = {}\n    builders = {}\n    for key in args.jsonl_keys:\n        output_bin_files[key] = \"{}_{}_{}.bin\".format(\n            args.output_prefix, key, \"document\"\n        )\n        output_idx_files[key] = \"{}_{}_{}.idx\".format(\n            args.output_prefix, key, \"document\"\n        )\n        builders[key] = indexed_dataset.make_builder(\n            output_bin_files[key],\n            impl=args.dataset_impl,\n            vocab_size=tokenizer.vocab_size,\n        )\n\n    # actually do tokenization\n    proc_start = time.time()\n    total_bytes_processed = 0\n    pbar = tqdm.tqdm()\n    for i, (doc, bytes_processed) in enumerate(encoded_docs, start=1):\n        total_bytes_processed += bytes_processed\n\n        # release semaphore so `yield_from_files` can add another file to the buffer\n        semaphore.release()\n\n        # add each tokenized document / sentence\n        for key, sentences in doc.items():\n            for sentence in sentences:\n                builders[key].add_item(np.array(sentence, dtype=builders[key].dtype))\n            # separate with eos token\n            builders[key].end_document()\n\n        # log progress\n        if i % args.log_interval == 0:\n            current = time.time()\n            elapsed = current - proc_start\n            mbs = total_bytes_processed / elapsed / 1024 / 1024\n            pbar.set_description(\n                f\"Processed {i}{'' if args.num_docs is None else '/' + str(args.num_docs)} documents ({i / elapsed :.2f} docs/s, {mbs:.2f} MB/s).\"\n            )\n            if i != 0:\n                pbar.update(args.log_interval)\n\n    # save output file\n    for key in args.jsonl_keys:\n        builders[key].finalize(output_idx_files[key])\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "tools/datasets/preprocess_data_with_chat_template.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nA script for processing a dataset such that chat templates are utilized in the creation of the data.\nThese are then used to perform instruction/chat model finetunes (for example, finetuning a model on only the assistant\nportions of a chatml dataset).\n\nThis follows the same output format as 'preprocess_data_with_mask.py' but using chat templates to generate the data.\nThis way we can support multiturn chat data in the finetuning process. instead of relying on a single turn of data.\n\nTo run this script, first edit `tools/datasets/corpora.py` such that the command to call\n `tools/datasets/preprocess_data_with_chat_template.py` is as follows:\n\n```\ncmd = f\"python tools/datasets/preprocess_data_with_with_chat_template.py \\\n    --input {jsonl_filepath} \\\n    --output-prefix {parent_folder}/{self.name} \\\n    --tokenizer-path {hf-tokenizer} \\\n    --jsonl-keys {jsonl_keys} \\\n    --dataset-impl mmap \\\n    --workers {self.num_workers} \"\n\nif self.only_last:\n    cmd += f\"--only-last \"\n\nif self.no_mask:\n    cmd += f\"--no-mask \"\n```\n\nThen, specify\n```\n\"train_data_paths\": [\"/path/to/dataset/name_text_document\"],\n\"label_data_paths\": [\"/path/to/dataset/name_label_document\"]\n```\nin your YML config. This will then allow for finetuning on the data with loss masks set appropriately.\n\n\"\"\"\n\nimport argparse\nimport multiprocessing\nimport os\nimport sys\n\nimport lm_dataformat as lmd\nimport numpy as np\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\n\nimport time\nimport tqdm\nimport jsonlines\n\nfrom megatron.data import indexed_dataset\nfrom threading import Semaphore\nfrom typing import List, Dict, Tuple\nfrom transformers import AutoTokenizer, PreTrainedTokenizer\n\n\ndef build_chat(\n    chat: List[Dict[str, str]],\n    generation_role: str,\n    apply_mask: bool,\n    tokenizer: PreTrainedTokenizer,\n    only_last_turn: bool = False,\n    for_rm: bool = False,\n) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Build a chat from a list of dictionaries. Each dictionary should have a \"role\" and \"content\" key, this follows the\n    Chat Template from https://huggingface.co/docs/transformers/main/en/chat_templating\n\n    :param chat: A list of dictionaries with \"role\" and \"content\" keys\n    :param generation_role: The role of the model generating the chat, usually \"assistant\"\n    :param apply_mask: Whether to apply a loss mask to the chat, if False, all tokens will be included in the loss\n    :param tokenizer: A HF tokenizer\n    :param only_last_turn: Whether to only include the last turn in the chat, needed for some fine-tuning tasks\n    \"\"\"\n    tokens = []\n    mask = []\n    if apply_mask is False:\n        tokens = tokenizer.apply_chat_template(chat)\n        mask = tokens\n        return tokens, mask\n    elif for_rm:\n        tokens = tokenizer.apply_chat_template(chat)\n        mask = [-100] * len(tokens)\n        if tokenizer.eos_token_id is not None:\n            # since this is processed in a causal format (input[:-1], mask[1:], we need to put two here...\n            mask.append(-100)\n            tokens.append(tokenizer.eos_token_id)\n            mask.append(tokenizer.eos_token_id)\n            tokens.append(tokenizer.eos_token_id)\n        else:\n            raise ValueError(\n                \"Tokenizer does not have an EOS token, unable to determine good mask, please edit and make your own.\"\n            )\n        return tokens, mask\n    for i, turn in enumerate(chat):\n        add_gen = (\n            False if i == len(chat) - 1 else chat[i + 1][\"role\"] == generation_role\n        )\n        chat_tokens = tokenizer.apply_chat_template(\n            chat[: i + 1], add_generation_prompt=add_gen\n        )[len(tokens) :]\n        # remove previous stuff...\n        tokens.extend(chat_tokens)\n        if only_last_turn and (i != len(chat) - 1):\n            mask.extend([-100] * len(chat_tokens))\n        elif apply_mask and (turn[\"role\"] != generation_role):\n            mask.extend([-100] * len(chat_tokens))\n        else:\n            mask.extend(chat_tokens)\n    if tokenizer.eos_token_id is not None:\n        mask.append(tokenizer.eos_token_id if mask[-1] != -100 else -100)\n        tokens.append(tokenizer.eos_token_id)\n    return tokens, mask\n\n\nclass Encoder(object):\n    def __init__(self, args):\n        self.args = args\n\n    def initializer(self):\n        # Use Encoder class as a container for global data\n        Encoder.tokenizer = AutoTokenizer.from_pretrained(self.args.tokenizer_path)\n\n    def encode(self, text):\n        ids = {}\n        for key in self.args.jsonl_keys:\n            text_ids, label_ids = build_chat(\n                text[key],\n                self.args.generation_role,\n                not self.args.no_mask,\n                Encoder.tokenizer,\n                self.args.only_last,\n                self.args.for_rm,\n            )\n            if self.args.reward_key is not None:\n                reward = text[self.args.reward_key]\n                if self.args.binary_reward:\n                    reward = [1] if reward else [-1]\n                elif type(reward) == float:\n                    reward = [reward]\n                ids[key] = (text_ids, label_ids, reward)\n            else:\n                ids[key] = (text_ids, label_ids, None)\n        return ids, len(text)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    group = parser.add_argument_group(title=\"input data\")\n    group.add_argument(\n        \"--input\",\n        type=str,\n        required=True,\n        help=\"Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated \"\n        \"list\",\n    )\n    group.add_argument(\n        \"--jsonl-keys\",\n        nargs=\"+\",\n        default=[\"conversation\"],\n        help=\"space separate listed of keys to extract from jsonl. Default: text\",\n    )\n    group.add_argument(\n        \"--no-mask\",\n        help=\"If set, this will not mask any tokens in the input data.\",\n        action=\"store_true\",\n    )\n    group.add_argument(\n        \"--for-rm\",\n        help=\"If set, this will mask everything except the last token in the chat.\",\n        action=\"store_true\",\n    )\n\n    group.add_argument(\n        \"--generation-role\",\n        type=str,\n        default=\"assistant\",\n        help=\"The role of the model generating the chat, usually 'assistant'. Default: assistant\",\n    )\n    group.add_argument(\n        \"--only-last\",\n        help=\"If set, this will mask everything except the last turn in the chat.\",\n        action=\"store_true\",\n    )\n    group.add_argument(\n        \"--reward-key\",\n        type=str,\n        default=None,\n        help=\"Optional: key to use for reward data in the input data.\",\n    )\n    group.add_argument(\n        \"--binary-reward\",\n        help=\"If set, this will treat the reward data as a boolean.\",\n        action=\"store_true\",\n    )\n    group.add_argument(\n        \"--num-docs\",\n        default=None,\n        help=\"Optional: Number of documents in the input data (if known) for an accurate progress bar.\",\n        type=int,\n    )\n    group = parser.add_argument_group(title=\"tokenizer\")\n    group.add_argument(\n        \"--tokenizer-path\",\n        type=str,\n        required=True,\n        help=\"Path to HF Tokenizer.\",\n    )\n    group.add_argument(\"--ftfy\", action=\"store_true\", help=\"Use ftfy to clean text\")\n    group = parser.add_argument_group(title=\"output data\")\n    group.add_argument(\n        \"--output-prefix\",\n        type=str,\n        required=True,\n        help=\"Path to binary output file without suffix\",\n    )\n    group.add_argument(\n        \"--dataset-impl\",\n        type=str,\n        default=\"mmap\",\n        choices=[\"lazy\", \"cached\", \"mmap\"],\n        help=\"Dataset implementation to use. Default: mmap\",\n    )\n\n    group = parser.add_argument_group(title=\"runtime\")\n    group.add_argument(\n        \"--workers\", type=int, default=1, help=\"Number of worker processes to launch\"\n    )\n    group.add_argument(\n        \"--log-interval\",\n        type=int,\n        default=100,\n        help=\"Interval between progress updates\",\n    )\n    args = parser.parse_args()\n    args.keep_empty = False\n\n    # some default/dummy values for the tokenizer\n    args.rank = 0\n    args.make_vocab_size_divisible_by = 128\n    args.model_parallel_size = 1\n\n    return args\n\n\ndef yield_from_files(fnames: list, semaphore):\n    \"\"\"\n    Iterator over input documents using lm_dataformat. Should be able to handle jsons / texts /\n    other compressed formats. Also filters out empty documents.\n\n    :param fnames: list of filenames\n    \"\"\"\n\n    def yielder(fname, semaphore):\n        with open(fname, encoding=\"utf-8\") as f:\n            reader = jsonlines.Reader(f)\n            for f in reader:\n                semaphore.acquire()\n                yield f\n\n    for fname in fnames:\n        semaphore.acquire()\n\n        yield from yielder(fname, semaphore)\n\n\ndef main():\n    args = get_args()\n    encoder = Encoder(args)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n    print(f\"Vocab size: {tokenizer.vocab_size}\")\n    print(f\"Output prefix: {args.output_prefix}\")\n\n    # build a semaphore object to stop `yield_from_files` from getting ahead of encoder.encode and\n    # hence building up memory\n    semaphore = Semaphore(10000 + args.workers)\n\n    # use multiprocessing to iterate over input documents\n    fin = yield_from_files(args.input.split(\",\"), semaphore)\n\n    if args.workers > 1:\n        pool = multiprocessing.Pool(args.workers, initializer=encoder.initializer)\n        encoded_docs = pool.imap(encoder.encode, fin, chunksize=25)\n    else:\n        encoder.initializer()\n        encoded_docs = (encoder.encode(doc) for doc in fin)\n\n    # make a dataset builder for each key in args.jsonl_keys\n    # each key will output to a different file beginning with args.output_prefix\n    output_bin_files = {}\n    output_idx_files = {}\n    builders = {}\n    for key in args.jsonl_keys:\n        output_bin_files[key] = \"{}_{}_{}.bin\".format(\n            args.output_prefix, key, \"document\"\n        )\n        output_idx_files[key] = \"{}_{}_{}.idx\".format(\n            args.output_prefix, key, \"document\"\n        )\n        builders[key] = indexed_dataset.make_builder(\n            output_bin_files[key],\n            impl=args.dataset_impl,\n            vocab_size=tokenizer.vocab_size,\n        )\n        builders[key]._dtype = np.int32\n        if not args.no_mask:\n            assert (\n                key + \"_label\" not in args.jsonl_keys\n            ), \"label should not be included as it will be generated according to the mask.\"\n            label_key = key + \"_label\"\n            output_bin_files[label_key] = \"{}_{}_{}.bin\".format(\n                args.output_prefix, label_key, \"document\"\n            )\n            output_idx_files[label_key] = \"{}_{}_{}.idx\".format(\n                args.output_prefix, label_key, \"document\"\n            )\n            builders[label_key] = indexed_dataset.make_builder(\n                output_bin_files[label_key],\n                impl=args.dataset_impl,\n                vocab_size=tokenizer.vocab_size,\n            )\n            builders[label_key]._dtype = np.int32\n        if args.reward_key is not None:\n            assert (\n                key + \"_reward\" not in args.jsonl_keys\n            ), \"reward should not be included as it will be generated from the data.\"\n            reward_key = key + \"_reward\"\n            output_bin_files[reward_key] = \"{}_{}_{}.bin\".format(\n                args.output_prefix, reward_key, \"document\"\n            )\n            output_idx_files[reward_key] = \"{}_{}_{}.idx\".format(\n                args.output_prefix, reward_key, \"document\"\n            )\n            builders[reward_key] = indexed_dataset.make_builder(\n                output_bin_files[reward_key],\n                impl=args.dataset_impl,\n                vocab_size=tokenizer.vocab_size,\n            )\n            builders[reward_key]._dtype = np.int32\n\n    # actually do tokenization\n    proc_start = time.time()\n    total_bytes_processed = 0\n    pbar = tqdm.tqdm()\n    for i, (doc, bytes_processed) in enumerate(encoded_docs, start=1):\n        total_bytes_processed += bytes_processed\n\n        # release semaphore so `yield_from_files` can add another file to the buffer\n        semaphore.release()\n\n        # add each tokenized document / sentence\n        for key, conv in doc.items():\n            tokens = conv[0]\n            token_mask = conv[1]\n            reward = conv[2]\n            builders[key].add_item(np.array(tokens, dtype=builders[key].dtype))\n            builders[key + \"_label\"].add_item(\n                np.array(token_mask, dtype=builders[key + \"_label\"].dtype)\n            )\n            if args.reward_key is not None:\n                builders[key + \"_reward\"].add_item(\n                    np.array(reward, dtype=builders[key + \"_reward\"].dtype)\n                )\n            # add indx...\n            builders[key].end_document()\n            builders[key + \"_label\"].end_document()\n            if args.reward_key is not None:\n                builders[key + \"_reward\"].end_document()\n            if i == 1:\n                print(\"key: \", key)\n                print(\"tokens: \", tokens)\n                print(\"token_mask: \", token_mask)\n                print(\"Reward: \", reward)\n        # log progress\n        if i % args.log_interval == 0:\n            current = time.time()\n            elapsed = current - proc_start\n            mbs = total_bytes_processed / elapsed / 1024 / 1024\n            pbar.set_description(\n                f\"Processed {i}{'' if args.num_docs is None else '/' + str(args.num_docs)} documents ({i / elapsed} docs/s, {mbs} MB/s).\"\n            )\n            if i != 0:\n                pbar.update(args.log_interval)\n\n    # save output file\n    update_keys = args.jsonl_keys\n    for key in update_keys:\n        builders[key].finalize(output_idx_files[key])\n        builders[key + \"_label\"].finalize(output_idx_files[key + \"_label\"])\n        if args.reward_key is not None:\n            builders[key + \"_reward\"].finalize(output_idx_files[key + \"_reward\"])\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "tools/datasets/preprocess_data_with_mask.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nA script for processing a dataset such that corresponding labels are also produced. These are then used to perform masked finetuning\n(for example, finetuning a model to only output the text following some delimiter in the finetuning dataset such as \"Answer: \"\nrather than generating the entire \"Question: ... Answer: \" turns of conversation.\n\nTo run this script, first edit `tools/datasets/corpora.py` such that the command to call `tools/datasets/preprocess_data.py` is as follows:\n\n```\ncmd = f\"python tools/datasets/preprocess_data_with_mask.py \\\n    --input {jsonl_filepath} \\\n    --output-prefix {parent_folder}/{self.name} \\\n    --vocab {self.vocab_file} \\\n    --dataset-impl mmap \\\n    --tokenizer-type {self.tokenizer_type} \\\n    --merge-file {self.merge_file} \\\n    --append-eod \\\n    --mask-before-token X,Y,Z \\\n    --workers {self.num_workers} \"\n\nif self.num_docs is not None:\n    cmd += f\"--num-docs {self.num_docs} \"\n\nif self.ftfy:\n    cmd += f\"--ftfy \"\n```\nwhere --mask-before-token must be the (comma-separated) list of tokens produced by encoding your delimiter string.\nUp to and including the first occurrence of this token sequence in a document, all tokens will have their loss mask zeroed out when the label dataset is provided to NeoX.\n\nThen, specify\n```\n\"train_data_paths\": [\"/path/to/dataset/name_text_document\"],\n\"label_data_paths\": [\"/path/to/dataset/name_label_document\"]\n```\nin your YML config. This will then allow for finetuning on the data with loss masks set appropriately.\n(However, be warned that NeoX packs documents to fill context windows, which may degrade performance in some finetuning situations where instead padding out to the context length may be preferred.)\n\"\"\"\n\nimport argparse\nimport multiprocessing\nimport os\nimport sys\nimport re\n\nimport lm_dataformat as lmd\nimport numpy as np\n\nsys.path.append(\n    os.path.abspath(\n        os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)\n    )\n)\nimport time\nimport tqdm\nimport torch\nimport ftfy\n\nfrom megatron.tokenizer import build_tokenizer\nfrom megatron.data import indexed_dataset\nfrom threading import Semaphore\nfrom functools import lru_cache\n\n\n@lru_cache(maxsize=None)\ndef build_nxt(pattern: tuple) -> tuple:\n    # The function is being cached. Use tuple to avoid the cache being tampered out of scope.\n    nxt = [0]\n    current = 1\n    match_idx = 0\n\n    while current < len(pattern):\n        if pattern[match_idx] == pattern[current]:\n            current += 1\n            match_idx += 1\n            nxt.append(match_idx)\n        elif match_idx != 0:\n            match_idx = nxt[match_idx - 1]\n        else:\n            nxt.append(0)\n            current += 1\n\n    return tuple(nxt)\n\n\ndef kmp(seq, pattern, first_appearance=False):\n    \"\"\"\n    Search for the location of a subsequence in a list. Not sure if there is a python built-in\n    implementation of kmp somewhere...\n    \"\"\"\n    nxt = build_nxt(tuple(pattern))\n    current = 0\n    match_idx = 0\n\n    matched = []\n\n    while current < len(seq):\n        if seq[current] == pattern[match_idx]:\n            current += 1\n            match_idx += 1\n        elif match_idx != 0:\n            match_idx = nxt[match_idx - 1]\n        else:\n            current += 1\n\n        if match_idx == len(pattern):\n            matched.append(current - len(pattern))\n            if first_appearance:\n                return matched\n            match_idx = nxt[match_idx - 1]\n\n    return matched\n\n\nclass Encoder(object):\n    def __init__(self, args):\n        self.args = args\n\n    def initializer(self):\n        # Use Encoder class as a container for global data\n        Encoder.tokenizer = build_tokenizer(self.args)\n\n    def encode(self, text):\n        if self.args.ftfy:\n            text = ftfy.fix_text(text)\n        if isinstance(text, str):\n            text = {\"text\": text}\n        ids = {}\n        for key in self.args.jsonl_keys:\n            doc_ids = []\n            text_ids = Encoder.tokenizer.tokenize(text[\"text\"])\n            if len(text_ids) > 0:\n                doc_ids.append(text_ids)\n            if self.args.append_eod:\n                doc_ids[-1].append(Encoder.tokenizer.eod)\n            ids[key] = doc_ids\n        return ids, len(text)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    group = parser.add_argument_group(title=\"input data\")\n    group.add_argument(\n        \"--input\",\n        type=str,\n        required=True,\n        help=\"Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated \"\n        \"list\",\n    )\n    group.add_argument(\n        \"--jsonl-keys\",\n        nargs=\"+\",\n        default=[\"text\"],\n        help=\"space separate listed of keys to extract from jsonl. Default: text\",\n    )\n    group.add_argument(\n        \"--mask-before-token\",\n        default=None,\n        help=\"apply loss masks before certain token(s). If multi-token pattern, separate by commas without space, e.g. --mask-before-token 0,1,1270 to use the token pattern [0,1,1270].\",\n        type=str,\n    )\n    group.add_argument(\n        \"--num-docs\",\n        default=None,\n        help=\"Optional: Number of documents in the input data (if known) for an accurate progress bar.\",\n        type=int,\n    )\n    group = parser.add_argument_group(title=\"tokenizer\")\n    group.add_argument(\n        \"--tokenizer-type\",\n        type=str,\n        required=True,\n        choices=[\n            \"HFGPT2Tokenizer\",\n            \"HFTokenizer\",\n            \"GPT2BPETokenizer\",\n            \"CharLevelTokenizer\",\n        ],\n        help=\"What type of tokenizer to use.\",\n    )\n    group.add_argument(\n        \"--vocab-file\", type=str, default=None, help=\"Path to the vocab file\"\n    )\n    group.add_argument(\n        \"--merge-file\",\n        type=str,\n        default=None,\n        help=\"Path to the BPE merge file (if necessary).\",\n    )\n    group.add_argument(\n        \"--append-eod\",\n        action=\"store_true\",\n        help=\"Append an <eod> token to the end of a document.\",\n    )\n    group.add_argument(\"--ftfy\", action=\"store_true\", help=\"Use ftfy to clean text\")\n    group = parser.add_argument_group(title=\"output data\")\n    group.add_argument(\n        \"--output-prefix\",\n        type=str,\n        required=True,\n        help=\"Path to binary output file without suffix\",\n    )\n    group.add_argument(\n        \"--dataset-impl\",\n        type=str,\n        default=\"mmap\",\n        choices=[\"lazy\", \"cached\", \"mmap\"],\n        help=\"Dataset implementation to use. Default: mmap\",\n    )\n\n    group = parser.add_argument_group(title=\"runtime\")\n    group.add_argument(\n        \"--workers\", type=int, default=1, help=\"Number of worker processes to launch\"\n    )\n    group.add_argument(\n        \"--log-interval\",\n        type=int,\n        default=100,\n        help=\"Interval between progress updates\",\n    )\n    args = parser.parse_args()\n    args.keep_empty = False\n\n    # some default/dummy values for the tokenizer\n    args.rank = 0\n    args.make_vocab_size_divisible_by = 128\n    args.model_parallel_size = 1\n\n    return args\n\n\ndef yield_from_files(fnames: list, semaphore):\n    \"\"\"\n    Iterator over input documents using lm_dataformat. Should be able to handle jsons / texts /\n    other compressed formats. Also filters out empty documents.\n\n    :param fnames: list of filenames\n    \"\"\"\n\n    def yielder(fname, semaphore):\n        for f in filter(lambda x: x, lmd.Reader(fname).stream_data()):\n            semaphore.acquire()\n            yield f\n\n    for fname in fnames:\n        semaphore.acquire()\n\n        yield from yielder(fname, semaphore)\n\n\ndef mask(sentence: list, pivot_tokens: list, include_pivot=True):\n    inds = kmp(sentence, pivot_tokens)\n    if not inds:\n        return sentence\n    index = inds[0]\n    if include_pivot:\n        index += len(pivot_tokens)\n\n    return [-100] * index + sentence[index:]\n\n\ndef main():\n    args = get_args()\n    encoder = Encoder(args)\n    tokenizer = build_tokenizer(args)\n    print(f\"Vocab size: {tokenizer.vocab_size}\")\n    print(f\"Output prefix: {args.output_prefix}\")\n\n    # build a semaphore object to stop `yield_from_files` from getting ahead of encoder.encode and\n    # hence building up memory\n    semaphore = Semaphore(10000 + args.workers)\n\n    # use multiprocessing to iterate over input documents\n    fin = yield_from_files(args.input.split(\",\"), semaphore)\n\n    if args.workers > 1:\n        pool = multiprocessing.Pool(args.workers, initializer=encoder.initializer)\n        encoded_docs = pool.imap(encoder.encode, fin, chunksize=25)\n    else:\n        encoder.initializer()\n        encoded_docs = (encoder.encode(doc) for doc in fin)\n\n    if args.mask_before_token is not None:\n        token_mask = [\n            int(re.sub(r\"[^0-9]\", \"\", r))\n            for r in args.mask_before_token.split(\",\")\n            if re.sub(r\"[^0-9]\", \"\", r)\n        ]\n    else:\n        token_mask = []\n\n    # make a dataset builder for each key in args.jsonl_keys\n    # each key will output to a different file beginning with args.output_prefix\n    output_bin_files = {}\n    output_idx_files = {}\n    builders = {}\n    for key in args.jsonl_keys:\n        output_bin_files[key] = \"{}_{}_{}.bin\".format(\n            args.output_prefix, key, \"document\"\n        )\n        output_idx_files[key] = \"{}_{}_{}.idx\".format(\n            args.output_prefix, key, \"document\"\n        )\n        builders[key] = indexed_dataset.make_builder(\n            output_bin_files[key],\n            impl=args.dataset_impl,\n            vocab_size=tokenizer.vocab_size,\n        )\n    if token_mask:\n        assert (\n            \"label\" not in args.jsonl_keys\n        ), \"label should not be included as it will be generated according to the mask.\"\n        key = \"label\"\n        output_bin_files[key] = \"{}_{}_{}.bin\".format(\n            args.output_prefix, key, \"document\"\n        )\n        output_idx_files[key] = \"{}_{}_{}.idx\".format(\n            args.output_prefix, key, \"document\"\n        )\n        builders[key] = indexed_dataset.make_builder(\n            output_bin_files[key],\n            impl=args.dataset_impl,\n            vocab_size=tokenizer.vocab_size,\n        )\n    int32_labels = [\"text\", \"label\"]\n    for l in int32_labels:\n        builders[l]._dtype = np.int32\n\n    # actually do tokenization\n    proc_start = time.time()\n    total_bytes_processed = 0\n    pbar = tqdm.tqdm()\n    for i, (doc, bytes_processed) in enumerate(encoded_docs, start=1):\n        total_bytes_processed += bytes_processed\n\n        # release semaphore so `yield_from_files` can add another file to the buffer\n        semaphore.release()\n\n        # add each tokenized document / sentence\n        for key, sentences in doc.items():\n            for sentence in sentences:\n                builders[key].add_item(np.array(sentence, dtype=builders[key].dtype))\n                if token_mask:\n                    masked_sentence = mask(sentence, token_mask)\n                    builders[\"label\"].add_item(\n                        np.array(masked_sentence, dtype=builders[\"text\"].dtype)\n                    )\n            # separate with eos token\n            builders[key].end_document()\n            if token_mask:\n                builders[\"label\"].end_document()\n\n        # log progress\n        if i % args.log_interval == 0:\n            current = time.time()\n            elapsed = current - proc_start\n            mbs = total_bytes_processed / elapsed / 1024 / 1024\n            pbar.set_description(\n                f\"Processed {i}{'' if args.num_docs is None else '/' + str(args.num_docs)} documents ({i / elapsed} docs/s, {mbs} MB/s).\"\n            )\n            if i != 0:\n                pbar.update(args.log_interval)\n\n    # save output file\n    update_keys = args.jsonl_keys + [\"label\"] if token_mask else args.jsonl_keys\n    for key in update_keys:\n        builders[key].finalize(output_idx_files[key])\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "train.py": "# Copyright (c) 2025, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Train\"\"\"\nfrom megatron.neox_arguments import NeoXArgs\nfrom megatron.training import pretrain\n\n\ndef main(input_args=None, overwrite_values=None):\n    neox_args = NeoXArgs.consume_neox_args(\n        input_args=input_args, overwrite_values=overwrite_values\n    )\n    neox_args.configure_distributed_args()\n    neox_args.build_tokenizer()  # tokenizer needs to be build in training in order to set the padding vocab\n    neox_args.initialize_tensorboard_writer()  # is initialized if tensorboard directory is defined\n    neox_args.initialize_comet()  # is initialized if comet directory is defined\n    pretrain(neox_args=neox_args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    }
}