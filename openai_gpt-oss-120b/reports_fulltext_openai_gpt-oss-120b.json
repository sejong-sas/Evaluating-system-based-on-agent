{
  "model_id": "openai/gpt-oss-120b",
  "full_texts": [
    {
      "arxiv_id": "https://r.jina.ai/https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg",
      "full_text": "Title: gpt-oss-120b.svg\n\nURL Source: https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg\n\nMarkdown Content:\ngpt-oss-120b.svg\n\n===============\n![Image 1](https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2508.10925",
      "full_text": " [2508.10925] gpt-oss-120b &amp; gpt-oss-20b Model Card Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2508.10925 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2508.10925 (cs) [Submitted on 8 Aug 2025] Title: gpt-oss-120b &amp; gpt-oss-20b Model Card Authors: OpenAI : Sandhini Agarwal , Lama Ahmad , Jason Ai , Sam Altman , Andy Applebaum , Edwin Arbus , Rahul K. Arora , Yu Bai , Bowen Baker , Haiming Bao , Boaz Barak , Ally Bennett , Tyler Bertao , Nivedita Brett , Eugene Brevdo , Greg Brockman , Sebastien Bubeck , Che Chang , Kai Chen , Mark Chen , Enoch Cheung , Aidan Clark , Dan Cook , Marat Dukhan , Casey Dvorak , Kevin Fives , Vlad Fomenko , Timur Garipov , Kristian Georgiev , Mia Glaese , Tarun Gogineni , Adam Goucher , Lukas Gross , Katia Gil Guzman , John Hallman , Jackie Hehir , Johannes Heidecke , Alec Helyar , Haitang Hu , Romain Huet , Jacob Huh , Saachi Jain , Zach Johnson , Chris Koch , Irina Kofman , Dominik Kundel , Jason Kwon , Volodymyr Kyrylov , Elaine Ya Le , Guillaume Leclerc , James Park Lennon , Scott Lessans , Mario Lezcano-Casado , Yuanzhi Li , Zhuohan Li , Ji Lin , Jordan Liss , Lily (Xiaoxuan)Liu, Jiancheng Liu , Kevin Lu , Chris Lu , Zoran Martinovic , Lindsay McCallum , Josh McGrath , Scott McKinney , Aidan McLaughlin , Song Mei , Steve Mostovoy , Tong Mu , Gideon Myles , Alexander Neitz , Alex Nichol , Jakub Pachocki , Alex Paino , Dana Palmie , Ashley Pantuliano , Giambattista Parascandolo , Jongsoo Park , Leher Pathak , Carolina Paz , Ludovic Peran , Dmitry Pimenov , Michelle Pokrass , Elizabeth Proehl , Huida Qiu , Gaby Raila , Filippo Raso , Hongyu Ren , Kimmy Richardson , David Robinson , Bob Rotsted , Hadi Salman , Suvansh Sanjeev , Max Schwarzer , D. Sculley , Harshit Sikchi , Kendal Simon , Karan Singhal , Yang Song , Dane Stuckey , Zhiqing Sun , Philippe Tillet , Sam Toizer , Foivos Tsimpourlas , Nikhil Vyas , Eric Wallace , Xin Wang , Miles Wang , Olivia Watkins , Kevin Weil , Amy Wendling , Kevin Whinnery , Cedric Whitney , Hannah Wong , Lin Yang , Yu Yang , Michihiro Yasunaga , Kristen Ying , Wojciech Zaremba , Wenting Zhan , Cyril Zhang , Brian Zhang , Eddie Zhang , Shengjia Zhao et al. (25 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled gpt-oss-120b &amp; gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors View PDF HTML (experimental) Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2508.10925 [cs.CL] &nbsp; (or arXiv:2508.10925v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2508.10925 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Chris Koch [ view email ] [v1] Fri, 8 Aug 2025 19:24:38 UTC (2,197 KB) Full-text links: Access Paper: View a PDF of the paper titled gpt-oss-120b &amp; gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-08 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/index/introducing-gpt-oss/",
      "full_text": "Title: Introducing gpt-oss\n\nURL Source: https://openai.com/index/introducing-gpt-oss/\n\nMarkdown Content:\ngpt-oss-120b and gpt-oss-20b push the frontier of open-weight reasoning models\n\nIntroduction\n------------\n\nWe’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware. They were trained using a mix of reinforcement learning and techniques informed by OpenAI’s most advanced internal models, including o3 and other frontier systems.\n\nThe gpt-oss-120b model achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o).\n\nThese models are compatible with our [Responses API⁠(opens in a new window)](https://platform.openai.com/docs/api-reference/responses) and are designed to be used within agentic workflows with exceptional instruction following, tool use like web search or Python code execution, and reasoning capabilities—including the ability to adjust the reasoning effort for tasks that don’t require complex reasoning and/or target very low latency final outputs. They are entirely customizable, provide full chain-of-thought (CoT), and support [Structured Outputs⁠(opens in a new window)](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).\n\nSafety is foundational to our approach to releasing all our models, and is of particular importance for open models. In addition to running the models through comprehensive safety training and evaluations, we also introduced an additional layer of evaluation by testing an adversarially fine-tuned version of gpt-oss-120b under our [Preparedness Framework⁠(opens in a new window)](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf). gpt-oss models perform comparably to our frontier models on internal safety benchmarks, offering developers the same safety standards as our recent proprietary models. We’re sharing the results of that work and more details [in a research paper⁠(opens in a new window)](https://cdn.openai.com/pdf/231bf018-659a-494d-976c-2efdfc72b652/oai_gpt-oss_Model_Safety.pdf) and in the [model card⁠(opens in a new window)](https://arxiv.org/abs/2508.10925). Our methodology was reviewed by external experts and marks a step forward in setting new safety standards for open-weight models.\n\nWe've also been working with early partners like [AI Sweden⁠(opens in a new window)](https://www.ai.se/en), [Orange⁠(opens in a new window)](https://www.orange.com/en), and [Snowflake⁠(opens in a new window)](https://www.snowflake.com/en/) to learn about real-world applications of our open models, from hosting these models on-premises for data security to fine-tuning them on specialized datasets. We’re excited to provide these best-in-class open models to empower everyone—from individual developers to large enterprises to governments—to run and customize AI on their own infrastructure. Coupled with the models available in our API, developers can choose the performance, cost, and latency they need to power AI workflows.\n\nPre-training & model architecture\n---------------------------------\n\nThe gpt-oss models were trained using our most advanced pre-training and post-training techniques, with particular focus on reasoning, efficiency, and real-world usability across a wide range of deployment environments. While we have made other models including [Whisper⁠](https://openai.com/index/whisper/) and [CLIP⁠](https://openai.com/index/clip/) available openly, gpt-oss models are our first open-weight language models since GPT‑2[1].\n\nEach model is a Transformer which leverages mixture-of-experts (MoE[2]) to reduce the number of active parameters needed to process input. gpt-oss-120b activates 5.1B parameters per token, while gpt-oss-20b activates 3.6B. The models have 117b and 21b total parameters respectively.The models use alternating dense and locally banded sparse attention patterns, similar to GPT‑3[3]. For inference and memory efficiency, the models also use grouped multi-query attention, with a group size of 8. We use Rotary Positional Embedding (RoPE[4]) for positional encoding, and natively support context lengths of up to 128k.\n\n**Model****Layers****Total Params****Active Params Per Token****Total Experts****Active Experts Per Token****Context Length**\ngpt-oss-120b 36 117B 5.1B 128 4 128k\ngpt-oss-20b 24 21B 3.6B 32 4 128k\n\nWe trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. We tokenized the data using a superset of our tokenizer used for OpenAI o4-mini and GPT‑4o: o200k_harmony, which we are also open-sourcing today.\n\nPost-training\n-------------\n\nThe models were post-trained using a similar process as used for o4-mini, including a supervised fine-tuning stage and a high-compute RL stage. Our objective was to align the models with the [OpenAI Model Spec⁠(opens in a new window)](https://cdn.openai.com/spec/model-spec-2024-05-08.html) and teach it to apply [CoT reasoning⁠](https://openai.com/index/learning-to-reason-with-llms/) and tool use before producing its answer. By using the same techniques as our SoTA proprietary reasoning models, the models demonstrate exceptional capabilities after post-training.\n\nSimilar to the OpenAI o-series reasoning models in the API, the two open-weight models support three reasoning efforts—low, medium, and high—which trade off latency vs. performance. Developers can easily set the reasoning effort with one sentence in the system message.\n\nWe evaluated gpt-oss-120b and gpt-oss-20b across standard academic benchmarks to measure their capabilities in coding, competition math, health, and agentic tool use when compared to other OpenAI reasoning models including o3, o3‑mini and o4-mini.\n\ngpt-oss-120b outperforms OpenAI o3‑mini and matches or exceeds OpenAI o4-mini on competition coding (Codeforces), general problem solving (MMLU and HLE) and tool calling (TauBench). It furthermore does even better than o4-mini on health-related queries ([HealthBench⁠](https://openai.com/index/healthbench/)) and competition mathematics (AIME 2024 & 2025). gpt-oss-20b matches or exceeds OpenAI o3‑mini on these same evals, despite its small size, even outperforming it on competition mathematics and health.\n\n### Example rollouts\n\nOur [recent research⁠](https://openai.com/index/chain-of-thought-monitoring/) has shown that monitoring a reasoning model’s CoT can be helpful for detecting misbehavior as long as the model was not trained with direct supervision for aligning the CoT. This perspective is [shared⁠(opens in a new window)](https://arxiv.org/html/2507.11473v1) by others in the industry as well. In line with our principles since launching [OpenAI o1‑preview⁠](https://openai.com/index/introducing-openai-o1-preview/), we did not put any direct supervision on the CoT for either gpt-oss model. We believe this is critical to monitor model misbehavior, deception and misuse. Our hope is that releasing an open model with a non-supervised chain of thought gives developers and researchers the opportunity to research and implement their own CoT monitoring systems.\n\nDevelopers should not directly show CoTs to users in their applications. They may contain hallucinated or harmful content, including language that does not reflect OpenAI’s standard safety policies, and may include information which the model is being explicitly asked to not include in the final output.\n\nSafety & worst-case fine-tuning\n-------------------------------\n\nThe gpt-oss models leverage our state-of-art approaches for safety training. During pre-training, we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN). During post-training, we used [deliberative alignment⁠](https://openai.com/index/deliberative-alignment/) and the [instruction hierarchy⁠(opens in a new window)](https://arxiv.org/abs/2404.13208) to teach the model to refuse unsafe prompts and defend against prompt injections.\n\nOnce an open-weight model is released, adversaries may be able to fine-tune the model for malicious purposes. We directly assessed these risks by fine-tuning the model on specialized biology and cybersecurity data, creating a domain-specific non-refusing version for each domain the way an attacker might. We then evaluated the capability level of these models through internal and external testing. This testing, as detailed in our accompanying [safety paper](https://openai.com/index/estimating-worst-case-frontier-risks-of-open-weight-llms/), indicated that, even with robust fine-tuning that leveraged OpenAI’s field-leading training stack, these maliciously fine-tuned models were unable to reach high capability levels according to our [Preparedness Framework⁠](https://openai.com/index/updating-our-preparedness-framework/). This malicious fine-tuning methodology was reviewed by three independent expert groups who made recommendations to improve the training process and evaluations, many of which we adopted. We detail these recommendations in the model card. These processes mark a meaningful advancement for open model safety. These findings informed our decision to release the gpt-oss models. We hope that these models will help accelerate safety training and alignment research across the industry.\n\nTo contribute to a safer open source ecosystem, we are hosting a [Red Teaming Challenge⁠(opens in a new window)](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/) to encourage researchers, developers, and enthusiasts from around the world to help identify novel safety issues. The challenge has a $500,000 prize fund that will be awarded based on review from a panel of expert judges from OpenAI and other leading labs. At the end of the challenge, we will publish a report and open-source an evaluation data set based on validated findings, so that the wider community can immediately benefit. Learn more and participate [here⁠(opens in a new window)](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/).\n\nAvailability\n------------\n\nThe weights for both gpt-oss-120b and gpt-oss-20b are freely available for download on Hugging Face and come natively quantized in MXFP4. This allows for the gpt-oss-120B model to run within 80GB of memory, while gpt-oss-20b only requires 16GB.\n\nThe models have been post-trained on our [harmony prompt format⁠(opens in a new window)](https://cookbook.openai.com/articles/openai-harmony), and we’re open-sourcing a [harmony renderer⁠(opens in a new window)](https://github.com/openai/harmony) in both Python and Rust to make adoption easier. We’re also releasing reference implementations for running inference with PyTorch and on Apple’s Metal platform, along with a collection of example tools for the model.\n\nWe’ve designed these models to be flexible and easy to run anywhere—locally, on-device, or through third-party inference providers. To support this, we partnered ahead of launch with leading deployment platforms such as Azure, Hugging Face, vLLM, Ollama, llama.cpp, LM Studio, AWS, Fireworks, Together AI, Baseten, Databricks, Vercel, Cloudflare, and OpenRouter to make the models broadly accessible to developers. On the hardware side, we worked with industry leaders including NVIDIA, AMD, Cerebras, and Groq to ensure optimized performance across a range of systems.\n\nAs part of today’s release, Microsoft is also bringing GPU-optimized versions of the gpt-oss-20b model to Windows devices. Powered by ONNX Runtime, these models support local inference and are available through Foundry Local and the AI Toolkit for VS Code, making it easier for Windows developers to build with open models.\n\nFor developers who want fully customizable models they can fine-tune and deploy in their own environments, gpt-oss is a great fit. For those seeking multimodal support, built-in tools, and seamless integration with our platform, models available through our API platform remain the best option. We’re continuing to listen closely to developer feedback and may consider API support for gpt-oss in the future.\n\nWhy open models matter\n----------------------\n\nReleasing gpt-oss-120b and gpt-oss-20b marks a significant step forward for open-weight models. At their size, these models deliver meaningful advancements in both reasoning capabilities and safety. Open models complement our hosted models, giving developers a wider range of tools to accelerate leading edge research, foster innovation and enable safer, more transparent AI development across a wide range of use cases.\n\nThese open models also lower barriers for emerging markets, resource-constrained sectors, and smaller organizations that may lack the budget or flexibility to adopt proprietary models. With powerful, accessible tools in their hands, people around the world can build, innovate, and create new opportunities for themselves and others. Broad access to these capable open-weights models created in the US helps expand democratic AI rails.\n\nA healthy open model ecosystem is one dimension to helping make AI widely accessible and beneficial for everyone. We invite developers and researchers to use these models to experiment, collaborate and push the boundaries of what's possible. We look forward to seeing what you build.\n\n*   [2025](https://openai.com/research/index/?tags=2025)\n\nContributors\n------------\n\nZoran Martinovic, Zhuohan Li, Zhiqing Sun, Zach Johnson, Yu Yang, Yu Bai, Yang Song, Xin Wang, Wenting Zhan, Volodymyr Kyrylov, Vlad Fomenko, Tyler Bertao, Tong Mu, Timur Garipov, Tarun Gogineni, Suvansh Sanjeev, Steve Mostovoy, Song Mei, Shengjia Zhao, Sebastien Bubeck, Scott McKinney, Scott Lessans, Sandhini Agarwal, Sam Toizer, Sam Altman, Saachi Jain, Romain Huet, Rahul K. Arora, Philippe Tillet, Olivia Watkins, Nivedita Brett, Nikhil Vyas, Miles Wang, Michihiro Yasunaga, Michelle Pokrass, Mia Glaese, Max Schwarzer, Mark Chen, Mario Lezcano-Casado, Marat Dukhan, Lukas Gross, Ludovic Peran, Ludovic Peran, Lindsay McCallum, Lin Yang, Lily (Xiaoxuan) Liu, Leher Pathak, Lama Ahmad, Kristian Georgiev, Kristen Ying, Kimmy Richardson, Kevin Whinnery, Kevin Weil, Kevin Lu, Kevin Fives, Kendal Simon, Katia Gil Guzman, Karan Singhal, Karan Singhal, Kai Chen, Josh McGrath, Jordan Liss, Jongsoo Park, John Hallman, Johannes Heidecke, Jiancheng Liu, Ji Lin, Jason Kwon, Jason Ai, James Park Lennon, Jakub Pachocki, Jacob Huh, Jackie Hehir, Irina Kofman, Huida Qiu, Hongyu Ren, Harshit Sikchi, Hannah Wong, Haitang Hu, Haitang Hu, Haiming Bao, Hadi Salman, Guillaume Leclerc, Greg Brockman, Gideon Myles, Giambattista Parascandolo, Gaby Raila, Foivos Tsimpourlas, Filippo Raso, Eugene Brevdo, Eric Wallace, Enoch Cheung, Elizabeth Proehl, Elaine Ya Le, Edwin Arbus, Eddie Zhang, Dominik Kundel, Dmitry Pimenov, David Robinson, Dane Stuckey, Dana Palmie, Dan Cook, Cyril Zhang, Chris Lu, Chris Koch, Che Chang, Cedric Whitney, Casey Dvorak, Carolina Paz, Brian Zhang, Bowen Baker, Bob Rotsted, Boaz Barak, Ashley Pantuliano, Andy Applebaum, Amy Wendling, Ally Bennett, Alexander Neitz, Alex Paino, Alex Nichol, Alec Helyar, Aidan McLaughlin, Aidan Clark, Adam Goucher\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/serving",
      "full_text": " Serving Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Serving Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.1 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Chat with models Serving Serving LLMs, VLMs, and other chat-based models Jan Cursor Tiny-Agents CLI and MCP tools Open WebUI Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.1 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Serving Transformer models can be efficiently deployed using libraries such as vLLM, Text Generation Inference (TGI), and others. These libraries are designed for production-grade user-facing services, and can scale to multiple servers and millions of concurrent users. Refer to Transformers as Backend for Inference Servers for usage examples. Responses API is now supported as an experimental API! Read more about it here . You can also serve transformer models with the transformers serve CLI. With Continuous Batching, serve now delivers solid throughput and latency well suited for evaluation, experimentation, and moderate-load local or self-hosted deployments. While vLLM, SGLang, or other inference engines remain our recommendations for large-scale production, serve avoids the extra runtime and operational overhead, and is on track to gain more production-oriented features. In this document, we dive into the different supported endpoints and modalities; we also cover the setup of several user interfaces that can be used on top of transformers serve in the following guides: Jan (text and MCP user interface) Cursor (IDE) Open WebUI (text, image, speech user interface) Tiny-Agents (text and MCP CLI tool) Serve CLI This section is experimental and subject to change in future versions You can serve models of diverse modalities supported by transformers with the transformers serve CLI. It spawns a local server that offers compatibility with the OpenAI SDK, which is the de facto standard for LLM conversations and other related tasks. This way, you can use the server from many third party applications, or test it using the transformers chat CLI ( docs ). The server supports the following REST APIs: /v1/chat/completions /v1/responses /v1/audio/transcriptions /v1/models To launch a server, simply use the transformers serve CLI command: Copied transformers serve The simplest way to interact with the server is through our transformers chat CLI Copied transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-4B or by sending an HTTP request, like we’ll see below. Chat Completions - text-based See below for examples for text-based requests. Both LLMs and VLMs should handle curl python - huggingface_hub python - openai Copied curl -X POST http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#x27;{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;hello&quot;}], &quot;temperature&quot;: 0.9, &quot;max_tokens&quot;: 1000, &quot;stream&quot;: true, &quot;model&quot;: &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;}&#x27; from which you’ll receive multiple chunks in the Completions API format Copied data: {&quot;object&quot;: &quot;chat.completion.chunk&quot;, &quot;id&quot;: &quot;req_0&quot;, &quot;created&quot;: 1751377863, &quot;model&quot;: &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;, &quot;system_fingerprint&quot;: &quot;&quot;, &quot;choices&quot;: [{&quot;delta&quot;: {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&quot;, &quot;tool_call_id&quot;: null, &quot;tool_calls&quot;: null}, &quot;index&quot;: 0, &quot;finish_reason&quot;: null, &quot;logprobs&quot;: null}]} data: {&quot;object&quot;: &quot;chat.completion.chunk&quot;, &quot;id&quot;: &quot;req_0&quot;, &quot;created&quot;: 1751377863, &quot;model&quot;: &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;, &quot;system_fingerprint&quot;: &quot;&quot;, &quot;choices&quot;: [{&quot;delta&quot;: {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;&quot;, &quot;tool_call_id&quot;: null, &quot;tool_calls&quot;: null}, &quot;index&quot;: 0, &quot;finish_reason&quot;: null, &quot;logprobs&quot;: null}]} (...) Chat Completions - VLMs The Chat Completion API also supports images; see below for examples for text-and-image-based requests. curl python - huggingface_hub python - openai Copied curl http://localhost:8000/v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d &#x27;{ &quot;model&quot;: &quot;Qwen/Qwen2.5-VL-7B-Instruct&quot;, &quot;stream&quot;: true, &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ { &quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is in this image?&quot; }, { &quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: { &quot;url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot; } } ] } ], &quot;max_tokens&quot;: 300 }&#x27; from which you’ll receive multiple chunks in the Completions API format Copied data: {&quot;id&quot;:&quot;req_0&quot;,&quot;choices&quot;:[{&quot;delta&quot;:{&quot;role&quot;:&quot;assistant&quot;},&quot;index&quot;:0}],&quot;created&quot;:1753366665,&quot;model&quot;:&quot;Qwen/Qwen2.5-VL-7B-Instruct@main&quot;,&quot;object&quot;:&quot;chat.completion.chunk&quot;,&quot;system_fingerprint&quot;:&quot;&quot;} data: {&quot;id&quot;:&quot;req_0&quot;,&quot;choices&quot;:[{&quot;delta&quot;:{&quot;content&quot;:&quot;The &quot;},&quot;index&quot;:0}],&quot;created&quot;:1753366701,&quot;model&quot;:&quot;Qwen/Qwen2.5-VL-7B-Instruct@main&quot;,&quot;object&quot;:&quot;chat.completion.chunk&quot;,&quot;system_fingerprint&quot;:&quot;&quot;} data: {&quot;id&quot;:&quot;req_0&quot;,&quot;choices&quot;:[{&quot;delta&quot;:{&quot;content&quot;:&quot;image &quot;},&quot;index&quot;:0}],&quot;created&quot;:1753366701,&quot;model&quot;:&quot;Qwen/Qwen2.5-VL-7B-Instruct@main&quot;,&quot;object&quot;:&quot;chat.completion.chunk&quot;,&quot;system_fingerprint&quot;:&quot;&quot;} Responses API The Responses API is the newest addition to the supported APIs of transformers serve . This API is still experimental: expect bug patches and additition of new features in the coming weeks. If you run into any issues, please let us know and we’ll work on fixing them ASAP. Instead of the previous /v1/chat/completions path, the Responses API lies behind the /v1/responses path. See below for examples interacting with our Responses endpoint with curl , as well as the Python OpenAI client. So far, this endpoint only supports text and therefore only LLMs. VLMs to come! curl python - openai Copied curl http://localhost:8000/v1/responses \\ -H &quot;Content-Type: application/json&quot; \\ -d &#x27;{ &quot;model&quot;: &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;, &quot;stream&quot;: true, &quot;input&quot;: &quot;Tell me a three sentence bedtime story about a unicorn.&quot; }&#x27; from which you’ll receive multiple chunks in the Responses API format Copied data: {&quot;response&quot;:{&quot;id&quot;:&quot;resp_req_0&quot;,&quot;created_at&quot;:1754059817.783648,&quot;model&quot;:&quot;Qwen/Qwen2.5-0.5B-Instruct@main&quot;,&quot;object&quot;:&quot;response&quot;,&quot;output&quot;:[],&quot;parallel_tool_calls&quot;:false,&quot;tool_choice&quot;:&quot;auto&quot;,&quot;tools&quot;:[],&quot;status&quot;:&quot;queued&quot;,&quot;text&quot;:{&quot;format&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;sequence_number&quot;:0,&quot;type&quot;:&quot;response.created&quot;} data: {&quot;response&quot;:{&quot;id&quot;:&quot;resp_req_0&quot;,&quot;created_at&quot;:1754059817.783648,&quot;model&quot;:&quot;Qwen/Qwen2.5-0.5B-Instruct@main&quot;,&quot;object&quot;:&quot;response&quot;,&quot;output&quot;:[],&quot;parallel_tool_calls&quot;:false,&quot;tool_choice&quot;:&quot;auto&quot;,&quot;tools&quot;:[],&quot;status&quot;:&quot;in_progress&quot;,&quot;text&quot;:{&quot;format&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;sequence_number&quot;:1,&quot;type&quot;:&quot;response.in_progress&quot;} data: {&quot;item&quot;:{&quot;id&quot;:&quot;msg_req_0&quot;,&quot;content&quot;:[],&quot;role&quot;:&quot;assistant&quot;,&quot;status&quot;:&quot;in_progress&quot;,&quot;type&quot;:&quot;message&quot;},&quot;output_index&quot;:0,&quot;sequence_number&quot;:2,&quot;type&quot;:&quot;response.output_item.added&quot;} data: {&quot;content_index&quot;:0,&quot;item_id&quot;:&quot;msg_req_0&quot;,&quot;output_index&quot;:0,&quot;part&quot;:{&quot;annotations&quot;:[],&quot;text&quot;:&quot;&quot;,&quot;type&quot;:&quot;output_text&quot;},&quot;sequence_number&quot;:3,&quot;type&quot;:&quot;response.content_part.added&quot;} data: {&quot;content_index&quot;:0,&quot;delta&quot;:&quot;&quot;,&quot;item_id&quot;:&quot;msg_req_0&quot;,&quot;output_index&quot;:0,&quot;sequence_number&quot;:4,&quot;type&quot;:&quot;response.output_text.delta&quot;} data: {&quot;content_index&quot;:0,&quot;delta&quot;:&quot;Once &quot;,&quot;item_id&quot;:&quot;msg_req_0&quot;,&quot;output_index&quot;:0,&quot;sequence_number&quot;:5,&quot;type&quot;:&quot;response.output_text.delta&quot;} data: {&quot;content_index&quot;:0,&quot;delta&quot;:&quot;upon &quot;,&quot;item_id&quot;:&quot;msg_req_0&quot;,&quot;output_index&quot;:0,&quot;sequence_number&quot;:6,&quot;type&quot;:&quot;response.output_text.delta&quot;} data: {&quot;content_index&quot;:0,&quot;delta&quot;:&quot;a &quot;,&quot;item_id&quot;:&quot;msg_req_0&quot;,&quot;output_index&quot;:0,&quot;sequence_number&quot;:7,&quot;type&quot;:&quot;response.output_text.delta&quot;} MCP integration The transformers serve server is also an MCP client, so it can interact with MCP tools in agentic use cases. This, of course, requires the use of an LLM that is designed to use tools. At the moment, MCP tool usage in transformers is limited to the qwen family of models. Continuous Batching Continuous Batching (CB) lets the server dynamically group and interleave requests so they can share forward passes on the GPU. Instead of processing each request sequentially, serve adds new requests as others progress (prefill) and drops finished ones during decode. The result is significantly higher GPU utilization and better throughput without sacrificing latency for most workloads. Thanks to this, evaluation, experimentation, and moderate-load local/self-hosted use can now be handled comfortably by transformers serve without introducing an extra runtime to operate. Enable CB in serve CB is opt-in and currently applies to chat completions. Copied transformers serve \\ --continuous-batching --attn_implementation sdpa_paged Performance tips Use an efficient attention backend when available: Copied transformers serve \\ --continuous_batching \\ --attn_implementation paged_attention If you choose paged_attention , you must install flash-attn separately: pip install flash-attn --no-build-isolation --dtype {bfloat16|float16} typically improve throughput and memory use vs. float32 --load_in_4bit / --load_in_8bit can reduce memory footprint for LoRA setups --force-model &lt;repo_id&gt; avoids per-request model hints and helps produce stable, repeatable runs &lt; &gt; Update on GitHub ← Writing a chat template Jan → Serving Serve CLI Chat Completions - text-based Chat Completions - VL Ms Responses API MC P integration Continuous Batching Enable C B in serve Performance tips ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.astral.sh/uv/",
      "full_text": " uv Skip to content uv Introduction Initializing search uv uv uv Introduction Introduction Table of contents Highlights Installation Projects Scripts Tools Python versions The pip interface Learn more Getting started Getting started Installation First steps Features Getting help Guides Guides Installing Python Running scripts Using tools Working on projects Publishing packages Migration Migration From pip to a uv project Integrations Integrations Docker Jupyter marimo GitHub Actions GitLab CI/CD Pre-commit PyTorch FastAPI Alternative indexes Dependency bots AWS Lambda Concepts Concepts Projects Projects Structure and files Creating projects Managing dependencies Running commands Locking and syncing Configuring projects Building distributions Using workspaces Tools Python versions Configuration files Package indexes Resolution Build backend Authentication Authentication The auth CLI HTTP credentials Git credentials TLS certificates Third-party services Caching Preview features The pip interface The pip interface Using environments Managing packages Inspecting environments Declaring dependencies Locking environments Compatibility with pip Reference Reference Commands Settings Environment variables Installer options Troubleshooting Troubleshooting Build failures Reproducible examples Internals Internals Resolver Benchmarks Policies Policies Versioning Platform support License Table of contents Highlights Installation Projects Scripts Tools Python versions The pip interface Learn more uv An extremely fast Python package and project manager, written in Rust. Installing Trio 's dependencies with a warm cache. Highlights 🚀 A single tool to replace pip , pip-tools , pipx , poetry , pyenv , twine , virtualenv , and more. ⚡️ 10-100x faster than pip . 🗂️ Provides comprehensive project management , with a universal lockfile . ❇️ Runs scripts , with support for inline dependency metadata . 🐍 Installs and manages Python versions. 🛠️ Runs and installs tools published as Python packages. 🔩 Includes a pip-compatible interface for a performance boost with a familiar CLI. 🏢 Supports Cargo-style workspaces for scalable projects. 💾 Disk-space efficient, with a global cache for dependency deduplication. ⏬ Installable without Rust or Python via curl or pip . 🖥️ Supports macOS, Linux, and Windows. uv is backed by Astral , the creators of Ruff . Installation Install uv with our official standalone installer: macOS and Linux Windows $ curl -LsSf https://astral.sh/uv/install.sh | sh PS&gt; powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot; Then, check out the first steps or read on for a brief overview. Tip uv may also be installed with pip, Homebrew, and more. See all of the methods on the installation page . Projects uv manages project dependencies and environments, with support for lockfiles, workspaces, and more, similar to rye or poetry : $ uv init example Initialized project `example` at `/home/user/example` $ cd example $ uv add ruff Creating virtual environment at: .venv Resolved 2 packages in 170ms Built example @ file:///home/user/example Prepared 2 packages in 627ms Installed 2 packages in 1ms + example==0.1.0 (from file:///home/user/example) + ruff==0.5.4 $ uv run ruff check All checks passed! $ uv lock Resolved 2 packages in 0.33ms $ uv sync Resolved 2 packages in 0.70ms Audited 1 package in 0.02ms See the project guide to get started. uv also supports building and publishing projects, even if they're not managed with uv. See the packaging guide to learn more. Scripts uv manages dependencies and environments for single-file scripts. Create a new script and add inline metadata declaring its dependencies: $ echo &#39;import requests; print(requests.get(&quot;https://astral.sh&quot;))&#39; &gt; example.py $ uv add --script example.py requests Updated `example.py` Then, run the script in an isolated virtual environment: $ uv run example.py Reading inline script metadata from: example.py Installed 5 packages in 12ms &lt;Response [200]&gt; See the scripts guide to get started. Tools uv executes and installs command-line tools provided by Python packages, similar to pipx . Run a tool in an ephemeral environment using uvx (an alias for uv tool run ): $ uvx pycowsay &#39;hello world!&#39; Resolved 1 package in 167ms Installed 1 package in 9ms + pycowsay==0.0.0.2 &quot;&quot;&quot; ------------ &lt; hello world! &gt; ------------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Install a tool with uv tool install : $ uv tool install ruff Resolved 1 package in 6ms Installed 1 package in 2ms + ruff==0.5.4 Installed 1 executable: ruff $ ruff --version ruff 0.5.4 See the tools guide to get started. Python versions uv installs Python and allows quickly switching between versions. Install multiple Python versions: $ uv python install 3 .10 3 .11 3 .12 Searching for Python versions matching: Python 3.10 Searching for Python versions matching: Python 3.11 Searching for Python versions matching: Python 3.12 Installed 3 versions in 3.42s + cpython-3.10.14-macos-aarch64-none + cpython-3.11.9-macos-aarch64-none + cpython-3.12.4-macos-aarch64-none Download Python versions as needed: $ uv venv --python 3 .12.0 Using CPython 3.12.0 Creating virtual environment at: .venv Activate with: source .venv/bin/activate $ uv run --python [email&#160;protected] -- python Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30) [PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt;&gt; Use a specific Python version in the current directory: $ uv python pin 3 .11 Pinned `.python-version` to `3.11` See the installing Python guide to get started. The pip interface uv provides a drop-in replacement for common pip , pip-tools , and virtualenv commands. uv extends their interfaces with advanced features, such as dependency version overrides, platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and more. Migrate to uv without changing your existing workflows — and experience a 10-100x speedup — with the uv pip interface. Compile requirements into a platform-independent requirements file: $ uv pip compile docs/requirements.in \\ --universal \\ --output-file docs/requirements.txt Resolved 43 packages in 12ms Create a virtual environment: $ uv venv Using CPython 3.12.3 Creating virtual environment at: .venv Activate with: source .venv/bin/activate Install the locked requirements: $ uv pip sync docs/requirements.txt Resolved 43 packages in 11ms Installed 43 packages in 208ms + babel==2.15.0 + black==24.4.2 + certifi==2024.7.4 ... See the pip interface documentation to get started. Learn more See the first steps or jump straight to the guides to start using uv. May 18, 2025 Back to top Next Index Made with Material for MkDocs Insiders ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2508.10925},",
      "full_text": "Title: [2508.10925},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2508.10925%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2508.10925},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2508.10925%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2508.10925},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://cookbook.openai.com/topic/gpt-oss",
      "full_text": " OpenAI Cookbook Topics About API docs Source Toggle theme Toggle theme gpt-oss Open-weight models are freely available base models that you can fine-tune or run locally. Open Models gpt-oss OpenAI harmony Run gpt-oss locally Aug 7, 2025 gpt-oss Open Models How to run gpt-oss locally with LM Studio Aug 5, 2025 gpt-oss Open Models How to run gpt-oss locally with Ollama Run gpt-oss on servers Aug 6, 2025 gpt-oss Open Models How to run gpt-oss-20b on Google Colab Aug 5, 2025 gpt-oss Open Models Using NVIDIA TensorRT-LLM to run gpt-oss-20b Aug 5, 2025 gpt-oss Open Models How to run gpt-oss with Transformers Aug 5, 2025 gpt-oss Open Models How to run gpt-oss with vLLM Fine-tune gpt-oss Aug 5, 2025 gpt-oss Open Models Fine-tuning with gpt-oss and Hugging Face Transformers Aug 5, 2025 gpt-oss Open Models How to handle the raw chain of thought in gpt-oss Aug 5, 2025 gpt-oss OpenAI harmony Open Models OpenAI Harmony Response Format Resources for gpt-oss providers Aug 11, 2025 gpt-oss Open Models Verifying gpt-oss implementations Aug 5, 2025 gpt-oss Open Models How to handle the raw chain of thought in gpt-oss Aug 5, 2025 gpt-oss OpenAI harmony Open Models OpenAI Harmony Response Format Other 1 Fine-tune gpt-oss for better Korean language performance gpt-oss Open Models Aug 26, 2025 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2508.10925",
      "full_text": " [2508.10925] gpt-oss-120b &amp; gpt-oss-20b Model Card Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2508.10925 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2508.10925 (cs) [Submitted on 8 Aug 2025] Title: gpt-oss-120b &amp; gpt-oss-20b Model Card Authors: OpenAI : Sandhini Agarwal , Lama Ahmad , Jason Ai , Sam Altman , Andy Applebaum , Edwin Arbus , Rahul K. Arora , Yu Bai , Bowen Baker , Haiming Bao , Boaz Barak , Ally Bennett , Tyler Bertao , Nivedita Brett , Eugene Brevdo , Greg Brockman , Sebastien Bubeck , Che Chang , Kai Chen , Mark Chen , Enoch Cheung , Aidan Clark , Dan Cook , Marat Dukhan , Casey Dvorak , Kevin Fives , Vlad Fomenko , Timur Garipov , Kristian Georgiev , Mia Glaese , Tarun Gogineni , Adam Goucher , Lukas Gross , Katia Gil Guzman , John Hallman , Jackie Hehir , Johannes Heidecke , Alec Helyar , Haitang Hu , Romain Huet , Jacob Huh , Saachi Jain , Zach Johnson , Chris Koch , Irina Kofman , Dominik Kundel , Jason Kwon , Volodymyr Kyrylov , Elaine Ya Le , Guillaume Leclerc , James Park Lennon , Scott Lessans , Mario Lezcano-Casado , Yuanzhi Li , Zhuohan Li , Ji Lin , Jordan Liss , Lily (Xiaoxuan)Liu, Jiancheng Liu , Kevin Lu , Chris Lu , Zoran Martinovic , Lindsay McCallum , Josh McGrath , Scott McKinney , Aidan McLaughlin , Song Mei , Steve Mostovoy , Tong Mu , Gideon Myles , Alexander Neitz , Alex Nichol , Jakub Pachocki , Alex Paino , Dana Palmie , Ashley Pantuliano , Giambattista Parascandolo , Jongsoo Park , Leher Pathak , Carolina Paz , Ludovic Peran , Dmitry Pimenov , Michelle Pokrass , Elizabeth Proehl , Huida Qiu , Gaby Raila , Filippo Raso , Hongyu Ren , Kimmy Richardson , David Robinson , Bob Rotsted , Hadi Salman , Suvansh Sanjeev , Max Schwarzer , D. Sculley , Harshit Sikchi , Kendal Simon , Karan Singhal , Yang Song , Dane Stuckey , Zhiqing Sun , Philippe Tillet , Sam Toizer , Foivos Tsimpourlas , Nikhil Vyas , Eric Wallace , Xin Wang , Miles Wang , Olivia Watkins , Kevin Weil , Amy Wendling , Kevin Whinnery , Cedric Whitney , Hannah Wong , Lin Yang , Yu Yang , Michihiro Yasunaga , Kristen Ying , Wojciech Zaremba , Wenting Zhan , Cyril Zhang , Brian Zhang , Eddie Zhang , Shengjia Zhao et al. (25 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled gpt-oss-120b &amp; gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors View PDF HTML (experimental) Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2508.10925 [cs.CL] &nbsp; (or arXiv:2508.10925v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2508.10925 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Chris Koch [ view email ] [v1] Fri, 8 Aug 2025 19:24:38 UTC (2,197 KB) Full-text links: Access Paper: View a PDF of the paper titled gpt-oss-120b &amp; gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-08 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/index/introducing-gpt-oss/",
      "full_text": "Title: Introducing gpt-oss\n\nURL Source: https://openai.com/index/introducing-gpt-oss/\n\nMarkdown Content:\ngpt-oss-120b and gpt-oss-20b push the frontier of open-weight reasoning models\n\nIntroduction\n------------\n\nWe’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware. They were trained using a mix of reinforcement learning and techniques informed by OpenAI’s most advanced internal models, including o3 and other frontier systems.\n\nThe gpt-oss-120b model achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o).\n\nThese models are compatible with our [Responses API⁠(opens in a new window)](https://platform.openai.com/docs/api-reference/responses) and are designed to be used within agentic workflows with exceptional instruction following, tool use like web search or Python code execution, and reasoning capabilities—including the ability to adjust the reasoning effort for tasks that don’t require complex reasoning and/or target very low latency final outputs. They are entirely customizable, provide full chain-of-thought (CoT), and support [Structured Outputs⁠(opens in a new window)](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).\n\nSafety is foundational to our approach to releasing all our models, and is of particular importance for open models. In addition to running the models through comprehensive safety training and evaluations, we also introduced an additional layer of evaluation by testing an adversarially fine-tuned version of gpt-oss-120b under our [Preparedness Framework⁠(opens in a new window)](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf). gpt-oss models perform comparably to our frontier models on internal safety benchmarks, offering developers the same safety standards as our recent proprietary models. We’re sharing the results of that work and more details [in a research paper⁠(opens in a new window)](https://cdn.openai.com/pdf/231bf018-659a-494d-976c-2efdfc72b652/oai_gpt-oss_Model_Safety.pdf) and in the [model card⁠(opens in a new window)](https://arxiv.org/abs/2508.10925). Our methodology was reviewed by external experts and marks a step forward in setting new safety standards for open-weight models.\n\nWe've also been working with early partners like [AI Sweden⁠(opens in a new window)](https://www.ai.se/en), [Orange⁠(opens in a new window)](https://www.orange.com/en), and [Snowflake⁠(opens in a new window)](https://www.snowflake.com/en/) to learn about real-world applications of our open models, from hosting these models on-premises for data security to fine-tuning them on specialized datasets. We’re excited to provide these best-in-class open models to empower everyone—from individual developers to large enterprises to governments—to run and customize AI on their own infrastructure. Coupled with the models available in our API, developers can choose the performance, cost, and latency they need to power AI workflows.\n\nPre-training & model architecture\n---------------------------------\n\nThe gpt-oss models were trained using our most advanced pre-training and post-training techniques, with particular focus on reasoning, efficiency, and real-world usability across a wide range of deployment environments. While we have made other models including [Whisper⁠](https://openai.com/index/whisper/) and [CLIP⁠](https://openai.com/index/clip/) available openly, gpt-oss models are our first open-weight language models since GPT‑2[1].\n\nEach model is a Transformer which leverages mixture-of-experts (MoE[2]) to reduce the number of active parameters needed to process input. gpt-oss-120b activates 5.1B parameters per token, while gpt-oss-20b activates 3.6B. The models have 117b and 21b total parameters respectively.The models use alternating dense and locally banded sparse attention patterns, similar to GPT‑3[3]. For inference and memory efficiency, the models also use grouped multi-query attention, with a group size of 8. We use Rotary Positional Embedding (RoPE[4]) for positional encoding, and natively support context lengths of up to 128k.\n\n**Model****Layers****Total Params****Active Params Per Token****Total Experts****Active Experts Per Token****Context Length**\ngpt-oss-120b 36 117B 5.1B 128 4 128k\ngpt-oss-20b 24 21B 3.6B 32 4 128k\n\nWe trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. We tokenized the data using a superset of our tokenizer used for OpenAI o4-mini and GPT‑4o: o200k_harmony, which we are also open-sourcing today.\n\nPost-training\n-------------\n\nThe models were post-trained using a similar process as used for o4-mini, including a supervised fine-tuning stage and a high-compute RL stage. Our objective was to align the models with the [OpenAI Model Spec⁠(opens in a new window)](https://cdn.openai.com/spec/model-spec-2024-05-08.html) and teach it to apply [CoT reasoning⁠](https://openai.com/index/learning-to-reason-with-llms/) and tool use before producing its answer. By using the same techniques as our SoTA proprietary reasoning models, the models demonstrate exceptional capabilities after post-training.\n\nSimilar to the OpenAI o-series reasoning models in the API, the two open-weight models support three reasoning efforts—low, medium, and high—which trade off latency vs. performance. Developers can easily set the reasoning effort with one sentence in the system message.\n\nWe evaluated gpt-oss-120b and gpt-oss-20b across standard academic benchmarks to measure their capabilities in coding, competition math, health, and agentic tool use when compared to other OpenAI reasoning models including o3, o3‑mini and o4-mini.\n\ngpt-oss-120b outperforms OpenAI o3‑mini and matches or exceeds OpenAI o4-mini on competition coding (Codeforces), general problem solving (MMLU and HLE) and tool calling (TauBench). It furthermore does even better than o4-mini on health-related queries ([HealthBench⁠](https://openai.com/index/healthbench/)) and competition mathematics (AIME 2024 & 2025). gpt-oss-20b matches or exceeds OpenAI o3‑mini on these same evals, despite its small size, even outperforming it on competition mathematics and health.\n\n### Example rollouts\n\nOur [recent research⁠](https://openai.com/index/chain-of-thought-monitoring/) has shown that monitoring a reasoning model’s CoT can be helpful for detecting misbehavior as long as the model was not trained with direct supervision for aligning the CoT. This perspective is [shared⁠(opens in a new window)](https://arxiv.org/html/2507.11473v1) by others in the industry as well. In line with our principles since launching [OpenAI o1‑preview⁠](https://openai.com/index/introducing-openai-o1-preview/), we did not put any direct supervision on the CoT for either gpt-oss model. We believe this is critical to monitor model misbehavior, deception and misuse. Our hope is that releasing an open model with a non-supervised chain of thought gives developers and researchers the opportunity to research and implement their own CoT monitoring systems.\n\nDevelopers should not directly show CoTs to users in their applications. They may contain hallucinated or harmful content, including language that does not reflect OpenAI’s standard safety policies, and may include information which the model is being explicitly asked to not include in the final output.\n\nSafety & worst-case fine-tuning\n-------------------------------\n\nThe gpt-oss models leverage our state-of-art approaches for safety training. During pre-training, we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN). During post-training, we used [deliberative alignment⁠](https://openai.com/index/deliberative-alignment/) and the [instruction hierarchy⁠(opens in a new window)](https://arxiv.org/abs/2404.13208) to teach the model to refuse unsafe prompts and defend against prompt injections.\n\nOnce an open-weight model is released, adversaries may be able to fine-tune the model for malicious purposes. We directly assessed these risks by fine-tuning the model on specialized biology and cybersecurity data, creating a domain-specific non-refusing version for each domain the way an attacker might. We then evaluated the capability level of these models through internal and external testing. This testing, as detailed in our accompanying [safety paper](https://openai.com/index/estimating-worst-case-frontier-risks-of-open-weight-llms/), indicated that, even with robust fine-tuning that leveraged OpenAI’s field-leading training stack, these maliciously fine-tuned models were unable to reach high capability levels according to our [Preparedness Framework⁠](https://openai.com/index/updating-our-preparedness-framework/). This malicious fine-tuning methodology was reviewed by three independent expert groups who made recommendations to improve the training process and evaluations, many of which we adopted. We detail these recommendations in the model card. These processes mark a meaningful advancement for open model safety. These findings informed our decision to release the gpt-oss models. We hope that these models will help accelerate safety training and alignment research across the industry.\n\nTo contribute to a safer open source ecosystem, we are hosting a [Red Teaming Challenge⁠(opens in a new window)](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/) to encourage researchers, developers, and enthusiasts from around the world to help identify novel safety issues. The challenge has a $500,000 prize fund that will be awarded based on review from a panel of expert judges from OpenAI and other leading labs. At the end of the challenge, we will publish a report and open-source an evaluation data set based on validated findings, so that the wider community can immediately benefit. Learn more and participate [here⁠(opens in a new window)](https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/).\n\nAvailability\n------------\n\nThe weights for both gpt-oss-120b and gpt-oss-20b are freely available for download on Hugging Face and come natively quantized in MXFP4. This allows for the gpt-oss-120B model to run within 80GB of memory, while gpt-oss-20b only requires 16GB.\n\nThe models have been post-trained on our [harmony prompt format⁠(opens in a new window)](https://cookbook.openai.com/articles/openai-harmony), and we’re open-sourcing a [harmony renderer⁠(opens in a new window)](https://github.com/openai/harmony) in both Python and Rust to make adoption easier. We’re also releasing reference implementations for running inference with PyTorch and on Apple’s Metal platform, along with a collection of example tools for the model.\n\nWe’ve designed these models to be flexible and easy to run anywhere—locally, on-device, or through third-party inference providers. To support this, we partnered ahead of launch with leading deployment platforms such as Azure, Hugging Face, vLLM, Ollama, llama.cpp, LM Studio, AWS, Fireworks, Together AI, Baseten, Databricks, Vercel, Cloudflare, and OpenRouter to make the models broadly accessible to developers. On the hardware side, we worked with industry leaders including NVIDIA, AMD, Cerebras, and Groq to ensure optimized performance across a range of systems.\n\nAs part of today’s release, Microsoft is also bringing GPU-optimized versions of the gpt-oss-20b model to Windows devices. Powered by ONNX Runtime, these models support local inference and are available through Foundry Local and the AI Toolkit for VS Code, making it easier for Windows developers to build with open models.\n\nFor developers who want fully customizable models they can fine-tune and deploy in their own environments, gpt-oss is a great fit. For those seeking multimodal support, built-in tools, and seamless integration with our platform, models available through our API platform remain the best option. We’re continuing to listen closely to developer feedback and may consider API support for gpt-oss in the future.\n\nWhy open models matter\n----------------------\n\nReleasing gpt-oss-120b and gpt-oss-20b marks a significant step forward for open-weight models. At their size, these models deliver meaningful advancements in both reasoning capabilities and safety. Open models complement our hosted models, giving developers a wider range of tools to accelerate leading edge research, foster innovation and enable safer, more transparent AI development across a wide range of use cases.\n\nThese open models also lower barriers for emerging markets, resource-constrained sectors, and smaller organizations that may lack the budget or flexibility to adopt proprietary models. With powerful, accessible tools in their hands, people around the world can build, innovate, and create new opportunities for themselves and others. Broad access to these capable open-weights models created in the US helps expand democratic AI rails.\n\nA healthy open model ecosystem is one dimension to helping make AI widely accessible and beneficial for everyone. We invite developers and researchers to use these models to experiment, collaborate and push the boundaries of what's possible. We look forward to seeing what you build.\n\n*   [2025](https://openai.com/research/index/?tags=2025)\n\nContributors\n------------\n\nZoran Martinovic, Zhuohan Li, Zhiqing Sun, Zach Johnson, Yu Yang, Yu Bai, Yang Song, Xin Wang, Wenting Zhan, Volodymyr Kyrylov, Vlad Fomenko, Tyler Bertao, Tong Mu, Timur Garipov, Tarun Gogineni, Suvansh Sanjeev, Steve Mostovoy, Song Mei, Shengjia Zhao, Sebastien Bubeck, Scott McKinney, Scott Lessans, Sandhini Agarwal, Sam Toizer, Sam Altman, Saachi Jain, Romain Huet, Rahul K. Arora, Philippe Tillet, Olivia Watkins, Nivedita Brett, Nikhil Vyas, Miles Wang, Michihiro Yasunaga, Michelle Pokrass, Mia Glaese, Max Schwarzer, Mark Chen, Mario Lezcano-Casado, Marat Dukhan, Lukas Gross, Ludovic Peran, Ludovic Peran, Lindsay McCallum, Lin Yang, Lily (Xiaoxuan) Liu, Leher Pathak, Lama Ahmad, Kristian Georgiev, Kristen Ying, Kimmy Richardson, Kevin Whinnery, Kevin Weil, Kevin Lu, Kevin Fives, Kendal Simon, Katia Gil Guzman, Karan Singhal, Karan Singhal, Kai Chen, Josh McGrath, Jordan Liss, Jongsoo Park, John Hallman, Johannes Heidecke, Jiancheng Liu, Ji Lin, Jason Kwon, Jason Ai, James Park Lennon, Jakub Pachocki, Jacob Huh, Jackie Hehir, Irina Kofman, Huida Qiu, Hongyu Ren, Harshit Sikchi, Hannah Wong, Haitang Hu, Haitang Hu, Haiming Bao, Hadi Salman, Guillaume Leclerc, Greg Brockman, Gideon Myles, Giambattista Parascandolo, Gaby Raila, Foivos Tsimpourlas, Filippo Raso, Eugene Brevdo, Eric Wallace, Enoch Cheung, Elizabeth Proehl, Elaine Ya Le, Edwin Arbus, Eddie Zhang, Dominik Kundel, Dmitry Pimenov, David Robinson, Dane Stuckey, Dana Palmie, Dan Cook, Cyril Zhang, Chris Lu, Chris Koch, Che Chang, Cedric Whitney, Casey Dvorak, Carolina Paz, Brian Zhang, Bowen Baker, Bob Rotsted, Boaz Barak, Ashley Pantuliano, Andy Applebaum, Amy Wendling, Ally Bennett, Alexander Neitz, Alex Paino, Alex Nichol, Alec Helyar, Aidan McLaughlin, Aidan Clark, Adam Goucher\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/open-models/",
      "full_text": "Title: Open models by OpenAI\n\nURL Source: https://openai.com/open-models/\n\nMarkdown Content:\nOpen models by OpenAI | OpenAI\n\n===============\n\n[Skip to main content](https://openai.com/open-models/#main)\n\nLog in\n\n[](https://openai.com/)\n\nSwitch to\n\n*   [ChatGPT(opens in a new window)](https://chatgpt.com/?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Sora(opens in a new window)](https://sora.com/)\n*   [API Platform(opens in a new window)](https://platform.openai.com/)\n\n[Home](https://openai.com/)\n*   [API Platform](https://openai.com/api/) \n*   [API Pricing](https://openai.com/api/pricing/) \n*   [Codex](https://openai.com/codex/) \n*   [Open Models](https://openai.com/open-models/) \n*   [Community(opens in a new window)](https://community.openai.com/) \n\n*   Research\n\nBack to main menu  \n\n    *   [Research Index](https://openai.com/research/index/)\n    *   [Research Overview](https://openai.com/research/)\n    *   [Research Residency](https://openai.com/residency/)\n    *   Latest Advancements\n    *   [GPT-5](https://openai.com/index/introducing-gpt-5/)\n    *   [OpenAI o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)\n    *   [GPT-4.5](https://openai.com/index/introducing-gpt-4-5/)\n    *   [OpenAI o1](https://openai.com/o1/)\n    *   [GPT-4o](https://openai.com/index/gpt-4o-system-card/)\n    *   [Sora](https://openai.com/index/sora-system-card/)\n\n*   Safety\n\nBack to main menu  \n\n    *   [Safety Approach](https://openai.com/safety/)\n    *   [Security & Privacy](https://openai.com/security-and-privacy/)\n\n*   [For Business](https://openai.com/business/)\n\nBack to main menu  \n\n    *   [Business Overview](https://openai.com/business/)\n    *   [Startups](https://openai.com/startups/)\n    *   [Solutions](https://openai.com/solutions/)\n    *   [ChatGPT Pricing](https://openai.com/business/chatgpt-pricing/)\n    *   [API Pricing](https://openai.com/api/pricing/)\n    *   [Contact Sales](https://openai.com/contact-sales/)\n\n*   For Developers\n\nBack to main menu  \n\n    *   [API Platform](https://openai.com/api/)\n    *   [API Pricing](https://openai.com/api/pricing/)\n    *   [Codex](https://openai.com/codex/)\n    *   [Open Models](https://openai.com/open-models/)\n    *   [Community(opens in a new window)](https://community.openai.com/)\n\n*   [ChatGPT](https://chatgpt.com/overview?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n\nBack to main menu  \n\n    *   [Explore ChatGPT](https://chatgpt.com/overview?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n    *   [Business](https://chatgpt.com/for-business/team?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n    *   [Enterprise](https://chatgpt.com/for-business/enterprise?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n    *   [Education](https://chatgpt.com/for-business/education?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n    *   [Pricing](https://chatgpt.com/pricing?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n    *   [Download](https://chatgpt.com/download?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n\n*   Sora\n\nBack to main menu  \n\n    *   [Sora Overview](https://openai.com/sora/)\n    *   [Features](https://openai.com/sora/#features)\n    *   [Pricing](https://openai.com/sora/#pricing)\n    *   [Help Center(opens in a new window)](https://help.openai.com/en/articles/9957612-generating-videos-on-sora)\n    *   [Sora Log in(opens in a new window)](https://sora.com/)\n\n*   [Stories](https://openai.com/stories/)\n*   Company\n\nBack to main menu  \n\n    *   [About Us](https://openai.com/about/)\n    *   [Our Charter](https://openai.com/charter/)\n    *   [Careers](https://openai.com/careers/)\n    *   [Brand Guidelines](https://openai.com/brand/)\n\n*   [News](https://openai.com/news/)\n\nLog in\n\nOpen models by OpenAI | OpenAI\n\nOpen models by OpenAI\n=====================\n\nAdvanced open-weight reasoning models to customize for any use case and run anywhere.\n\n[Download on Hugging Face(opens in a new window)](https://huggingface.co/openai/gpt-oss-120b)[View on GitHub(opens in a new window)](https://github.com/openai/gpt-oss)[Try our models(opens in a new window)](https://gpt-oss.com/)\n\n[![Image 2](https://cdn.openai.com/ctf-cdn/open-models-card-bg.png?w=3840&q=50&fm=webp) -0.59 0.30 0.18 0.19 -0.64 ### gpt-oss-120b A large open model designed to run in data centers and on high-end desktops and laptops. Start building(opens in a new window) (opens in a new window)](https://huggingface.co/openai/gpt-oss-120b)[![Image 3](https://cdn.openai.com/ctf-cdn/open-models-card-bg.png?w=3840&q=50&fm=webp) -0.59 0.30 0.18 0.19 -0.64 ### gpt-oss-20b A medium-sized open model that can run on most desktops and laptops. Start building(opens in a new window) (opens in a new window)](https://huggingface.co/openai/gpt-oss-20b)\n\n![Image 4: Icon representing unlocking](https://images.ctfassets.net/kftzwdyauwt9/1TKGZmhwnWDTOyOhxWUOvk/4f7fdaf7831728aeb9c7fd304acb2f08/unlock.svg?w=3840&q=90)\n\n#### Permissive license\n\nThese models are supported by the Apache 2.0 license. Build freely without worrying about copyleft restrictions or patent risk—whether you're experimenting, customizing, or deploying commercially.\n\n![Image 5: Agentic code icon](https://images.ctfassets.net/kftzwdyauwt9/5OEoJzrBuSmHTiDu71MN8G/b2b66bd76644a1d359339524b226c48c/marker-code.svg?w=3840&q=90)\n\n#### Designed for agentic tasks\n\nLeverage powerful instruction following and tool use within the chain-of-thought, including web search and Python code execution.\n\n![Image 6: Customization icon](https://images.ctfassets.net/kftzwdyauwt9/49qTMsFaInUYQoz4ErRQmJ/456cc3f1188cf6419d4d4e2c69f21a7f/settings-slider.svg?w=3840&q=90)\n\n#### Deeply customizable\n\nAdjust the reasoning effort to low, medium, or high. Plus, customize the models to adapt to your use case with full-parameter fine-tuning.\n\n![Image 7: Tasks icon](https://images.ctfassets.net/kftzwdyauwt9/7KzuoeyT7FOOxiB79UDnmn/1c635e90af3c4cb9f802cccbcbdaa7bb/tasks.svg?w=3840&q=90)\n\n#### Full chain-of-thought\n\nAccess the full chain-of-thought for easier debugging and higher trust in model outputs.\n\n### Interactive demo\n\nWe built a simple playground where developers can try both models in the browser.\n\n[Try it now(opens in a new window)](https://gpt-oss.com/)\n\n![Image 8: gpt-oss playground](https://images.ctfassets.net/kftzwdyauwt9/7bo2qwyIvkSO95HdEu6rs0/c5143e5c4c5e4a26c91617589b776a04/gpt-oss-screenshot.png?w=3840&q=90&fm=webp)\n\nModel performance\n-----------------\n\n*   [Read the research blog](https://openai.com/index/introducing-gpt-oss/)\n\n**gpt-oss-120b****gpt-oss-20b****OpenAI o3****OpenAI o4-mini**\nReasoning & knowledge\nMMLU**90.0****85.3**93.4 93.0\nGPQA Diamond**80.1****71.5**83.3 81.4\nHumanity’s Last Exam**19.0****17.3**24.9 17.7\nCompetition math\nAIME 2024**96.6****96.0**95.2 98.7\nAIME 2025**97.9****98.7**98.4 99.5\n\nAdvancing safety standards for open models\n------------------------------------------\n\n*   [Read our model system card](https://openai.com/index/gpt-oss-model-card/)\n\n![Image 9: Icon representing security](https://images.ctfassets.net/kftzwdyauwt9/3XJ4lZho5mBYyUjEYfVNOK/d05d2727dcde6ec570e8b0a298a6ef9e/shield-lock.svg?w=3840&q=90)\n\n#### Safety is foundational to our open models\n\nEach model has completed thorough safety training and evaluation to help developers keep users safe.\n\n![Image 10: Icon representing scientific testing](https://images.ctfassets.net/kftzwdyauwt9/1aA2FTnaCAcL47TDkJT4Zj/288035e728aa47ef16798794e30212e6/flask.svg?w=3840&q=90)\n\n#### Comprehensive safety testing\n\nWe rigorously tested a maliciously fine-tuned version of gpt-oss-120b under our Preparedness Framework, and found that it doesn’t reach high capability levels. These training and testing methods were reviewed and informed by external safety experts, and mark a meaningful advancement in open model safety standards.\n\nOur partners\n------------\n\nWe’re working with leading deployment and hardware companies to offer these models to the open-source community.\n\nResources\n---------\n\n[### How to use gpt-oss with Transformers Learn more](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n[### How to run gpt-oss locally with Ollama Learn more](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n[### How to use gpt-oss with vLLM Learn more](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n[### Learn about the OpenAI harmony response format Learn more](https://cookbook.openai.com/articles/openai-harmony)\n\n[### How to run gpt-oss locally with LM Studio Learn more](https://cookbook.openai.com/articles/gpt-oss/run-locally-lmstudio)\n\n[### How to run gpt-oss-20b on Google Colab Learn more](https://cookbook.openai.com/articles/gpt-oss/run-colab)\n\n[### Verifying gpt-oss implementations Learn more](https://cookbook.openai.com/articles/gpt-oss/verifying-implementations)\n\n[### Find more guides about gpt-oss Learn more](https://cookbook.openai.com/topic/gpt-oss)\n\n![Image 11: \"\"](https://images.ctfassets.net/kftzwdyauwt9/6DDrb5MvXlt6FGWS3ONz8s/7b600a9b59476abb169715fb64707550/Media.png?w=3840&q=90&fm=webp)\n\n### We want to hear from you\n\nShare feedback and feature requests that may guide future open models. Please note: you won’t receive a direct response. For support and discussion, visit our Hugging Face Community.\n\nEmail address*\n\n \n\nPlease share your feedback, including your specific use case.*\n\n \n\nSubmit feedback\n\n*\n\nEmail Address: \n\n*\n\nOpen Model Additional Info: \n\nSubmit\n\nStart building\n--------------\n\n[Download on Hugging Face(opens in a new window)](https://huggingface.co/openai/gpt-oss-120b)[View on GitHub(opens in a new window)](https://github.com/openai/gpt-oss)\n\nOur Research\n*   [Research Index](https://openai.com/research/index/)\n*   [Research Overview](https://openai.com/research/)\n*   [Research Residency](https://openai.com/residency/)\n\nLatest Advancements\n*   [GPT-5](https://openai.com/gpt-5/)\n*   [OpenAI o3](https://openai.com/index/introducing-o3-and-o4-mini/)\n*   [OpenAI o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)\n*   [GPT-4o](https://openai.com/index/gpt-4o-system-card/)\n*   [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)\n*   [Sora](https://openai.com/index/sora-system-card/)\n\nSafety\n*   [Safety Approach](https://openai.com/safety/)\n*   [Security & Privacy](https://openai.com/security-and-privacy/)\n*   [Trust & Transparency](https://openai.com/trust-and-transparency/)\n\nChatGPT\n*   [Explore ChatGPT(opens in a new window)](https://chatgpt.com/overview?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Business](https://chatgpt.com/business/business-plan?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Enterprise](https://chatgpt.com/business/enterprise?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Education](https://chatgpt.com/business/education?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Pricing(opens in a new window)](https://chatgpt.com/pricing?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n*   [Download(opens in a new window)](https://chatgpt.com/download?openaicom-did=f148d5ff-e3e9-466a-b09e-f54ba821c706&openaicom_referred=true)\n\nSora\n*   [Sora Overview](https://openai.com/sora/)\n*   [Features](https://openai.com/sora/#features)\n*   [Pricing](https://openai.com/sora/#pricing)\n*   [Sora log in(opens in a new window)](https://sora.com/)\n\nAPI Platform\n*   [Platform Overview](https://openai.com/api/)\n*   [Pricing](https://openai.com/api/pricing/)\n*   [API log in(opens in a new window)](https://platform.openai.com/login)\n*   [Documentation(opens in a new window)](https://platform.openai.com/docs/overview)\n*   [Developer Forum(opens in a new window)](https://community.openai.com/)\n\nFor Business\n*   [Business Overview](https://openai.com/business/)\n*   [Solutions](https://openai.com/solutions/)\n*   [Contact Sales](https://openai.com/contact-sales/)\n\nCompany\n*   [About Us](https://openai.com/about/)\n*   [Our Charter](https://openai.com/charter/)\n*   [Careers](https://openai.com/careers/)\n*   [Brand](https://openai.com/brand/)\n\nSupport\n*   [Help Center(opens in a new window)](https://help.openai.com/)\n\nMore\n*   [News](https://openai.com/news/)\n*   [Stories](https://openai.com/stories/)\n*   [Livestreams](https://openai.com/live/)\n*   [Podcast](https://openai.com/podcast/)\n\nTerms & Policies\n*   [Terms of Use](https://openai.com/policies/terms-of-use/)\n*   [Privacy Policy](https://openai.com/policies/privacy-policy/)\n*   [Other Policies](https://openai.com/policies/)\n\n[(opens in a new window)](https://x.com/OpenAI)[(opens in a new window)](https://www.youtube.com/OpenAI)[(opens in a new window)](https://www.linkedin.com/company/openai)[(opens in a new window)](https://github.com/openai)[(opens in a new window)](https://www.instagram.com/openai/)[(opens in a new window)](https://www.tiktok.com/@openai)[(opens in a new window)](https://discord.gg/openai)\n\nOpenAI © 2015–2025 Manage Cookies\n\nEnglish United States\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://cookbook.openai.com/articles/gpt-oss/run-transformers",
      "full_text": " How to run gpt-oss with Transformers Topics About API docs Source Toggle theme Toggle theme Aug 5, 2025 How to run gpt-oss with Transformers Dominik Kundel (OpenAI) Open in GitHub Pick your model Quick setup Create an Open AI Responses / Chat Completions endpoint Quick inference with pipeline Advanced inference with `.generate()` Chat template and tool calling Multi-GPU &amp; distributed inference The Transformers library by Hugging Face provides a flexible way to load and run large language models locally or on a server. This guide will walk you through running OpenAI gpt-oss-20b or OpenAI gpt-oss-120b using Transformers, either with a high-level pipeline or via low-level generate calls with raw token IDs. We&#x27;ll cover the use of OpenAI gpt-oss-20b or OpenAI gpt-oss-120b with the high-level pipeline abstraction, low-level `generate` calls, and serving models locally with `transformers serve`, with in a way compatible with the Responses API. In this guide we’ll run through various optimised ways to run the gpt-oss models via Transformers. Bonus: You can also fine-tune models via transformers, check out our fine-tuning guide here . Pick your model Both gpt-oss models are available on Hugging Face: openai/gpt-oss-20b ~16GB VRAM requirement when using MXFP4 Great for single high-end consumer GPUs openai/gpt-oss-120b Requires ≥60GB VRAM or multi-GPU setup Ideal for H100-class hardware Both are MXFP4 quantized by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards. If you use bfloat16 instead of MXFP4, memory consumption will be larger (~48 GB for the 20b parameter model). Quick setup Install dependencies It’s recommended to create a fresh Python environment. Install transformers, accelerate, as well as the Triton kernels for MXFP4 compatibility: pip install -U transformers accelerate torch triton== 3.4 kernels (Optional) Enable multi-GPU If you’re running large models, use Accelerate or torchrun to handle device mapping automatically. Create an Open AI Responses / Chat Completions endpoint To launch a server, simply use the transformers serve CLI command: transformers serve The simplest way to interact with the server is through the transformers chat CLI transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b or by sending an HTTP request with cURL, e.g. curl -X POST http://localhost:8000/v1/responses -H &quot;Content-Type: application/json&quot; -d &#x27;{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;hello&quot;}], &quot;temperature&quot;: 0.9, &quot;max_tokens&quot;: 1000, &quot;stream&quot;: true, &quot;model&quot;: &quot;openai/gpt-oss-20b&quot;}&#x27; Additional use cases, like integrating transformers serve with Cursor and other tools, are detailed in the documentation . Quick inference with pipeline The easiest way to run the gpt-oss models is with the Transformers high-level pipeline API: from transformers import pipeline generator = pipeline( &quot;text-generation&quot; , model = &quot;openai/gpt-oss-20b&quot; , torch_dtype = &quot;auto&quot; , device_map = &quot;auto&quot; # Automatically place on available GPUs ) messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Explain what MXFP4 quantization is.&quot; }, ] result = generator( messages, max_new_tokens = 200 , temperature = 1.0 , ) print (result[ 0 ][ &quot;generated_text&quot; ]) Advanced inference with .generate() If you want more control, you can load the model and tokenizer manually and invoke the .generate() method: from transformers import AutoModelForCausalLM, AutoTokenizer model_name = &quot;openai/gpt-oss-20b&quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype = &quot;auto&quot; , device_map = &quot;auto&quot; ) messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Explain what MXFP4 quantization is.&quot; }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt = True , return_tensors = &quot;pt&quot; , return_dict = True , ).to(model.device) outputs = model.generate( ** inputs, max_new_tokens = 200 , temperature = 0.7 ) print (tokenizer.decode(outputs[ 0 ])) Chat template and tool calling OpenAI gpt-oss models use the harmony response format for structuring messages, including reasoning and tool calls. To construct prompts you can use the built-in chat template of Transformers. Alternatively, you can install and use the openai-harmony library for more control. To use the chat template: from transformers import AutoModelForCausalLM, AutoTokenizer model_name = &quot;openai/gpt-oss-20b&quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, device_map = &quot;auto&quot; , torch_dtype = &quot;auto&quot; , ) messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;Always respond in riddles&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;What is the weather like in Madrid?&quot; }, ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt = True , return_tensors = &quot;pt&quot; , return_dict = True , ).to(model.device) generated = model.generate( ** inputs, max_new_tokens = 100 ) print (tokenizer.decode(generated[ 0 ][inputs[ &quot;input_ids&quot; ].shape[ - 1 ] :])) To integrate the openai-harmony library to prepare prompts and parse responses, first install it like this: pip install openai-harmony Here’s an example of how to use the library to build your prompts and encode them to tokens: import json from openai_harmony import ( HarmonyEncodingName, load_harmony_encoding, Conversation, Message, Role, SystemContent, DeveloperContent ) from transformers import AutoModelForCausalLM, AutoTokenizer encoding = load_harmony_encoding(HarmonyEncodingName. HARMONY_GPT_OSS ) # Build conversation convo = Conversation.from_messages([ Message.from_role_and_content(Role. SYSTEM , SystemContent.new()), Message.from_role_and_content( Role. DEVELOPER , DeveloperContent.new().with_instructions( &quot;Always respond in riddles&quot; ) ), Message.from_role_and_content(Role. USER , &quot;What is the weather like in SF?&quot; ) ]) # Render prompt prefill_ids = encoding.render_conversation_for_completion(convo, Role. ASSISTANT ) stop_token_ids = encoding.stop_tokens_for_assistant_actions() # Load model model_name = &quot;openai/gpt-oss-20b&quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = &quot;auto&quot; , device_map = &quot;auto&quot; ) # Generate outputs = model.generate( input_ids = [prefill_ids], max_new_tokens = 128 , eos_token_id = stop_token_ids ) # Parse completion tokens completion_ids = outputs[ 0 ][ len (prefill_ids):] entries = encoding.parse_messages_from_completion_tokens(completion_ids, Role. ASSISTANT ) for message in entries: print (json.dumps(message.to_dict(), indent = 2 )) Note that the Developer role in Harmony maps to the system prompt in the chat template. Multi-GPU &amp; distributed inference The large gpt-oss-120b fits on a single H100 GPU when using MXFP4. If you want to run it on multiple GPUs, you can: Use tp_plan=&quot;auto&quot; for automatic placement and tensor parallelism Launch with accelerate launch or torchrun for distributed setups Leverage Expert Parallelism Use specialised Flash attention kernels for faster inference from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.distributed import DistributedConfig import torch model_path = &quot;openai/gpt-oss-120b&quot; tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side = &quot;left&quot; ) device_map = { # Enable Expert Parallelism &quot;distributed_config&quot; : DistributedConfig( enable_expert_parallel = 1 ), # Enable Tensor Parallelism &quot;tp_plan&quot; : &quot;auto&quot; , } model = AutoModelForCausalLM.from_pretrained( model_path, torch_dtype = &quot;auto&quot; , attn_implementation = &quot;kernels-community/vllm-flash-attn3&quot; , ** device_map, ) messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Explain how expert parallelism works in large language models.&quot; } ] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt = True , return_tensors = &quot;pt&quot; , return_dict = True , ).to(model.device) outputs = model.generate( ** inputs, max_new_tokens = 1000 ) # Decode and print response = tokenizer.decode(outputs[ 0 ]) print ( &quot;Model response:&quot; , response.split( &quot;&lt;|channel|&gt;final&lt;|message|&gt;&quot; )[ - 1 ].strip()) You can then run this on a node with four GPUs via torchrun --nproc_per_node=4 generate.py ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.astral.sh/uv/",
      "full_text": " uv Skip to content uv Introduction Initializing search uv uv uv Introduction Introduction Table of contents Highlights Installation Projects Scripts Tools Python versions The pip interface Learn more Getting started Getting started Installation First steps Features Getting help Guides Guides Installing Python Running scripts Using tools Working on projects Publishing packages Migration Migration From pip to a uv project Integrations Integrations Docker Jupyter marimo GitHub Actions GitLab CI/CD Pre-commit PyTorch FastAPI Alternative indexes Dependency bots AWS Lambda Concepts Concepts Projects Projects Structure and files Creating projects Managing dependencies Running commands Locking and syncing Configuring projects Building distributions Using workspaces Tools Python versions Configuration files Package indexes Resolution Build backend Authentication Authentication The auth CLI HTTP credentials Git credentials TLS certificates Third-party services Caching Preview features The pip interface The pip interface Using environments Managing packages Inspecting environments Declaring dependencies Locking environments Compatibility with pip Reference Reference Commands Settings Environment variables Installer options Troubleshooting Troubleshooting Build failures Reproducible examples Internals Internals Resolver Benchmarks Policies Policies Versioning Platform support License Table of contents Highlights Installation Projects Scripts Tools Python versions The pip interface Learn more uv An extremely fast Python package and project manager, written in Rust. Installing Trio 's dependencies with a warm cache. Highlights 🚀 A single tool to replace pip , pip-tools , pipx , poetry , pyenv , twine , virtualenv , and more. ⚡️ 10-100x faster than pip . 🗂️ Provides comprehensive project management , with a universal lockfile . ❇️ Runs scripts , with support for inline dependency metadata . 🐍 Installs and manages Python versions. 🛠️ Runs and installs tools published as Python packages. 🔩 Includes a pip-compatible interface for a performance boost with a familiar CLI. 🏢 Supports Cargo-style workspaces for scalable projects. 💾 Disk-space efficient, with a global cache for dependency deduplication. ⏬ Installable without Rust or Python via curl or pip . 🖥️ Supports macOS, Linux, and Windows. uv is backed by Astral , the creators of Ruff . Installation Install uv with our official standalone installer: macOS and Linux Windows $ curl -LsSf https://astral.sh/uv/install.sh | sh PS&gt; powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot; Then, check out the first steps or read on for a brief overview. Tip uv may also be installed with pip, Homebrew, and more. See all of the methods on the installation page . Projects uv manages project dependencies and environments, with support for lockfiles, workspaces, and more, similar to rye or poetry : $ uv init example Initialized project `example` at `/home/user/example` $ cd example $ uv add ruff Creating virtual environment at: .venv Resolved 2 packages in 170ms Built example @ file:///home/user/example Prepared 2 packages in 627ms Installed 2 packages in 1ms + example==0.1.0 (from file:///home/user/example) + ruff==0.5.4 $ uv run ruff check All checks passed! $ uv lock Resolved 2 packages in 0.33ms $ uv sync Resolved 2 packages in 0.70ms Audited 1 package in 0.02ms See the project guide to get started. uv also supports building and publishing projects, even if they're not managed with uv. See the packaging guide to learn more. Scripts uv manages dependencies and environments for single-file scripts. Create a new script and add inline metadata declaring its dependencies: $ echo &#39;import requests; print(requests.get(&quot;https://astral.sh&quot;))&#39; &gt; example.py $ uv add --script example.py requests Updated `example.py` Then, run the script in an isolated virtual environment: $ uv run example.py Reading inline script metadata from: example.py Installed 5 packages in 12ms &lt;Response [200]&gt; See the scripts guide to get started. Tools uv executes and installs command-line tools provided by Python packages, similar to pipx . Run a tool in an ephemeral environment using uvx (an alias for uv tool run ): $ uvx pycowsay &#39;hello world!&#39; Resolved 1 package in 167ms Installed 1 package in 9ms + pycowsay==0.0.0.2 &quot;&quot;&quot; ------------ &lt; hello world! &gt; ------------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Install a tool with uv tool install : $ uv tool install ruff Resolved 1 package in 6ms Installed 1 package in 2ms + ruff==0.5.4 Installed 1 executable: ruff $ ruff --version ruff 0.5.4 See the tools guide to get started. Python versions uv installs Python and allows quickly switching between versions. Install multiple Python versions: $ uv python install 3 .10 3 .11 3 .12 Searching for Python versions matching: Python 3.10 Searching for Python versions matching: Python 3.11 Searching for Python versions matching: Python 3.12 Installed 3 versions in 3.42s + cpython-3.10.14-macos-aarch64-none + cpython-3.11.9-macos-aarch64-none + cpython-3.12.4-macos-aarch64-none Download Python versions as needed: $ uv venv --python 3 .12.0 Using CPython 3.12.0 Creating virtual environment at: .venv Activate with: source .venv/bin/activate $ uv run --python [email&#160;protected] -- python Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30) [PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt;&gt; Use a specific Python version in the current directory: $ uv python pin 3 .11 Pinned `.python-version` to `3.11` See the installing Python guide to get started. The pip interface uv provides a drop-in replacement for common pip , pip-tools , and virtualenv commands. uv extends their interfaces with advanced features, such as dependency version overrides, platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and more. Migrate to uv without changing your existing workflows — and experience a 10-100x speedup — with the uv pip interface. Compile requirements into a platform-independent requirements file: $ uv pip compile docs/requirements.in \\ --universal \\ --output-file docs/requirements.txt Resolved 43 packages in 12ms Create a virtual environment: $ uv venv Using CPython 3.12.3 Creating virtual environment at: .venv Activate with: source .venv/bin/activate Install the locked requirements: $ uv pip sync docs/requirements.txt Resolved 43 packages in 11ms Installed 43 packages in 208ms + babel==2.15.0 + black==24.4.2 + certifi==2024.7.4 ... See the pip interface documentation to get started. Learn more See the first steps or jump straight to the guides to start using uv. May 18, 2025 Back to top Next Index Made with Material for MkDocs Insiders ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://cookbook.openai.com/articles/gpt-oss/run-vllm",
      "full_text": " How to run gpt-oss with vLLM Topics About API docs Source Toggle theme Toggle theme Aug 5, 2025 How to run gpt-oss with vLLM Dominik Kundel (OpenAI) Open in GitHub Pick your model Quick Setup Use the API Using tools (function calling) Agents SDK Integration Using vLLM for direct sampling vLLM is an open-source, high-throughput inference engine designed to efficiently serve large language models (LLMs) by optimizing memory usage and processing speed. This guide will walk you through how to use vLLM to set up gpt-oss-20b or gpt-oss-120b on a server to serve gpt-oss as an API for your applications, and even connect it to the Agents SDK. Note that this guide is meant for server applications with dedicated GPUs like NVIDIA’s H100s. For local inference on consumer GPUs, check out our Ollama or LM Studio guides. Pick your model vLLM supports both model sizes of gpt-oss: openai/gpt-oss-20b The smaller model Only requires about 16GB of VRAM openai/gpt-oss-120b Our larger full-sized model Best with ≥60GB VRAM Can fit on a single H100 or multi-GPU setups Both models are MXFP4 quantized out of the box. Quick Setup Install vLLM vLLM recommends using uv to manage your Python environment. This will help with picking the right implementation based on your environment. Learn more in their quickstart . To create a new virtual environment and install vLLM run: uv venv --python 3.12 --seed source .venv/bin/activate uv pip install --pre vllm== 0.10 .1+gptoss \\ --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\ --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\ --index-strategy unsafe-best-match Start up a server and download the model vLLM provides a serve command that will automatically download the model from HuggingFace and spin up an OpenAI-compatible server on localhost:8000 . Run the following command depending on your desired model size in a terminal session on your server. # For 20B vllm serve openai/gpt-oss-20b # For 120B vllm serve openai/gpt-oss-120b Use the API vLLM exposes a Chat Completions-compatible API and a Responses-compatible API so you can use the OpenAI SDK without changing much. Here’s a Python example: from openai import OpenAI client = OpenAI( base_url = &quot;http://localhost:8000/v1&quot; , api_key = &quot;EMPTY&quot; ) result = client.chat.completions.create( model = &quot;openai/gpt-oss-20b&quot; , messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a helpful assistant.&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Explain what MXFP4 quantization is.&quot; } ] ) print (result.choices[ 0 ].message.content) response = client.responses.create( model = &quot;openai/gpt-oss-120b&quot; , instructions = &quot;You are a helfpul assistant.&quot; , input = &quot;Explain what MXFP4 quantization is.&quot; ) print (response.output_text) If you’ve used the OpenAI SDK before, this will feel instantly familiar and your existing code should work by changing the base URL. Using tools (function calling) vLLM supports function calling and giving the model browsing capabilities. Function calling works through both the Responses and Chat Completions APIs. Example of invoking a function via Chat Completions: tools = [ { &quot;type&quot; : &quot;function&quot; , &quot;function&quot; : { &quot;name&quot; : &quot;get_weather&quot; , &quot;description&quot; : &quot;Get current weather in a given city&quot; , &quot;parameters&quot; : { &quot;type&quot; : &quot;object&quot; , &quot;properties&quot; : { &quot;city&quot; : { &quot;type&quot; : &quot;string&quot; }}, &quot;required&quot; : [ &quot;city&quot; ] }, }, } ] response = client.chat.completions.create( model = &quot;openai/gpt-oss-120b&quot; , messages = [{ &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;What&#x27;s the weather in Berlin right now?&quot; }], tools = tools ) print (response.choices[ 0 ].message) Since the models can perform tool calling as part of the chain-of-thought (CoT) it’s important for you to return the reasoning returned by the API back into a subsequent call to a tool call where you provide the answer until the model reaches a final answer. Agents SDK Integration Want to use gpt-oss with OpenAI’s Agents SDK ? Both Agents SDK enable you to override the OpenAI base client to point to vLLM for your self-hosted models. Alternatively, for the Python SDK you can also use the LiteLLM integration to proxy to vLLM. Here’s a Python Agents SDK example: uv pip install openai-agents import asyncio from openai import AsyncOpenAI from agents import Agent, Runner, function_tool, OpenAIResponsesModel, set_tracing_disabled set_tracing_disabled( True ) @function_tool def get_weather (city: str ): print ( f &quot;[debug] getting weather for { city } &quot; ) return f &quot;The weather in { city } is sunny.&quot; async def main (model: str , api_key: str ): agent = Agent( name = &quot;Assistant&quot; , instructions = &quot;You only respond in haikus.&quot; , model = OpenAIResponsesModel( model = &quot;openai/gpt-oss-120b&quot; , openai_client = AsyncOpenAI( base_url = &quot;http://localhost:8000/v1&quot; , api_key = &quot;EMPTY&quot; , ), ) tools = [get_weather], ) result = await Runner.run(agent, &quot;What&#x27;s the weather in Tokyo?&quot; ) print (result.final_output) if __name__ == &quot;__main__&quot; : asyncio.run(main()) Using vLLM for direct sampling Aside from running vLLM using vllm serve as an API server, you can use the vLLM Python library to control inference directly. If you are using vLLM for sampling directly it’s important to ensure that your input prompts follow the harmony response format as the model will not function correctly otherwise. You can use the openai-harmony SDK for this. uv pip install openai-harmony Afterwards you can use harmony to encode and parse the tokens generated by vLLM’s generate function. import json from openai_harmony import ( HarmonyEncodingName, load_harmony_encoding, Conversation, Message, Role, SystemContent, DeveloperContent, ) from vllm import LLM , SamplingParams # --- 1) Render the prefill with Harmony --- encoding = load_harmony_encoding(HarmonyEncodingName. HARMONY_GPT_OSS ) convo = Conversation.from_messages( [ Message.from_role_and_content(Role. SYSTEM , SystemContent.new()), Message.from_role_and_content( Role. DEVELOPER , DeveloperContent.new().with_instructions( &quot;Always respond in riddles&quot; ), ), Message.from_role_and_content(Role. USER , &quot;What is the weather like in SF?&quot; ), ] ) prefill_ids = encoding.render_conversation_for_completion(convo, Role. ASSISTANT ) # Harmony stop tokens (pass to sampler so they won&#x27;t be included in output) stop_token_ids = encoding.stop_tokens_for_assistant_actions() # --- 2) Run vLLM with prefill --- llm = LLM( model = &quot;openai/gpt-oss-120b&quot; , trust_remote_code = True , ) sampling = SamplingParams( max_tokens = 128 , temperature = 1 , stop_token_ids = stop_token_ids, ) outputs = llm.generate( prompt_token_ids = [prefill_ids], # batch of size 1 sampling_params = sampling, ) # vLLM gives you both text and token IDs gen = outputs[ 0 ].outputs[ 0 ] text = gen.text output_tokens = gen.token_ids # &lt;-- these are the completion token IDs (no prefill) # --- 3) Parse the completion token IDs back into structured Harmony messages --- entries = encoding.parse_messages_from_completion_tokens(output_tokens, Role. ASSISTANT ) # &#x27;entries&#x27; is a sequence of structured conversation entries (assistant messages, tool calls, etc.). for message in entries: print ( f &quot; { json.dumps(message.to_dict()) } &quot; ) ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama",
      "full_text": " How to run gpt-oss locally with Ollama Topics About API docs Source Toggle theme Toggle theme Aug 5, 2025 How to run gpt-oss locally with Ollama Dominik Kundel (OpenAI) Open in GitHub Pick your model Quick setup Chat with gpt-oss Use the API Using tools (function calling) Responses API workarounds Agents SDK integration Want to get OpenAI gpt-oss running on your own hardware? This guide will walk you through how to use Ollama to set up gpt-oss-20b or gpt-oss-120b locally, to chat with it offline, use it through an API, and even connect it to the Agents SDK. Note that this guide is meant for consumer hardware, like running a model on a PC or Mac. For server applications with dedicated GPUs like NVIDIA’s H100s, check out our vLLM guide . Pick your model Ollama supports both model sizes of gpt-oss: gpt-oss-20b The smaller model Best with ≥16GB VRAM or unified memory Perfect for higher-end consumer GPUs or Apple Silicon Macs gpt-oss-120b Our larger full-sized model Best with ≥60GB VRAM or unified memory Ideal for multi-GPU or beefy workstation setup A couple of notes: These models ship MXFP4 quantized out the box and there is currently no other quantization You can offload to CPU if you’re short on VRAM, but expect it to run slower. Quick setup Install Ollama → Get it here Pull the model you want: # For 20B ollama pull gpt-oss:20b # For 120B ollama pull gpt-oss:120b Chat with gpt-oss Ready to talk to the model? You can fire up a chat in the app or the terminal: ollama run gpt-oss:20b Ollama applies a chat template out of the box that mimics the OpenAI harmony format . Type your message and start the conversation. Use the API Ollama exposes a Chat Completions-compatible API , so you can use the OpenAI SDK without changing much. Here’s a Python example: from openai import OpenAI client = OpenAI( base_url = &quot;http://localhost:11434/v1&quot; , # Local Ollama API api_key = &quot;ollama&quot; # Dummy key ) response = client.chat.completions.create( model = &quot;gpt-oss:20b&quot; , messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a helpful assistant.&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Explain what MXFP4 quantization is.&quot; } ] ) print (response.choices[ 0 ].message.content) If you’ve used the OpenAI SDK before, this will feel instantly familiar. Alternatively, you can use the Ollama SDKs in Python or JavaScript directly. Using tools (function calling) Ollama can: Call functions Use a built-in browser tool (in the app) Example of invoking a function via Chat Completions: tools = [ { &quot;type&quot; : &quot;function&quot; , &quot;function&quot; : { &quot;name&quot; : &quot;get_weather&quot; , &quot;description&quot; : &quot;Get current weather in a given city&quot; , &quot;parameters&quot; : { &quot;type&quot; : &quot;object&quot; , &quot;properties&quot; : { &quot;city&quot; : { &quot;type&quot; : &quot;string&quot; }}, &quot;required&quot; : [ &quot;city&quot; ] }, }, } ] response = client.chat.completions.create( model = &quot;gpt-oss:20b&quot; , messages = [{ &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;What&#x27;s the weather in Berlin right now?&quot; }], tools = tools ) print (response.choices[ 0 ].message) Since the models can perform tool calling as part of the chain-of-thought (CoT) it’s important for you to return the reasoning returned by the API back into a subsequent call to a tool call where you provide the answer until the model reaches a final answer. Responses API workarounds Ollama doesn’t (yet) support the Responses API natively. If you do want to use the Responses API you can use Hugging Face’s Responses.js proxy to convert Chat Completions to Responses API. For basic use cases you can also run our example Python server with Ollama as the backend. This server is a basic example server and does not have the pip install gpt-oss python -m gpt_oss.responses_api.serve \\ --inference_backend=ollama \\ --checkpoint gpt-oss:20b Agents SDK integration Want to use gpt-oss with OpenAI’s Agents SDK ? Both Agents SDK enable you to override the OpenAI base client to point to Ollama using Chat Completions or your Responses.js proxy for your local models. Alternatively, you can use the built-in functionality to point the Agents SDK against third party models. Python: Use LiteLLM to proxy to Ollama through LiteLLM TypeScript: Use AI SDK with the ollama adapter Here’s a Python Agents SDK example using LiteLLM: import asyncio from agents import Agent, Runner, function_tool, set_tracing_disabled from agents.extensions.models.litellm_model import LitellmModel set_tracing_disabled( True ) @function_tool def get_weather (city: str ): print ( f &quot;[debug] getting weather for { city } &quot; ) return f &quot;The weather in { city } is sunny.&quot; async def main (model: str , api_key: str ): agent = Agent( name = &quot;Assistant&quot; , instructions = &quot;You only respond in haikus.&quot; , model = LitellmModel( model = &quot;ollama/gpt-oss:120b&quot; , api_key = api_key), tools = [get_weather], ) result = await Runner.run(agent, &quot;What&#x27;s the weather in Tokyo?&quot; ) print (result.final_output) if __name__ == &quot;__main__&quot; : asyncio.run(main()) ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://cookbook.openai.com/articles/openai-harmony",
      "full_text": " OpenAI Harmony Response Format Topics About API docs Source Toggle theme Toggle theme Aug 5, 2025 OpenAI Harmony Response Format Dominik Kundel (OpenAI) Open in GitHub Concepts Harmony renderer library Prompt format The gpt-oss models were trained on the harmony response format for defining conversation structures, generating reasoning output and structuring function calls. If you are not using gpt-oss directly but through an API or a provider like Ollama, you will not have to be concerned about this as your inference solution will handle the formatting. If you are building your own inference solution, this guide will walk you through the prompt format. The format is designed to mimic the OpenAI Responses API, so if you have used that API before, this format should hopefully feel familiar to you. gpt-oss should not be used without using the harmony format, as it will not work correctly. Concepts Roles Every message that the model processes has a role associated with it. The model knows about five types of roles: Role Purpose system A system message is used to specify reasoning effort, meta information like knowledge cutoff and built-in tools developer The developer message is used to provide information about the instructions for the model (what is normally considered the “system prompt”) and available function tools user Typically representing the input to the model assistant Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is. tool Messages representing the output of a tool call. The specific tool name will be used as the role inside a message. These roles also represent the information hierarchy that the model applies in case there are any instruction conflicts: system &gt; developer &gt; user &gt; assistant &gt; tool Channels Assistant messages can be output in three different “channels”. These are being used to separate between user-facing responses and internal facing messages. Channel Purpose final Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model. analysis These are messages that are being used by the model for its chain of thought (CoT). Important: Messages in the analysis channel do not adhere to the same safety standards as final messages do. Avoid showing these to end-users. commentary Any function tool call will typically be triggered on the commentary channel while built-in tools will normally be triggered on the analysis channel. However, occasionally built-in tools will still be output to commentary . Occasionally this channel might also be used by the model to generate a preamble to calling multiple functions. Harmony renderer library We recommend using our harmony renderer through PyPI or crates.io when possible as it will automatically handle rendering your messages in the right format and turning them into tokens for processing by the model. Below is an example of using the renderer to construct a system prompt and a short conversation. from openai_harmony import ( Author, Conversation, DeveloperContent, HarmonyEncodingName, Message, Role, SystemContent, ToolDescription, load_harmony_encoding, ReasoningEffort ) encoding = load_harmony_encoding(HarmonyEncodingName. HARMONY_GPT_OSS ) system_message = ( SystemContent.new() .with_reasoning_effort(ReasoningEffort. HIGH ) .with_conversation_start_date( &quot;2025-06-28&quot; ) ) developer_message = ( DeveloperContent.new() .with_instructions( &quot;Always respond in riddles&quot; ) .with_function_tools( [ ToolDescription.new( &quot;get_current_weather&quot; , &quot;Gets the current weather in the provided location.&quot; , parameters = { &quot;type&quot; : &quot;object&quot; , &quot;properties&quot; : { &quot;location&quot; : { &quot;type&quot; : &quot;string&quot; , &quot;description&quot; : &quot;The city and state, e.g. San Francisco, CA&quot; , }, &quot;format&quot; : { &quot;type&quot; : &quot;string&quot; , &quot;enum&quot; : [ &quot;celsius&quot; , &quot;fahrenheit&quot; ], &quot;default&quot; : &quot;celsius&quot; , }, }, &quot;required&quot; : [ &quot;location&quot; ], }, ), ] ) ) convo = Conversation.from_messages( [ Message.from_role_and_content(Role. SYSTEM , system_message), Message.from_role_and_content(Role. DEVELOPER , developer_message), Message.from_role_and_content(Role. USER , &quot;What is the weather in Tokyo?&quot; ), Message.from_role_and_content( Role. ASSISTANT , &#x27;User asks: &quot;What is the weather in Tokyo?&quot; We need to use get_current_weather tool.&#x27; , ).with_channel( &quot;analysis&quot; ), Message.from_role_and_content(Role. ASSISTANT , &#x27;{&quot;location&quot;: &quot;Tokyo&quot;}&#x27; ) .with_channel( &quot;commentary&quot; ) .with_recipient( &quot;functions.get_current_weather&quot; ) .with_content_type( &quot;&lt;|constrain|&gt; json&quot; ), Message.from_author_and_content( Author.new(Role. TOOL , &quot;functions.get_current_weather&quot; ), &#x27;{ &quot;temperature&quot;: 20, &quot;sunny&quot;: true }&#x27; , ).with_channel( &quot;commentary&quot; ), ] ) tokens = encoding.render_conversation_for_completion(convo, Role. ASSISTANT ) # After receiving a token response # Do not pass in the stop token parsed_response = encoding.parse_messages_from_completion_tokens(new_tokens, Role. ASSISTANT ) Additionally the openai_harmony library also includes a StreamableParser for parsing and decoding as the model is generating new tokens. This can be helpful for example to stream output and handle unicode characters during decoding. from openai_harmony import ( load_harmony_encoding, Role, StreamableParser, HarmonyEncodingName ) encoding = load_harmony_encoding(HarmonyEncodingName. HARMONY_GPT_OSS ) stream = StreamableParser(encoding, role = Role. ASSISTANT ) tokens = [ 200005 , 35644 , 200008 , 1844 , 31064 , 25 , 392 , 4827 , 382 , 220 , 17 , 659 , 220 , 17 , 16842 , 12295 , 81645 , 13 , 51441 , 6052 , 13 , 200007 , 200006 , 173781 , 200005 , 17196 , 200008 , 17 , 659 , 220 , 17 , 314 , 220 , 19 , 13 , 200002 ] for token in tokens: stream.process(token) print ( &quot;--------------------------------&quot; ) print ( &quot;current_role&quot; , stream.current_role) print ( &quot;current_channel&quot; , stream.current_channel) print ( &quot;last_content_delta&quot; , stream.last_content_delta) print ( &quot;current_content_type&quot; , stream.current_content_type) print ( &quot;current_recipient&quot; , stream.current_recipient) print ( &quot;current_content&quot; , stream.current_content) Prompt format If you choose to build your own renderer, you’ll need to adhere to the following format. Special Tokens The model uses a set of special tokens to identify the structure of your input. If you are using tiktoken these tokens are encoded in the o200k_harmony encoding. All special tokens follow the format &lt;|type|&gt; . Special token Purpose Token ID &lt;|start|&gt; Indicates the beginning of a message . Followed by the “header” information of a message starting with the role 200006 &lt;|end|&gt; Indicates the end of a message 200007 &lt;|message|&gt; Indicates the transition from the message “header” to the actual content 200008 &lt;|channel|&gt; Indicates the transition to the channel information of the header 200005 &lt;|constrain|&gt; Indicates the transition to the data type definition in a tool call 200003 &lt;|return|&gt; Indicates the model is done with sampling the response message. A valid “stop token” indicating that you should stop inference. 200002 &lt;|call|&gt; Indicates the model wants to call a tool. A valid “stop token” indicating that you should stop inference. 200012 Message format The harmony response format consists of “messages” with the model potentially generating multiple messages in one go. The general structure of a message is as follows: &lt;|start|&gt;{header}&lt;|message|&gt;{content}&lt;|end|&gt; The {header} contains a series of meta information including the role . &lt;|end|&gt; represents the end of a fully completed message but the model might also use other stop tokens such as &lt;|call|&gt; for tool calling and &lt;|return|&gt; to indicate the model is done with the completion. Chat conversation format Following the message format above the most basic chat format consists of a user message and the beginning of an assistant message. Example input &lt;|start|&gt;user&lt;|message|&gt;What is 2 + 2?&lt;|end|&gt; &lt;|start|&gt;assistant The output will begin by specifying the channel . For example analysis to output the chain of thought. The model might output multiple messages (primarily chain of thought messages) for which it uses the &lt;|end|&gt; token to separate them. Once its done generating it will stop with either a &lt;|return|&gt; token indicating it’s done generating the final answer, or &lt;|call|&gt; indicating that a tool call needs to be performed. In either way this indicates that you should stop inference. Example output &lt;|channel|&gt;analysis&lt;|message|&gt;User asks: &quot;What is 2 + 2?&quot; Simple arithmetic. Provide answer.&lt;|end|&gt; &lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;2 + 2 = 4.&lt;|return|&gt; The final channel will contain the answer to your user’s request. Check out the reasoning section for more details on the chain-of-thought. Implementation note: &lt;|return|&gt; is a decode-time stop token only. When you add the assistant’s generated reply to conversation history for the next turn, replace the trailing &lt;|return|&gt; with &lt;|end|&gt; so that stored messages are fully formed as &lt;|start|&gt;{header}&lt;|message|&gt;{content}&lt;|end|&gt; . Prior messages in prompts should therefore end with &lt;|end|&gt; . For supervised targets/training examples, ending with &lt;|return|&gt; is appropriate; for persisted history, normalize to &lt;|end|&gt; . System message format The system message is used to provide general information to the system. This is different to what might be considered the “system prompt” in other prompt formats. For that, check out the developer message format . We use the system message to define: The identity of the model — This should always stay as You are ChatGPT, a large language model trained by OpenAI. If you want to change the identity of the model, use the instructions in the developer message . Meta dates — Specifically the Knowledge cutoff: and the Current date: The reasoning effort — As specified on the levels high , medium , low Available channels — For the best performance this should map to analysis , commentary , and final . Built-in tools — The model has been trained on both a python and browser tool. Check out the built-in tools section for details. If you are defining functions, it should also contain a note that all function tool calls must go to the commentary channel. For the best performance stick to this format as closely as possible. Example system message The most basic system message you should use is the following: &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Valid channels: analysis, commentary, final. Channel must be included for every message.&lt;|end|&gt; If functions calls are present in the developer message section, use: &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Valid channels: analysis, commentary, final. Channel must be included for every message. Calls to these tools must go to the commentary channel: &#x27;functions&#x27;.&lt;|end|&gt; Developer message format The developer message represents what is commonly considered the “system prompt”. It contains the instructions that are provided to the model and optionally a list of function tools available for use or the output format you want the model to adhere to for structured outputs . If you are not using function tool calling your developer message would just look like this: &lt;|start|&gt;developer&lt;|message|&gt;# Instructions {instructions}&lt;|end|&gt; Where {instructions} is replaced with your “system prompt”. For defining function calling tools, check out the dedicated section . For defining an output format to be used in structured outputs, check out this section of the guide . Reasoning The gpt-oss models are reasoning models. By default, the model will do medium level reasoning. To control the reasoning you can specify in the system message the reasoning level as low , medium , or high . The recommended format is: Reasoning: high The model will output its raw chain-of-thought (CoT) as assistant messages into the analysis channel while the final response will be output as final . For example for the question What is 2 + 2? the model output might look like this: &lt;|channel|&gt;analysis&lt;|message|&gt;User asks: &quot;What is 2 + 2?&quot; Simple arithmetic. Provide answer.&lt;|end|&gt; &lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;2 + 2 = 4.&lt;|return|&gt; In this case the CoT is User asks: “What is 2 + 2?” Simple arithmetic. Provide answer. And the actual answer is: 2 + 2 = 4 Important: The model has not been trained to the same safety standards in the chain-of-thought as it has for final output. You should not show the chain-of-thought to your users, as they might contain harmful content. Learn more in the model card . Handling reasoning output in subsequent sampling In general, you should drop any previous CoT content on subsequent sampling if the responses by the assistant ended in a message to the final channel. Meaning if our first input was this: &lt;|start|&gt;user&lt;|message|&gt;What is 2 + 2?&lt;|end|&gt; &lt;|start|&gt;assistant and resulted in the output: &lt;|channel|&gt;analysis&lt;|message|&gt;User asks: &quot;What is 2 + 2?&quot; Simple arithmetic. Provide answer.&lt;|end|&gt; &lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;2 + 2 = 4.&lt;|return|&gt; For the model to work properly, the input for the next sampling should be &lt;|start|&gt;user&lt;|message|&gt;What is 2 + 2?&lt;|end|&gt; &lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;2 + 2 = 4.&lt;|end|&gt; &lt;|start|&gt;user&lt;|message|&gt;What about 9 / 2?&lt;|end|&gt; &lt;|start|&gt;assistant The exception for this is tool/function calling. The model is able to call tools as part of its chain-of-thought and because of that, we should pass the previous chain-of-thought back in as input for subsequent sampling. Check out the function calling section for a complete example. Function calling Defining available tools All functions that are available to the model should be defined in the developer message in a dedicated Tools section. To define the functions we use a TypeScript-like type syntax and wrap the functions into a dedicated functions namespace. It’s important to stick to this format closely to improve accuracy of function calling. You can check out the harmony renderer codebase for more information on how we are turning JSON schema definitions for the arguments into this format but some general formatting practices: Define every function as a type {function_name} = () =&gt; any if it does not receive any arguments For functions that receive an argument name the argument _ and inline the type definition Add comments for descriptions in the line above the field definition Always use any as the return type Keep an empty line after each function definition Wrap your functions into a namespace, generally functions is the namespace you should use to not conflict with other tools that the model might have been trained on. Here’s a complete input example including the definition of two functions: &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Valid channels: analysis, commentary, final. Channel must be included for every message. Calls to these tools must go to the commentary channel: &#x27;functions&#x27;.&lt;|end|&gt;&lt;|start|&gt;developer&lt;|message|&gt;# Instructions Use a friendly tone. # Tools ## functions namespace functions { // Gets the location of the user. type get_location = () =&gt; any; // Gets the current weather in the provided location. type get_current_weather = (_: { // The city and state, e.g. San Francisco, CA location: string, format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius }) =&gt; any; // Gets the current weather in the provided list of locations. type get_multiple_weathers = (_: { // List of city and state, e.g. [&quot;San Francisco, CA&quot;, &quot;New York, NY&quot;] locations: string[], format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius }) =&gt; any; } // namespace functions&lt;|end|&gt;&lt;|start|&gt;user&lt;|message|&gt;What is the weather like in SF?&lt;|end|&gt;&lt;|start|&gt;assistant Receiving tool calls If the model decides to call a tool it will define a recipient in the header of the message using the format to={name} . For example, if it decides to trigger the get_current_weather function from above it would specify to=functions.get_current_weather in the header and commentary as the channel as specified in the system message . The recipient might be defined in the role or channel section of the header. The model might also specify a &lt;|constrain|&gt; token to indicate the type of input for the tool call. In this case since it’s being passed in as JSON the &lt;|constrain|&gt; is set to json . &lt;|channel|&gt;analysis&lt;|message|&gt;Need to use function get_current_weather.&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;commentary to=functions.get_current_weather &lt;|constrain|&gt;json&lt;|message|&gt;{&quot;location&quot;:&quot;San Francisco&quot;}&lt;|call|&gt; Handling tool calls After the function call was handled we need to provide the output back to the model by specifying a new tool message with the output after the call message. A tool message has the following format: &lt;|start|&gt;{toolname} to=assistant&lt;|channel|&gt;commentary&lt;|message|&gt;{output}&lt;|end|&gt; So in our example above &lt;|start|&gt;functions.get_current_weather to=assistant&lt;|channel|&gt;commentary&lt;|message|&gt;{&quot;sunny&quot;: true, &quot;temperature&quot;: 20}&lt;|end|&gt; Once you have gathered the output for the tool calls you can run inference with the complete content: &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Valid channels: analysis, commentary, final. Channel must be included for every message. Calls to these tools must go to the commentary channel: &#x27;functions&#x27;.&lt;|end|&gt;&lt;|start|&gt;developer&lt;|message|&gt;# Instructions Use a friendly tone. # Tools ## functions namespace functions { // Gets the location of the user. type get_location = () =&gt; any; // Gets the current weather in the provided location. type get_current_weather = (_: { // The city and state, e.g. San Francisco, CA location: string, format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius }) =&gt; any; // Gets the current weather in the provided list of locations. type get_multiple_weathers = (_: { // List of city and state, e.g. [&quot;San Francisco, CA&quot;, &quot;New York, NY&quot;] locations: string[], format?: &quot;celsius&quot; | &quot;fahrenheit&quot;, // default: celsius }) =&gt; any; } // namespace functions&lt;|end|&gt;&lt;|start|&gt;user&lt;|message|&gt;What is the weather like in SF?&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;analysis&lt;|message|&gt;Need to use function get_current_weather.&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;commentary to=functions.get_current_weather &lt;|constrain|&gt;json&lt;|message|&gt;{&quot;location&quot;:&quot;San Francisco&quot;}&lt;|call|&gt; &lt;|start|&gt;functions.get_current_weather to=assistant&lt;|channel|&gt;commentary&lt;|message|&gt;{&quot;sunny&quot;: true, &quot;temperature&quot;: 20}&lt;|end|&gt;&lt;|start|&gt;assistant As you can see above we are passing not just the function out back into the model for further sampling but also the previous chain-of-thought (“Need to use function get_current_weather.”) to provide the model with the necessary information to continue its chain-of-thought or provide the final answer. Preambles At times the model might choose to generate a “preamble” to inform the user about the tools it is about to call. For example, when it plans to call multiple tools. If this is the case it will generate an assistant message on the commentary channel that, unlike the chain-of-thought, is intended to be shown to the end-user. &lt;|channel|&gt;analysis&lt;|message|&gt;{long chain of thought}&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;commentary&lt;|message|&gt;**Action plan**: 1. Generate an HTML file 2. Generate a JavaScript for the Node.js server 3. Start the server --- Will start executing the plan step by step&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;commentary to=functions.generate_file&lt;|constrain|&gt;json&lt;|message|&gt;{&quot;template&quot;: &quot;basic_html&quot;, &quot;path&quot;: &quot;index.html&quot;}&lt;|call|&gt; In this case the model generated an action plan to inform the user about the multiple steps it is about to execute. Structured output To control the output behavior of the model, you can define a response format at the end of the developer message with the following structure: # Response Formats ## {format name} // {description or context} {schema}&lt;|end|&gt; The format name functions similar to the name you can specify for your schema in the Responses API and the schema is a JSON Schema. As an example, here’s a developer message that defines a schema for a shopping list: &lt;|start|&gt;developer&lt;|message|&gt;# Instructions You are a helpful shopping assistant # Response Formats ## shopping_list {&quot;properties&quot;:{&quot;items&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;description&quot;:&quot;entries on the shopping list&quot;,&quot;items&quot;:{&quot;type&quot;:&quot;string&quot;}}},&quot;type&quot;:&quot;object&quot;}&lt;|end|&gt;&lt;|start|&gt;user&lt;|message|&gt;I need to buy coffee, soda and eggs&lt;|end|&gt;&lt;|start|&gt;assistant This prompt alone will, however, only influence the model’s behavior but doesn’t guarantee the full adherence to the schema. For this you still need to construct your own grammar and enforce the schema during sampling. Built-in tools During the training of the gpt-oss models, they were trained with two common tools to browse for information and execute python code to improve its results. If you are trying to build this functionality, you should use the format below to improve reliability and accuracy. These tools should be defined in the system message not in the developer message by adding a # Tools section. Browser tool To define the browser tool add it to the system prompt section: &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Tools ## browser // Tool for browsing. // The `cursor` appears in brackets before each browsing display: `[{cursor}]`. // Cite information from the tool using the following format: // `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`. // Do not quote more than 10 words directly from the tool output. // sources=web (default: web) namespace browser { // Searches for information related to `query` and displays `topn` results. type search = (_: { query: string, topn?: number, // default: 10 source?: string, }) =&gt; any; // Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines. // Valid link ids are displayed with the formatting: `【{id}†.*】`. // If `cursor` is not provided, the most recent page is implied. // If `id` is a string, it is treated as a fully qualified URL associated with `source`. // If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available. // Use this function without `id` to scroll to a new location of an opened page. type open = (_: { id?: number | string, // default: -1 cursor?: number, // default: -1 loc?: number, // default: -1 num_lines?: number, // default: -1 view_source?: boolean, // default: false source?: string, }) =&gt; any; // Finds exact matches of `pattern` in the current page, or the page given by `cursor`. type find = (_: { pattern: string, cursor?: number, // default: -1 }) =&gt; any; } // namespace browser # Valid channels: analysis, commentary, final. Channel must be included for every message.&lt;|end|&gt; If the model decides to call actions in the browser it will use the same format as for function calls with two notable exceptions: Requests will be made to the analysis channel The recipient will be browser.search , browser.open , browser.find respectively Python tool &lt;|start|&gt;system&lt;|message|&gt;You are ChatGPT, a large language model trained by OpenAI. Knowledge cutoff: 2024-06 Current date: 2025-06-28 Reasoning: high # Tools ## python Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files). When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at &#x27;/mnt/data&#x27; can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster. # Valid channels: analysis, commentary, final. Channel must be included for every message.&lt;|end|&gt; If the model decides to execute Python code it will use the same format as for function calls with two notable exceptions: Requests will be made to the analysis channel The recipient will always be python ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2508.10925},",
      "full_text": "Title: [2508.10925},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2508.10925%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2508.10925},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2508.10925%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2508.10925},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    }
  ]
}