{
  "2-3 (API)": "The available material shows that open-source users can interact with the target model through two complementary, publicly described entry-points. First, the sentence “You can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format” confirms that the model is directly loadable inside the Hugging Face Transformers ecosystem. Simply specifying the model name in a standard pipeline or chat template is enough; the library then attaches the prescribed Harmony response format without any additional user work, giving developers a drop-in, \"GPT-like\" experience. Second, the short line “vllm serve openai/gpt-oss-120b” demonstrates that the model can also be hosted through the vLLM serving stack. vLLM exposes an OpenAI-compatible HTTP endpoint, so the example implicitly tells users that one shell command will spin up a network service around gpt-oss-120b, again mimicking the familiar OpenAI API surface. Together these two snippets convey that the model is immediately usable either locally (via Transformers) or as a remotely accessible service (via vLLM) without needing bespoke wrappers, custom serialization, or manual response-formatting code.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony)."
    },
    {
      "source": "[readme]",
      "quote": "vllm serve openai/gpt-oss-120b"
    }
  ],
  "3-1 (Pre-training)": "The pre-training–era information emphasises three technical points. (1) Model scale and intent: “`gpt-oss-120b` — for production, general purpose, high reasoning use cases … (117B parameters with 5.1B active parameters).” This clarifies that although the model is branded “120 b”, its MoE architecture activates only 5.1 b parameters per forward pass, allowing large-model expressiveness while keeping compute per token moderate. (2) Hardware footprint: that same sentence plus the follow-on text about fitting “into a single 80 GB GPU (like NVIDIA H100 or AMD MI300X)” underline a key design goal—running real-world, high-quality reasoning workloads on one modern datacentre GPU instead of multi-node clusters. (3) Quantisation methodology: the line beginning “MXFP4 quantization: The models were post-trained with MXFP4 quantization of the MoE weights” explains how that single-GPU feasibility is achieved. By applying MXFP4 (a mixed-precision, 4-bit format) after core training, the developers shrink memory requirements for the MoE experts, unlocking single-card deployment for gpt-oss-120b and even 16 GB consumer-class operation for the sibling 20 b variant. Finally, the JSON fragment '\"model_type\": \"gpt_oss\"' records the formal identifier used inside config files, showing that the pre-training artefacts are packaged under the custom gpt_oss architecture stamp.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)"
    },
    {
      "source": "[readme]",
      "quote": "**MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory."
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"gpt_oss\","
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning guidance is succinct yet clear. One quote states, “Both gpt-oss models can be fine-tuned for a variety of specialized use cases,” signalling that the authors encourage domain adaptation rather than prescribing a fixed, frozen checkpoint. The next sentence gives concrete hardware expectations: “This larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller `gpt-oss-20b` can even be fine-tuned on consumer hardware.” In practice, this means organisations with access to a single high-end datacentre GPU can run full-parameter or LoRA-style updates on the 120 b variant, while individual researchers or edge deployers with commodity cards can still adapt the smaller model. Although no learning-rate schedules, dataset sizes, or adapter techniques are explicitly listed, the quotes collectively establish that the checkpoints, tokenizer, and software stack expose all hooks necessary for supervised fine-tuning across a wide performance-to-cost spectrum.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Both gpt-oss models can be fine-tuned for a variety of specialized use cases."
    },
    {
      "source": "[readme]",
      "quote": "This larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware."
    }
  ],
  "3-3 (Reinforcement Learning)": "The provided quotations contain no statements that mention RLHF, PPO, DPO, or any other reinforcement-learning procedure for `gpt-oss-120b`; therefore no RL-specific methodology, hyper-parameters, or reward models can be summarised.",
  "3-3 (Reinforcement Learning)__evidence": []
}