{
  "1-5 (Architecture)": "The provided material gives several concrete facts about the architecture of the target model gpt-oss-120b. First, it explicitly calls the model “production, general-purpose, [for] high-reasoning use cases” and clarifies that it is designed to fit on a single 80 GB GPU such as an NVIDIA H100 or AMD MI300X. The size is stated as “117 B parameters with 5.1 B active parameters,” confirming that the model is a Mixture-of-Experts (MoE) system in which only a subset of weights (5.1 B) are active per token. Second, the quotes specify that the weights underwent “MXFP4 quantization,” which is a post-training 4-bit quantization scheme applied specifically to the MoE weights; this compression technique is the reason the full 117 B-parameter model can still execute on a single 80 GB card. Finally, the JSON snippet \"model_type\": \"gpt_oss\" confirms the internal configuration flag used to identify the architecture within tooling. Together, these statements establish that gpt-oss-120b is a 117-billion-parameter MoE transformer whose experts are stored in MXFP4, yielding 5.1 B active parameters at inference and enabling single-GPU deployment.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)"
    },
    {
      "source": "[readme]",
      "quote": "* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory."
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"gpt_oss\","
    }
  ],
  "1-6 (Tokenizer)": "No tokenizer details are disclosed for gpt-oss-120b in the supplied quotes.",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Hardware guidance in the quotes centers on single-GPU operation. The text states that gpt-oss-120b “fit[s] into a single 80 GB GPU (like NVIDIA H100 or AMD MI300X).” It reiterates that “This larger model gpt-oss-120b can be fine-tuned on a single H100 node,” highlighting that full-parameter fine-tuning is feasible without multi-node clusters as long as a single H100 is available. By comparison, the same sentence notes that the 20 B variant can be fine-tuned on consumer hardware, underscoring that the 120 B version remains constrained to datacenter-class 80 GB accelerators. No multi-GPU or TPU information is given; the sole documented compute target is an 80 GB H100 or MI300X.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)"
    },
    {
      "source": "[readme]",
      "quote": "This larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware."
    }
  ],
  "2-2 (Software)": "For the software stack, the quotes explicitly mention that users “can use gpt-oss-120b … with Transformers,” referring to the Hugging Face Transformers library. They additionally instruct readers to consult the “reference implementations in the gpt-oss repository” for details on “how to use this model with PyTorch and Triton.” Therefore, the documented training/inference ecosystem includes (1) Hugging Face Transformers as the high-level API, (2) PyTorch as the core deep-learning framework, and (3) Triton for optimized kernel execution. No specific version numbers or other libraries are supplied in the excerpt.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers."
    },
    {
      "source": "[readme]",
      "quote": "To learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation)."
    }
  ]
}