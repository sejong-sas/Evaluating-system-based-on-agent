{
  "model": "openai/gpt-oss-120b",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Multiple quotes state the model is released under the unmodified Apache-2.0 licence, which grants all four freedoms."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored technical report / model card is quoted (BibTeX entry and PDF sections clearly authored by OpenAI)."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Training hardware is quoted: “NVIDIA H100 GPUs … 2.1 million H100-hours.”  Both type and quantity/compute are given."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack mentions PyTorch, Triton kernels and Flash-Attention.  Some components beyond the base framework are named, but no versions/configs → Semi-Open."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: no official API found. OpenAI's gpt-oss-120b is an open-weight model not served via OpenAI's official API; major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "High-level statements about MoE pre-training and MXFP4 are given, but not enough detail to be fully reproducible (no schedules/objectives) → Semi-Open."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Quotes merely say the model *can* be fine-tuned; they do not reveal methods, hyper-parameters, or pipelines."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "RL is mentioned, but no methodological detail is provided."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "No disclosure of sources, sizes, or makeup of the pre-training corpus. Adjusted to Semi-Open: quotes indicate partial disclosure."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Sources (biorisk expert data, cybersecurity CTF environments) are named, but no sizes/licensing/availability details are given → Semi-Open."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data details are disclosed."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Filtering of CBRN-related content during pre-training is stated, but no algorithms, thresholds or multi-step pipeline are provided.  Counts as partial disclosure."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Multiple quotes state the model is released under the unmodified Apache-2.0 licence, which grants all four freedoms."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored technical report / model card is quoted (BibTeX entry and PDF sections clearly authored by OpenAI)."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Training hardware is quoted: “NVIDIA H100 GPUs … 2.1 million H100-hours.”  Both type and quantity/compute are given."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack mentions PyTorch, Triton kernels and Flash-Attention.  Some components beyond the base framework are named, but no versions/configs → Semi-Open."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: no official API found. OpenAI's gpt-oss-120b is an open-weight model not served via OpenAI's official API; major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "High-level statements about MoE pre-training and MXFP4 are given, but not enough detail to be fully reproducible (no schedules/objectives) → Semi-Open."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Quotes merely say the model *can* be fine-tuned; they do not reveal methods, hyper-parameters, or pipelines."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "RL is mentioned, but no methodological detail is provided."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "No disclosure of sources, sizes, or makeup of the pre-training corpus. Adjusted to Semi-Open: quotes indicate partial disclosure."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Sources (biorisk expert data, cybersecurity CTF environments) are named, but no sizes/licensing/availability details are given → Semi-Open."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data details are disclosed."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Filtering of CBRN-related content during pre-training is stated, but no algorithms, thresholds or multi-step pipeline are provided.  Counts as partial disclosure."
    }
  },
  "final_score_10pt": 5.312,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 8.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}