{
  "4-1 (Pre-training Data)": "The facebook/opt-125m model is reported to share the same pre-training corpus as the larger OPT family. According to the quotes, that corpus was built for “OPT-175B” but explicitly “was used to pre-train the OPT models,” which include the 125 M-parameter variant. The corpus is a union of five publicly available sources: (1) the three text datasets originally employed by RoBERTa, (2) a subset of The Pile, and (3) the Pushshift.io Reddit dump as processed in Roller et al. 2021. The selection strategy emphasized “breadth and availability.” Documentation referenced in the Data Card (Appendix C) covers further details, and the same data were held constant while model size varied “from 125M to 175B,” confirming that the 125 M model inherits this exact mixture of sources.",
  "4-2 (Fine-tuning Data)": "For downstream safety work, “OPT-175B” was fine-tuned on carefully curated dialogue datasets—specifically BlenderBot 1 and R2C2. Evaluation shows that models subjected to this fine-tuning exhibit “overall lower toxicity” and slightly outperform a 2.7 B-parameter Reddit baseline on SaferDialogues and Unit-Test benchmarks in both Safe and Adversarial modes. While the quote references the 175 B-parameter size, it directly links the observed benefits to the curated datasets themselves, indicating the nature and purpose of the fine-tuning data used within the OPT series.",
  "4-3 (Reinforcement Learning Data)": "The supplied quotations contain no statements that mention any reinforcement-learning–specific datasets, generation procedures, or accessibility information for facebook/opt-125m or other OPT models.",
  "4-4 (Data Filtering)": "The data pipeline for OPT models included an explicit “data curation and deduplication” stage staffed by Shuhoi Chen, Myle Ott, and Stephen Roller. Although the quote does not disclose numerical thresholds, tool names, or classifier criteria, it confirms that a dedicated sub-team carried out systematic cleaning and duplicate removal before training the models.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/C.1 Motivation]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/D.3 Data, Limitations, and Recommendations]",
      "quote": "Training data for OPT-175B was selected based on a combination of breadth and availability."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "This dataset was used to pre-train the OPT models."
    },
    {
      "source": "[sections/Datasheet]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of ﬁve datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[sections/D.1 Model Details]",
      "quote": "OPT-175B was trained with AdamW for parameter sizes from 125M to 175B. See the Data Card (Appendix C) for information about training data and Section 2.2 - 2.5 for information about the training process."
    },
    {
      "source": "[sections/D.3 Data, Limitations, and Recommendations]",
      "quote": "Data selection for training: Training data for OPT-175B was selected based on a combination of breadth and availability."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We observe that OPT-175B has similar performance as the Reddit 2.7B model across both SaferDialogues and the Unit Tests, with OPT-175B performing marginally better in the Safe and Adversarial settings. Consistent with Roller et al. (2021) and Xu et al. (2020), we ﬁnd that the models ﬁne-tuned on curated dialogue datasets (BlenderBot 1, R2C2) have overall lower toxicity."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Contributions]",
      "quote": "• Training and monitoring OPT-175B: Mikel Artetxe, Moya Chen, Naman Goyal, Punit Singh Koura, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Stephen Roller, Susan Zhang\n• Data curation and deduplication: Shuhoi Chen, Myle Ott, Stephen Roller"
    }
  ]
}