{
  "model_id": "facebook/opt-125m",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2205.01068",
      "full_text": " [2205.01068] OPT: Open Pre-trained Transformer Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2205.01068 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2205.01068 (cs) [Submitted on 2 May 2022 ( v1 ), last revised 21 Jun 2022 (this version, v4)] Title: OPT: Open Pre-trained Transformer Language Models Authors: Susan Zhang , Stephen Roller , Naman Goyal , Mikel Artetxe , Moya Chen , Shuohui Chen , Christopher Dewan , Mona Diab , Xian Li , Xi Victoria Lin , Todor Mihaylov , Myle Ott , Sam Shleifer , Kurt Shuster , Daniel Simig , Punit Singh Koura , Anjali Sridhar , Tianlu Wang , Luke Zettlemoyer View a PDF of the paper titled OPT: Open Pre-trained Transformer Language Models, by Susan Zhang and 18 other authors View PDF Abstract: Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2205.01068 [cs.CL] &nbsp; (or arXiv:2205.01068v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2205.01068 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Susan Zhang [ view email ] [v1] Mon, 2 May 2022 17:49:50 UTC (9,196 KB) [v2] Tue, 3 May 2022 15:04:06 UTC (9,190 KB) [v3] Thu, 5 May 2022 11:44:30 UTC (7,822 KB) [v4] Tue, 21 Jun 2022 17:04:40 UTC (7,832 KB) Full-text links: Access Paper: View a PDF of the paper titled OPT: Open Pre-trained Transformer Language Models, by Susan Zhang and 18 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-05 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2205.01068.pdf",
      "full_text": "OPT: Open Pre-trained Transformer Language Models\nSusan Zhang∗, Stephen Roller∗, Naman Goyal∗,\nMikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,\nXi Victoria Lin, Todor Mihaylov, Myle Ott†, Sam Shleifer†, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer\nMeta AI\n{susanz,roller,naman}@fb.com\nAbstract\nLarge language models,\nwhich are often\ntrained for hundreds of thousands of compute\ndays, have shown remarkable capabilities for\nzero- and few-shot learning. Given their com-\nputational cost, these models are difﬁcult to\nreplicate without signiﬁcant capital. For the\nfew that are available through APIs, no ac-\ncess is granted to the full model weights, mak-\ning them difﬁcult to study. We present Open\nPre-trained Transformers (OPT), a suite of\ndecoder-only pre-trained transformers ranging\nfrom 125M to 175B parameters, which we aim\nto fully and responsibly share with interested\nresearchers. We show that OPT-175B is com-\nparable to GPT-3,1 while requiring only 1/7th\nthe carbon footprint to develop. We are also\nreleasing our logbook detailing the infrastruc-\nture challenges we faced, along with code for\nexperimenting with all of the released models.\n1\nIntroduction\nLarge language models (LLMs) trained on massive\ntext collections have shown surprising emergent\ncapabilities to generate text and perform zero- and\nfew-shot learning (Brown et al., 2020; Lieber et al.,\n2021; Smith et al., 2022; Rae et al., 2021; Chowd-\nhery et al., 2022). While in some cases the public\ncan interact with these models through paid APIs,\nfull model access is currently limited to only a\nfew highly resourced labs.2 This restricted access\nhas limited researchers’ ability to study how and\nwhy these large language models work, hindering\n∗Equal contribution.\n†Work done while at Meta AI.\n1Following Brown et al. (2020), we use GPT-3 to refer to\nboth the 175B model and the smaller scale models as well.\n2Exceptions include work by EleutherAI, who released\ndense models up to 20B in size (Black et al., 2022),\nSalesforce (Nijkamp et al., 2022), and Meta AI, who re-\nleased dense models up to 13B and sparse models up to\n1.1T (Artetxe et al., 2021).\nThere is also ongoing work\nfrom the BigScience workshop (https://bigscience.\nhuggingface.co/), which aims to open source very large\nmultilingual language models and datasets.\nprogress on improving known challenges in areas\nsuch as robustness, bias, and toxicity.\nIn this technical report, we present Open Pre-\ntrained Transformers (OPT), a suite of decoder-\nonly pre-trained transformers ranging from 125M\nto 175B parameters, which we aim to fully and\nresponsibly share with interested researchers. We\ntrain the OPT models to roughly match the per-\nformance and sizes of the GPT-3 class of models,\nwhile also applying the latest best practices in data\ncollection and efﬁcient training. Our aim in de-\nveloping this suite of OPT models is to enable re-\nproducible and responsible research at scale, and\nto bring more voices to the table in studying the\nimpact of these LLMs. Deﬁnitions of risk, harm,\nbias, and toxicity, etc., should be articulated by the\ncollective research community as a whole, which is\nonly possible when models are available for study.\nWe are releasing all of our models between\n125M and 66B parameters, and will provide full\nresearch access to OPT-175B upon request. Ac-\ncess will be granted to academic researchers; those\nafﬁliated with organizations in government, civil\nsociety, and academia; and those in industry re-\nsearch laboratories. We are also releasing both the\nlogbook of our model creation as well as our code-\nbase, metaseq,3 which enabled training OPT-175B\non 992 80GB A100 GPUs, reaching 147 TFLOP/s\nutilization per GPU. From this implementation, and\nfrom using the latest generation of NVIDIA hard-\nware, we are able to develop OPT-175B using only\n1/7th the carbon footprint of GPT-3. While this is a\nsigniﬁcant achievement, the energy cost of creating\nsuch a model is still nontrivial, and repeated efforts\nto replicate a model of this size will only amplify\nthe growing compute footprint of these LLMs.\nWe believe the entire AI community — aca-\ndemic researchers, civil society, policymakers, and\nindustry — must work together to develop clear\n3https://github.com/facebookresearch/\nmetaseq\narXiv:2205.01068v4  [cs.CL]  21 Jun 2022\n\nModel\n#L\n#H\ndmodel\nLR\nBatch\n125M\n12\n12\n768\n6.0e−4\n0.5M\n350M\n24\n16\n1024\n3.0e−4\n0.5M\n1.3B\n24\n32\n2048\n2.0e−4\n1M\n2.7B\n32\n32\n2560\n1.6e−4\n1M\n6.7B\n32\n32\n4096\n1.2e−4\n2M\n13B\n40\n40\n5120\n1.0e−4\n4M\n30B\n48\n56\n7168\n1.0e−4\n4M\n66B\n64\n72\n9216\n0.8e−4\n2M\n175B\n96\n96\n12288\n1.2e−4\n2M\nTable 1: Model architecture details. We report the\nnumber of layers (#L), number of attention heads (#H),\nand the embedding size (dmodel). We also report the\npeak Learning Rate (LR) and global batch size in num-\nber of tokens (Batch).\nguidelines around responsible AI in general and\nresponsible LLMs in particular, given their cen-\ntrality in many downstream language applications.\nA much broader segment of the AI community\nneeds access to these models in order to conduct\nreproducible research and collectively drive the\nﬁeld forward. With the release of OPT-175B and\nsmaller-scale baselines, we hope to increase the di-\nversity of voices deﬁning the ethical considerations\nof such technologies.\n2\nMethod\n2.1\nModels\nWe present results on eight Transformer language\nmodels ranging from 125 million to 175 billion\nparameters. Architectural details are displayed in\nTable 1. In the interest of transparency, and to re-\nduce risk of training instabilities, our models and\nhyperparameters largely follow Brown et al. (2020),\nwith variations in batch size mostly to obtain in-\ncreased computational efﬁciency.\n2.2\nTraining Setup\nFor weight initialization, we follow the same set-\ntings provided in the Megatron-LM codebase,4 us-\ning a normal distribution with zero mean and stan-\ndard deviation of 0.006. Standard deviation for\noutput layers are scaled by a 1.0/\n√\n2L term where\nL is the total number of layers. All bias terms are\ninitialized as 0, and all models are trained with\nReLU activation and a sequence length of 2048.\n4https://github.com/NVIDIA/\nMegatron-LM/blob/main/examples/pretrain_\ngpt3_175B.sh\nWe use an AdamW optimizer (Loshchilov and\nHutter, 2017) with (β1, β2) set to (0.9, 0.95), and\nweight decay of 0.1. We follow a linear learning\nrate schedule, warming up from 0 to the maximum\nlearning rate over the ﬁrst 2000 steps in OPT-175B,\nor over 375M tokens in our smaller baselines, and\ndecaying down to 10% of the maximum LR over\n300B tokens. A number of mid-ﬂight changes\nto LR were also required (see Section 2.5). Our\nbatch sizes range from 0.5M to 4M depending on\nthe model size (see Table 1) and is kept constant\nthroughout the course of training.\nWe use a dropout of 0.1 throughout, but we\ndo not apply any dropout to embeddings.\nWe\nclip gradient norms at 1.0, except for some mid-\nﬂight changes that reduce this threshold down\nfrom 1.0 to 0.3 (see Section 2.5).\nWe also in-\nclude a gradient predivide factor to reduce the risk\nof over/underﬂows when computing the gradient\nacross all ranks (splitting the division by the world\nsize of N into two division operations by\n√\nN).\n2.3\nPre-training Corpus\nThe pre-training corpus contains a concatenation\nof datasets used in RoBERTa (Liu et al., 2019b),\nthe Pile (Gao et al., 2021a), and PushShift.io Red-\ndit (Baumgartner et al., 2020; Roller et al., 2021).\nAll corpora were previously collected or ﬁltered\nto contain predominantly English text, but a small\namount of non-English data is still present within\nthe corpus via CommonCrawl.\nWe removed duplicated documents across all\ndatasets by ﬁltering out documents via Min-\nhashLSH (Rajaraman and Ullman, 2011) with a\nJaccard similarity ≥.95. We found the Pile was\nparticularly full of duplicate documents, and ad-\nvise future researchers using the Pile to perform\nadditional de-duplication processing.\nWe tokenize all corpora using the GPT-2 byte\nlevel BPE tokenizer (Sennrich et al., 2016; Radford\net al., 2019; Brown et al., 2020). Our ﬁnal corpus\ncontains roughly 180B tokens.\nRoBERTa\nWe included the BookCorpus (Zhu\net al., 2015) and Stories (Trinh and Le, 2018) sub-\nsets of the RoBERTa corpus and utilized an up-\ndated version of CCNews, containing news stories\ncrawled through September 28, 2021. This CC-\nNews v2 corpus was preprocessed the same way as\nthe original RoBERTa CCNews (Liu et al., 2019b).\nThe Pile\nWe included a subset of the Pile\n(Gao et al., 2021a), including: CommonCrawl,\n\nDM Mathematics,\nProject Gutenberg,\nHack-\nerNews, OpenSubtitles, OpenWebText2, USPTO\nand Wikipedia. Other subsets of the Pile were elim-\ninated as we found they increased the risk of insta-\nbilities, as measured by tendency to cause spikes\nin gradient norms at the 1.3B scale, or were other-\nwise deemed unsuitable. All subsets went through\nadditional ad-hoc whitespace normalization.\nPushShift.io Reddit\nWe included a subset of\nthe Pushshift.io corpus produced by Baumgart-\nner et al. (2020) and previously used by Roller\net al. (2021). To convert the conversational trees\ninto language-model-accessible documents, we ex-\ntracted the longest chain of comments in each\nthread and discarded all other paths in the tree.\nThis reduced the corpus by about 66%.\n2.4\nTraining Efﬁciency\nWe trained OPT-175B on 992 80GB A100 GPUs,\nby utilizing Fully Sharded Data Parallel (Artetxe\net al., 2021) with Megatron-LM Tensor Parallelism\n(Shoeybi et al., 2019). We achieve utilization of up\nto 147 TFLOP/s per GPU. We keep Adam state in\nFP32, since we shard it across all hosts, while the\nmodel weights remained in FP16. To avoid under-\nﬂows, we used dynamic loss scaling, as described\nin Micikevicius et al. (2017).\n2.5\nTraining Processes\nHere we describe signiﬁcant training process ad-\njustments that arose during OPT-175B pre-training.\nHardware Failures\nWe faced a signiﬁcant num-\nber of hardware failures in our compute cluster\nwhile training OPT-175B. In total, hardware fail-\nures contributed to at least 35 manual restarts and\nthe cycling of over 100 hosts over the course of 2\nmonths. During manual restarts, the training run\nwas paused, and a series of diagnostics tests were\nconducted to detect problematic nodes. Flagged\nnodes were then cordoned off and training was re-\nsumed from the last saved checkpoint. Given the\ndifference between the number of hosts cycled out\nand the number of manual restarts, we estimate 70+\nautomatic restarts due to hardware failures.\nLoss Divergences\nLoss divergences were also an\nissue in our training run. When the loss diverged,\nwe found that lowering the learning rate and restart-\ning from an earlier checkpoint allowed for the job\nto recover and continue training. We noticed a cor-\nrelation between loss divergence, our dynamic loss\n0k\n20k\n40k\n60k\n80k\n100k\n120k\n140k\nIterations\n0.0e-4\n0.2e-4\n0.4e-4\n0.6e-4\n0.8e-4\n1.0e-4\n1.2e-4\nLearning Rate\nEmpirical Learning Rate\nFigure 1: Empirical LR schedule. We found that low-\nering learning rate was helpful for avoiding instabili-\nties.\n0k\n20k\n40k\n60k\n80k\n100k\n120k\n140k\nIterations\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\n10.0\nPerplexity\nValidation Perplexity\nFigure 2: Validation Perplexity. Our mid-ﬂight LR\nchanges had clear effects on validation perplexity.\nscalar crashing to 0, and the l2-norm of the activa-\ntions of the ﬁnal layer spiking. These observations\nled us to pick restart points for which our dynamic\nloss scalar was still in a “healthy” state (≥1.0),\nand after which our activation norms would trend\ndownward instead of growing unboundedly. Our\nempirical LR schedule is shown in Figure 1. Early\nin training, we also noticed that lowering gradient\nclipping from 1.0 to 0.3 helped with stability; see\nour released logbook for exact details. Figure 2\nshows our validation loss with respect to training\niterations.\nOther Mid-ﬂight Changes\nWe conducted a\nnumber of other experimental mid-ﬂight changes\nto handle loss divergences. These included: switch-\ning to vanilla SGD (optimization plateaued quickly,\nand we reverted back to AdamW); resetting the dy-\nnamic loss scalar (this helped recover some but not\nall divergences); and switching to a newer version\nof Megatron (this reduced pressure on activation\nnorms and improved throughput).\n\n3\nEvaluations\n3.1\nPrompting & Few-Shot\nWe evaluate our model on 16 standard NLP tasks\nutilized in the literature: HellaSwag (Zellers et al.,\n2019), StoryCloze (Mostafazadeh et al., 2016),\nPIQA (Bisk et al., 2020), ARC Easy and Challenge\n(Clark et al., 2018), OpenBookQA (Mihaylov et al.,\n2018), WinoGrad (Levesque et al., 2011), Wino-\nGrande (Sakaguchi et al., 2020), and SuperGLUE\n(Wang et al., 2019). We follow GPT-3 (Brown\net al., 2020) by using their prompts and overall ex-\nperimental setup. We compare primarily to GPT-3,\nhaving aimed to re-implement their evaluation set-\ntings, but include reported performance of other\nLLMs on a per-task basis when available (Lieber\net al., 2021; Rae et al., 2021; Hoffmann et al., 2022;\nBlack et al., 2022)\nWe report performance in accuracy (omitting F1\nfor MultiRC and ReCoRD for consistency in eval-\nuation metrics). For the Winograd Schema Chal-\nlenge (WSC) task in the SuperGLUE benchmark,\nwe follow (Brown et al., 2020) and formulate the\ntask as multiple choice questions, which is known\nto affect performance (Liu et al., 2020).\nZero-shot\nOverall average zero-shot perfor-\nmance across all 14 tasks may be seen in Figure 3.\nOverall, we see our average performance follows\nthe trend of GPT-3. However, performance can\nvary radically across the tasks: for a full break-\ndown, see Appendix A. Note that we intentionally\nremoved MultiRC and WIC from these averages, as\nthese datasets seem to systematically favor GPT-3\nor OPT disproportionately.\nOur performance roughly matched GPT-3 for 10\ntasks, and underperformed in 3 tasks (ARC Chal-\nlenge and MultiRC). In 3 tasks (CB, BoolQ, WSC),\nwe ﬁnd both GPT and OPT models display unpre-\ndictable behavior with respect to scale, likely due\nto the small size of the validation set in these 3\ntasks (56, 277, and 104 examples, respectively).\nIn WIC, we see that the OPT models always out-\nperform the GPT-3 models, though the numbers\nreported by Brown et al. (2020) also seem question-\nable, given WIC being a binary classiﬁcation task.5\nFor MultiRC, we are unable to replicate the GPT-3\nresults using the Davinci API6 within our evalua-\ntion setup, suggesting differences in the methods\n5Brown et al. (2020) reports 0% accuracy on WIC, which\nimplies 100% accuracy if the classiﬁcation was inverted.\n6https://beta.openai.com/docs/engines/\noverview\n108\n109\n1010\n1011\nParameters\n50\n55\n60\n65\n70\nAvg. Accuracy\nAverage across 14 NLP Tasks (Zero-Shot)\nOPT\nGPT\nFigure 3:\nZero-shot NLP Evaluation Averages.\nAcross a variety of tasks and model sizes, OPT largely\nmatches the reported averages of GPT-3. However, per-\nformance varies greatly per task: see Appendix A.\n108\n109\n1010\n1011\nParameters\n50\n55\n60\n65\n70\n75\nAvg. Accuracy\nAverage across 14 NLP Tasks\nShot\n0\n1\n32\nSeries\nOPT\nGPT\nFigure 4:\nMulti-shot performance.\nOPT perfor-\nmance for one- and few-shot lags behind GPT-3 mod-\nels, but performance depends heavily per task; see Ap-\npendix A.\nof evaluation on this task. For BoolQ and WSC,\nwe note that both OPT and GPT models seem to\nhover around majority-class accuracy, suggesting\nsmall perturbations in probability masses may be\ndominating the evaluations.\nChinchilla (Hoffmann et al., 2022) and Gopher\n(Rae et al., 2021) perform roughly consistently\nwith others for their parameter sizes, while PaLM\n(Chowdhery et al., 2022) generally performs better\nacross all settings, even when controlling for num-\nber of parameters. We speculate the high perfor-\nmance of PaLM comes predominantly from higher\nquality and diversity of pre-training data.\nOne-shot and Few-shot\nAverage multi-shot in-\ncontext performance is shown in Figure 4 (again,\nomitting MultiRC and WIC), with detailed perfor-\nmances shown in Appendix A. Across the average\n\nof all metrics, we ﬁnd that OPT models perform\nsimilarly to GPT-3 models. However, as with zero-\nshot, breaking down these results per task shows\na different story: in the same set of 10 datasets as\nzero-shot, we see similar performance across the\ntwo models. Some of the remaining datasets show\ninconsistent performance with respect to model\nsize for both OPT and GPT-3 models (BoolQ, CB,\nWSC, RTE). In MultiRC, we consistently see un-\nderperformance of OPT models compared to GPT-\n3 models. Similar to our zero-shot evaluation, we\nhypothesize our one- and few-shot evaluation setup\nmay differ signiﬁcantly from Brown et al. (2020).\n3.2\nDialogue\nGiven that LLMs are known to be an integral com-\nponent of modern dialogue models (Adiwardana\net al., 2020; Roller et al., 2021; Thoppilan et al.,\n2022; Rae et al., 2021; Chowdhery et al., 2022), we\nadditionally evaluate OPT-175B on several open\nsource dialogue datasets. In particular, we fol-\nlow Roller et al. (2021), and evaluate on ConvAI2\n(Dinan et al., 2020b), Wizard of Wikipedia (Di-\nnan et al., 2019b), Empathetic Dialogues (Rashkin\net al., 2019), and Blended Skill Talk (Smith et al.,\n2020). We additionally evaluate on the more recent\nWizard of Internet dataset (Komeili et al., 2021).\nWe focus our comparisons primarily against ex-\nisting open source dialogue models including the\nﬁne-tuned BlenderBot 1 (Roller et al., 2021) and\nits pre-training counterpart Reddit 2.7B. We also\ncompare against the ﬁne-tuned R2C2 BlenderBot,\na 2.7B parameter BlenderBot-like model trained by\nShuster et al. (2022).\nWe report Perplexity and Unigram F1 (UF1)\noverlap, following the metrics of the ConvAI2 com-\npetition (Dinan et al., 2020b). To control for dif-\nferent tokenization in each of the models, we nor-\nmalize all perplexities to be in the space of the\nGPT-2 tokenizer (Radford et al., 2019). We also\nnote which models are supervised with respect to\nthese dialogue tasks and which are unsupervised.\nFor OPT-175B, all generations are performed using\ngreedy decoding up to a maximum of 32 tokens.\nWe do not attempt to prompt the model at all except\nfor alternating “Person 1:” and “Person 2:” lines of\ndialogue. The remaining models use the generation\nparameters found in BlenderBot 1.\nResults are shown in Table 2.\nWe see that\nOPT-175B signiﬁcantly outperforms the also-\nunsupervised Reddit 2.7B model on all tasks, and\nperforms competitively with the fully supervised\nBlenderBot 1 model, especially in the ConvAI2\ndataset. On the Wizard-of-Internet dataset, which\nis fully unsupervised for all models, we see that\nOPT-175B obtains the lowest perplexity but still\nhas lower UF1 than the models with Wizard-of-\nWikipedia supervision.\nWe were somewhat surprised that the evaluations\nof the unsupervised OPT-175B model were as com-\npetitive as BlenderBot 1 on the ConvAI2 dataset.\nThis may indicate leakage of the ConvAI2 dataset\ninto the general pre-training corpus or even into the\nvalidation data as evaluated in Table 2. To address\nconcerns of leakage, we searched our pre-training\ncorpus for the ﬁrst conversation in the ConvAI2\ndataset, but we did not ﬁnd any overlap. We addi-\ntionally evaluated OPT-175B on the ConvAI2 hid-\nden test set, which has never been publicly released,\nand achieved 10.7 ppl and .185 UF1, matching the\nperformance of the validation set. Furthermore, we\nevaluated OPT-175B on a subset of the ConvAI2-\nlike MultiSessionChat (MSC) dataset (Xu et al.,\n2021b) and obtained a perplexity of 9.7 and UF1\nof .177, indicating the model is generalizing well\nacross multiple PersonaChat-like datasets. Since\nboth MSC and WoI datasets were released after the\nCommonCrawl snapshot used in pre-training cor-\npus, there is minimal risk of leakage. We conclude\nthat OPT-175B has a strong ability to maintain a\nconsistent persona across conversations, a behav-\nior also highlighted in LaMDA (Thoppilan et al.,\n2022).\n4\nBias & Toxicity Evaluations\nTo understand the potential harm of OPT-175B,\nwe evaluate a series of benchmarks related to hate\nspeech detection, stereotype awareness, and toxic\ncontent generation. While there may be shortcom-\nings in these benchmarks (Blodgett et al., 2021; Ja-\ncobs and Wallach, 2021), these measurements pro-\nvide a ﬁrst step towards understanding the limita-\ntions of OPT-175B. We compare primarily against\nGPT-3 Davinci, as these benchmarks were not yet\navailable to be included in Brown et al. (2020).\n4.1\nHate Speech Detection\nUsing the ETHOS dataset provided in Mollas et al.\n(2020) and instrumented by Chiu and Alexander\n(2021), we measure the ability of OPT-175B to\nidentify whether or not certain English statements\nare racist or sexist (or neither). In the zero-, one-,\n\nPerplexity (↓)\nUnigram F1 (↑)\nModel\nEval\nC2\nWW\nED\nBST\nWoI\nC2\nWW\nED\nBST\nWoI\nReddit 2.7B\nUnsup.\n18.9\n21.0\n11.6\n17.4\n18.0\n.126\n.133\n.135\n.133\n.124\nBlenderBot 1\nSup.\n10.2\n12.5\n9.0\n11.9\n14.7\n.183\n.189\n.192\n.178\n.154\nR2C2 BlenderBot\nSup.\n10.5\n12.4\n9.1\n11.7\n14.6\n.205\n.198\n.197\n.186\n.160\nOPT-175B\nUnsup.\n10.8\n13.3\n10.3\n12.1\n12.0\n.185\n.152\n.149\n.162\n.147\nTable 2: Dialogue Evaluations. OPT-175B, in a fully unsupervised setting, performs competitively against fully\nsupervised models.\nSetup\nDavinci\nOPT-175B\nZero-shot\n.628\n.667\nOne-shot\n.616\n.713\nFew-shot (binary)\n.354\n.759\nFew-shot (multiclass)\n.672\n.812\nTable 3: Hate speech detection. F1 scores of detect-\ning hate speech between Davinci and OPT-175B. OPT-\n175B considerably outperforms Davinci in all settings.\nand few-shot binary cases, the model is presented\nwith text and asked to consider whether the text is\nracist or sexist and provide a yes/no response. In\nthe few-shot multiclass setting, the model is asked\nto provide a yes/no/neither response.\nResults are presented in Table 3. With all of\nour one-shot through few-shot conﬁgurations, OPT-\n175B performs considerably better than Davinci.\nWe speculate this occurs from two sources: (1)\nevaluating via the Davinci API may be bringing\nin safety control mechanisms beyond the original\n175B GPT-3 model used in Brown et al. (2020);\nand (2) the signiﬁcant presence of unmoderated\nsocial media discussions in the pre-training dataset\nhas provided additional inductive bias to aid in such\nclassiﬁcation tasks.\n4.2\nCrowS-Pairs\nDeveloped for masked language models, CrowS-\nPairs (Nangia et al., 2020) is a crowdsourced bench-\nmark aiming to measure intrasentence level biases\nin 9 categories: gender, religion, race/color, sex-\nual orientation, age, nationality, disability, physical\nappearance, and socioeconomic status. Each exam-\nple consists of a pair of sentences representing a\nstereotype, or anti-stereotype, regarding a certain\ngroup, with the goal of measuring model preference\ntowards stereotypical expressions. Higher scores\nindicate higher bias exhibited by a model.\nCategory\nGPT-3\nOPT-175B\nGender\n62.6\n65.7\nReligion\n73.3\n68.6\nRace/Color\n64.7\n68.6\nSexual orientation\n76.2\n78.6\nAge\n64.4\n67.8\nNationality\n61.6\n62.9\nDisability\n76.7\n76.7\nPhysical appearance\n74.6\n76.2\nSocioeconomic status\n73.8\n76.2\nOverall\n67.2\n69.5\nTable 4: CrowS-Pairs evaluation. Lower is better for\nall categories, indicating more fairness. The OPT-175B\nmodel performs worse than Davinci in most categories.\nWhen compared with Davinci in Table 4, OPT-\n175B appears to exhibit more stereotypical biases\nin almost all categories except for religion. Again,\nthis is likely due to differences in training data;\nNangia et al. (2020) showed that Pushshift.io Red-\ndit corpus has a higher incidence rate for stereo-\ntypes and discriminatory text than other corpora\n(e.g. Wikipedia).\nGiven this is a primary data\nsource for OPT-175B, the model may have learned\nmore discriminatory associations, which directly\nimpacts its performance on CrowS-Pairs.\n4.3\nStereoSet\nFollowing Lieber et al. (2021) and Artetxe et al.\n(2021), we use StereoSet (Nadeem et al., 2021)\nto measure stereotypical bias across 4 categories:\nprofession, gender, religion, and race. In addition\nto intrasentence measurement (similar to CrowS-\nPairs), StereoSet includes measurement at the inter-\nsentence level to test a model’s ability to incorpo-\nrate additional context. To account for a potential\ntrade-off between bias detection and language mod-\neling capability, StereoSet includes two metrics:\n\nCategory\nDavinci\nOPT-175B\nProf.\nLMS (↑)\n78.4\n74.1\nSS (↓)\n63.4\n62.6\nICAT (↑)\n57.5\n55.4\nGend.\nLMS (↑)\n75.6\n74.0\nSS (↓)\n66.5\n63.6\nICAT (↑)\n50.6\n53.8\nReli.\nLMS (↑)\n80.8\n84.0\nSS (↓)\n59.0\n59.0\nICAT (↑)\n66.3\n68.9\nRace\nLMS (↑)\n77.0\n74.9\nSS (↓)\n57.4\n56.8\nICAT (↑)\n65.7\n64.8\nOverall\nLMS (↑)\n77.6\n74.8\nSS (↓)\n60.8\n59.9\nICAT (↑)\n60.8\n60.0\nTable 5: StereoSet Evaluations. Davinci and OPT-\n175B perform similarly across all evaluations.\nLanguage Modeling Score (LMS) and Stereotype\nScore (SS), which are then combined to form the\nIdealized Context Association Test score (ICAT).\nUnlike Lieber et al. (2021), we normalize scores\nby token count, rather than character count, which\nthey report improves metrics for several models.\nResults are shown in Table 5.\nWe see that\nDavinci and OPT-175B exhibit similar scores on\naggregate (overall ICAT is very close between the\ntwo). In particular, Davinci outperforms in the\nareas of profession and race, while OPT-175B out-\nperforms in the areas of Gender and Religion. OPT-\n175B performs better across the board on the SS\nmetric, while Davinci generally outperforms on the\nLMS metric.\n4.4\nRealToxicityPrompts\nWe evaluate the tendency of OPT-175B to respond\nwith toxic language via the RealToxicityPrompts\n(Gehman et al., 2020) dataset. Following PaLM\n(Chowdhery et al., 2022), we sample 25 genera-\ntions of 20 tokens using nucleus sampling (Holtz-\nman et al., 2020) (p = 0.9) for each of 10, 000\nrandomly sampled prompts from RTP, and report\nmean toxicity probabilities of the continuations,\nstratiﬁed across bucketed toxicities of the original\nprompts. For comparison, we report bucketed toxi-\ncity rates from Davinci and PaLM.\nResults are shown in Figure 5. Overall, we see\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrompt Toxicity Probability (Binned)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nToxicity Probability of Continuation (TPC)\nToxicity Probability of Prompt (TPP)\nOPT 175B\nDavinci\nPaLM\nFigure 5: RealToxicityPompts.\nOPT-175B is more\nlikely to generate toxic responses than either Davinci\nor PaLM. Consistent with prior work, toxicity rates in-\ncrease as prompt toxicity increases.\nthat OPT-175B has a higher toxicity rate than ei-\nther PaLM or Davinci. We also observe that all\n3 models have increased likelihood of generating\ntoxic continuations as the toxicity of the prompt\nincreases, which is consistent with the observations\nof Chowdhery et al. (2022). As with our exper-\niments in hate speech detection, we suspect the\ninclusion of unmoderated social media texts in the\npre-training corpus raises model familiarity with,\nand therefore propensity to generate and detect,\ntoxic text. This strong awareness of toxic language\nmay or may not be desirable depending on the\nspeciﬁc requirements of downstream applications.\nFuture applications of OPT-175B should consider\nthis aspect of the model, and take additional miti-\ngations, or avoid usage entirely as appropriate.\n4.5\nDialogue Safety Evaluations\nFinally, we compare OPT-175B on two Dialogue\nSafety evaluations. The ﬁrst, SaferDialogues (Ung\net al., 2021), measures the ability to recover from\nexplicit safety failures, usually in the form of apol-\nogizing or recognizing its mistake. The second, the\nSafety Bench Unit Tests (Dinan et al., 2021), mea-\nsures how unsafe a model’s response is, stratiﬁed\nacross 4 levels of topic sensitivity: Safe, Realis-\ntic, Unsafe, and Adversarial. As with the other\ndialogue evaluations (Section 3.2), we compare to\nseveral existing open source dialogue models.\nResults for both experiments are shown in Ta-\nble 6. We observe that OPT-175B has similar per-\nformance as the Reddit 2.7B model across both\nSaferDialogues and the Unit Tests, with OPT-175B\nperforming marginally better in the Safe and Adver-\nsarial settings. Consistent with Roller et al. (2021)\n\nSafe. Dia.\nUnit Tests (↓)\nModel\nPPL\nF1\nSa\nRe\nUn\nAd\nReddit 2.7B\n16.2\n.140\n.300\n.261\n.450\n.439\nBlenderBot 1\n12.4\n.161\n.028\n.150\n.250\n.194\nR2C2 BlenderBot\n13.8\n.160\n.022\n.133\n.289\n.222\nOPT-175B\n14.7\n.141\n.033\n.261\n.567\n.283\nTable 6: Dialogue Responsible AI evaluations. OPT-\n175B is roughly on par with the Reddit 2.7B model, but\nperforms worse in the Unsafe setting.\nand Xu et al. (2020), we ﬁnd that the models ﬁne-\ntuned on curated dialogue datasets (BlenderBot 1,\nR2C2) have overall lower toxicity. We conclude\nthat future experimentation of OPT-175B for dia-\nlogue should contain explicit ﬁne-tuning on curated\ndatasets in order to improve the safety proﬁle.\n5\nLimitations\nIn Sections 3.1 and 4, we carried out extensive\nevaluation of all released models at varying scales.\nWe saw parity in performance for standard evalu-\nation datasets used in the GPT-3 models. More-\nover, we performed safety, bias, and inclusion eval-\nuations, again seeing largely comparable perfor-\nmance with some variations in toxicity and hate\nspeech detection. However, such evaluations may\nnot fully characterize the complete limitations of\nthese models. In general, we qualitatively observe\nthat OPT-175B suffers from the same limitations\nnoted in other LLMs (Brown et al., 2020; Lieber\net al., 2021; Thoppilan et al., 2022; Rae et al., 2021;\nSmith et al., 2022; Chowdhery et al., 2022; Bender\net al., 2021).\nIn particular, we found OPT-175B does not work\nwell with declarative instructions or point-blank\ninterrogatives. Prompting with such instructions\ntends to produce a simulation of a dialogue begin-\nning with such an instruction, rather than an execu-\ntion of the instruction. Future work into instruction\nlearning, in the vein of InstructGPT (Ouyang et al.,\n2022), may alleviate these limitations.\nOPT-175B also tends to be repetitive and can eas-\nily get stuck in a loop. While sampling can reduce\nthe incidence rate of repetitive behavior (Holtz-\nman et al., 2020), we anecdotally found it did not\neliminate it entirely when only one generation is\nsampled. Future work may wish to incorporate\nmore modern strategies for reducing repetition and\nimproving diversity, such as unlikelihood training\n(Welleck et al., 2020) or best-ﬁrst decoding (Meis-\nter et al., 2020).\nSimilar to other LLMs, OPT-175B can produce\nfactually incorrect statements (Adiwardana et al.,\n2020; Brown et al., 2020; Roller et al., 2021; Rae\net al., 2021; Chowdhery et al., 2022; Thoppilan\net al., 2022). This can be particularly harmful in\napplications where information accuracy is critical,\nsuch as healthcare and scientiﬁc discovery (Wei-\ndinger et al., 2021b). Recently, several efforts have\nreported that retrieval-augmented models can im-\nprove factual correctness of LLMs (Lewis et al.,\n2020; Komeili et al., 2021; Thoppilan et al., 2022;\nBorgeaud et al., 2021; Shuster et al., 2022; Nakano\net al., 2021). We believe OPT-175B will also bene-\nﬁt from retrieval-augmentation in future iterations.\nAs shown in Section 4, we also ﬁnd OPT-175B\nhas a high propensity to generate toxic language\nand reinforce harmful stereotypes, even when pro-\nvided with a relatively innocuous prompt (Gehman\net al., 2020), and adversarial prompts are trivial to\nﬁnd (Dinan et al., 2021). There has been a great\ndeal of work on mitigations for toxicity and bi-\nases (Dathathri et al., 2019; Dinan et al., 2019a;\nSheng et al., 2019; Dinan et al., 2020a; Liu et al.,\n2019a; Krause et al., 2020; Xu et al., 2020; Liang\net al., 2021; Dinan et al., 2021; Xu et al., 2021a;\nDhamala et al., 2021; Schick et al., 2021; Ouyang\net al., 2022). Depending on downstream applica-\ntions, future uses of OPT-175B may need to employ\nthese or novel mitigation approaches, especially be-\nfore any real world deployment. Given our primary\ngoal as a replication of GPT-3, we choose not to\napply these mitigations in this ﬁrst release.\nIn summary, we still believe this technology is\npremature for commercial deployment. Despite\nincluding data sheets and model cards, we believe\nmore scrutiny should be afforded to the training\ndata with additional data characterization and se-\nlection criteria in order to use data responsibly. The\ncurrent practice is to feed the model with as much\ndata as possible and minimal selection within these\ndatasets. Despite having comprehensive evalua-\ntions, we would ideally have more streamlined and\nconsistent evaluation setups to ensure replicability\nand reproducibility of evaluation scenarios. Dif-\nferences in prompting styles and number of shots\nfor in-context learning could create variations that\nlead to different results. We hope that the public\nrelease of the OPT models will enable many more\nresearchers to work on these important issues.\n\n6\nConsiderations for Release\nFollowing the recommendations for individual re-\nsearchers generated by the Partnership for AI,7\nalong with the governance guidance outlined by\nNIST,8 we are disclosing all of the details in-\nvolved in training OPT-175B through our log-\nbook,9 our code, and providing researchers access\nto model weights for OPT-175B, along with a suite\nof smaller baselines mirroring the setup for OPT-\n175B. We aim to be fully accountable for the devel-\nopment lifecycle of OPT-175B, and only through\nincreasing transparency around LLM development\ncan we start understanding the limitations and risks\nof LLMs before broader deployment occurs.\nBy sharing a detailed account of our day-to-day\ntraining process, we disclose not only how much\ncompute was used to train the current version of\nOPT-175B, but also the human overhead required\nwhen underlying infrastructure or the training pro-\ncess itself becomes unstable at scale. These details\nare generally omitted from previous publications,\nlikely due to the inability to fully ablate changes\nmade mid-ﬂight (without drastically increasing the\ncompute budget). We hope that by revealing how\ncertain ad-hoc design decisions were made, we can\nimprove upon these practices in the future, and col-\nlectively increase the experimental robustness in\ndeveloping models at this scale.\nOutside of these notes, the metaseq codebase\nitself is the ﬁnal source of truth in many of our\nimplementation details. By releasing our develop-\nment codebase, we aim to shed light on any imple-\nmentation detail that may have been omitted from\nbeing explicitly enumerated in this paper, as it is\neither considered a detail of standard practice in\nthe ﬁeld, or is simply a detail we failed to account\nfor. This current codebase is also the only known\nopen-source implementation of training a decoder-\nonly transformer that is ≥175B parameters without\nthe use of pipeline paralellism on NVIDIA GPUs.\nTo enable experimentation at 175B scale, we are\nproviding researchers with direct access to the pa-\nrameters of OPT-175B. The reasoning here is two-\nfold: enable Responsible AI research into LLMs\nwhile simultaneously reducing the environmental\n7https://partnershiponai.org/paper/\nresponsible-publication-recommendations/\n8https://nvlpubs.nist.gov/nistpubs/\nSpecialPublications/NIST.SP.1270.pdf\n9https://github.com/facebookresearch/\nmetaseq/blob/main/projects/OPT/\nchronicles/OPT175B_Logbook.pdf\nimpact of pursuing research at this scale. There is a\ngrowing body of work detailing ethical and social\nrisks from deploying language models with emer-\ngent capabilities at scale (Weidinger et al., 2021a;\nBommasani et al., 2021; Dinan et al., 2021; Kenton\net al., 2021). By limiting access to OPT-175B to\nthe research community with a non-commercial\nlicense, we aim to focus development efforts on\nquantifying the limitations of the LLMs ﬁrst, be-\nfore broader commercial deployment occurs.\nFurthermore, there exists signiﬁcant compute\nand carbon cost to reproduce models of this size.\nWhile OPT-175B was developed with an estimated\ncarbon emissions footprint (CO2eq) of 75 tons,10\nGPT-3 was estimated to use 500 tons (Patterson\net al., 2021), while Gopher required 380 tons (Rae\net al., 2021). These estimates are not universally re-\nported, and the accounting methodologies for these\ncalculations are also not standardized. In addition,\nmodel training is only one component of the over-\nall carbon footprint of AI systems; we must also\nconsider experimentation and eventual downstream\ninference cost, all of which contribute to the grow-\ning energy footprint of creating large-scale models\n(Wu et al., 2022). By releasing our logbook, we\nhope to highlight the gap between a theoretical car-\nbon cost estimate that assumes no hardware failures\nor training instabilities, versus one that aims to in-\nclude the entire LLM development lifecycle. We\nneed to understand the manufacturing (or embod-\nied) carbon of these systems (Gupta et al., 2021)\nas they grow increasingly more complex, and we\nhope that our paper can help future work in deﬁn-\ning additional factors to consider when measuring\nthe impact of scale on the environment.\nSimilarly, by producing a set of baselines across\na wide range of scales, we hope to enable the\nbroader research community to study the impact\nand limitations of these models with respect to\nscale alone. As reported in Hoffmann et al. (2022),\nmany of these LLMs may have been under-trained\nas a function of the amount of training data used,\nwhich implies that incorporating more data and con-\ntinuing to train these baseline models may continue\nto improve performance. There is also evidence\nthat step-function changes in capabilities may oc-\ncur at a scale that is much smaller than 175B (Wei\net al., 2021), indicating a need to examine a wider\nrange of scales for different research applications.\n10With ablations, baselines and downtime, our own esti-\nmates of total cost is roughly 2× higher.\n\n7\nRelated Work\nSince the publication of the Transformer architec-\nture (Vaswani et al., 2017) and BERT (Devlin et al.,\n2019), the ﬁeld of NLP has experienced a massive\nshift towards the use of LLMs with self-supervised\npre-training. Multiple masked langauge models,\nincluding T5 (Raffel et al., 2020) and Megatron-\nLM (Shoeybi et al., 2019), have shown consistent\nimprovements through scale. These scaling gains\ncome not only from growing the total number of\nparameters in the models, but also the amount and\nquality of pre-training data (Liu et al., 2019b; Hoff-\nmann et al., 2022).\nAuto-regressive language models (Mikolov et al.,\n2009) have seen the largest growth in model size,\nfrom 117M parameters (Radford et al., 2018) to\nover 500B parameters (Smith et al., 2022; Chowd-\nhery et al., 2022). The resulting massive improve-\nment in generative ﬂuency and quality was ﬁrst\ncharacterized in GPT-2 (Radford et al., 2019) and\nfurther improved with GPT-3 (Brown et al., 2020)\nand later models. Although a variety of very large\n(over 100B parameters) generative models have\nnow been trained (Lieber et al., 2021; Rae et al.,\n2021; Thoppilan et al., 2022; Smith et al., 2022;\nChowdhery et al., 2022), they are all closed source\nand accessible only internally or via paid API ser-\nvices. There are a few notable efforts towards open\nsourcing LLMs from non-proﬁt research organiza-\ntions including EleutherAI (Black et al., 2022) and\nBigScience.11 These models differ from the OPT\nmodels in pre-training data, target languages and\nmodel scale, making it possible for the community\nto compare different pre-training strategies.\nSince Brown et al. (2020), the primary evalu-\nation criterion for LLMs has been prompt-based\n(Black et al., 2022; Rae et al., 2021; Chowdhery\net al., 2022), as is also performed in this paper.\nThis is largely due to the convenience of evaluat-\ning on many tasks without specialized task-speciﬁc\nﬁne-tuning. Prompting itself has a long history:\ncloze evaluations go back several decades (Cham-\nbers and Jurafsky, 2008; Mostafazadeh et al., 2016).\nMore recently, prompting or masked inﬁlling has\nbeen used to probe models for knowledge (Petroni\net al., 2019) or perform a variety of NLP tasks\n(Radford et al., 2019; Brown et al., 2020). There\nhas also been work on eliciting prompting behav-\nior in smaller models (Schick and Schütze, 2020;\n11https://huggingface.co/bigscience/\ntr11-176B-ml-logs/tensorboard\nGao et al., 2021b; Li and Liang, 2021; Lester et al.,\n2021; Scao and Rush, 2021), improving the ﬂexi-\nbility of prompting (Shin et al., 2020), and under-\nstanding why and how prompting works (Liu et al.,\n2021; Min et al., 2022).\nRecent efforts have shown gains by ﬁne-tuning\nmodels to directly respond to instruction-style\nprompting (Wei et al., 2021; Min et al., 2021; Sanh\net al., 2021; Ouyang et al., 2022). However, ef-\nfective prompt engineering remains an open re-\nsearch challenge. Results vary signiﬁcantly and\nunpredictably with the selection of the prompt (Lu\net al., 2021), and models do not seem to understand\nthe prompts as fully as we expect (Webson and\nPavlick, 2021). Furthermore, it is challenging to\nwrite prompts without a development set, which\nleads to questions about the extent to which we\nare actually achieving zero- or few-shot learning in\npractice (Perez et al., 2021). We do not attempt to\naddress these concerns of prompting, and instead\nonly aim to provide evaluation of OPT-175B in ex-\nisting settings. However, we hope the full release of\nOPT-175B will enable others to better study these\nchallenges in the future.\n8\nConclusion\nIn this technical report, we introduced OPT, a col-\nlection of auto-regressive language models ranging\nin size from 125M to 175B parameters. Our goal\nwas to replicate the performance and sizes of the\nGPT-3 class of models, while also applying the\nlatest best practices in data curation and training\nefﬁciency. We described training details, evaluated\nperformance in a number of NLP and dialogue set-\ntings, and characterized behaviors with respect to\nbias, toxicity and hate speech. We also described\nmany other limitations the models have, and dis-\ncussed a wide set of considerations for responsibly\nreleasing the models. We believe the entire AI\ncommunity would beneﬁt from working together\nto develop guidelines for responsible LLMs, and\nwe hope that broad access to these types of models\nwill increase the diversity of voices deﬁning the\nethical considerations of such technologies.\nAcknowledgements\nWe would like to thank Scott Jeschonek, Giri Anan-\ntharaman, Diego Sarina, Joaquin Colombo, Chris\nBray, Stephen Roylance, Kalyan Saladi, Shubho\nSengupta, and Brian O’Horo for helping to remove\ninfrastructure blockers along the way; Percy Liang,\n\nRishi Bommasani, and Emily Dinan for discus-\nsions on responsible release practices; Carole-Jean\nWu for discussions on sustainability and carbon\nfootprint considerations; Srini Iyer, Ramakanth Pa-\nsunuru, and Shruti Bhosale for previous contribu-\ntions to evaluations; Benjamin Lefaudeux, Geeta\nChauhan, Natalia Gimelshein, Horace He, and Sam\nGross for discussions on performance improvement\nwork; Emily Dinan, Carole-Jean Wu, Daniel McK-\ninnon, and Mark Tygert for feedback on this draft;\nAntoine Bordes, Joelle Pineau, Mary Williamson,\nNecip Fazil Ayan, Armand Joulin, Sergey Edunov,\nMelanie Kambadur, Zornitsa Kozareva, Ves Stoy-\nanov, Vitaliy Liptchinsky, Rahul Iyer, Jing Xu, Ja-\nson Weston, and many others for supporting this\nproject internally.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020.\nTowards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\nGiri Anantharaman, Xian Li, Shuohui Chen, Halil\nAkin, Mandeep Baines, Louis Martin, Xing Zhou,\nPunit Singh Koura, Brian O’Horo, Jeff Wang, Luke\nZettlemoyer, Mona T. Diab, Zornitsa Kozareva, and\nVes Stoyanov. 2021.\nEfﬁcient large scale lan-\nguage modeling with mixtures of experts.\nCoRR,\nabs/2112.10684.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. CoRR, abs/2001.08435.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.\nOn the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language.\nPro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(05):7432–7439.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. Gpt-neox-20b: An open-\nsource autoregressive language model.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyp-\ning Norwegian salmon: An inventory of pitfalls in\nfairness benchmark datasets. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1004–1015, Online. As-\nsociation for Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, Erik Brynjolfsson, Shya-\nmal Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri Chatterji, Annie S. Chen, Kathleen Creel,\nJared Quincy Davis, Dorottya Demszky, Chris Don-\nahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie,\nKaran Goel, Noah D. Goodman, Shelby Grossman,\nNeel Guha, Tatsunori Hashimoto, Peter Henderson,\nJohn Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Juraf-\nsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark S. Krass, Ranjay Krishna, Rohith Kudi-\ntipudi, and et al. 2021.\nOn the opportunities and\nrisks of foundation models. CoRR, abs/2108.07258.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared\nD\nKaplan,\nPrafulla\nDhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNathanael Chambers and Dan Jurafsky. 2008. Unsuper-\nvised learning of narrative event chains. In Proceed-\nings of ACL-08: HLT, pages 789–797, Columbus,\nOhio. Association for Computational Linguistics.\nKe-Li Chiu and Rohan Alexander. 2021.\nDetect-\ning hate speech with gpt-3.\narXiv preprint\narXiv:2103.12407.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\n\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018.\nThink you have solved question\nanswering?\ntry arc, the AI2 reasoning challenge.\nCoRR, abs/1803.05457.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: A simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL).\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya\nKrishna, Yada Pruksachatkun, Kai-Wei Chang, and\nRahul Gupta. 2021.\nBold: Dataset and metrics\nfor measuring biases in open-ended language gen-\neration. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 862–872.\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021.\nAnticipating safety issues\nin e2e conversational ai: Framework and tooling.\narXiv preprint arXiv:2107.03451.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020a.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 8173–8188, On-\nline. Association for Computational Linguistics.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019a. Build it break it ﬁx it for\ndialogue safety: Robustness from adversarial human\nattack. arXiv preprint arXiv:1908.06083.\nEmily Dinan,\nVarvara Logacheva,\nValentin\nMa-\nlykh, Alexander Miller, Kurt Shuster, Jack Ur-\nbanek, Douwe Kiela, Arthur Szlam, Iulian Serban,\nRyan Lowe, Shrimai Prabhumoye, Alan W. Black,\nAlexander Rudnicky, Jason Williams, Joelle Pineau,\nMikhail Burtsev, and Jason Weston. 2020b.\nThe\nsecond conversational intelligence challenge (Con-\nvAI2). In The NeurIPS ’18 Competition, pages 187–\n208, Cham. Springer International Publishing.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019b. Wiz-\nard of Wikipedia: Knowledge-powered conversa-\ntional agents. In Proceedings of the International\nConference on Learning Representations.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021a. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021b.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 3816–3830. Association for Computa-\ntional Linguistics.\nTimnit\nGebru,\nJamie\nMorgenstern,\nBriana\nVec-\nchione,\nJennifer\nWortman\nVaughan,\nHanna\nWallach,\nHal Daumé III, and Kate Crawford.\n2021.\nDatasheets for datasets.\nCommun. ACM,\n64(12):86–92.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020.\nRealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nUdit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse,\nHsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and\nCarole-Jean Wu. 2021. Chasing carbon: The elu-\nsive environmental footprint of computing. IEEE In-\nternational Symposium on High-Performance Com-\nputer Architecture (HPCA 2021).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\n\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2020. The curious case of neural text degener-\nation. ArXiv, abs/1904.09751.\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measure-\nment and fairness. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 375–385, New York, NY,\nUSA. Association for Computing Machinery.\nZachary Kenton, Tom Everitt, Laura Weidinger, Ia-\nson Gabriel, Vladimir Mikulik, and Geoffrey Irv-\ning. 2021. Alignment of language agents. CoRR,\nabs/2103.14659.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2021.\nInternet-augmented dialogue generation.\nCoRR, abs/2107.07566.\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shaﬁq Joty, Richard\nSocher, and Nazneen Fatema Rajani. 2020. GEDI:\nGenerative discriminator guided sequence genera-\ntion. arXiv preprint arXiv:2009.06367.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. CoRR, abs/2104.08691.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical Formalizations of\nCommonsense Reasoning, volume 46, page 47.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning:\nOptimizing Continuous Prompts for Generation.\npages 4582–4597.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency,\nand Ruslan Salakhutdinov. 2021.\nTowards under-\nstanding and mitigating social biases in language\nmodels.\nIn International Conference on Machine\nLearning, pages 6565–6576. PMLR.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021.\nJurassic-1: Technical details and\nevaluation. Technical report, AI21 Labs.\nHaochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao\nLiu, and Jiliang Tang. 2019a. Does gender matter?\ntowards fairness in dialogue systems. arXiv preprint\narXiv:1910.10486.\nHaokun Liu, William Huang, Dhara Mungra, and\nSamuel R. Bowman. 2020. Precise task formaliza-\ntion matters in Winograd schema evaluations.\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8275–8280, Online. Association for Computa-\ntional Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021.\nWhat\nmakes good in-context examples for gpt-3? CoRR,\nabs/2101.06804.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017.\nFixing\nweight decay regularization in adam.\nCoRR,\nabs/1711.05101.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021.\nFantastically\nordered prompts and where to ﬁnd them: Overcom-\ning few-shot prompt order sensitivity.\nClara Meister, Tim Vieira, and Ryan Cotterell. 2020.\nBest-ﬁrst beam search. Transactions of the Associa-\ntion for Computational Linguistics, 8:795–809.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, et al. 2017.\nMixed precision\ntraining. arXiv preprint arXiv:1710.03740.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. CoRR, abs/1809.02789.\nTomas Mikolov, Jiri Kopecky, Lukas Burget, Ondrej\nGlembek, et al. 2009.\nNeural network based lan-\nguage models for highly inﬂective languages.\nIn\n2009 IEEE international conference on acoustics,\nspeech and signal processing, pages 4725–4728.\nIEEE.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn\nin context.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022.\nRethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2018.\nModel cards for model reporting.\nCoRR, abs/1810.03993.\n\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Kar-\nlos, and Grigorios Tsoumakas. 2020.\nETHOS:\nan online hate speech detection dataset.\nCoRR,\nabs/2006.08328.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe,\nDevi Parikh,\nDhruv Batra,\nLucy Vander-\nwende, Pushmeet Kohli, and James F. Allen. 2016.\nA corpus and evaluation framework for deeper\nunderstanding of commonsense stories.\nCoRR,\nabs/1604.01696.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet:\nMeasuring stereotypical bias in pre-\ntrained language models. In Association for Com-\nputational Linguistics (ACL).\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback.\narXiv preprint\narXiv:2112.09332.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R Bowman. 2020.\nCrows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. arXiv preprint arXiv:2010.00133.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. A conversational paradigm\nfor program synthesis. arXiv preprint.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\narXiv preprint arXiv:2104.10350.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. Ad-\nvances in Neural Information Processing Systems,\n34.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning. Technical\nreport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant M. Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake A. Hecht-\nman, Laura Weidinger, Iason Gabriel, William S.\nIsaac, Edward Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis,\nKoray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. CoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21:1–67.\nAnand Rajaraman and Jeffrey David Ullman. 2011.\nMining of massive datasets. Cambridge University\nPress.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019.\nTowards empathetic open-\ndomain conversation models: A new benchmark and\ndataset.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5370–5381, Florence, Italy. Association\nfor Computational Linguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021.\nRecipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artiﬁcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 8732–\n8740. AAAI Press.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\nGao, Tali Bers, Thomas Wolf, and Alexander M.\nRush. 2021.\nMultitask prompted training enables\nzero-shot task generalization.\nTeven Le Scao and Alexander M. Rush. 2021. How\nmany data points is a prompt worth?\npages 2627–\n2636.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. CoRR, abs/2009.07118.\nTimo Schick, Sahana Udupa, and Hinrich Schütze.\n2021. Self-diagnosis and self-debiasing: A proposal\nfor reducing corpus-based bias in nlp. Transactions\nof the Association for Computational Linguistics,\n9:1408–1424.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts.\npages 4222–\n4235.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason We-\nston. 2022.\nLanguage models that seek for\nknowledge: Modular search & generation for di-\nalogue and prompt completion.\narXiv preprint\narXiv:2203.13224.\nEric Smith, Mary Williamson, Kurt Shuster, Jason We-\nston, and Y-Lan Boureau. 2020. Can you put it all\ntogether: Evaluating conversational agents’ ability\nto blend skills. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics. ACL.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022.\nUsing deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, abs/2201.11990.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nTrieu H. Trinh and Quoc V. Le. 2018.\nA sim-\nple method for commonsense reasoning.\nCoRR,\nabs/1806.02847.\nMegan Ung, Jing Xu, and Y-Lan Boureau. 2021. Safer-\ndialogues: Taking feedback gracefully after conver-\nsational safety failures. ArXiv, abs/2110.07518.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems.\nAlex Wang,\nYada Pruksachatkun,\nNikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts? arXiv preprint arXiv:2109.01247.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2021.\nFinetuned\nlanguage models are zero-shot learners.\nCoRR,\nabs/2109.01652.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\n\nHaas, Laura Rimell, Lisa Anne Hendricks, William\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021a.\nEthical and social risks of harm\nfrom language models.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021b. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training.\nIn\nInternational Conference on Learning Representa-\ntions.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, Michael Gschwind, Anurag Gupta,\nMyle Ott, Anastasia Melnikov, Salvatore Candido,\nDavid Brooks, Geeta Chauhan, Benjamin Lee,\nHsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Ba-\nlandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim\nHazelwood. 2022.\nSustainable AI: environmental\nimplications, challenges and opportunities. In Pro-\nceedings of the Conference on Machine Learning\nand Systems.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots.\narXiv preprint\narXiv:2010.07079.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason We-\nston, and Emily Dinan. 2021a. Bot-adversarial dia-\nlogue for safe conversational agents. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2950–2968,\nOnline. Association for Computational Linguistics.\nJing Xu, Arthur Szlam, and Jason Weston. 2021b. Be-\nyond goldﬁsh memory:\nLong-term open-domain\nconversation. arXiv preprint arXiv:2107.07567.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019.\nHellaswag: Can\na machine really ﬁnish your sentence?\nIn Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4791–4800. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. CoRR, abs/1506.06724.\n\nA\nAdditional Evaluations\n.\n108\n109\n1010\n1011\n1012\n30\n40\n50\n60\n70\n80\nAccuracy\nHellaSwag\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nStoryCloze\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nPIQA\n108\n109\n1010\n1011\n1012\n40\n45\n50\n55\n60\n65\n70\nARC (Easy)\n108\n109\n1010\n1011\n1012\n30\n35\n40\n45\n50\nAccuracy\nARC (Challenge)\n108\n109\n1010\n1011\n1012\n30\n35\n40\n45\n50\n55\nOpenBookQA\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\n80\nWinogrande\n108\n109\n1010\n1011\n1012\n60\n65\n70\n75\n80\n85\n90\nWinograd\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\n80\n85\nAccuracy\nBoolQ\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\nCB\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\n90\nCOPA\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\n60\nWIC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\n75\n80\n85\n90\nAccuracy\nWSC\n108\n109\n1010\n1011\n1012\nParameters\n5\n10\n15\n20\n25\nMultiRC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\nRTE\n108\n109\n1010\n1011\n1012\nParameters\n70\n75\n80\n85\n90\nReCoRD\nOPT\nGPT\nPaLM\nChinchilla\nGopher\nEleuther\nJurassic\nFigure 6: Zero-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons where available.\nWe ﬁnd that across most tasks, GPT-3 models and OPT models perform similarly, but some tasks display highly\nerratic behavior.\n\n108\n109\n1010\n1011\n1012\n30\n40\n50\n60\n70\n80\nAccuracy\nHellaSwag\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\nStoryCloze\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nPIQA\n108\n109\n1010\n1011\n1012\n40\n45\n50\n55\n60\n65\n70\n75\nARC (Easy)\n108\n109\n1010\n1011\n1012\n25\n30\n35\n40\n45\n50\nAccuracy\nARC (Challenge)\n108\n109\n1010\n1011\n1012\n35\n40\n45\n50\n55\n60\n65\nOpenBookQA\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\nWinogrande\n108\n109\n1010\n1011\n1012\n60\n65\n70\n75\n80\n85\n90\nWinograd\n108\n109\n1010\n1011\n1012\n45\n50\n55\n60\n65\n70\n75\nAccuracy\nBoolQ\n108\n109\n1010\n1011\n1012\n0\n20\n40\n60\n80\nCB\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\n90\nCOPA\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\nWIC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\n75\nAccuracy\nWSC\n108\n109\n1010\n1011\n1012\nParameters\n5\n10\n15\n20\n25\n30\nMultiRC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\nRTE\n108\n109\n1010\n1011\n1012\nParameters\n70\n75\n80\n85\n90\nReCoRD\nShot\n0\n1\n32\nSeries\nOPT\nGPT\nFigure 7: Multishot-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons to the\nGPT-3 reported performance. As with zero-shot, performance is roughly similar for most tasks, with some tasks\ndemonstrating erratic behavior.\n\nB\nContributions\nPre-training\n• Initial planning: Susan Zhang\n• Training infrastructure and initial ablations: Naman Goyal, Myle Ott, Stephen Roller, Sam Shleifer,\nSusan Zhang\n• Training efﬁciency: Naman Goyal, Myle Ott, Sam Shleifer\n• Data curation and deduplication: Shuhoi Chen, Myle Ott, Stephen Roller\n• Training and monitoring OPT-175B: Mikel Artetxe, Moya Chen, Naman Goyal, Punit Singh Koura,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Stephen Roller, Susan Zhang\n• Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang\nEvaluations\n• NLP: Xian Li, Xi Victoria Lin, Todor Mihaylov, Stephen Roller, Anjali Sridhar\n• Dialogue: Stephen Roller\n• Responsible AI Evaluations: Punit Singh Koura, Stephen Roller, Tianlu Wang\nPaper writing:\nMoya Chen, Stephen Roller, Luke Zettlemoyer, Susan Zhang\nCode release preparation:\nChristopher Dewan, Susan Zhang\nResponsible AI conduct:\nMona Diab, Susan Zhang\nC\nDatasheet\nWe follow the recommendations of Gebru et al. (2021) and provide a data card for the dataset used to\ntrain the OPT models.\nC.1\nMotivation\n• For what purpose was the dataset created? Was there a speciﬁc task in mind? Was there a\nspeciﬁc gap that needed to be ﬁlled? Please provide a description. The pre-training data for\ntraining the OPT-175B model was created by a union of ﬁve datasets, including three datasets used\nby RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io\nReddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021).\nThese purpose of creating this dataset was to pre-train the language model on a broad corpus of text,\nwith emphasis on human-generated text.\n• Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)? Meta AI.\n• Who funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number. Meta AI.\n• Any other comments? No.\n\nC.2\nComposition\n• What do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description. The instances are\ntextual documents. The overall dataset is composed from a union of the following datasets:\n– BookCorpus (Zhu et al., 2015) consists of more than 10K unpublished books\n– CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data ﬁltered to match the\nstory-like style of Winograd schemas\n– The Pile (Gao et al., 2021a) from which the following was included:\n* Pile-CC\n* OpenWebText2\n* USPTO\n* Project Gutenberg\n* OpenSubtitles\n* Wikipedia\n* DM Mathematics\n* HackerNews\n– Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021).\n– CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n• How many instances are there in total (of each type, if appropriate)? The training data contains\n180B tokens corresponding to 800 GB of data.\n• Does the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? If the dataset is a sample, then what is the larger set? Is the\nsample representative of the larger set (e.g., geographic coverage)? If so, please describe how\nthis representativeness was validated/veriﬁed. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were\nwithheld or unavailable). The CC-stories dataset contains a subset of CommonCrawl data ﬁltered\nto match the story-like style of Winograd schemas. The remainder of the dataset was collected from\nthe above sources, reformatted, and deduplicated.\n• What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description. Each instance consists of raw text data.\n• Is there a label or target associated with each instance? If so, please provide a description. No.\n• Is any information missing from individual instances? If so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not\ninclude intentionally removed information, but might include, e.g., redacted text. No.\n• Are relationships between individual instances made explicit (e.g., users’ movie ratings, social\nnetwork links)? If so, please describe how these relationships are made explicit. There are no\nexplicit relationships between individual instances.\n• Are there recommended data splits (e.g., training, development/validation, testing)? If so,\nplease provide a description of these splits, explaining the rationale behind them. We hold out\na random validation set of approximately 200MB from the pretraining data, sampled proportionally\nto each dataset’s size in the pretraining corpus.\n\n• Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a\ndescription. Outside of naturally occurring duplication from potential overlaps between the datasets,\nthere are no other redundancies, errors, or sources of noise that we add.\n• Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? It’s self-contained.\n• Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? If so, please describe why. Parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety.\n• Does the dataset relate to people? If not, you may skip the remaining questions in this section.\nSome documents of this data relate to people, such as news articles, Wikipedia descriptions, etc.\n• Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how\nthese subpopulations are identiﬁed and provide a description of their respective distributions\nwithin the dataset. No, the dataset does not explicitly include subpopulation identiﬁcation.\n• Any other comments? No.\nC.3\nCollection Process\n• How was the data associated with each instance acquired? Was the data directly observ-\nable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/ derived from other data (e.g., part-of-speech tags, model-based guesses for age or\nlanguage)? If data was reported by subjects or indirectly inferred/derived from other data,\nwas the data validated/veriﬁed? If so, please describe how. N/A. The dataset is a union of ﬁve\npublicly available datasets.\n• What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\nsensor, manual human curation, software program, software API)? How were these mecha-\nnisms or procedures validated? The data was downloaded from the internet.\n• If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with speciﬁc sampling probabilities)? Please see previous answers for how the\ndataset was created.\n• Who was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)? This data is\nmined, ﬁltered and sampled by machines.\n• Over what timeframe was the data collected? Does this timeframe match the creation time-\nframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not,\nplease describe the timeframe in which the data associated with the instances was created. The\nCC-News dataset contains English news articles crawled between September 2016 and September\n2021.\n• Does the dataset relate to people? If not, you may skip the remainder of the questions in this\nsection. No.\n• Did you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)? N/A.\n• Were the individuals in question notiﬁed about the data collection? If so, please describe (or\nshow with screenshots or other information) how notice was provided, and provide a link or\nother access point to, or otherwise reproduce, the exact language of the notiﬁcation itself. N/A.\n\n• Did the individuals in question consent to the collection and use of their data? If so, please\ndescribe (or show with screenshots or other information) how consent was requested and pro-\nvided, and provide a link or other access point to, or otherwise reproduce, the exact language\nto which the individuals consented. N/A.\n• If consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses? If so, please provide a description, as well as a\nlink or other access point to the mechanism (if appropriate). N/A.\n• Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a\ndata protection impact analysis) been conducted? If so, please provide a description of this\nanalysis, including the outcomes, as well as a link or other access point to any supporting\ndocumentation. Some toxicity and bias evaluations were performed. Please refer to the main\ndocument and the model card for these details.\n• Any other comments? No.\nC.4\nPreprocessing/cleaning/labeling\n• Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, to-\nkenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)? If so, please provide a description. If not, you may skip the remainder\nof the questions in this section. The component datasets went through standard cleaning and\nre-formatting practices, including removing repetitive/non-informative text like “Chapter One,” or\n“This ebook by Project Gutenberg.”\n• Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to sup-\nport unanticipated future uses)? If so, please provide a link or other access point to the “raw”\ndata. The “raw” component datasets is publicly available in their respective locations (more details\ncan be seen in the respective papers linked in references).\n• Any other comments? No.\nC.5\nUses\n• Has the dataset been used for any tasks already? If so, please provide a description. Yes, this\ndataset was used to pre-train the OPT models.\n• Is there a repository that links to any or all papers or systems that use the dataset? If so,\nplease provide a link or other access point. https://github.com/facebookresearch/\nmetaseq\n• What (other) tasks could the dataset be used for? This data can be used to pre-train language\nmodels, which are foundation to many current and future language tasks.\n• Is there anything about the composition of the dataset or the way it was collected and prepro-\ncessed/cleaned/labeled that might impact future uses? For example, is there anything that a\nfuture user might need to know to avoid uses that could result in unfair treatment of individ-\nuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g.,\nﬁnancial harms, legal risks) If so, please provide a description. Is there anything a future user\ncould do to mitigate these undesirable harms? The pipeline for creating this dataset paves a way\nfor building a scalable infrastructure for mining datasets.\n• Are there tasks for which the dataset should not be used? If so, please provide a description.\nNone that we are currently aware of.\n• Any other comments? No.\n\nC.6\nDistribution\n• Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? If so, please provide a description.\nNot at this time.\n• How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the\ndataset have a digital object identiﬁer (DOI)? N/A.\n• When will the dataset be distributed? N/A.\n• Will the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms\nor ToU, as well as any fees associated with these restrictions. N/A.\n• Do any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any supporting documentation. N/A.\n• Any other comments? No.\nC.7\nMaintenance\n• Who is supporting/hosting/maintaining the dataset? Meta AI.\n• How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Refer\nto the main document.\n• Is there an erratum? If so, please provide a link or other access point. N/A.\n• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete in-\nstances)? If so, please describe how often, by whom, and how updates will be communicated\nto users (e.g., mailing list, GitHub)? No current plan for updating.\n• If the dataset relates to people, are there applicable limits on the retention of the data as-\nsociated with the instances (e.g., were individuals in question told that their data would be\nretained for a ﬁxed period of time and then deleted)? If so, please describe these limits and\nexplain how they will be enforced. N/A.\n• Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users. N/A.\n• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism\nfor them to do so? If so, please provide a description. Will these contributions be validated/\nveriﬁed? If so, please describe how. If not, why not? Is there a process for communicating/ dis-\ntributing these contributions to other users? If so, please provide a description. No mechanism\nis available right now.\n• Any other comments? No.\nD\nModel Card\nFollowing Mitchell et al. (2018), we provide a model card for OPT-175B.\n\nD.1\nModel Details\n• Person or organization developing model: OPT-175B was developed by Meta AI.\n• Model date: OPT-175B was released on May 3, 2022.\n• Model version: OPT-175B described in this paper is version 1.0.0.\n• Model type: OPT-175B is a large decoder-only transformer language model.\n• Information about training algorithms, parameters, fairness constraints or other applied ap-\nproaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to\n175B. See the Data Card (Appendix C) for information about training data and Section 2.2 - 2.5 for\ninformation about the training process.\n• Paper or other resource for more information: See the rest of this paper for more details on\nOPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also\navailable in metaseq, our open-source repository.12\n• License: OPT-175B and the smaller baseline models are made available through a non-commercial\nuse license agreement provided in our model license.13\n• Where to send questions or comments about the model: Please contact the corresponding authors\n{susanz,roller,namangoyal}@fb.com for any questions or comments.\nD.2\nIntended Use\n• Primary intended uses: We release OPT-175B for research into Language Models, especially as it\npertains to Responsible AI. See Section 6 for more detailed Considerations for Release. Information\non how to use the model can be found at metaseq, our open-source repository.\n• Primary intended users: We primarily target researchers and the related research community.\n• Out-of-scope use cases: OPT-175B is not released for production use or real-world deployments.\nAs we note in Section 5, OPT-175B, like similar large language models, has a variety of shortcomings\nthat make it premature for commercial use.\nD.3\nData, Limitations, and Recommendations\n• Data selection for training: Training data for OPT-175B was selected based on a combination of\nbreadth and availability. See our Data Card (Appendix C) for more detailed information on the data\nused to train our model.\n• Data selection for evaluation: Evaluations in this paper were chosen to provide comparable perfor-\nmance assessments relative to similar scale models in the literature. Given concerns in the community\naround safety and fairness of large language models in general, we also explicitly provide evaluations\non Responsible AI (see Section 4).\n• Limitations: Like other large language models for which the diversity (or lack thereof) of training\ndata induces downstream impact on the quality of our model, OPT-175B has limitations in terms\nof bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\nhallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\nlarge language models. By releasing with a non-commercial license, we also hope to increase\ncommunication, transparency, and study of the problems of large language models, especially in\nareas which may not be aligned with commercial interests. See Section 5 for a more detailed\ndiscussion of limitations of OPT-175B.\n12https://github.com/facebookresearch/metaseq/\n13https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.\nmd\n\n• Recommendations for future work: See Section 6 for more about our Considerations for Release,\nincluding a discussion of potential avenues of research enabled by opening our model to more of\nthe research community. We hope that the release of OPT-175B, as well as information around our\nmodel training process, will increase open science around both large language models in speciﬁc and\nnatural language processing and deep learning in general.\n\nE\nSample Model Outputs\nFor all sample outputs, the initial prompt is given in bold and the remainder is the continuation. These\nexample outputs were intentionally selected to highlight both successes and failures of the OPT-175B\nmodel.\nFigure 8: Poetry generation. We have observed the model can write entertaining poetry on topics such as dodos,\nsamosas, and performance reviews. However, we struggled to get the model to observe rhyme or meter.\nFigure 9: Conversation generation. OPT-175B adopts a patriotic personality when prompted as the Statue of\nLiberty. However, the model also devolves into somewhat simple and linguistically repetitive generations further\ninto the conversation.\n\nFigure 10: Basic few-shot translation example. OPT was not intentionally trained to be multilingual, but we\nfound anecdotally it has limited success with simple translations in German, Spanish, French, and Chinese.\n\nFigure 11: Paper writing example. Prompting with \"1. Introduction\" generally yielded more interesting results\ncompared to prompting with “Abstract.” Our prompt here was inspired by the ﬁrst sentence of the seminal ResNet\nwork (He et al., 2016).\n\nFigure 12: Arithmetic. We observe mistakes when extending from addition to other operations.\n\nFigure 13: Python programming. Simply switching out a variable name can alter the generated output.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2005.14165",
      "full_text": " [2005.14165] Language Models are Few-Shot Learners Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2005.14165 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2005.14165 (cs) [Submitted on 28 May 2020 ( v1 ), last revised 22 Jul 2020 (this version, v4)] Title: Language Models are Few-Shot Learners Authors: Tom B. Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert-Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M. Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , Dario Amodei View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF Abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Comments: 40+32 pages Subjects: Computation and Language (cs.CL) Cite as: arXiv:2005.14165 [cs.CL] &nbsp; (or arXiv:2005.14165v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2005.14165 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tom B Brown [ view email ] [v1] Thu, 28 May 2020 17:29:03 UTC (6,995 KB) [v2] Mon, 1 Jun 2020 17:08:53 UTC (6,997 KB) [v3] Fri, 5 Jun 2020 02:52:35 UTC (6,998 KB) [v4] Wed, 22 Jul 2020 19:47:17 UTC (6,998 KB) Full-text links: Access Paper: View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2020-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 74 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Tom B. Brown Nick Ryder Jared Kaplan Prafulla Dhariwal Arvind Neelakantan &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    }
  ]
}