{
  "1-1 (Weights)": "The documentation explicitly states that “Open Pretrained Transformers (OPT) … rang[e] from 125M to 175B parameters,” and that the authors “aim to fully and responsibly share” those checkpoints “with interested researchers.”  A concrete usage snippet shows a user calling “pipeline('text-generation', model=\"facebook/opt-125m\")”, confirming that the facebook/opt-125m weights are already hosted on-line and can be downloaded and instantiated directly through the standard Hugging Face interface.  Taken together, these two quotes indicate that the 125 M-parameter model weights are publicly available and freely accessible to anyone who wishes to load them.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "> We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\n> to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[readme]",
      "quote": ">>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")"
    }
  ],
  "1-2 (Code)": "A single sentence notes that “OPT was first introduced in [Open Pre-trained Transformer Language Models] and first released in [metaseq's repository] on May 3rd 2022 by Meta AI.”  This directly ties the initial public code drop to Meta’s metaseq GitHub repository and therefore confirms that code relevant to the OPT family—including the 125 M checkpoint—was made openly available on that date. (The quote does not differentiate pre-training, fine-tuning, or inference stages, but it does verify that some implementation for OPT appeared in an openly accessible repository.)",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    }
  ],
  "1-3 (License)": "Multiple fragments refer to an “OPT-175B LICENSE AGREEMENT,” indicating that the entire OPT suite—including the 125 M model—falls under that same bespoke license.  The text specifies redistribution conditions: “Together with any copies … you must provide … a copy of this License, and … the following attribution notice: ‘OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.’”  The agreement is formally “between you … and Meta Platforms, Inc.” and is stored in the project as a “LICENSE.md” file.  These excerpts establish that users receive the software under Meta’s custom OPT license and must propagate both the license text and the exact attribution statement whenever they share the model or derivative works.",
  "1-3 (License)__evidence": [
    {
      "source": "[license_file]",
      "quote": "<h2 align=\"center\"> OPT-175B LICENSE AGREEMENT </h2>"
    },
    {
      "source": "[license_file]",
      "quote": "Together with any copies of the Software Products (as well as derivative works thereof or works incorporating the Software Products) that you distribute, you must provide (i) a copy of this License, and (ii) the following attribution notice: “OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.”"
    },
    {
      "source": "[license_file]",
      "quote": "eprint={2205.01068},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n\n[license_file]\n<h2 align=\"center\"> OPT-175B LICENSE AGREEMENT </h2>\n\nThis License Agreement (as may be amended in accordance with this License Agreement, **“License”**), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (**“Licensee”** or **“you”**) and Meta Platforms, Inc."
    },
    {
      "source": "[readme]",
      "quote": "tion (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this License. If you do not h"
    },
    {
      "source": "[readme]",
      "quote": "eta that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.**\n<br><br>\n1. **LICENSE GRANT**\n<br><br>\n a. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Meta grants you"
    },
    {
      "source": "[readme]",
      "quote": "ntation solely for use in connection with the license to the Software granted above. \n<br><br>\n c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Meta and"
    },
    {
      "source": "[readme]",
      "quote": "WILL META BE LIABLE TO YOU (A) UNDER ANY THEORY OF LIABILITY, WHETHER BASED IN CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, WARRANTY, OR OTHERWISE UNDER THIS LICENSE, OR (B) FOR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, PUNITIVE OR SPECIAL DAMAGES OR LOST PROFITS, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE SOFTWARE PRODUCTS, THEIR CON"
    },
    {
      "source": "[readme]",
      "quote": "ate this License, in whole or in part, at any time upon notice (including electronic) to you.\n<br><br>\n c. The following sections survive termination of this License: 2 (Restrictions), 3 (Attribution), 4 (Disclaimers), 5 (Limitation on Liability), 6 (Indemnification) 7 (Termination; Survival), 8 (Third Party Materials), 9 (Trademarks), 10 (Applicable Law; Dispute Resolution), and 11 ("
    },
    {
      "source": "[readme]",
      "quote": "with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers’ ability to study how and\n> why these large language models work, hindering progress on"
    },
    {
      "source": "[readme]",
      "quote": "under Meta’s copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign or sublicense this License or any"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE.md"
    }
  ],
  "1-4 (Paper)": "The main technical reference is the arXiv paper “Open Pre-trained Transformer Language Models” (arXiv:2205.01068), explicitly cited as the work in which “OPT was first introduced.”  This same sentence also notes the public release of code in the metaseq repository, reinforcing that the paper and repository together constitute the authoritative sources of information for the facebook/opt-125m model.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    }
  ]
}