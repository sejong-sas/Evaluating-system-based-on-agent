{
  "1-1 (가중치 Weights)": "이 항목은 모델의 가중치와 관련된 다양한 체크포인트들의 공개 및 이용 방법에 대해 상세히 설명하고 있습니다. 사용자는 사전 학습된 모델 체크포인트(형태: PP=12, TP=4, DP=4)를 다운로드할 수 있으며, 모델 구조를 변경하려는 경우에는 별도의 universal checkpoint도 함께 다운로드해야 합니다. 또한, 추가적인 파인튜닝 작업을 진행하려는 경우에 대비한 파인튜닝 체크포인트(형태: PP=72, TP=1, DP=4)도 제공되고 있어, 각 용도에 맞게 적절한 가중치를 선택하여 활용할 수 있도록 공개되어 있습니다.",
  "1-2 (코드 Code)": "이 항목은 훈련 및 추론을 위한 코드의 설정과 공개된 범위를 자세하게 설명하고 있습니다. 사용자는 GitHub 저장소에서 ‘t0loading’ 브랜치를 통해 Megatron-DeepSpeed 리포지토리를 클론(clone) 받고, 제공된 설치 가이드를 참고하여 필요한 패키지들과 환경을 구축할 수 있습니다. 또한, 명시적으로 create_xp3x.py라는 스크립트 파일의 존재가 언급되어 있어, 사용자들이 해당 코드를 활용하여 실험을 진행하거나 추가적인 개발을 할 수 있는 범위와 구체적인 구성 요소들을 파악할 수 있습니다.",
  "1-3 (라이선스 License)": "이 항목에서는 소프트웨어의 사용 및 배포에 관한 법적 권한을 명시한 라이선스 정보를 제공하고 있습니다. Apache License Version 2.0 (2004년 1월 버전)을 기반으로 하며, 해당 라이선스는 사용, 수정, 배포 등 다양한 권한을 명시적으로 허용하고 있습니다. LICENSE.md 파일에 관련 정보가 포함되어 있어, 사용자들이 라이선스 문서를 통해 상세한 권한 및 조건들을 확인할 수 있도록 구성되었습니다.",
  "1-4 (논문 Paper)": "이 항목은 해당 프로젝트와 관련된 공식 논문 및 보고서에 관한 정보와 링크를 제공하고 있습니다. 구체적으로, BLOOMZ, mT0, xP3 모델의 생성에 사용된 모든 주요 구성 요소들에 대한 개요를 제공하는 논문, 'Crosslingual Generalization through Multitask Finetuning'에 대한 상세한 정보와 함께 arXiv 링크가 제시되어 있습니다. 또한, 추가 자료로서 포스터(PDF 형식) 파일이 포함되어 있어, 연구 결과 및 발표 자료에 접근하여 관련 내용을 심도 있게 이해할 수 있도록 도와줍니다.",
  "1-1 (가중치 Weights)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal). If you want to continue finetuning, you should use [our finetuned checkpoint](https://huggingface.co/bigscience/bloomz-optimizer-states), which is of shape PP=72, TP=1, DP=4."
    }
  ],
  "1-2 (코드 Code)__evidence": [
    {
      "source": "readme",
      "quote": "Setup the training code: `git clone -b t0loading https://github.com/bigscience-workshop/Megatron-DeepSpeed` & follow its [setup guide](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/t0loading#get-started-fast) to create an environment with necessary packages."
    },
    {
      "source": "files",
      "quote": "create_xp3x.py"
    }
  ],
  "1-3 (라이선스 License)__evidence": [
    {
      "source": "license_files",
      "quote": "# LICENSE.md\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/"
    },
    {
      "source": "license_files",
      "quote": "LICENSE.md"
    }
  ],
  "1-4 (논문 Paper)__evidence": [
    {
      "source": "readme",
      "quote": "This repository provides an overview of all components used for the creation of BLOOMZ & mT0 and xP3 introduced in the paper [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)."
    },
    {
      "source": "files",
      "quote": "plotstables/XMTF_ACL2023_POSTER.pdf"
    }
  ]
}