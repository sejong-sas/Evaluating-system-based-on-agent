{
  "1-1 (가중치 Weights)": "이 항목은 모델의 가중치와 관련된 다양한 체크포인트들의 공개 및 이용 방법에 대해 상세히 설명하고 있습니다. 사용자는 사전 학습된 모델 체크포인트(형태: PP=12, TP=4, DP=4)를 다운로드할 수 있으며, 모델 구조를 변경하려는 경우에는 별도의 universal checkpoint도 함께 다운로드해야 합니다. 또한, 추가적인 파인튜닝 작업을 진행하려는 경우에 대비한 파인튜닝 체크포인트(형태: PP=72, TP=1, DP=4)도 제공되고 있어, 각 용도에 맞게 적절한 가중치를 선택하여 활용할 수 있도록 공개되어 있습니다.",
  "1-2 (코드 Code)": "이 항목은 훈련 및 추론을 위한 코드의 설정과 공개된 범위를 자세하게 설명하고 있습니다. 사용자는 GitHub 저장소에서 ‘t0loading’ 브랜치를 통해 Megatron-DeepSpeed 리포지토리를 클론(clone) 받고, 제공된 설치 가이드를 참고하여 필요한 패키지들과 환경을 구축할 수 있습니다. 또한, 명시적으로 create_xp3x.py라는 스크립트 파일의 존재가 언급되어 있어, 사용자들이 해당 코드를 활용하여 실험을 진행하거나 추가적인 개발을 할 수 있는 범위와 구체적인 구성 요소들을 파악할 수 있습니다.",
  "1-3 (라이선스 License)": "이 항목에서는 소프트웨어의 사용 및 배포에 관한 법적 권한을 명시한 라이선스 정보를 제공하고 있습니다. Apache License Version 2.0 (2004년 1월 버전)을 기반으로 하며, 해당 라이선스는 사용, 수정, 배포 등 다양한 권한을 명시적으로 허용하고 있습니다. LICENSE.md 파일에 관련 정보가 포함되어 있어, 사용자들이 라이선스 문서를 통해 상세한 권한 및 조건들을 확인할 수 있도록 구성되었습니다.",
  "1-4 (논문 Paper)": "이 항목은 해당 프로젝트와 관련된 공식 논문 및 보고서에 관한 정보와 링크를 제공하고 있습니다. 구체적으로, BLOOMZ, mT0, xP3 모델의 생성에 사용된 모든 주요 구성 요소들에 대한 개요를 제공하는 논문, 'Crosslingual Generalization through Multitask Finetuning'에 대한 상세한 정보와 함께 arXiv 링크가 제시되어 있습니다. 또한, 추가 자료로서 포스터(PDF 형식) 파일이 포함되어 있어, 연구 결과 및 발표 자료에 접근하여 관련 내용을 심도 있게 이해할 수 있도록 도와줍니다.",
  "1-1 (가중치 Weights)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal). If you want to continue finetuning, you should use [our finetuned checkpoint](https://huggingface.co/bigscience/bloomz-optimizer-states), which is of shape PP=72, TP=1, DP=4."
    }
  ],
  "1-2 (코드 Code)__evidence": [
    {
      "source": "readme",
      "quote": "Setup the training code: `git clone -b t0loading https://github.com/bigscience-workshop/Megatron-DeepSpeed` & follow its [setup guide](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/t0loading#get-started-fast) to create an environment with necessary packages."
    },
    {
      "source": "files",
      "quote": "create_xp3x.py"
    }
  ],
  "1-3 (라이선스 License)__evidence": [
    {
      "source": "license_files",
      "quote": "# LICENSE.md\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/"
    },
    {
      "source": "license_files",
      "quote": "LICENSE.md"
    }
  ],
  "1-4 (논문 Paper)__evidence": [
    {
      "source": "readme",
      "quote": "This repository provides an overview of all components used for the creation of BLOOMZ & mT0 and xP3 introduced in the paper [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)."
    },
    {
      "source": "files",
      "quote": "plotstables/XMTF_ACL2023_POSTER.pdf"
    }
  ],
  "1-5 (아키텍처 Architecture)": "이 인용문은 모델 아키텍처와 관련하여 사전 학습된 체크포인트의 구성에 대해 상세히 설명합니다. 기본 체크포인트는 PP=12, TP=4, DP=4의 구성으로 제공되며, 모델을 재구성하려면 별도의 유니버설 체크포인트를 다운로드해야 한다고 명시되어 있습니다. 또한, 추가적인 미세 조정(finétuning)을 원할 경우 PP=72, TP=1, DP=4의 구조를 가지는 미세 조정된 체크포인트를 사용하도록 안내하고 있어, 모델의 병렬 처리 및 구성 변화에 관한 구체적인 정보를 제공합니다.",
  "1-6 (토크나이저 Tokenizer)": "제공된 인용문에는 토크나이저의 종류나 공개 여부에 대한 구체적인 정보가 포함되어 있지 않아, 해당 항목에 대한 추가 세부 사항을 확인할 수 없습니다.",
  "2-1 (하드웨어 Hardware)": "해당 인용문은 훈련 스크립트 내에서 SLURM 환경 설정과 관련된 부분의 수정 필요성을 강조합니다. 구체적으로는 노드 수, GPU 수, 실행 시간 등의 #SBATCH 변수를 조정해야 하며, 이에 대한 자세한 가이드가 GitHub의 SLURM 가이드를 통해 제공되고 있음을 언급합니다. 이는 훈련에 사용되는 하드웨어 자원의 관리 및 최적화를 위한 중요한 정보를 포함하고 있습니다.",
  "2-2 (소프트웨어 Software)": "이 인용문은 훈련 코드 설정에 관해 구체적으로 설명하고 있으며, 'Megatron-DeepSpeed' 저장소를 t0loading 분기로 클론한 후, 제공된 setup guide를 따라 필요한 소프트웨어 패키지 및 프레임워크 버전을 갖춘 환경을 구성하는 방법을 안내합니다. 이를 통해 훈련 소프트웨어의 설치 및 초기화 과정 전반에 걸친 세부 사항이 전달되고 있습니다.",
  "1-5 (아키텍처 Architecture)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal). If you want to continue finetuning, you should use [our finetuned checkpoint](https://huggingface.co/bigscience/bloomz-optimizer-states), which is of shape PP=72, TP=1, DP=4."
    }
  ],
  "1-6 (토크나이저 Tokenizer)__evidence": [],
  "2-1 (하드웨어 Hardware)__evidence": [
    {
      "source": "readme",
      "quote": "Important parts of the script to modify are:\n- `#SBATCH` variables, such as nodes, gpus, time, etc. - Our SLURM guide is [here](https://github.com/bigscience-workshop/bigscience/tree/master/jz/slurm#slurm-how-to)"
    }
  ],
  "2-2 (소프트웨어 Software)__evidence": [
    {
      "source": "readme",
      "quote": "Setup the training code: `git clone -b t0loading https://github.com/bigscience-workshop/Megatron-DeepSpeed` & follow its [setup guide](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/t0loading#get-started-fast) to create an environment with necessary packages."
    }
  ],
  "2-3 (API)": "이 항목과 관련된 API 정보, 문서, 사용 예가 제공되지 않았습니다. 즉, 모델 API의 존재, 문서화 혹은 활용 예시 등에 관한 구체적인 내용은 주어진 인용문에 포함되어 있지 않습니다.",
  "3-1 (사전학습 Pre-training)": "이 항목에서는 사전학습과 관련해 모델의 체크포인트 다운로드 방법과 관련 링크들을 제공하고 있습니다. 구체적으로, pretrained 모델을 다운로드 할 수 있는 체크포인트 링크가 제공되며, 모델의 세부 구조가 PP=12, TP=4, DP=4로 정의되어 있음을 언급합니다. 또한 만약 모델의 구조(reshape)를 변경하고자 한다면, 해당 universal checkpoint 역시 다운로드 받아야 한다고 설명하여, 사전학습에 사용되는 모델의 상태와 구성 정보를 자세히 전달합니다.",
  "3-2 (파인튜닝 Fine-tuning)": "파인튜닝 방법과 파이프라인에 관한 설명에서는 구체적인 트레이닝 스크립트 설정 및 실행 절차가 기술되어 있습니다. SLURM 스크립트를 활용해 큰 스케일 모델 학습을 진행하는데, 관련 스크립트는 'bigscience-workshop/bigscience/train/tr13-mtf' 경로에서 확인 가능하며, 이 작업은 'xp3capmixnewcodelonglossseq'라는 식별자로 참조됩니다. 이를 통해 파인튜닝 작업을 수행하기 위한 구체적인 코드와 실행 방법을 제공하고 있음을 알 수 있습니다.",
  "3-3 (강화학습 Reinforcement Learning)": "이 항목에서는 RLHF나 DPO와 같은 강화학습 기법에 관한 구체적인 내용이 제공되지 않았습니다. 따라서 강화학습과 관련된 방법이나 절차에 대한 정보는 주어진 인용문에서 확인할 수 없습니다.",
  "2-3 (API)__evidence": [],
  "3-1 (사전학습 Pre-training)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal)."
    }
  ],
  "3-2 (파인튜닝 Fine-tuning)__evidence": [
    {
      "source": "readme",
      "quote": "Setup & run the training script: We use SLURM scripts available at [bigscience-workshop/bigscience/train/tr13-mtf] and referred to as `xp3capmixnewcodelonglossseq`."
    }
  ],
  "3-3 (강화학습 Reinforcement Learning)__evidence": [],
  "4-1 (사전학습 데이터 Pre-training Data)": "이 항목에 제공된 인용문에는 사전학습 데이터의 종류, 규모 및 출처에 관한 어떠한 정보도 포함되어 있지 않습니다. 따라서, 사전학습에 사용된 데이터에 대한 구체적 내용이나 세부 정보가 명시되어 있지 않음을 알 수 있습니다.",
  "4-2 (파인튜닝 데이터 Fine-tuning Data)": "제공된 인용문은 파인튜닝 데이터로 사용된 xP3 데이터셋에 대한 정보를 포함하고 있습니다. xP3 데이터셋은 https://huggingface.co/datasets/bigscience/xP3 링크를 통해 접근 가능하며, 총 13개의 학습 태스크로 구성되어 있으며, 46개 언어에 걸쳐 데이터를 포함하고 있습니다. 데이터셋은 특히 영어 프롬프트를 사용하여 구성되어 있어, 파인튜닝에 필요한 다중 언어와 다양한 작업 형식의 대표적인 예시를 제공하고 있습니다.",
  "4-3 (강화학습 데이터 Reinforcement Learning Data)": "강화학습 데이터와 관련해서는 어떠한 구체적인 인용문이나 내용이 제공되지 않았습니다. 이로 인해 강화학습 데이터의 구성이나 출처에 대한 정보는 제시되지 않은 상태입니다.",
  "4-4 (데이터 필터링 Data Filtering)": "인용문은 데이터 필터링 과정과 기준에 대한 상세한 정보를 제공하고 있습니다. 주요 내용으로는 테스트 세트에 숨겨진 라벨이 있을 경우 발생할 수 있는 노이즈 문제를 방지하기 위해 특정 프롬프트들을 스킵하는 방식에 대해 설명하고 있습니다. 예를 들어, piqa와 같은 데이터셋에서는 테스트 라벨이 모두 -1로 구성되어 있어 리스트 인덱스 처리 과정에서 항상 동일한 결과(노이즈 샘플)를 발생시킬 수 있으므로, 이를 회피하기 위해 해당 프롬프트들이 스킵되도록 설정되어 있습니다. 또한, 다양한 데이터셋(common_gen, piqa, qasc, imdb, glue/qqp, super_glue/record, kilt_tasks/hotpotqa, cosmos_qa, clue/tnews, clue/csl, clue/cmrc2018, clue/drcd, hellaswag 등)에 대해 'test' 혹은 'unsupervised' 구간에서 특정 프롬프트들이 스킵되도록 상세하게 명시되어 있으며, 각 데이터셋마다 스킵 대상 프롬프트의 리스트를 제공함으로써, 불필요한 노이즈를 제거하고 보다 정제된 데이터셋을 구성하는 과정이 드러납니다.",
  "4-1 (사전학습 데이터 Pre-training Data)__evidence": [],
  "4-2 (파인튜닝 데이터 Fine-tuning Data)__evidence": [
    {
      "source": "readme",
      "quote": "<td><a href=https://huggingface.co/datasets/bigscience/xP3>xP3</a></t> \n<td>Mixture of 13 training tasks in 46 languages with English prompts</td>"
    }
  ],
  "4-3 (강화학습 데이터 Reinforcement Learning Data)__evidence": [],
  "4-4 (데이터 필터링 Data Filtering)__evidence": [
    {
      "source": "py_files/create_xp3x.py",
      "quote": "# Some datasets have test sets with hidden labels which will still compile but only to noise\n# e.g. piqa test labels are all [-1] which still works on list indices resulting in \n# noise samples where the label is always the same\nSKIP_PROMPTS = {\n    \"common_gen\": {\"test\": [\"all\"]},\n    \"piqa\": {\"test\": [\"all\"]},\n    \"qasc\": {\"test\": [\"all\"]},\n    \"imdb\": {\"unsupervised\": [\"all\"]},\n    \"glue/qqp\": {\"test\": [\"all\"]},\n    \"super_glue/record\": {\"test\": [\"all\"]},\n    \"qasc\": {\"test\": [\"all\"]},\n    'kilt_tasks/hotpotqa': {\"test\": [\"all\"]},\n    \"cosmos_qa\": {\"test\": [\n        \"description_context_question_answer_text\", \n        \"description_context_question_text\",\n        \"description_context_question_answer_id\",\n        \"context_answer_to_question\",\n        \"context_description_question_answer_text\",\n        \"context_description_question_answer_id\",\n        \"context_question_description_answer_id\",\n        \"context_description_question_text\",\n        \"context_question_description_answer_text\",\n        \"only_question_answer\",\n        \"no_prompt_id\",\n        \"context_question_description_text\",\n        \"no_prompt_text\"\n        ]},\n    \"clue/tnews\": {\"test\": [\"all\"]},\n    \"clue/csl\": {\"test\": [\"all\"]},\n    \"clue/cmrc2018\": {\"test\": [\"generate_question\", \"in_an_exam\", \"answer_in_the_passage\", \"answer_following_question\", \"xp3longcontinue\"]},\n    \"clue/drcd\": {\"test\": [\"generate_question\", \"in_an_exam\", \"answer_in_the_passage\", \"answer_following_question\", \"xp3longcontinue\"]},\n    \"hellaswag\": {\"test\": [\n        \"complete_first_then\", \"Topic of the context\", \"Open-ended completion\", \"Randomized prompts template\", \"Appropriate continuation - Yes or No\", \"Predict ending with hint\", \"Open-ended start\", \"Reversed appropriate continuation - Yes or No\", \"how_ends\", \"if_begins_how_continues\"\n    ]}\n}"
    }
  ]
}