{
  "1-1 (가중치 Weights)": "이 항목은 모델의 사전 학습된 가중치가 어떻게 공개되고 배포되는지를 자세히 설명합니다. 사용자는 HuggingFace 플랫폼에서 제공된 체크포인트 링크를 통해 가중치를 다운로드할 수 있으며, 이 체크포인트는 PP=12, TP=4, DP=4의 파라미터 구성을 갖추고 있음을 명시합니다. 또한, 모델의 재구성을 원하는 사용자는 별도의 '유니버설 체크포인트'를 추가로 다운로드해야 함을 안내하여, 프로젝트의 가중치 구조 및 다중 병렬 처리 설정에 대한 세부 정보를 제공합니다.",
  "1-2 (코드 Code)": "이 항목은 모델 훈련 및 실행에 필요한 코드를 공개하는 방법을 상세하게 설명합니다. 특히, xP3의 확장 버전인 xP3x에 관한 내용으로, 사용자는 GitHub 저장소의 'xp3x' 브랜치를 pip 명령을 통해 직접 설치할 수 있습니다. 더불어, 'create_xp3x.py'라는 이름의 스크립트 파일이 포함되어 있어, 해당 코드가 데이터셋 생성 및 처리와 관련된 구체적인 프로세스를 담고 있음을 보여줍니다.",
  "1-3 (라이선스 License)": "이 항목은 프로젝트에 적용되는 라이선스 정보를 명확하게 전달합니다. LICENSE.md 파일을 통해, 프로젝트가 Apache License 버전 2.0 (2004년 1월 버전) 하에 배포되고 있음을 확인할 수 있으며, 이 라이선스는 사용, 변경, 배포, 상업적 이용 등 다양한 권리를 허용하는 조항들을 포함하고 있습니다. 이는 사용자가 프로젝트를 어떻게 활용할 수 있는지에 대해 명확한 지침을 제공합니다.",
  "1-4 (논문 Paper)": "이 항목은 모델과 관련된 문서 자료들을 포괄적으로 제공하고 있음을 설명합니다. 저장소에는 BLOOMZ, mT0, xP3와 같은 모델의 구성 요소들을 다룬 발표 자료와 기술 보고서, 공식 논문 등이 포함되어 있으며, 특히 'Crosslingual Generalization through Multitask Finetuning'이라는 논문이 그 기반이 됨을 명시하고 있습니다. 또한, 추가 문서인 포스터 파일(plotstables/XMTF_ACL2023_POSTER.pdf)을 통해 연구 방법론 및 결과를 더욱 심도 있게 이해할 수 있도록 배포되고 있습니다.",
  "1-1 (가중치 Weights)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal)."
    }
  ],
  "1-2 (코드 Code)__evidence": [
    {
      "source": "readme",
      "quote": "For the new extension of xP3, [xP3x](https://huggingface.co/datasets/Muennighoff/xP3x), the process is largely the same except:\n\n1. Install the `xp3x` branch instead i.e. `pip install git+https://github.com/Muennighoff/promptsource.git@xp3x`\n3. The creation script is in this repository & named `create_xp3x.py`."
    },
    {
      "source": "files",
      "quote": "create_xp3x.py"
    }
  ],
  "1-3 (라이선스 License)__evidence": [
    {
      "source": "license_files",
      "quote": "# LICENSE.md\n                                 Apache License\n                           Version 2.0, January 2004"
    },
    {
      "source": "files",
      "quote": "LICENSE.md"
    }
  ],
  "1-4 (논문 Paper)__evidence": [
    {
      "source": "readme",
      "quote": "This repository provides an overview of all components used for the creation of BLOOMZ & mT0 and xP3 introduced in the paper [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)."
    },
    {
      "source": "files",
      "quote": "plotstables/XMTF_ACL2023_POSTER.pdf"
    }
  ],
  "1-5 (아키텍처 Architecture)": "제공된 quote에 따르면, pretrained 모델의 체크포인트를 다운로드하는 과정을 설명하고 있으며, 모델의 구조 상세 정보로 PP=12, TP=4, DP=4의 형태를 갖고 있음을 명시하고 있습니다. 이는 모델의 파이프라인 병렬 처리, 텐서 병렬 처리, 데이터 병렬 처리의 구성을 나타내며, 만약 모델의 구조를 재설계하거나 재배열할 필요가 있을 경우 별도로 제공된 universal checkpoint도 함께 다운로드해야 함을 알려줍니다.",
  "1-6 (토크나이저 Tokenizer)": "해당 항목에 대해서는 quote가 제공되지 않아, 사용된 토크나이저의 이름, 구조, 다운로드 가능 여부 등 어떠한 세부 정보도 확인할 수 없습니다.",
  "2-1 (하드웨어 Hardware)": "quote에 하드웨어에 관한 내용이 전혀 제공되지 않았으므로, 모델 훈련에 사용된 하드웨어 종류, 수량 또는 계산 자원 규모에 관한 구체적인 정보는 확인할 수 없습니다.",
  "2-2 (소프트웨어 Software)": "제공된 quote는 훈련 코드를 설정하는 방법을 상세히 설명합니다. 특히, 'Megatron-DeepSpeed' 저장소의 t0loading 브랜치를 git clone 명령어를 통해 복제한 후, 제공된 setup guide 링크에 따라 필요한 패키지들을 설치하여 환경을 구축하도록 안내하고 있습니다. 이로써, 사용된 소프트웨어 프레임워크, 라이브러리 및 환경 설정에 대한 기본적인 구성 정보가 드러납니다.",
  "1-5 (아키텍처 Architecture)__evidence": [
    {
      "source": "readme",
      "quote": "Download the pretrained model [checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states), which is of shape PP=12, TP=4, DP=4. If you'd like to reshape the model you will also need to download [the universal checkpoint](https://huggingface.co/bigscience/bloom-optimizer-states/tree/global_step95000_universal)."
    }
  ],
  "1-6 (토크나이저 Tokenizer)__evidence": [],
  "2-1 (하드웨어 Hardware)__evidence": [],
  "2-2 (소프트웨어 Software)__evidence": [
    {
      "source": "readme",
      "quote": "Setup the training code: `git clone -b t0loading https://github.com/bigscience-workshop/Megatron-DeepSpeed` & follow its [setup guide](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/t0loading#get-started-fast) to create an environment with necessary packages."
    }
  ],
  "2-3 (API)": "제공된 quote에는 모델이 접근 가능한 API와 관련된 정보가 포함되어 있지 않습니다. 즉, gpt api나 gemini api와 같이 라이브러리 형태가 아닌 외부 API의 존재 여부, 문서 링크, 사용 예제, 공개 여부 등과 관련된 구체적인 내용에 대해서는 언급된 사항이 없습니다.",
  "3-1 (사전학습 Pre-training)": "해당 quote에서는 사전학습에 사용된 방법론, 진행 절차, 데이터 흐름, 하이퍼파라미터 설정 등 사전학습 관련 세부 정보에 관한 언급이 전혀 없으므로, 이 항목에 대한 구체적인 정보는 제공되지 않았습니다.",
  "3-2 (파인튜닝 Fine-tuning)": "제공된 quote에 따르면, 파인튜닝 단계에서는 먼저 SLURM 스크립트를 사용하여 훈련 스크립트를 설정하고 실행하는 방식이 채택되었습니다. 구체적으로, bigscience-workshop의 GitHub 저장소 내 train/tr13-mtf 디렉토리에서 'xp3capmixnewcodelonglossseq'라는 이름의 스크립트를 참조하며, 이 스크립트는 예시로 bloomz 모델을 훈련하기 위해 사용된 SLURM 스크립트로 확인됩니다. 이는 모델 파인튜닝을 위한 자동화된 환경 구성 및 스크립트 기반 실행 방법을 보여줍니다. 또한, 두 번째 quote에서는 구글 리서치에서 제공하는 T5x 파인튜닝 문서를 참조하여, pretrained mT5 모델과 xP3 데이터셋을 활용한 파인튜닝 절차를 따르도록 권장하고 있음을 알 수 있습니다. 이로써, 파인튜닝 과정이 체계적이고 재현 가능한 파이프라인을 통해 진행된다는 점을 강조하고 있습니다.",
  "3-3 (강화학습 Reinforcement Learning)": "제공된 quote에서는 RLHF나 DPO와 같은 강화학습 알고리즘의 사용 여부, 구체적인 방식, 절차 및 설정값 등 강화학습과 관련된 모든 상세 정보가 포함되어 있지 않습니다. 따라서 해당 항목에 대한 구체적인 내용은 확인할 수 없습니다.",
  "2-3 (API)__evidence": [],
  "3-1 (사전학습 Pre-training)__evidence": [],
  "3-2 (파인튜닝 Fine-tuning)__evidence": [
    {
      "source": "readme",
      "quote": "Setup & run the training script: We use SLURM scripts available at [bigscience-workshop/bigscience/train/tr13-mtf] and referred to as `xp3capmixnewcodelonglossseq`. E.g. [this is the script launched to train bloomz](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm)."
    },
    {
      "source": "readme",
      "quote": "Follow the finetuning instructions [here](https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md) making sure to use pretrained mT5 models & the xP3 dataset."
    }
  ],
  "3-3 (강화학습 Reinforcement Learning)__evidence": [],
  "4-1 (사전학습 데이터 Pre-training Data)": "이 항목은 사전학습 단계에서 사용된 데이터의 구체적인 파일 이름들을 나열하고 있습니다. 제공된 두 개의 파일, 'xp3capmixnewcodelong_train.txt'와 'xp3capmixnewcodelong_validation_pretr.txt'는 각각 훈련과 검증을 위해 구성된 데이터셋을 나타냅니다. 이 파일 이름을 통해 데이터셋이 코드와 캡션 또는 혼합 데이터를 포함할 가능성을 유추할 수 있으며, 데이터의 수집, 구성 방식 및 사용 범위에 관한 상세 정보는 파일명 자체를 통해 일부 밝혀지고 있습니다.",
  "4-2 (파인튜닝 데이터 Fine-tuning Data)": "파인튜닝 데이터는 13개의 서로 다른 훈련 작업이 혼합되어 있으며, 이 작업들은 총 46개 언어에 걸쳐 제공되고 있습니다. 영어 프롬프트를 기반으로 구성되어 있어 각 작업의 명령어와 지시사항이 영어로 제공되는 것이 특징입니다. 이를 통해 다국어 환경에서의 모델 성능 향상을 목표로 한 데이터셋의 구성 및 다양성을 엿볼 수 있습니다.",
  "4-3 (강화학습 데이터 Reinforcement Learning Data)": "현재 강화학습에 사용된 데이터셋에 관한 인용문은 제공되지 않았으며, 강화학습 단계에 활용된 데이터에 대한 구체적인 파일 목록이나 구성 정보, 출처 및 생성 방식 등의 세부 내용은 포함되어 있지 않습니다.",
  "4-4 (데이터 필터링 Data Filtering)": "데이터 필터링 항목은 SKIP_PROMPTS라는 변수 내에 정의된 상세한 필터링 기준을 포함하고 있습니다. 이 설정은 여러 데이터셋 및 작업(task)에 대해 테스트 단계 또는 특정 분류(예: 'unsupervised')에서 특정 프롬프트들을 건너뛰도록 설계되어 있습니다. 인용문에는 'common_gen', 'piqa', 'qasc', 'imdb', 'glue/qqp', 'super_glue/record', 'kilt_tasks/hotpotqa', 'cosmos_qa', 'clue/tnews', 'clue/csl', 'clue/cmrc2018', 'clue/drcd', 'hellaswag' 등 다양한 작업들이 언급되며, 각 작업마다 어떤 프롬프트 유형들이 제외될지 구체적으로 명시되어 있습니다. 이와 같이 데이터 필터링 과정은 모델 학습 시 불필요하거나 부적절한 프롬프트를 사전에 제거함으로써 최종 성능에 미치는 영향을 최소화하려는 의도를 반영합니다.",
  "4-1 (사전학습 데이터 Pre-training Data)__evidence": [
    {
      "source": "files",
      "quote": "xp3capmixnewcodelong_train.txt"
    },
    {
      "source": "files",
      "quote": "xp3capmixnewcodelong_validation_pretr.txt"
    }
  ],
  "4-2 (파인튜닝 데이터 Fine-tuning Data)__evidence": [
    {
      "source": "readme",
      "quote": "Mixture of 13 training tasks in 46 languages with English prompts"
    }
  ],
  "4-3 (강화학습 데이터 Reinforcement Learning Data)__evidence": [],
  "4-4 (데이터 필터링 Data Filtering)__evidence": [
    {
      "source": "py_files/create_xp3x.py",
      "quote": "SKIP_PROMPTS = {\n    \"common_gen\": {\"test\": [\"all\"]},\n    \"piqa\": {\"test\": [\"all\"]},\n    \"qasc\": {\"test\": [\"all\"]},\n    \"imdb\": {\"unsupervised\": [\"all\"]},\n    \"glue/qqp\": {\"test\": [\"all\"]},\n    \"super_glue/record\": {\"test\": [\"all\"]},\n    \"qasc\": {\"test\": [\"all\"]},\n    'kilt_tasks/hotpotqa': {\"test\": [\"all\"]},\n    \"cosmos_qa\": {\"test\": [\"description_context_question_answer_text\", \"description_context_question_text\", \"description_context_question_answer_id\", \"context_answer_to_question\", \"context_description_question_answer_text\", \"context_description_question_answer_id\", \"context_question_description_answer_id\", \"context_description_question_text\", \"context_question_description_answer_text\", \"only_question_answer\", \"no_prompt_id\", \"context_question_description_text\", \"no_prompt_text\"]},\n    \"clue/tnews\": {\"test\": [\"all\"]},\n    \"clue/csl\": {\"test\": [\"all\"]},\n    \"clue/cmrc2018\": {\"test\": [\"generate_question\", \"in_an_exam\", \"answer_in_the_passage\", \"answer_following_question\", \"xp3longcontinue\"]},\n    \"clue/drcd\": {\"test\": [\"generate_question\", \"in_an_exam\", \"answer_in_the_passage\", \"answer_following_question\", \"xp3longcontinue\"]},\n    \"hellaswag\": {\"test\": [\"complete_first_then\", \"Topic of the context\", \"Open-ended completion\", \"Randomized prompts template\", \"Appropriate continuation - Yes or No\", \"Predict ending with hint\", \"Open-ended start\", \"Reversed appropriate continuation - Yes or No\", \"how_ends\", \"if_begins_how_continues\"]},\n}"
    }
  ]
}