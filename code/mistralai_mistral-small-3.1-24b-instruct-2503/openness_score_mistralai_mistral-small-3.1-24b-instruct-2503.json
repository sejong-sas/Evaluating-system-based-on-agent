{
  "model": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights hosted on Hugging Face or equivalent"
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No code files."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The readme explicitly states 'license: apache-2.0', which permits use, modification, redistribution, and commercial use."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "A blog post is referenced for additional details. While this is official documentation, it is a blog/announcement rather than a full technical paper."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture disclosed on the model card/config"
    },
    "1-6 Tokenizer": {
      "score": 1,
      "reason": "Tokenizer details available on the model card/config"
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Detailed hardware requirements are disclosed (e.g., deployment on an RTX 4090, GPU RAM requirements), which demonstrates full disclosure of hardware details."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "The software stack is clearly detailed, including specific library names and version requirements (vllm and mistral_common), ensuring reproducibility."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "No concrete public API evidence (no endpoint/key/example)."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete pre-training method details (objectives/schedules/hyperparameters/pipeline). Only data scale/types/hardware were mentioned → Closed (0) by tight policy."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "No concrete fine-tuning method details (objectives/schedules/hyperparameters/pipeline). Only data scale/types/hardware were mentioned → Closed (0) by tight policy."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "There is no evidence quoted regarding any reinforcement learning methods. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.0,
      "reason": "No details or quotes about the pre-training dataset (sources, quantities, licensing) are provided. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.0,
      "reason": "There is no evidence or quotes disclosing what fine-tuning data was used. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No evidence is provided regarding reinforcement learning data. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "There is no evidence or quoted information about how data filtering or cleaning was performed. No direct quote evidence; defaulting to Closed by strict rule."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights hosted on Hugging Face or equivalent"
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No code files."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The readme explicitly states 'license: apache-2.0', which permits use, modification, redistribution, and commercial use."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "A blog post is referenced for additional details. While this is official documentation, it is a blog/announcement rather than a full technical paper."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture disclosed on the model card/config"
    },
    "1-6 Tokenizer": {
      "score": 1,
      "reason": "Tokenizer details available on the model card/config"
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Detailed hardware requirements are disclosed (e.g., deployment on an RTX 4090, GPU RAM requirements), which demonstrates full disclosure of hardware details."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "The software stack is clearly detailed, including specific library names and version requirements (vllm and mistral_common), ensuring reproducibility."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "No concrete public API evidence (no endpoint/key/example)."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete pre-training method details (objectives/schedules/hyperparameters/pipeline). Only data scale/types/hardware were mentioned → Closed (0) by tight policy."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "No concrete fine-tuning method details (objectives/schedules/hyperparameters/pipeline). Only data scale/types/hardware were mentioned → Closed (0) by tight policy."
    },
    "4-1 Pre-training Data": {
      "score": 0.0,
      "reason": "No details or quotes about the pre-training dataset (sources, quantities, licensing) are provided. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.0,
      "reason": "There is no evidence or quotes disclosing what fine-tuning data was used. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "There is no evidence or quoted information about how data filtering or cleaning was performed. No direct quote evidence; defaulting to Closed by strict rule."
    }
  },
  "final_score_10pt": 4.643,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "not_used"
    },
    "excluded": [
      "3-3 Reinforcement Learning",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 14,
    "raw_sum": 6.5,
    "scale": "10/14",
    "code_detection_reason": "No code files.",
    "pretrain_sources_used": false
  }
}