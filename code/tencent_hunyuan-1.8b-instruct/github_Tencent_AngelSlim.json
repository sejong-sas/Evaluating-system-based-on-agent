{
    "repo": "Tencent/AngelSlim",
    "branch": "main",
    "files": [
        ".github/workflows/code-format.yml",
        ".github/workflows/codeql.yml",
        ".github/workflows/deploy.yml",
        ".gitignore",
        ".pre-commit-config.yaml",
        ".readthedocs.yaml",
        "LICENSE",
        "README.md",
        "README_en.md",
        "angelslim/__init__.py",
        "angelslim/compressor/__init__.py",
        "angelslim/compressor/cache/__init__.py",
        "angelslim/compressor/cache/cache.py",
        "angelslim/compressor/cache/cachepipeline.py",
        "angelslim/compressor/cache/deepcache.py",
        "angelslim/compressor/cache/teacache.py",
        "angelslim/compressor/compressor_factory.py",
        "angelslim/compressor/distill/__init__.py",
        "angelslim/compressor/quant/__init__.py",
        "angelslim/compressor/quant/core/__init__.py",
        "angelslim/compressor/quant/core/config.py",
        "angelslim/compressor/quant/core/dit_hook.py",
        "angelslim/compressor/quant/core/fp8_analyse_tools.py",
        "angelslim/compressor/quant/core/hook.py",
        "angelslim/compressor/quant/core/metrics.py",
        "angelslim/compressor/quant/core/packing_utils.py",
        "angelslim/compressor/quant/core/quant_func.py",
        "angelslim/compressor/quant/core/sample_func.py",
        "angelslim/compressor/quant/core/save.py",
        "angelslim/compressor/quant/modules/__init__.py",
        "angelslim/compressor/quant/modules/awq/__init__.py",
        "angelslim/compressor/quant/modules/awq/auto_clip.py",
        "angelslim/compressor/quant/modules/awq/auto_scale.py",
        "angelslim/compressor/quant/modules/awq/awq.py",
        "angelslim/compressor/quant/modules/awq/search.py",
        "angelslim/compressor/quant/modules/catcher.py",
        "angelslim/compressor/quant/modules/fp8/__init__.py",
        "angelslim/compressor/quant/modules/fp8/fp8.py",
        "angelslim/compressor/quant/modules/gptq/__init__.py",
        "angelslim/compressor/quant/modules/gptq/gptq.py",
        "angelslim/compressor/quant/modules/gptq/gptq_module.py",
        "angelslim/compressor/quant/modules/helper_layer.py",
        "angelslim/compressor/quant/modules/int8/__init__.py",
        "angelslim/compressor/quant/modules/int8/int8.py",
        "angelslim/compressor/quant/modules/smooth/__init__.py",
        "angelslim/compressor/quant/modules/smooth/smooth.py",
        "angelslim/compressor/quant/observers/__init__.py",
        "angelslim/compressor/quant/observers/abs_max_activation.py",
        "angelslim/compressor/quant/observers/abs_max_weight.py",
        "angelslim/compressor/quant/observers/base_observer.py",
        "angelslim/compressor/quant/observers/ema_activation.py",
        "angelslim/compressor/quant/observers/groupwise_weight.py",
        "angelslim/compressor/quant/observers/hist_activation.py",
        "angelslim/compressor/quant/observers/observer.py",
        "angelslim/compressor/quant/ptq.py",
        "angelslim/compressor/sparsity/__init__.py",
        "angelslim/compressor/speculative_decoding/__init__.py",
        "angelslim/data/__init__.py",
        "angelslim/data/base_dataset.py",
        "angelslim/data/dataloader.py",
        "angelslim/data/multimodal_dataset.py",
        "angelslim/data/text_dataset.py",
        "angelslim/engine.py",
        "angelslim/models/__init__.py",
        "angelslim/models/base_model.py",
        "angelslim/models/diffusion/__init__.py",
        "angelslim/models/diffusion/flux.py",
        "angelslim/models/llm/__init__.py",
        "angelslim/models/llm/deepseek.py",
        "angelslim/models/llm/hunyuan_dense.py",
        "angelslim/models/llm/hunyuan_moe.py",
        "angelslim/models/llm/kimi_k2.py",
        "angelslim/models/llm/llama.py",
        "angelslim/models/llm/modeling_deepseek.py",
        "angelslim/models/llm/qwen.py",
        "angelslim/models/model_factory.py",
        "angelslim/models/vlm/__init__.py",
        "angelslim/models/vlm/qwen_vl.py",
        "angelslim/tokenizer/__init__.py",
        "angelslim/tokenizer/kimi_k2.py",
        "angelslim/utils/__init__.py",
        "angelslim/utils/config_parser.py",
        "angelslim/utils/default_compress_config.py",
        "angelslim/utils/utils.py",
        "configs/deepseek_r1/fp8_static/deepseek_r1_fp8_static.yaml",
        "configs/deepseek_r1/fp8_static/deepseek_r1_fp8_static_low_memmory.yaml",
        "configs/deepseek_r1/w4a8_fp8/deepseek_r1_w4a8_fp8.yaml",
        "configs/deepseek_r1/w4a8_fp8/deepseek_r1_w4a8_fp8_low_memmory.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_dynamic/deepseek_r1_distill_qwen-14b_fp8_dynamic.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_dynamic/deepseek_r1_distill_qwen-1_5b_fp8_dynamic.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_dynamic/deepseek_r1_distill_qwen-32b_fp8_dynamic.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_dynamic/deepseek_r1_distill_qwen-7b_fp8_dynamic.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_static/deepseek_r1_distill_qwen-14b_fp8_static.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_static/deepseek_r1_distill_qwen-1_5b_fp8_static.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_static/deepseek_r1_distill_qwen-32b_fp8_static.yaml",
        "configs/deepseek_r1_distill_qwen/fp8_static/deepseek_r1_distill_qwen-7b_fp8_static.yaml",
        "configs/deepseek_r1_distill_qwen/int4_awq/deepseek_r1_distill_qwen-14b_int4_awq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_awq/deepseek_r1_distill_qwen-1_5b_int4_awq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_awq/deepseek_r1_distill_qwen-32b_int4_awq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_awq/deepseek_r1_distill_qwen-7b_int4_awq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_gptq/deepseek_r1_distill_qwen-14b_int4_gptq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_gptq/deepseek_r1_distill_qwen-1_5b_int4_gptq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_gptq/deepseek_r1_distill_qwen-32b_int4_gptq.yaml",
        "configs/deepseek_r1_distill_qwen/int4_gptq/deepseek_r1_distill_qwen-7b_int4_gptq.yaml",
        "configs/flux/flux-1-schnell_deepcache.yaml",
        "configs/flux/flux-1-schnell_teacache.yaml",
        "configs/hunyuan/fp8_dynamic/hunyuan_0_5b_dense_fp8_dynamic.yaml",
        "configs/hunyuan/fp8_dynamic/hunyuan_1_8b_dense_fp8_dynamic.yaml",
        "configs/hunyuan/fp8_dynamic/hunyuan_4b_dense_fp8_dynamic.yaml",
        "configs/hunyuan/fp8_dynamic/hunyuan_7b_dense_fp8_dynamic.yaml",
        "configs/hunyuan/fp8_dynamic/hunyuan_a13b_fp8_dynamic.yaml",
        "configs/hunyuan/fp8_static/hunyuan_0_5b_dense_fp8_static.yaml",
        "configs/hunyuan/fp8_static/hunyuan_1_8b_dense_fp8_static.yaml",
        "configs/hunyuan/fp8_static/hunyuan_4b_dense_fp8_static.yaml",
        "configs/hunyuan/fp8_static/hunyuan_7b_dense_fp8_static.yaml",
        "configs/hunyuan/fp8_static/hunyuan_a13b_fp8_static.yaml",
        "configs/hunyuan/fp8_static/hunyuan_a13b_fp8_static_low_memory.yaml",
        "configs/hunyuan/int4_awq/hunyuan-a13b_int4_awq.yaml",
        "configs/hunyuan/int4_awq/hunyuan_0_5b_dense_int4_awq.yaml",
        "configs/hunyuan/int4_awq/hunyuan_1_8b_dense_int4_awq.yaml",
        "configs/hunyuan/int4_awq/hunyuan_4b_dense_int4_awq.yaml",
        "configs/hunyuan/int4_awq/hunyuan_7b_dense_int4_awq.yaml",
        "configs/hunyuan/int4_gptq/hunyuan_0_5b_dense_int4_gptq.yaml",
        "configs/hunyuan/int4_gptq/hunyuan_1_8b_dense_int4_gptq.yaml",
        "configs/hunyuan/int4_gptq/hunyuan_4b_dense_int4_gptq.yaml",
        "configs/hunyuan/int4_gptq/hunyuan_7b_dense_int4_gptq.yaml",
        "configs/hunyuan/int4_gptq/hunyuan_a13b_int4_gptq.yaml",
        "configs/kimi_k2/fp8_static/kimi_k2_fp8_static.yaml",
        "configs/kimi_k2/fp8_static/kimi_k2_fp8_static_low_memmory.yaml",
        "configs/kimi_k2/w4a8_fp8/kimi_k2_w4a8_fp8.yaml",
        "configs/kimi_k2/w4a8_fp8/kimi_k2_w4a8_fp8_low_memmory.yaml",
        "configs/qwen2_5/fp8_dynamic/qwen2_5-1_5b_instruct_fp8_dynamic.yaml",
        "configs/qwen2_5/fp8_dynamic/qwen2_5-32b_instruct_fp8_dynamic.yaml",
        "configs/qwen2_5/fp8_dynamic/qwen2_5-7b_instruct_fp8_dynamic.yaml",
        "configs/qwen2_5/fp8_static/qwen2_5-1_5b_instruct_fp8_static.yaml",
        "configs/qwen2_5/fp8_static/qwen2_5-32b_instruct_fp8_static.yaml",
        "configs/qwen2_5/fp8_static/qwen2_5-7b_fp8_static.yaml",
        "configs/qwen2_5/fp8_static/qwen2_5-7b_fp8_static_low_memory.yaml",
        "configs/qwen2_5/fp8_static/qwen2_5-7b_instruct_fp8_static.yaml",
        "configs/qwen2_5/int4_awq/qwen2_5-1_5b_int4_awq.yaml",
        "configs/qwen2_5/int4_awq/qwen2_5-32b_int4_awq.yaml",
        "configs/qwen2_5/int4_awq/qwen2_5-7b_int4_awq.yaml",
        "configs/qwen2_5/int4_gptq/qwen2_5-1_5b_int4_gptq.yaml",
        "configs/qwen2_5/int4_gptq/qwen2_5-32b_int4_gptq.yaml",
        "configs/qwen2_5/int4_gptq/qwen2_5-7b_int4_gptq.yaml",
        "configs/qwen2_5_vl/fp8_dynamic/qwen2_5_vl-32b_fp8_dynamic.yaml",
        "configs/qwen2_5_vl/fp8_dynamic/qwen2_5_vl-3b_fp8_dynamic.yaml",
        "configs/qwen2_5_vl/fp8_dynamic/qwen2_5_vl-72b_fp8_dynamic.yaml",
        "configs/qwen2_5_vl/fp8_dynamic/qwen2_5_vl-7b_fp8_dynamic.yaml",
        "configs/qwen2_5_vl/fp8_static/qwen2_5_vl-32b_fp8_static.yaml",
        "configs/qwen2_5_vl/fp8_static/qwen2_5_vl-3b_fp8_static.yaml",
        "configs/qwen2_5_vl/fp8_static/qwen2_5_vl-72b_fp8_static.yaml",
        "configs/qwen2_5_vl/fp8_static/qwen2_5_vl-7b_fp8_static.yaml",
        "configs/qwen2_5_vl/int4_awq/qwen2_5_vl-32b_int4_awq.yaml",
        "configs/qwen2_5_vl/int4_awq/qwen2_5_vl-3b_int4_awq.yaml",
        "configs/qwen2_5_vl/int4_awq/qwen2_5_vl-72b_int4_awq.yaml",
        "configs/qwen2_5_vl/int4_awq/qwen2_5_vl-72b_int4_awq_low_memory.yaml",
        "configs/qwen2_5_vl/int4_awq/qwen2_5_vl-7b_int4_awq.yaml",
        "configs/qwen2_5_vl/int4_gptq/qwen2_5_vl-32b_int4_gptq.yaml",
        "configs/qwen2_5_vl/int4_gptq/qwen2_5_vl-3b_int4_gptq.yaml",
        "configs/qwen2_5_vl/int4_gptq/qwen2_5_vl-72b_int4_gptq.yaml",
        "configs/qwen2_5_vl/int4_gptq/qwen2_5_vl-7b_int4_gptq.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-0_6b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-14b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-1_7b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-32b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-4b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-8b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-a22b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_dynamic/qwen3-a3b_fp8_dynamic.yaml",
        "configs/qwen3/fp8_static/qwen3-0_6b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-0_6b_fp8_static_analyse.yaml",
        "configs/qwen3/fp8_static/qwen3-14b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-1_7b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-32b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-4b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-8b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-a22b_fp8_static.yaml",
        "configs/qwen3/fp8_static/qwen3-a22b_fp8_static_low_memroy.yaml",
        "configs/qwen3/fp8_static/qwen3-a3b_fp8_static.yaml",
        "configs/qwen3/int4_awq/qwen3-0_6b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-14b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-1_7b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-32b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-4b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-8b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-a22b_int4_awq.yaml",
        "configs/qwen3/int4_awq/qwen3-a3b_int4_awq.yaml",
        "configs/qwen3/int4_gptq/qwen3-0_6b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-14b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-1_7b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-32b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-4b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-8b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-a22b_int4_gptq.yaml",
        "configs/qwen3/int4_gptq/qwen3-a3b_int4_gptq.yaml",
        "configs/qwen3/int8_dynamic/qwen3-0_6b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-14b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-1_7b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-32b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-4b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-8b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-a22b_int8_dynamic.yaml",
        "configs/qwen3/int8_dynamic/qwen3-a3b_int8_dynamic.yaml",
        "configs/qwen3/smooth_int8/qwen3-0_6b_int8_dynamic_smooth.yaml",
        "configs/qwen3/smooth_int8/qwen3-14b_int8_dynamic_smooth.yaml",
        "configs/qwen3/smooth_int8/qwen3-1_7b_int8_dynamic_smooth.yaml",
        "configs/qwen3/smooth_int8/qwen3-32b_int8_dynamic_smooth.yaml",
        "configs/qwen3/smooth_int8/qwen3-4b_int8_dynamic_smooth.yaml",
        "configs/qwen3/smooth_int8/qwen3-8b_int8_dynamic_smooth.yaml",
        "configs/qwq/fp8_dynamic/qwq-32b_fp8_dynamic.yaml",
        "configs/qwq/fp8_static/qwq-32b_fp8_static.yaml",
        "configs/qwq/int4_awq/qwq-32b_int4_awq.yaml",
        "configs/qwq/int4_gptq/qwq-32b_int4_gptq.yaml",
        "dataset/README.md",
        "dataset/multimodal_fake_data/fake_data.json",
        "dataset/multimodal_fake_data/images/0.png",
        "dataset/multimodal_fake_data/images/1.png",
        "dataset/sharegpt_gpt4/sharegpt_gpt4_256.jsonl",
        "dataset/sharegpt_gpt4_qwen/sharegpt_gpt4-qwen3_a22B_output.jsonl",
        "deploy/lm_eval.sh",
        "deploy/lmms_eval.sh",
        "deploy/offline.py",
        "deploy/openai.sh",
        "deploy/run_sglang.sh",
        "deploy/run_vllm.sh",
        "docs/Makefile",
        "docs/README.md",
        "docs/make.bat",
        "docs/requirements.txt",
        "docs/source/assets/angel_slim_wechat.png",
        "docs/source/assets/architecture.png",
        "docs/source/assets/fp8_scale_analyse.png",
        "docs/source/assets/fp8_weight_analyse.png",
        "docs/source/assets/logos/angelslim_icon.png",
        "docs/source/assets/logos/angelslim_logo.png",
        "docs/source/assets/logos/angelslim_logo_light.png",
        "docs/source/conf.py",
        "docs/source/deployment/deploy.md",
        "docs/source/design/architecture.md",
        "docs/source/design/index.md",
        "docs/source/design/prepare_config.md",
        "docs/source/design/prepare_dataset.md",
        "docs/source/design/update_algorithm.md",
        "docs/source/features/cache/deepcache.md",
        "docs/source/features/cache/index.md",
        "docs/source/features/cache/teacache.md",
        "docs/source/features/quantization/awq.md",
        "docs/source/features/quantization/fp8.md",
        "docs/source/features/quantization/gptq.md",
        "docs/source/features/quantization/index.md",
        "docs/source/features/quantization/int8.md",
        "docs/source/features/speculative_decoding/eagle.md",
        "docs/source/features/speculative_decoding/index.md",
        "docs/source/getting_started/installation.md",
        "docs/source/getting_started/quickstrat.md",
        "docs/source/index.md",
        "docs/source/models/deepseek/deepseek_quant.md",
        "docs/source/models/flux/flux_cache.md",
        "docs/source/models/hunyuan/hunyuan_quant.md",
        "docs/source/models/kimi_k2/kimi_k2_quant.md",
        "docs/source/models/qwen/qwen_quant.md",
        "docs/source/models/qwenvl/qwenvl_quant.md",
        "docs/source/performance/quantization/benchmarks.md",
        "docs/source/performance/speculative_decoding/benchmarks.md",
        "requirements.txt",
        "setup.py",
        "tools/fp8_quant_analyse.py",
        "tools/run.py"
    ],
    "license_files": {
        "LICENSE": "Tencent is pleased to support the open source community by making AngelSlim-model compression tools available. \n\nCopyright (C) 2025 Tencent.  All rights reserved. \n\nAngelSlim-model compression tools is licensed under the Apache License Version 2.0 except for the third-party components listed below.\n\n\nTerms of the Apache License Version 2.0:\n--------------------------------------------------------------------\nApache License \n\nVersion 2.0, January 2004\n\nhttp://www.apache.org/licenses/ \n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and \n\nYou must cause any modified files to carry prominent notices stating that You changed the files; and \n\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and \n\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. \n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. \n\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\n\n\nOther dependencies and licenses:\n\n\nOpen Source Software Licensed under the MIT License:\n--------------------------------------------------------------------\n1. awq\nCopyright (c) 2023 MIT HAN Lab\n\n2. gptq\nCopyright (c) 2023 潘其威(William)\n\n\nTerms of the MIT License:\n--------------------------------------------------------------------\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
    },
    "readme": "简体中文 | [English](README_en.md)\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/source/assets/logos/angelslim_logo_light.png\">\n    <img alt=\"AngelSlim\" src=\"./docs/source/assets/logos/angelslim_logo.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\n致力于打造更易用、更全面和更高效的大模型压缩工具包\n</h3>\n\n<p align=\"center\">\n          📖 <a href=\"https://angelslim.readthedocs.io/\">Documentation</a>&nbsp&nbsp | &nbsp&nbsp🤗 <a href=\"https://huggingface.co/AngelSlim\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/organization/AngelSlim\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"./docs/source/assets/angel_slim_wechat.png\">WeChat (微信)</a> | &nbsp&nbsp🫨 <a href=\"https://discord.com/invite/dHVNeuNdFt\">Discord</a>\n<br>\n</p>\n\n\n## 目录\n- [最新进展](#最新进展)\n- [主要特性](#主要特性)\n- [支持模型](#支持模型)\n- [如何使用](#如何使用)\n  - [安装 AngelSlim](#安装-AngelSlim)\n  - [快速开始](#快速开始)\n  - [部署与测试](#部署与测试)\n- [Benchmark](#benchmark)\n- [许可协议](#许可协议)\n- [引用](#引用)\n- [技术交流](#技术交流)\n\n## 📣最新进展\n- [25/08/06] 我们支持了`Hunyuan 0.5B/1.8B/4B/7B`和`Qwen2.5VL 3B/7B/32B/72B`的FP8、INT4量化，支持了`DeepSeek-R1/V3`和`Kimi-K2`模型的`FP8-Static`、`W4A8-FP8`量化。我们还开源了`Hunyuan 1.8B/4B/7B`系列模型的Eagle3权重。\n- [25/07/04] 我们支持了`Hunyuan/Qwen2.5/Qwen3/DeepSeek-R1-Distill-Qwen`等模型的量化，包含INT8、FP8、INT4等算法。\n我们还开源了`Qwen3`系列模型的Eagle3权重。\n\nComing soon：\n- [ ] Diffusion模型压缩支持\n- [ ] 投机采样新算法发布\n\n## 🌟主要特性\n\n- **高度集成化**：本工具将主流的压缩算法集成到工具，开发者可一键式调用，具有很好的易用性。\n- **持续算法创新**：本工具除了集成工业界使用最广的算法，还持续自研更好的压缩算法，并且会陆续开源。\n- **追求极致性能**：在模型压缩流程、压缩算法部署方面，本工具持续端到端优化，例如单卡GPU可量化Qwen3-235B和Deepseek-R1。\n\n## 💼支持模型\n\n### 量化\n目前已支持文生文任务Hunyuan-Dense、Hunyuan-MoE、Qwen3-Dense、Qwen3-MoE、Qwen2.5、DeepSeek-R1蒸馏Qwen模型、QwQ等系列的主要模型：\n\n| 模型名      | FP8-Dynamic       | FP8-Static        | INT8-Dynamic | INT4-GPTQ         | INT4-AWQ          |\n| ---------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n| [Hunyuan-Dense](https://huggingface.co/collections/tencent/hunyuan-dense-model-6890632cda26b19119c9c5e7)   |      ✅           |         ✅           | ✅           |    ✅               |         ✅           |\n| [Hunyuan-MoE](https://huggingface.co/collections/tencent/hunyuan-a13b-685ec38e5b46321e3ea7c4be)   |      ✅           |         ✅           | ✅           |    ✅               |         ✅           |\n| [Qwen3-Dense](https://huggingface.co/collections/AngelSlim/qwen3-quant-68652e26da31740739d154f8)               |      ✅           |         ✅           | ✅           |    ✅               |         ✅           |\n| [Qwen3-MoE](https://huggingface.co/collections/AngelSlim/qwen3-quant-68652e26da31740739d154f8)            |      ✅           |         ✅           | ✅           |     ✅             |        ✅            |\n| [Qwen2.5](https://huggingface.co/collections/AngelSlim/qwen2-25-quant-68652d6cbdf5c0d4b1c4499a)            |      ✅           |         ✅           | ✅           |     ✅             |        ✅            |\n| [DeepSeek-R1-Distill-Qwen](https://huggingface.co/collections/AngelSlim/deepseek-r1-distill-quant-68652f16a9c206b030b05f7f) |      ✅           |         ✅           | ✅           |      ✅             |        ✅            |\n| [QwQ](https://huggingface.co/collections/AngelSlim/qwen3-quant-68652e26da31740739d154f8) |      ✅           |         ✅           |       ✅             | ✅           |       ✅            |\n\n### 投机采样\n\n#### Eagle3\n目前已开源Qwen3和Hunyuan系列模型的Eagle3权重。\n\n| Qwen3  Models   | Hunyuan Models     |\n| ----------|----------|\n| ✅ [Qwen3-1.7B](https://huggingface.co/AngelSlim/Qwen3-1.7B_eagle3)    |✅ [Hunyuan-1.8B-Instruct](https://huggingface.co/AngelSlim/Hunyuan-1.8B-Instruct_eagle3)    |\n| ✅ [Qwen3-4B](https://huggingface.co/AngelSlim/Qwen3-4B_eagle3)        |✅ [Hunyuan-4B-Instruct](https://huggingface.co/AngelSlim/Hunyuan-4B-Instruct_eagle3)        |\n| ✅ [Qwen3-8B](https://huggingface.co/AngelSlim/Qwen3-8B_eagle3)        |✅ [Hunyuan-7B-Instruct](https://huggingface.co/AngelSlim/Hunyuan-7B-Instruct_eagle3)        |\n| ✅ [Qwen3-14B](https://huggingface.co/AngelSlim/Qwen3-14B_eagle3)      |\n| ✅ [Qwen3-32B](https://huggingface.co/AngelSlim/Qwen3-32B_eagle3)      |\n| ✅ [Qwen3-30B-A3B](https://huggingface.co/AngelSlim/Qwen3-a3B_eagle3)  |\n\n\n\n\n\n## 🛎️如何使用\n\n### 安装 AngelSlim\n\n推荐使用`pip`直接安装最新稳定版`AngelSlim`：\n\n```shell\npip install angelslim\n```\n\n也可以选择克隆代码仓库后，以可编辑的方式从源代码安装：\n\n```shell\ncd AngelSlim && python setup.py install\n```\n\n更详细的安装说明可参考[安装文档](https://angelslim.readthedocs.io/zh-cn/latest/getting_started/installation.html)。\n\n### 快速开始\n\n完成安装`AngelSlim`后，您可以通过以下脚本快速开始，完成`Qwen3-1.7B`模型的静态`FP8`量化：\n\n- 一键式启动\n\n  ```shell\n  python3 tools/run.py -c configs/qwen3/fp8_static/qwen3-1_7b_fp8_static.yaml\n  ```\n\n  该示例将会加载`HugggingFace`模型， 使用`config`配置的`dataset`数据进行激活值校准，量化产出模型权重.\n\n- 源码启动\n\n  对`Qwen3-1.7B`完成动态`FP8`量化：\n\n  ```python\n  from angelslim.engine import Engine\n\n  slim_engine = Engine()\n  # Prepare model\n  slim_engine.prepare_model(model_name=\"Qwen\", model_path=\"Qwen/Qwen3-1.7B\",)\n  # Initialize compressor\n  slim_engine.prepare_compressor(\"PTQ\", default_method=\"fp8_dynamic\")\n  # Compress model\n  slim_engine.run()\n  # Save compressed model\n  slim_engine.save(\"./output\")\n  ```\n\n详情请参考[快速开始文档](https://angelslim.readthedocs.io/zh-cn/latest/getting_started/quickstrat.html)。\n\n### 部署与测试\n\n#### 1. 离线推理\n\n如果需要通过`transformers`加载量化模型，请在量化模型配置的`global`中设置`deploy_backend: huggingface`，或者直接手动将量化产出模型路径下`config.json`配置中的`ignored_layers`字段改为`ignore`。\n\n测试`transformers`加载量化模型离线推理：\n\n```shell\npython deploy/offline.py $MODEL_PATH\n```\n\n其中 `MODEL_PATH` 为量化产出模型路径。\n\n#### 2. 服务部署\n\n支持通过以下推理框架部署 OpenAI 兼容的 API 服务：\n\n**vLLM**\n\n[vLLM](https://github.com/vllm-project/vllm) 服务启动脚本，建议版本`vllm>=0.8.5.post1`，部署MOE INT8量化模型需要`vllm>=0.9.2`。\n\n```shell\nbash deploy/run_vllm.sh $MODEL_PATH\n```\n\n**SGLang**\n\n[SGLang](https://github.com/sgl-project/sglang) 服务启动脚本，建议版本 `sglang>=0.4.6.post1`：\n\n```shell\nbash deploy/run_sglang.sh $MODEL_PATH\n```\n\n#### 3. 服务调用\n\n通过 [OpenAI 格式](https://platform.openai.com/docs/api-reference/introduction) 接口发起请求：\n\n```shell\nbash deploy/openai.sh $MODEL_PATH\n```\n\n#### 4. 效果验证\n\n使用 [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) 评估量化模型精度，建议版本`lm-eval>=0.4.8`：\n\n```shell\nbash deploy/lm_eval.sh $MODEL_PATH\n```\n\n详细操作指南请参阅[部署文档](https://angelslim.readthedocs.io/zh-cn/latest/deployment/deploy.html)。\n\n## 📈Benchmark\n\n### （1）量化\n\n下面只展示了部分模型的效果测试情况，完整Benchmark可以参考[Benchmark文档](https://angelslim.readthedocs.io/zh-cn/latest/performance/quantization/benchmarks.html)\n\n#### Hunyuan系列模型\n\nHunyuan-Instruct的`BF16`、`FP8`、`INT4-GPTQ`、`INT4-AWQ`在`OlympiadBench`、`AIME 2024`、`DROP`、`GPQA-Diamond`上的评测结果如下：\n\n<table>\n  <thead>\n    <tr><th>Model</th><th>Quantization</th><th>OlympiadBench</th><th>AIME 2024</th><th>DROP</th><th>GPQA-Diamond</th></tr>\n  </thead>\n  <tbody>\n    <tr><td rowspan=\"4\">Hunyuan-A13B-Instruct</td>\n    <td>BF16</td><td>82.7</td><td>87.30</td><td>91.1</td><td>71.2</td></tr>\n    <tr><td>FP8-Static</td><td>83.0</td><td>86.7</td><td>91.1</td><td>-</td></tr>\n    <tr><td>Int4-GPTQ</td><td>82.7</td><td>86.7</td><td>91.1</td><td>-</td></tr>\n    <tr><td>Int4-AWQ</td><td>82.6</td><td>85.6</td><td>91.0</td><td>-</td></tr>\n  </tbody>\n  <tbody>\n    <tr><td rowspan=\"4\">Hunyuan-7B-Instruct</td>\n    <td>BF16</td>          <td>76.5</td><td>81.1</td><td>85.9</td><td>60.1</td></tr>\n    <tr><td>FP8-Static</td><td>76.6</td><td>80.9</td><td>86.0</td><td>60.1</td></tr>\n    <tr><td>Int4-GPTQ</td><td>76.2</td><td>81.0</td><td>85.7</td><td>60.0</td></tr>\n    <tr><td>Int4-AWQ</td><td>76.4</td><td>80.9</td><td>85.9</td><td>60.1</td></tr>\n  </tbody>\n  <tbody>\n    <tr><td rowspan=\"4\">Hunyuan-4B-Instruct</td>\n    <td>BF16</td>          <td>73.1</td><td>78.3</td><td>78.2</td><td>61.1</td></tr>\n    <tr><td>FP8-Static</td><td>73.1</td><td>76.6</td><td>78.3</td><td>60.2</td></tr>\n    <tr><td>Int4-GPTQ</td><td>72.9</td><td>-</td><td>78.1</td><td>58.1</td></tr>\n    <tr><td>Int4-AWQ</td><td>72.8</td><td>-</td><td>78.2</td><td>-</td></tr>\n  </tbody>\n  <tbody>\n    <tr><td rowspan=\"4\">Hunyuan-1.8B-Instruct</td>\n    <td>BF16</td>          <td>63.4</td><td>56.7</td><td>76.7</td><td>47.2</td></tr>\n    <tr><td>FP8-Static</td><td>62.5</td><td>55.2</td><td>75.1</td><td>47.7</td></tr>\n    <tr><td>Int4-GPTQ</td><td>60.9</td><td>-</td><td>73.0</td><td>44.4</td></tr>\n    <tr><td>Int4-AWQ</td><td>61.7</td><td>-</td><td>71.7</td><td>43.6</td></tr>\n  </tbody>\n  <tbody>\n    <tr><td rowspan=\"4\">Hunyuan-0.5B-Instruct</td>\n    <td>BF16</td>          <td>29.6</td><td>17.2</td><td>52.8</td><td>23.3</td></tr>\n    <tr><td>FP8-Static</td><td>29.6</td><td>17.2</td><td>51.6</td><td>22.5</td></tr>\n    <tr><td>Int4-GPTQ</td><td>26.8</td><td>-</td><td>50.9</td><td>23.3</td></tr>\n    <tr><td>Int4-AWQ</td><td>26.3</td><td>-</td><td>48.9</td><td>23.3</td></tr>\n  </tbody>\n</table>\n\n\n#### Qwen3系列模型\n\nQwen3系列模型的`BF16`、`FP8-Static`、`FP8-Dynamic`、`INT8-Dynamic`、`INT4-GPTQ`、`INT4-AWQ`在`CEVAL`、`MMLU`、`GSM8K`、`HUMANEVAL`上的评测结果如下：\n\n<table>\n  <thead>\n    <tr><th>Model</th><th>Quantization</th><th>CEVAL</th><th>MMLU</th><th>GSM8K</th><th>HUMANEVAL</th></tr>\n  </thead>\n  <tbody>\n    <tr><td rowspan=\"4\">Qwen3-0.6B</td><td>BF16</td><td>45.84</td><td>47.21</td><td>42.99</td><td>19.51</td></tr>\n    <tr><td>FP8-Static</td><td>45.99</td><td>46.87</td><td>38.06</td><td>18.90</td></tr>\n    <tr><td>FP8-Dynamic</td><td>45.99</td><td>46.93</td><td>38.29</td><td>20.73</td></tr>\n    <tr><td>INT8-Dynamic</td><td>45.17</td><td>46.95</td><td>41.17</td><td>21.34</td></tr>\n    <tr><td rowspan=\"6\">Qwen3-8B</td><td>BF16</td><td>79.27</td><td>74.78</td><td>87.79</td><td>63.41</td></tr>\n    <tr><td>FP8-Static</td><td>78.23</td><td>74.79</td><td>86.96</td><td>62.20</td></tr>\n    <tr><td>FP8-Dynamic</td><td>78.45</td><td>74.75</td><td>87.64</td><td>62.80</td></tr>\n    <tr><td>INT8-Dynamic</td><td>78.01</td><td>74.84</td><td>86.96</td><td>67.07</td></tr>\n    <tr><td>INT4-GPTQ</td><td>77.19</td><td>73.26</td><td>86.43</td><td>62.20</td></tr>\n    <tr><td>INT4-AWQ</td><td>76.15</td><td>73.59</td><td>86.96</td><td>63.41</td></tr>\n    <tr><td rowspan=\"6\">Qwen3-14B</td><td>BF16</td><td>83.06</td><td>78.90</td><td>88.40</td><td>55.49</td></tr>\n    <tr><td>FP8-Static</td><td>82.62</td><td>78.57</td><td>89.46</td><td>57.32</td></tr>\n    <tr><td>FP8-Dynamic</td><td>82.24</td><td>78.92</td><td>88.32</td><td>52.44</td></tr>\n    <tr><td>INT8-Dynamic</td><td>81.87</td><td>78.13</td><td>86.28</td><td>56.10</td></tr>\n    <tr><td>INT4-GPTQ</td><td>81.05</td><td>78.02</td><td>87.34</td><td>57.93</td></tr>\n    <tr><td>INT4-AWQ</td><td>82.02</td><td>77.68</td><td>84.23</td><td>61.59</td></tr>\n    <tr><td rowspan=\"5\">Qwen3-32B</td><td>BF16</td><td>86.55</td><td>82.00</td><td>74.53</td><td>37.80</td></tr>\n    <tr><td>FP8-Static</td><td>86.92</td><td>81.78</td><td>70.20</td><td>39.63</td></tr>\n    <tr><td>FP8-Dynamic</td><td>86.55</td><td>81.89</td><td>70.43</td><td>38.41</td></tr>\n    <tr><td>INT4-GPTQ</td><td>86.18</td><td>81.01</td><td>-</td><td>43.29</td></tr>\n    <tr><td>INT4-AWQ</td><td>86.18</td><td>81.54</td><td>-</td><td>36.59</td></tr>\n    <tr><td rowspan=\"4\">Qwen3-30B-A3B</td><td>BF16</td><td>83.66</td><td>79.36</td><td>89.99</td><td>31.71</td></tr>\n    <tr><td>FP8-Static</td><td>83.95</td><td>79.47</td><td>89.01</td><td>31.10</td></tr>\n    <tr><td>FP8-Dynamic</td><td>84.10</td><td>79.40</td><td>89.16</td><td>32.93</td></tr>\n    <tr><td>INT8-Dynamic</td><td>83.36</td><td>79.48</td><td>89.16</td><td>34.15</td></tr>\n    <tr><td rowspan=\"4\">Qwen3-235B-A22B</td><td>BF16</td><td>89.60</td><td>86.28</td><td>85.29</td><td>27.44</td></tr>\n    <tr><td>FP8-Static</td><td>89.67</td><td>86.19</td><td>86.96</td><td>27.44</td></tr>\n    <tr><td>FP8-Dynamic</td><td>89.67</td><td>86.18</td><td>85.22</td><td>28.05</td></tr>\n    <tr><td>INT8-Dynamic</td><td>88.93</td><td>86.20</td><td>86.20</td><td>23.78</td></tr>\n    <tr><td rowspan=\"5\">QwQ-32B</td><td>BF16</td><td>85.74</td><td>82.03</td><td>73.31</td><td>42.68</td></tr>\n    <tr><td>FP8-Static</td><td>85.44</td><td>81.91</td><td>75.36</td><td>42.68</td></tr>\n    <tr><td>FP8-Dynamic</td><td>85.07</td><td>81.93</td><td>75.66</td><td>42.07</td></tr>\n    <tr><td>INT4-GPTQ</td><td>84.03</td><td>81.26</td><td>68.23</td><td>45.73</td></tr>\n    <tr><td>INT4-AWQ</td><td>83.58</td><td>81.01</td><td>68.69</td><td>43.29</td></tr>\n  </tbody>\n</table>\n\n#### Qwen2.5VL系列模型\n\nQwen2.5VL系列模型的`BF16`、`FP8-Static`、`FP8-Dynamic`、`INT4-GPTQ`、`INT4-AWQ`在`MMMU_VAL`、`DocVQA_VAL`、`ChartQA_TEST`上的评测结果如下：\n\n<table>\n  <thead>\n    <tr><th>Model</th><th>Quantization</th><th>MMMU_VAL</th><th>MMLDocVQA_VALU</th><th>ChartQA_TEST</th></tr>\n  </thead>\n  <tbody>\n    <tr><td rowspan=\"5\">Qwen2.5VL-3B</td><td>BF16</td><td>47.11</td><td>78.57</td><td>80.32</td></tr>\n    <tr><td>FP8-Static</td><td>47.33</td><td>79.34</td><td>79.68</td></tr>\n    <tr><td>FP8-Dynamic</td><td>45.99</td><td>46.93</td><td>38.29</td></tr>\n    <tr><td>INT4-GPTQ</td><td>46.56</td><td>77.20</td><td>78.96</td></tr>\n    <tr><td>INT4-AWQ</td><td>45.78</td><td>-</td><td>79.60</td></tr>\n   <tr><td rowspan=\"5\">Qwen2.5VL-7B</td><td>BF16</td><td>45.44</td><td>89.71</td><td>84.64</td></tr>\n    <tr><td>FP8-Static</td><td>47.00</td><td>89.83</td><td>85.92</td></tr>\n    <tr><td>FP8-Dynamic</td><td>47.22</td><td>89.80</td><td>88.64</td></tr>\n    <tr><td>INT4-GPTQ</td><td>46.67</td><td>90.45</td><td>-</td></tr>\n    <tr><td>INT4-AWQ</td><td>45.67</td><td>89.28</td><td>-</td></tr>\n    <tr><td rowspan=\"5\">Qwen2.5VL-32B</td><td>BF16</td><td>57.00</td><td>90.03</td><td>-</td></tr>\n    <tr><td>FP8-Static</td><td>57.00</td><td>89.88</td><td>-</td></tr>\n    <tr><td>FP8-Dynamic</td><td>56.44</td><td>89.88</td><td>-</td></tr>\n    <tr><td>INT4-GPTQ</td><td>55.22</td><td>89.80 </td><td>-</td></tr>\n    <tr><td>INT4-AWQ</td><td>55.22</td><td>90.30</td><td>-</td></tr>\n    <tr><td rowspan=\"5\">Qwen2.5VL-72B</td><td>BF16</td><td>58.78</td><td>94.39</td><td>85.60</td></tr>\n    <tr><td>FP8-Static</td><td>57.89</td><td>94.41</td><td>85.84</td></tr>\n    <tr><td>FP8-Dynamic</td><td>58.67</td><td>94.38</td><td>85.60</td></tr>\n    <tr><td>INT4-GPTQ</td><td>57.56</td><td>94.46</td><td>86.48</td></tr>\n    <tr><td>INT4-AWQ</td><td>58.78</td><td>94.19</td><td>87.28</td></tr>\n  </tbody>\n</table>\n\n#### DeepSeek系列模型\n\nDeepSeek-R1-0528模型的`FP8-Block-Wise`、`W4A8-FP8`在`GPQA Diamond`、`AIME 2024`、`SimpleQA`、`LiveCodeBench`上的评测结果如下：\n\n<table>\n  <thead>\n    <tr><th>Model</th><th>Quantization</th><th>GPQA Diamond</th><th>AIME 2024</th><th>SimpleQA</th><th>LiveCodeBench</th></tr>\n  </thead>\n  <tbody>\n    <tr><td rowspan=\"6\">DeepSeek-R1-0528</td><td>FP8-Block-Wise</td><td>78.28</td><td>88.67</td><td>27.8</td><td>77.1</td></tr>\n    <tr><td>W4A8-FP8</td><td>77.37</td><td>88.67</td><td>26.83</td><td>78.86</td></tr>\n  </tbody>\n</table>\n\n> **备注**：\n> - 以上评测结果使用TRT-LLM框架部署测试5次求平均\n> - 评测时使用的超参如下:\n> ```json\n>{\n>  \"top_k\": 20,\n>  \"top_p\": 0.6,\n>  \"temperature\": 0.7,\n>  \"output_seq_len\": 32768,\n>  \"max_input_seq_len\": 16384\n>}\n>```\n\n\n#### 其他模型\n\n其他模型的`BF16`、`FP8-Static`、`FP8-Dynamic`、`INT4-GPTQ`、`INT4-AWQ`在`CEVAL`、`MMLU`、`GSM8K`上的评测结果如下：\n\n<table>\n  <thead>\n    <tr><th>Model</th><th>Quantization</th><th>CEVAL</th><th>MMLU</th><th>GSM8K</th></tr>\n  </thead>\n  <tbody>\n    <tr><td rowspan=\"3\">Qwen2.5-1.5B-Instruct</td><td>BF16</td><td>67.01</td><td>60.05</td><td>54.28</td></tr>\n    <tr><td>FP8-Static</td><td>66.27</td><td>60.23</td><td>-</td></tr>\n    <tr><td>FP8-Dynamic</td><td>66.79</td><td>60.08</td><td>51.71</td></tr>\n    <tr><td rowspan=\"5\">Qwen2.5-7B-Instruct</td><td>BF16</td><td>81.20</td><td>74.55</td><td>79.98</td></tr>\n    <tr><td>FP8-Static</td><td>81.13</td><td>74.03</td><td>79.30</td></tr>\n    <tr><td>FP8-Dynamic</td><td>80.31</td><td>74.07</td><td>79.00</td></tr>\n    <tr><td>INT4-GPTQ</td><td>79.05</td><td>73.05</td><td>74.75</td></tr>\n    <tr><td>INT4-AWQ</td><td>79.35</td><td>73.22</td><td>79.38</td></tr>\n    <tr><td rowspan=\"5\">Qwen2.5-32B-Instruct</td><td>BF16</td><td>87.30</td><td>83.21</td><td>81.73</td></tr>\n    <tr><td>FP8-Static</td><td>87.59</td><td>83.08</td><td>81.58</td></tr>\n    <tr><td>FP8-Dynamic</td><td>87.30</td><td>83.04</td><td>81.58</td></tr>\n    <tr><td>INT4-GPTQ</td><td>86.70</td><td>82.45</td><td>82.03</td></tr>\n    <tr><td>INT4-AWQ</td><td>87.00</td><td>82.64</td><td>-</td></tr>\n    <tr><td rowspan=\"5\">DeepSeek-R1-Distill-Qwen-7B</td><td>BF16</td><td>53.49</td><td>53.80</td><td>75.74</td></tr>\n    <tr><td>FP8-Static</td><td>53.57</td><td>54.17</td><td>76.19</td></tr>\n    <tr><td>FP8-Dynamic</td><td>52.97</td><td>54.13</td><td>74.15</td></tr>\n    <tr><td>INT4-GPTQ</td><td>51.86</td><td>52.44</td><td>75.89</td></tr>\n    <tr><td>INT4-AWQ</td><td>53.49</td><td>53.70</td><td>-</td></tr>\n    <tr><td rowspan=\"5\">DeepSeek-R1-Distill-Qwen-14B</td><td>BF16</td><td>77.71</td><td>74.28</td><td>85.67</td></tr>\n    <tr><td>FP8-Static</td><td>77.56</td><td>74.66</td><td>86.73</td></tr>\n    <tr><td>FP8-Dynamic</td><td>76.82</td><td>74.63</td><td>87.11</td></tr>\n    <tr><td>INT4-GPTQ</td><td>74.29</td><td>72.37</td><td>84.61</td></tr>\n    <tr><td>INT4-AWQ</td><td>74.81</td><td>73.00</td><td>86.05</td></tr>\n    <tr><td rowspan=\"5\">DeepSeek-R1-Distill-Qwen-32B</td><td>BF16</td><td>84.18</td><td>80.89</td><td>87.41</td></tr>\n    <tr><td>FP8-Static</td><td>83.43</td><td>80.90</td><td>87.57</td></tr>\n    <tr><td>FP8-Dynamic</td><td>83.73</td><td>81.10</td><td>86.43</td></tr>\n    <tr><td>INT4-GPTQ</td><td>84.10</td><td>79.80</td><td>86.73</td></tr>\n    <tr><td>INT4-AWQ</td><td>82.84</td><td>80.15</td><td>87.19</td></tr>\n  </tbody>\n</table>\n\n### （2）投机采样\n#### Qwen3 Series Models\nQwen3系列的Eagle3模型在MT-bench/HunmanEval/GSM8K/Alpaca上的加速结果如下：\n\n<table>\n  <thead>\n    <tr>\n        <th>&nbsp</th><th>&nbsp</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">MT-bench</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">HumanEval</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">GSM8K</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">Alpaca</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">Mean</th></tr>\n    <tr><th>Temperature</th><th>Model</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th></tr>\n  </thead>\n  <tbody>\n    <!-- <tr><td colspan=\"12\" style=\"text-align: center; vertical-align: middle;\"><strong>Temperature=0</strong></td></tr> -->\n    <tr><td rowspan=\"6\"><strong>T=0</strong></td>\n    <td>Qwen3-1.7B</td><td>2.05x</td><td>2.81</td><td>2.07x</td><td>2.93</td><td>2.11x</td><td>2.98</td><td>1.93x</td><td>2.69</td><td>2.04x</td><td>2.85</td></tr>\n    <tr> <td>Qwen3-4B</td><td>2.21x</td><td>3.01</td><td>2.36x</td><td>3.24</td><td>2.42x</td><td>3.13</td><td>2.32x</td><td>2.75</td><td>2.33x</td><td>3.03</td></tr>\n    <tr><td>Qwen3-8B</td><td>2.63x</td><td>3.65</td><td>2.76x</td><td>3.85</td><td>2.82x</td><td>3.90</td><td>2.62x</td><td>3.48</td><td>2.70x</td><td>3.72</td></tr>\n    <tr><td>Qwen3-14B</td><td>2.23x</td><td>3.30</td><td>2.53x</td><td>3.74</td><td>2.56x</td><td>3.79</td><td>2.16x</td><td>3.13</td><td>2.37x</td><td>3.49</td></tr>\n    <tr><td>Qwen3-32B</td><td>2.39x</td><td>2.78</td><td>2.37x</td><td>2.81</td><td>2.47x</td><td>2.92</td><td>2.42x</td><td>2.53</td><td>2.41x</td><td>2.76</td></tr>\n    <tr><td>Qwen3-30B-A3B</td><td>2.84x</td><td>3.63</td><td>2.27x</td><td>3.09</td><td>2.64x</td><td>3.42</td><td>2.83x</td><td>3.56</td><td>2.64x</td><td>3.42</td></tr>\n    <!-- <tr><td colspan=\"12\" style=\"text-align: center; vertical-align: middle;\"><strong>Temperature=1</strong></td></tr> -->\n    <tr><td rowspan=\"6\"><strong>T=1</strong></td>\n    <td>Qwen3-1.7B</td><td>1.74x</td><td>2.53</td><td>1.86x</td><td>2.70</td><td>1.82x</td><td>2.69</td><td>1.72x</td><td>2.46</td><td>1.93x</td><td>2.60</td></tr>\n    <tr><td>Qwen3-4B</td><td>1.93x</td><td>2.60</td><td>2.00x</td><td>2.84</td><td>2.11x</td><td>2.82</td><td>2.34x</td><td>2.50</td><td>1.75x</td><td>2.69</td></tr>\n    <tr><td>Qwen3-8B</td><td>1.98x</td><td>2.75</td><td>2.25x</td><td>3.11</td><td>2.31x</td><td>3.15</td><td>2.10x</td><td>2.76</td><td>2.90x</td><td>2.94</td></tr>\n    <tr><td>Qwen3-14B</td><td>1.71x</td><td>2.61</td><td>1.95x</td><td>2.87</td><td>2.04x</td><td>3.08</td><td>1.68x</td><td>2.55</td><td>2.90x</td><td>2.78</td></tr>\n    <tr><td>Qwen3-32B</td><td>1.62x</td><td>1.91</td><td>1.71x</td><td>2.05</td><td>1.78x</td><td>2.10</td><td>1.80x</td><td>1.95</td><td>1.62x</td><td>2.00</td></tr>\n    <tr><td>Qwen3-30B-A3B</td><td>1.91x</td><td>2.46</td><td>2.00x</td><td>2.64</td><td>1.90x</td><td>2.53</td><td>1.80x</td><td>2.32</td><td>1.90x</td><td>2.48</td></tr>\n  </tbody>\n</table>\n\nHunyuan系列的Eagle3模型在MT-bench/HunmanEval/GSM8K/Alpaca上的加速结果如下：\n\n<table>\n  <thead>\n    <tr>\n        <th>&nbsp</th><th>&nbsp</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">MT-bench</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">HumanEval</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">GSM8K</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">Alpaca</th>\n        <th colspan=\"2\" style=\"text-align: center; vertical-align: middle;\">Mean</th></tr>\n    <tr><th>Temperature</th><th>Model</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th><th>Speedup</th><th>τ</th></tr>\n  </thead>\n  <tbody>\n    <!-- <tr><td colspan=\"12\" style=\"text-align: center; vertical-align: middle;\"><strong>Temperature=0</strong></td></tr> -->\n    <tr><td rowspan=\"3\"><strong>T=0</strong></td>\n    <td>Hunyuan-1.8B-Instruct</td><td>1.97x</td><td>2.90</td><td>2.58x</td><td>3.73</td><td>2.61x</td><td>3.71</td><td>1.71x</td><td>2.43</td><td>2.22x</td><td>3.19</td></tr>\n    <tr> <td>Hunyuan-4B-Instruct</td><td>1.77x</td><td>2.60</td><td>2.64x</td><td>3.35</td><td>2.14x</td><td>3.17</td><td>1.72x</td><td>2.57</td><td>2.07x</td><td>2.92</td></tr>\n    <tr><td>Hunyuan-7B-Instruct</td><td>2.22x</td><td>3.58</td><td>3.59x</td><td>5.47</td><td>2.96x</td><td>4.68</td><td>1.64x</td><td>2.56</td><td>2.60x</td><td>4.07</td></tr>\n    <!-- <tr><td colspan=\"12\" style=\"text-align: center; vertical-align: middle;\"><strong>Temperature=1</strong></td></tr> -->\n    <tr><td rowspan=\"3\"><strong>T=1</strong></td>\n    <td>Hunyuan-1.8B-Instruct</td><td>1.58x</td><td>2.36</td><td>2.35x</td><td>3.56</td><td>2.23x</td><td>3.38</td><td>1.26x</td><td>1.87</td><td>1.86x</td><td>2.79</td></tr>\n    <tr><td>Hunyuan-4B-Instruct</td><td>1.36x</td><td>2.05</td><td>1.97x</td><td>2.86</td><td>1.72x</td><td>2.68</td><td>1.14x</td><td>1.76</td><td>1.55x</td><td>2.34</td></tr>\n    <tr><td>Hunyuan-7B-Instruct</td><td>1.90x</td><td>3.11</td><td>3.12x</td><td>5.09</td><td>2.74x</td><td>4.34</td><td>1.47x</td><td>2.39</td><td>2.31x</td><td>3.73</td></tr>\n  </tbody>\n</table>\n\n## 📝许可协议\n本项目的代码依照 [License for AngelSlim](LICENSE) 协议开源。\n\n## 🔗引用\n```\n@software{AngelSlim2025,\n    title={{AngelSlim}},\n    author={Tencent AngelSlim Project Contributors},\n    year={2025},\n    month={7},\n    url={https://github.com/Tencent/AngelSlim},\n}\n```\n\n## 💬技术交流\n\n- AngelSlim正在快速迭代更新中，后续会推出更多的功能，有问题或建议欢迎通过[GitHub Issues](https://github.com/Tencent/AngelSlim/issues)给我们提issue，或者加入[微信技术交流群](./docs/source/assets/angel_slim_wechat.png)。\n",
    "py_files": {
        "angelslim/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .engine import Engine  # noqa: F401\n",
        "angelslim/compressor/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .cache import Cache  # noqa: F401\nfrom .compressor_factory import CompressorFactory  # noqa: F401\nfrom .quant import PTQ  # noqa: F401\n",
        "angelslim/compressor/cache/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .cache import Cache  # noqa: F401\nfrom .cachepipeline import FluxCachePipeline  # noqa: F401\nfrom .deepcache import FluxDeepCacheHelper  # noqa: F401\nfrom .teacache import TeaCache  # noqa: F401\n",
        "angelslim/compressor/cache/cache.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ..compressor_factory import CompressorFactory\nfrom .deepcache import FluxDeepCacheHelper\nfrom .teacache import TeaCache\n\n__all__ = [\"Cache\"]\n\n\n@CompressorFactory.register\nclass Cache:\n    def __init__(self, model, slim_config=None):\n        \"\"\"\n        Diffusion Cache class for caching model layers during inference.\n        Args:\n            model(nn.Module, required): the model to be cached.\n            slim_config(dict, required): the configuration for caching.\n                - compress_config: the configuration for compression.\n                - global_config: the global configuration for the model.\n        \"\"\"\n        self.cache_model = model\n        # init cache config of model\n        self.init_cache(slim_config[\"compress_config\"])\n\n    def init_cache(self, slim_config):\n        \"\"\"Initialize the cache for the FLUX model.\n        Args:\n            slim_config (dict): Configuration for the cache.\n        \"\"\"\n        cache_config = slim_config.cache\n        if cache_config.name == \"DeepCache\":\n            if self.cache_model.model_type == \"flux\":\n                if cache_config.use_cache_helper:\n                    self.cache_model.cache_helper = FluxDeepCacheHelper(\n                        pipe_model=self.cache_model,\n                        no_cache_steps=cache_config.no_cache_steps,\n                        no_cache_block_id=cache_config.no_cache_block_id,\n                    )\n        elif cache_config.name == \"TeaCache\":\n            if not cache_config.use_cache_helper:\n                l1_distance = cache_config.accumulated_rel_l1_distance\n                self.cache_module = TeaCache(\n                    self.cache_model.model,\n                    model_type=self.cache_model.model_type,\n                    cnt=cache_config.cnt,\n                    num_steps=cache_config.num_steps,\n                    rel_l1_thresh=cache_config.rel_l1_thresh,\n                    accumulated_rel_l1_distance=l1_distance,\n                )\n        else:\n            raise ValueError(f\"Unsupported cache method: {cache_config.name}. \")\n",
        "angelslim/compressor/cache/cachepipeline.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\nimport torch\nfrom diffusers import FluxPipeline\nfrom diffusers.pipelines.flux.pipeline_flux import calculate_shift, retrieve_timesteps\nfrom diffusers.pipelines.flux.pipeline_output import FluxPipelineOutput\n\n__all__ = [\"FluxCachePipeline\"]\n\n\nclass FluxCachePipeline(FluxPipeline):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.cache_helper = None\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 28,\n        timesteps: List[int] = None,\n        guidance_scale: float = 3.5,\n        num_images_per_prompt: Optional[int] = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],  # noqa: B006\n        max_sequence_length: int = 512,\n    ):\n        height = height or self.default_sample_size * self.vae_scale_factor\n        width = width or self.default_sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            prompt_2,\n            height,\n            width,\n            prompt_embeds=prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n            max_sequence_length=max_sequence_length,\n        )\n\n        self._guidance_scale = guidance_scale\n        self._joint_attention_kwargs = joint_attention_kwargs\n        self._interrupt = False\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n\n        lora_scale = (\n            self.joint_attention_kwargs.get(\"scale\", None)\n            if self.joint_attention_kwargs is not None\n            else None\n        )\n        (\n            prompt_embeds,\n            pooled_prompt_embeds,\n            text_ids,\n        ) = self.encode_prompt(\n            prompt=prompt,\n            prompt_2=prompt_2,\n            prompt_embeds=prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            max_sequence_length=max_sequence_length,\n            lora_scale=lora_scale,\n        )\n\n        # 4. Prepare latent variables\n        num_channels_latents = self.transformer.config.in_channels // 4\n        latents, latent_image_ids = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 5. Prepare timesteps\n        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n        image_seq_len = latents.shape[1]\n        mu = calculate_shift(\n            image_seq_len,\n            self.scheduler.config.base_image_seq_len,\n            self.scheduler.config.max_image_seq_len,\n            self.scheduler.config.base_shift,\n            self.scheduler.config.max_shift,\n        )\n        timesteps, num_inference_steps = retrieve_timesteps(\n            self.scheduler,\n            num_inference_steps,\n            device,\n            timesteps,\n            sigmas,\n            mu=mu,\n        )\n        num_warmup_steps = max(\n            len(timesteps) - num_inference_steps * self.scheduler.order, 0\n        )\n        self._num_timesteps = len(timesteps)\n\n        # handle guidance\n        if self.transformer.config.guidance_embeds:\n            guidance = torch.full(\n                [1], guidance_scale, device=device, dtype=torch.float32\n            )\n            guidance = guidance.expand(latents.shape[0])\n        else:\n            guidance = None\n\n        # 6. Denoising loop\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                if self.interrupt:\n                    continue\n                if self.cache_helper:\n                    self.cache_helper.cur_timestep = i\n\n                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n\n                noise_pred = self.transformer(\n                    hidden_states=latents,\n                    timestep=timestep / 1000,\n                    guidance=guidance,\n                    pooled_projections=pooled_prompt_embeds,\n                    encoder_hidden_states=prompt_embeds,\n                    txt_ids=text_ids,\n                    img_ids=latent_image_ids,\n                    joint_attention_kwargs=self.joint_attention_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents_dtype = latents.dtype\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, return_dict=False\n                )[0]\n\n                if latents.dtype != latents_dtype:\n                    if torch.backends.mps.is_available():\n                        latents = latents.to(latents_dtype)\n\n                if callback_on_step_end is not None:\n                    callback_kwargs = {}\n                    for k in callback_on_step_end_tensor_inputs:\n                        callback_kwargs[k] = locals()[k]\n                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                    latents = callback_outputs.pop(\"latents\", latents)\n                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    progress_bar.update()\n\n                # if XLA_AVAILABLE:\n                #     xm.mark_step()\n\n        if output_type == \"latent\":\n            image = latents\n\n        else:\n            latents = self._unpack_latents(\n                latents, height, width, self.vae_scale_factor\n            )\n            latents = (\n                latents / self.vae.config.scaling_factor\n            ) + self.vae.config.shift_factor\n            image = self.vae.decode(latents, return_dict=False)[0]\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return FluxPipelineOutput(images=image)\n",
        "angelslim/compressor/cache/deepcache.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"FluxDeepCacheHelper\", \"HunyuanDitDeepCacheHelper\"]\n\n\nclass DeepCacheHelper(object):\n    def __init__(\n        self,\n        pipe_model=None,\n        timesteps=None,\n        no_cache_steps=None,\n        no_cache_block_id=None,\n        no_cache_layer_id=None,\n    ):\n        if pipe_model is not None:\n            self.pipe_model = pipe_model\n        if timesteps is not None:\n            self.timesteps = timesteps\n        if no_cache_steps is not None:\n            self.no_cache_steps = no_cache_steps\n        if no_cache_block_id is not None:\n            self.no_cache_block_id = no_cache_block_id\n        if no_cache_layer_id is not None:\n            self.no_cache_layer_id = no_cache_layer_id\n        self.set_default_blocktypes()\n        self.set_model_type()\n\n    def set_default_blocktypes(self, default_blocktypes=None):\n        self.default_blocktypes = default_blocktypes\n\n    def set_model_type(self, model_type=\"flux\"):\n        self.model_type = model_type\n\n    def enable(self):\n        assert self.pipe_model is not None\n        self.reset_states()\n        self.wrap_modules()\n\n    def disable(self):\n        self.unwrap_modules()\n        self.reset_states()\n\n    def is_skip_step(self, block_i, layer_i, blocktype):\n        self.start_timestep = (\n            self.cur_timestep if self.start_timestep is None else self.start_timestep\n        )  # For some pipeline that the first timestep != 0\n\n        if self.cur_timestep - self.start_timestep in self.no_cache_steps:\n            return False\n        if blocktype in self.default_blocktypes:\n            if block_i in self.no_cache_block_id[blocktype]:\n                return False\n            else:\n                return True\n        return True\n\n    def wrap_model_forward(self):\n        pass\n\n    def wrap_block_forward(self, block, block_name, block_i, layer_i, blocktype):\n        self.function_dict[(blocktype, block_name, block_i, layer_i)] = block.forward\n\n        def wrapped_forward(*args, **kwargs):\n            skip = self.is_skip_step(block_i, layer_i, blocktype)\n            result = (\n                self.cached_output[(blocktype, block_name, block_i, layer_i)]\n                if skip\n                else self.function_dict[(blocktype, block_name, block_i, layer_i)](\n                    *args, **kwargs\n                )\n            )\n            if not skip:\n                self.cached_output[(blocktype, block_name, block_i, layer_i)] = result\n            return result\n\n        block.forward = wrapped_forward\n\n    def wrap_modules(self):\n        pass\n\n    def unwrap_modules(self):\n        pass\n\n    def reset_states(self):\n        self.cur_timestep = 0\n        self.function_dict = {}\n        self.cached_output = {}\n        self.start_timestep = None\n\n\nclass FluxDeepCacheHelper(DeepCacheHelper):\n    def set_default_blocktypes(self, default_blocktypes=None):\n        self.default_blocktypes = [\"single\"]\n\n    def wrap_model_forward(self):\n        self.function_dict[\"model_forward\"] = self.pipe_model.forward\n\n        def wrapped_forward(*args, **kwargs):\n            result = self.function_dict[\"model_forward\"](*args, **kwargs)\n            return result\n\n        self.pipe_model.forward = wrapped_forward\n\n    def wrap_modules(self):\n        # 1. wrap flux forward\n        self.wrap_model_forward()\n        # 2. wrap double forward\n        for block_i, block in enumerate(self.pipe_model.transformer_blocks):\n            self.wrap_block_forward(block, \"block\", block_i, 0, blocktype=\"double\")\n\n        # 3. wrap single forward\n        for block_i, block in enumerate(self.pipe_model.single_transformer_blocks):\n            self.wrap_block_forward(block, \"block\", block_i, 0, blocktype=\"single\")\n\n    def unwrap_modules(self):\n        # 1. model forward\n        self.pipe_model.forward = self.function_dict[\"model_forward\"]\n        # 2. block forward\n        for block_i, block in enumerate(self.pipe_model.transformer_blocks):\n            block.forward = self.function_dict[(\"double\", \"block\", block_i, 0)]\n\n        # 3. single block forward\n        block_num = len(self.pipe_model.single_transformer_blocks)\n        for block_i, block in enumerate(self.pipe_model.single_transformer_blocks):\n            block.forward = self.function_dict[\n                (\"single\", \"block\", block_num - block_i - 1, 0)\n            ]\n\n\nclass HunyuanDitDeepCacheHelper(DeepCacheHelper):\n    def set_default_blocktypes(self, default_blocktypes=None):\n        self.default_blocktypes = [\"no_skip_block\", \"skip_block\"]\n\n    def set_model_type(self, model_type=\"hunyuandit\"):\n        self.model_type = model_type\n\n    def wrap_model_forward(self):\n        self.function_dict[\"model_forward\"] = self.pipe_model.forward\n\n        def wrapped_forward(*args, **kwargs):\n            result = self.function_dict[\"model_forward\"](*args, **kwargs)\n            return result\n\n        self.pipe_model.forward = wrapped_forward\n\n    def wrap_modules(self):\n        # 1. wrap model forward\n        self.wrap_model_forward()\n        # 2. wrap block forward\n        block_num = len(self.pipe_model.blocks)\n        for block_i, block in enumerate(self.pipe_model.blocks):\n            if block_i <= block_num // 2:\n                self.wrap_block_forward(\n                    block, \"block\", block_i, 0, blocktype=\"no_skip_block\"\n                )\n            else:\n                self.wrap_block_forward(\n                    block, \"block\", block_num - block_i - 1, 0, blocktype=\"skip_block\"\n                )\n\n    def unwrap_modules(self):\n        # 1. model forward\n        self.pipe_model.forward = self.function_dict[\"model_forward\"]\n        # 2. block forward\n        block_num = len(self.pipe_model.blocks)\n        for block_i, block in enumerate(self.pipe_model.blocks):\n            if block_i <= block_num // 2:\n                block.forward = self.function_dict[\n                    (\"no_skip_block\", \"block\", block_i, 0)\n                ]\n            else:\n                block.forward = self.function_dict[\n                    (\"skip_block\", \"block\", block_num - block_i - 1, 0)\n                ]\n",
        "angelslim/compressor/cache/teacache.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# TeaCache refer from: https://github.com/ali-vilab/TeaCache\n\nimport types\nfrom typing import Any, Dict, Optional, Union\n\nimport numpy as np\nimport torch\nfrom diffusers.models.modeling_outputs import Transformer2DModelOutput\nfrom diffusers.utils import (\n    USE_PEFT_BACKEND,\n    is_torch_version,\n    scale_lora_layers,\n    unscale_lora_layers,\n)\n\nfrom ...utils import print_info\n\n\nclass TeaCache:\n    def __init__(\n        self,\n        pipe_model,\n        model_type=\"flux\",\n        cnt=0,\n        num_steps=50,\n        rel_l1_thresh=0.6,\n        accumulated_rel_l1_distance=0.0,\n        previous_modulated_input=None,\n        previous_residual=None,\n    ):\n        \"\"\"\n        Initialize the TeaCache for Diffusion model.\n        Args:\n            pipe_model: The Diffusion model pipeline.\n            model_type: The type of the model, default is \"flux\".\n            cnt: Counter for the number of steps.\n            num_steps: Total number of steps for caching.\n            rel_l1_thresh: Relative L1 threshold for caching. 0.25 for 1.5x speedup,\n                0.4 for 1.8x speedup, 0.6 for 2.0x speedup, 0.8 for 2.25x speedup.\n            accumulated_rel_l1_distance: Accumulated relative L1 distance.\n            previous_modulated_input: Previous modulated input for caching.\n            previous_residual: Previous residual for caching.\n        \"\"\"\n        self.pipe_model = pipe_model\n        self.previous_modulated_input = previous_modulated_input\n        self.previous_residual = previous_residual\n        self.cnt = cnt\n        self.num_steps = num_steps\n        self.rel_l1_thresh = rel_l1_thresh\n        self.accumulated_rel_l1_distance = accumulated_rel_l1_distance\n        self._init_teacache(model_type)\n\n    def _init_teacache(self, model_type):\n        if model_type == \"flux\":\n            teacache_forward = flux_teacache_forward\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}.\")\n        self.pipe_model.transformer.forward = types.MethodType(\n            teacache_forward, self.pipe_model.transformer\n        )\n        self.pipe_model.transformer.__class__.enable_teacache = True\n        self.pipe_model.transformer.__class__.cnt = self.cnt\n        self.pipe_model.transformer.__class__.num_steps = self.num_steps\n        self.pipe_model.transformer.__class__.rel_l1_thresh = self.rel_l1_thresh\n        self.pipe_model.transformer.__class__.accumulated_rel_l1_distance = (\n            self.accumulated_rel_l1_distance\n        )\n        self.pipe_model.transformer.__class__.previous_modulated_input = (\n            self.previous_modulated_input\n        )\n        self.pipe_model.transformer.__class__.previous_residual = self.previous_residual\n\n\ndef flux_teacache_forward(\n    self,\n    hidden_states: torch.Tensor,\n    encoder_hidden_states: torch.Tensor = None,\n    pooled_projections: torch.Tensor = None,\n    timestep: torch.LongTensor = None,\n    img_ids: torch.Tensor = None,\n    txt_ids: torch.Tensor = None,\n    guidance: torch.Tensor = None,\n    joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_block_samples=None,\n    controlnet_single_block_samples=None,\n    return_dict: bool = True,\n    controlnet_blocks_repeat: bool = False,\n) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n    \"\"\"\n    The [`FluxTransformer2DModel`] forward method.\n\n    Args:\n        hidden_states (`torch.FloatTensor` of shape\n            `(batch size, channel, height, width)`): Input `hidden_states`.\n        encoder_hidden_states (`torch.FloatTensor` of shape\n            `(batch size, sequence_len, embed_dims)`): Conditional embeddings\n            (embeddings computed from the input conditions such as prompts) to use.\n        pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`)\n            Embeddings projected from the embeddings of input conditions.\n        timestep ( `torch.LongTensor`):\n            Used to indicate denoising step.\n        block_controlnet_hidden_states: (`list` of `torch.Tensor`): A list of tensors\n            that if specified are added to the residuals of transformer blocks.\n        joint_attention_kwargs (`dict`, optional): A kwargs dictionary that if specified\n            is passed along to the AttentionProcessor as defined under self.processor in\n            https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py. # noqa: E501\n        return_dict (`bool`, optional, defaults to `True`): Whether or not to return a\n            [~models.transformer_2d.Transformer2DModelOutput] instead of a plain tuple.\n\n    Returns:\n        If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`]\n        is returned, otherwise a `tuple` where the first element is the sample tensor.\n    \"\"\"\n    if joint_attention_kwargs is not None:\n        joint_attention_kwargs = joint_attention_kwargs.copy()\n        lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n    else:\n        lora_scale = 1.0\n\n    if USE_PEFT_BACKEND:\n        # weight the lora layers by setting `lora_scale` for each PEFT layer\n        scale_lora_layers(self, lora_scale)\n    else:\n        if (\n            joint_attention_kwargs is not None\n            and joint_attention_kwargs.get(\"scale\", None) is not None\n        ):\n            print_info(\n                \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"  # noqa: E501\n            )\n\n    hidden_states = self.x_embedder(hidden_states)\n\n    timestep = timestep.to(hidden_states.dtype) * 1000\n    if guidance is not None:\n        guidance = guidance.to(hidden_states.dtype) * 1000\n    else:\n        guidance = None\n\n    temb = (\n        self.time_text_embed(timestep, pooled_projections)\n        if guidance is None\n        else self.time_text_embed(timestep, guidance, pooled_projections)\n    )\n    encoder_hidden_states = self.context_embedder(encoder_hidden_states)\n\n    if txt_ids.ndim == 3:\n        print_info(\n            \"Passing `txt_ids` 3d torch.Tensor is deprecated.\"\n            \"Please remove the batch dimension and pass it as a 2d torch Tensor\"\n        )\n        txt_ids = txt_ids[0]\n    if img_ids.ndim == 3:\n        print_info(\n            \"Passing `img_ids` 3d torch.Tensor is deprecated.\"\n            \"Please remove the batch dimension and pass it as a 2d torch Tensor\"\n        )\n        img_ids = img_ids[0]\n\n    ids = torch.cat((txt_ids, img_ids), dim=0)\n    image_rotary_emb = self.pos_embed(ids)\n\n    if (\n        joint_attention_kwargs is not None\n        and \"ip_adapter_image_embeds\" in joint_attention_kwargs\n    ):\n        ip_adapter_image_embeds = joint_attention_kwargs.pop(\"ip_adapter_image_embeds\")\n        ip_hidden_states = self.encoder_hid_proj(ip_adapter_image_embeds)\n        joint_attention_kwargs.update({\"ip_hidden_states\": ip_hidden_states})\n\n    if self.enable_teacache:\n        inp = hidden_states.clone()\n        temb_ = temb.clone()\n        modulated_inp, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n            self.transformer_blocks[0].norm1(inp, emb=temb_)\n        )\n        if self.cnt == 0 or self.cnt == self.num_steps - 1:\n            should_calc = True\n            self.accumulated_rel_l1_distance = 0\n        else:\n            coefficients = [\n                4.98651651e02,\n                -2.83781631e02,\n                5.58554382e01,\n                -3.82021401e00,\n                2.64230861e-01,\n            ]\n            rescale_func = np.poly1d(coefficients)\n            self.accumulated_rel_l1_distance += rescale_func(\n                (\n                    (modulated_inp - self.previous_modulated_input).abs().mean()\n                    / self.previous_modulated_input.abs().mean()\n                )\n                .cpu()\n                .item()\n            )\n            if self.accumulated_rel_l1_distance < self.rel_l1_thresh:\n                should_calc = False\n            else:\n                should_calc = True\n                self.accumulated_rel_l1_distance = 0\n        self.previous_modulated_input = modulated_inp\n        self.cnt += 1\n        if self.cnt == self.num_steps:\n            self.cnt = 0\n\n    if self.enable_teacache:\n        if not should_calc:\n            hidden_states += self.previous_residual\n        else:\n            ori_hidden_states = hidden_states.clone()\n            for index_block, block in enumerate(self.transformer_blocks):\n                if torch.is_grad_enabled() and self.gradient_checkpointing:\n\n                    def create_custom_forward(module, return_dict=None):\n                        def custom_forward(*inputs):\n                            if return_dict is not None:\n                                return module(*inputs, return_dict=return_dict)\n                            else:\n                                return module(*inputs)\n\n                        return custom_forward\n\n                    ckpt_kwargs: Dict[str, Any] = (\n                        {\"use_reentrant\": False}\n                        if is_torch_version(\">=\", \"1.11.0\")\n                        else {}\n                    )\n                    encoder_hidden_states, hidden_states = (\n                        torch.utils.checkpoint.checkpoint(\n                            create_custom_forward(block),\n                            hidden_states,\n                            encoder_hidden_states,\n                            temb,\n                            image_rotary_emb,\n                            **ckpt_kwargs,\n                        )\n                    )\n\n                else:\n                    encoder_hidden_states, hidden_states = block(\n                        hidden_states=hidden_states,\n                        encoder_hidden_states=encoder_hidden_states,\n                        temb=temb,\n                        image_rotary_emb=image_rotary_emb,\n                        joint_attention_kwargs=joint_attention_kwargs,\n                    )\n\n                # controlnet residual\n                if controlnet_block_samples is not None:\n                    interval_control = len(self.transformer_blocks) / len(\n                        controlnet_block_samples\n                    )\n                    interval_control = int(np.ceil(interval_control))\n                    # For Xlabs ControlNet.\n                    if controlnet_blocks_repeat:\n                        hidden_states = (\n                            hidden_states\n                            + controlnet_block_samples[\n                                index_block % len(controlnet_block_samples)\n                            ]\n                        )\n                    else:\n                        hidden_states = (\n                            hidden_states\n                            + controlnet_block_samples[index_block // interval_control]\n                        )\n            hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n\n            for index_block, block in enumerate(self.single_transformer_blocks):\n                if torch.is_grad_enabled() and self.gradient_checkpointing:\n\n                    def create_custom_forward(module, return_dict=None):\n                        def custom_forward(*inputs):\n                            if return_dict is not None:\n                                return module(*inputs, return_dict=return_dict)\n                            else:\n                                return module(*inputs)\n\n                        return custom_forward\n\n                    ckpt_kwargs: Dict[str, Any] = (\n                        {\"use_reentrant\": False}\n                        if is_torch_version(\">=\", \"1.11.0\")\n                        else {}\n                    )\n                    hidden_states = torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(block),\n                        hidden_states,\n                        temb,\n                        image_rotary_emb,\n                        **ckpt_kwargs,\n                    )\n\n                else:\n                    hidden_states = block(\n                        hidden_states=hidden_states,\n                        temb=temb,\n                        image_rotary_emb=image_rotary_emb,\n                        joint_attention_kwargs=joint_attention_kwargs,\n                    )\n\n                # controlnet residual\n                if controlnet_single_block_samples is not None:\n                    interval_control = len(self.single_transformer_blocks) / len(\n                        controlnet_single_block_samples\n                    )\n                    interval_control = int(np.ceil(interval_control))\n                    hidden_states[:, encoder_hidden_states.shape[1] :, ...] = (\n                        hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n                        + controlnet_single_block_samples[\n                            index_block // interval_control\n                        ]\n                    )\n\n            hidden_states = hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n            self.previous_residual = hidden_states - ori_hidden_states\n    else:\n        for index_block, block in enumerate(self.transformer_blocks):\n            if torch.is_grad_enabled() and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                ckpt_kwargs: Dict[str, Any] = (\n                    {\"use_reentrant\": False} if is_torch_version(\">=\", \"1.11.0\") else {}\n                )\n                encoder_hidden_states, hidden_states = (\n                    torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(block),\n                        hidden_states,\n                        encoder_hidden_states,\n                        temb,\n                        image_rotary_emb,\n                        **ckpt_kwargs,\n                    )\n                )\n\n            else:\n                encoder_hidden_states, hidden_states = block(\n                    hidden_states=hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    temb=temb,\n                    image_rotary_emb=image_rotary_emb,\n                    joint_attention_kwargs=joint_attention_kwargs,\n                )\n\n            # controlnet residual\n            if controlnet_block_samples is not None:\n                interval_control = len(self.transformer_blocks) / len(\n                    controlnet_block_samples\n                )\n                interval_control = int(np.ceil(interval_control))\n                # For Xlabs ControlNet.\n                if controlnet_blocks_repeat:\n                    hidden_states = (\n                        hidden_states\n                        + controlnet_block_samples[\n                            index_block % len(controlnet_block_samples)\n                        ]\n                    )\n                else:\n                    hidden_states = (\n                        hidden_states\n                        + controlnet_block_samples[index_block // interval_control]\n                    )\n        hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n\n        for index_block, block in enumerate(self.single_transformer_blocks):\n            if torch.is_grad_enabled() and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                ckpt_kwargs: Dict[str, Any] = (\n                    {\"use_reentrant\": False} if is_torch_version(\">=\", \"1.11.0\") else {}\n                )\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(block),\n                    hidden_states,\n                    temb,\n                    image_rotary_emb,\n                    **ckpt_kwargs,\n                )\n\n            else:\n                hidden_states = block(\n                    hidden_states=hidden_states,\n                    temb=temb,\n                    image_rotary_emb=image_rotary_emb,\n                    joint_attention_kwargs=joint_attention_kwargs,\n                )\n\n            # controlnet residual\n            if controlnet_single_block_samples is not None:\n                interval_control = len(self.single_transformer_blocks) / len(\n                    controlnet_single_block_samples\n                )\n                interval_control = int(np.ceil(interval_control))\n                hidden_states[:, encoder_hidden_states.shape[1] :, ...] = (\n                    hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n                    + controlnet_single_block_samples[index_block // interval_control]\n                )\n\n        hidden_states = hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n\n    hidden_states = self.norm_out(hidden_states, temb)\n    output = self.proj_out(hidden_states)\n\n    if USE_PEFT_BACKEND:\n        # remove `lora_scale` from each PEFT layer\n        unscale_lora_layers(self, lora_scale)\n\n    if not return_dict:\n        return (output,)\n\n    return Transformer2DModelOutput(sample=output)\n",
        "angelslim/compressor/compressor_factory.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Callable, Dict, Optional, Type, Union\n\nfrom ..utils import print_info\n\n\nclass CompressorFactory:\n    \"\"\"\n    Factory class for model compression methods with flexible registration.\n    Supports both explicit name registration and direct class name registration.\n    \"\"\"\n\n    _compress_methods: Dict[str, Type[Any]] = {}\n\n    @classmethod\n    def register(cls, name: Optional[Union[str, Callable]] = None) -> Callable:\n        \"\"\"Decorator to register compression methods. Supports two usage patterns:\n        1. @CompressorFactory.register(\"explicit_name\")\n        2. @CompressorFactory.register (uses class name as key)\n        \"\"\"\n\n        # Handler for direct class registration (@CompressorFactory.register)\n        def register_class(compress_cls: Type[Any]) -> Type[Any]:\n            \"\"\"Register a class using its own name as the key\"\"\"\n            key = compress_cls.__name__\n            if key in cls._compress_methods:\n                print_info(\n                    f\"Compression method '{key}' already exists, will be overwritten.\"\n                )\n            cls._compress_methods[key] = compress_cls\n            return compress_cls\n\n        # Handler for named registration (@CompressorFactory.register(\"name\"))\n        def register_with_name(key: str) -> Callable[[Type[Any]], Type[Any]]:\n            \"\"\"Decorator that registers a class with a custom key\"\"\"\n\n            def decorator(compress_cls: Type[Any]) -> Type[Any]:\n                if key in cls._compress_methods:\n                    print_info(f\"register '{key}' already exists, will be overwritten.\")\n                cls._compress_methods[key] = compress_cls\n                return compress_cls\n\n            return decorator\n\n        # Determine registration type based on input\n        if name is None:\n            # Case 1: Direct class registration (@CompressorFactory.register)\n            return register_class\n        elif isinstance(name, str):\n            # Case 2: Explicit name registration (@CompressorFactory.register(\"name\"))\n            return register_with_name(name)\n        elif callable(name):\n            # Case 3: Direct class registration (called without parentheses)\n            return register_class(name)\n        else:\n            raise TypeError(\"Invalid argument type for registration\")\n\n    @classmethod\n    def create(cls, name: str, model: Any, slim_config: Any) -> Any:\n        \"\"\"Create compressor instance\"\"\"\n        if name not in cls._compress_methods:\n            available = list(cls._compress_methods.keys())\n            raise ValueError(\n                f\"Compression method '{name}' not registered. Available: {available}\"\n            )\n        return cls._compress_methods[name](model, slim_config)\n\n    @classmethod\n    def get_available_compressor(cls) -> list:\n        return list(cls._compress_methods.keys())\n",
        "angelslim/compressor/distill/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .core import *  # noqa: F401 F403\nfrom .modules import *  # noqa: F401 F403\nfrom .observers import *  # noqa: F401 F403\nfrom .ptq import PTQ  # noqa: F401\n",
        "angelslim/compressor/quant/core/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config import *  # noqa: F401 F403\nfrom .hook import DiTHook, PTQHook  # noqa: F401\nfrom .metrics import mse_loss, snr_loss  # noqa: F401\nfrom .packing_utils import dequantize_gemm, pack_weight_to_int8  # noqa: F401\nfrom .quant_func import *  # noqa: F401 F403\nfrom .sample_func import EMASampler, MultiStepSampler  # noqa: F401\nfrom .save import DeepseekV3HfPTQSave  # noqa: F401\nfrom .save import DeepseekV3PTQSaveTRTLLM  # noqa: F401\nfrom .save import PTQPTMSave  # noqa: F401\nfrom .save import PTQSaveVllmHF  # noqa: F401\nfrom .save import PTQTorchSave  # noqa: F401\nfrom .save import PTQvLLMSaveHF  # noqa: F401\nfrom .save import PTQVLMSaveVllmHF  # noqa: F401\n",
        "angelslim/compressor/quant/core/config.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nfrom ..observers import (\n    AbsMaxChannelWiseWeightObserver,\n    AbsMaxGroupWiseWeightObserver,\n    AbsmaxPerchannelObserver,\n    AbsmaxPertensorObserver,\n)\n\nACT_OBSERVERS_CLASS = {\n    \"per-tensor\": AbsmaxPertensorObserver,\n    \"per-channel\": AbsmaxPerchannelObserver,\n}\nWEIGHT_OBSERVERS_CLASS = {\n    \"per-tensor\": AbsmaxPertensorObserver,\n    \"per-channel\": AbsMaxChannelWiseWeightObserver,\n    \"per-group\": AbsMaxGroupWiseWeightObserver,\n}\n\nKVCACHE_OBSERVERS_CLASS = {\"per-channel\": AbsmaxPerchannelObserver}\n\n\nclass QuantConfig:\n    r\"\"\"\n    Configure how to quantize a model or a part of the model. It will map each layer to\n    an instance of observers by the settings.\n\n    Args:\n        config: The quant config.\n    Examples:\n        .. code-block:: python\n\n            >>> from slim.quant import QuantConfig\n            >>> q_config = QuantConfig(yaml_config)\n    \"\"\"\n\n    def __init__(self, config, global_config=None):\n        # quant_algo change\n        self.act_observer = None\n        self.weight_observer = None\n        self.kv_cache_observer = None\n\n        quantization_args = config.quantization\n        self.quant_algo = quantization_args.name\n        self.quant_bit = quantization_args.bits\n        self.max_seq_length = global_config.max_seq_length\n        self.quant_helpers = quantization_args.quant_helpers\n        act_quant_method = quantization_args.quant_method.get(\"activation\", None)\n        weight_quant_method = quantization_args.quant_method[\"weight\"]\n\n        if \"fp8\" in self.quant_algo:\n            is_dynamic = \"dynamic\" if \"dynamic\" in self.quant_algo else \"static\"\n            assert (\n                is_dynamic or act_quant_method is not None\n            ), \"[Error] fp8_static need act_quant_method\"\n            self.act_observer = (\n                ACT_OBSERVERS_CLASS[act_quant_method]\n                if \"static\" in is_dynamic\n                else None\n            )\n            self.weight_observer = WEIGHT_OBSERVERS_CLASS[weight_quant_method]\n            self.kv_cache_observer = None\n\n            if \"w4a8\" in self.quant_algo:\n                group_size = (\n                    128\n                    if quantization_args.quant_method[\"group_size\"] == -1\n                    else quantization_args.quant_method[\"group_size\"]\n                )\n                self.quant_algo_info = {\n                    \"w\": f\"int4_{weight_quant_method}\",\n                    \"w_group_size\": group_size,\n                    \"ignore_layers\": quantization_args.ignore_layers,\n                }\n            else:\n                self.quant_algo_info = {\n                    \"w\": f\"fp8_{weight_quant_method}\",\n                    \"ignore_layers\": quantization_args.ignore_layers,\n                }\n\n            if act_quant_method is not None:\n                self.quant_algo_info[\"a\"] = f\"fp8_{act_quant_method}-{is_dynamic}\"\n            self.hidden_size = global_config.hidden_size\n            self.model_arch_type = global_config.model_arch_type\n            self.low_memory = config.quantization.low_memory\n            self.quant_analyse = config.quantization.quant_analyse\n            self.quant_vit = config.quantization.quant_vit\n        elif \"int8\" in self.quant_algo:\n            is_dynamic = \"dynamic\" if \"dynamic\" in self.quant_algo else \"static\"\n            assert (\n                is_dynamic or act_quant_method is not None\n            ), \"[Error] int8_static need act_quant_method\"\n            self.act_observer = (\n                ACT_OBSERVERS_CLASS[act_quant_method]\n                if \"static\" in is_dynamic\n                else None\n            )\n            self.weight_observer = WEIGHT_OBSERVERS_CLASS[weight_quant_method]\n            self.kv_cache_observer = None\n            self.quant_algo_info = {\n                \"w\": f\"int8_{weight_quant_method}\",\n                \"ignore_layers\": quantization_args.ignore_layers,\n            }\n            if act_quant_method is not None:\n                self.quant_algo_info[\"a\"] = f\"int8_{act_quant_method}-{is_dynamic}\"\n            self.hidden_size = global_config.hidden_size\n            self.model_arch_type = global_config.model_arch_type\n            self.low_memory = config.quantization.low_memory\n            self.quant_analyse = config.quantization.quant_analyse\n        elif \"int4_awq\" in self.quant_algo:\n            self.act_observer = None\n            self.weight_observer = None\n            self.kv_cache_observer = None\n            group_size = (\n                128\n                if quantization_args.quant_method[\"group_size\"] == -1\n                else quantization_args.quant_method[\"group_size\"]\n            )\n            self.quant_algo_info = {\n                \"zero_point\": quantization_args.quant_method[\"zero_point\"],\n                \"group_size\": int(group_size),\n                \"mse_range\": quantization_args.quant_method[\"mse_range\"],\n            }\n            self.hidden_size = global_config.hidden_size\n            self.model_arch_type = global_config.model_arch_type\n            self.low_memory = config.quantization.low_memory\n        elif \"int4_gptq\" in self.quant_algo:\n            self.act_observer = None\n            self.weight_observer = None\n            self.kv_cache_observer = None\n            group_size = (\n                128\n                if quantization_args.quant_method[\"group_size\"] == -1\n                else quantization_args.quant_method[\"group_size\"]\n            )\n            self.quant_algo_info = {\n                \"group_size\": group_size,\n                \"ignore_layers\": quantization_args.ignore_layers,\n            }\n            self.hidden_size = global_config.hidden_size\n\n        if \"smooth\" in self.quant_helpers:\n            self.smooth_alpha = quantization_args.smooth_alpha\n            self.smooth_observer = ACT_OBSERVERS_CLASS[\"per-channel\"]\n        self.custom_observe_layers_names = \"default\"\n\n    def custom_observe_layers(\n        self,\n        names: List,\n        act_observer=\"default\",\n        weight_observer=\"default\",\n        kv_cache_observer=\"default\",\n    ):\n        \"\"\"\n        name supports fuzzy search.\n        \"\"\"\n        self.custom_observe_layers_names = names\n        self.act_observer = (\n            act_observer if act_observer in ACT_OBSERVERS_CLASS else self.act_observer\n        )\n        self.weight_observer = (\n            weight_observer\n            if weight_observer in WEIGHT_OBSERVERS_CLASS\n            else self.weight_observer\n        )\n        self.kv_cache_observer = (\n            kv_cache_observer\n            if kv_cache_observer in KVCACHE_OBSERVERS_CLASS\n            else self.kv_cache_observer\n        )\n",
        "angelslim/compressor/quant/core/dit_hook.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nimport torch\nimport torch.nn as nn\n\n\ndef filter_func(name):\n    pattern = re.compile(\n        r\".*(mlp_t5|pooler|style_embedder|x_embedder|t_embedder|extra_embedder).*\"\n    )\n    return pattern.match(name) is not None\n\n\nclass DiTHook:\n    def __init__(self, model, use_transformer_engine=False):\n        \"\"\"\n        Args:\n            model(nn.Moudle, required): the model to be quant\n        \"\"\"\n        self.model = model\n        self.input_activation = {}\n        self.output_activation = {}\n        self.input_activation_cnt = {}\n        self.output_activation_cnt = {}\n        self.use_transformer_engine = use_transformer_engine\n        self._apply_hook()\n\n    def _apply_hook(self):\n        self._forward_hook_list = []\n        for name, sub_layer in self.model.named_modules():\n            if filter_func(name):\n                continue\n            instance_list = nn.Linear\n            if isinstance(sub_layer, instance_list):\n                forward_pre_hook_handle = sub_layer.register_forward_hook(\n                    self._forward_pre_hook\n                )\n                self._forward_hook_list.append(forward_pre_hook_handle)\n\n    def _forward_pre_hook(self, layer, input, output):\n        layer_name = \"\"\n        for name, module in self.model.named_modules():\n            if filter_func(name):\n                continue\n            if module == layer:\n                layer_name = name\n                break\n        x = (\n            output[0].detach().cpu()\n            if isinstance(output, tuple)\n            else output.detach().cpu()\n        )\n        self.output_activation[layer_name] = (\n            self.output_activation.get(layer_name, torch.zeros(x.shape).to(x.dtype)) + x\n        )\n        self.output_activation_cnt[layer_name] = (\n            self.output_activation_cnt.get(layer_name, 0) + 1\n        )\n        y = (\n            input[0].detach().cpu()\n            if isinstance(input, tuple)\n            else input.detach().cpu()\n        )\n        self.input_activation[layer_name] = (\n            self.input_activation.get(layer_name, torch.zeros(y.shape).to(y.dtype)) + y\n        )\n        self.input_activation_cnt[layer_name] = (\n            self.input_activation_cnt.get(layer_name, 0) + 1\n        )\n\n    def remove_hook(self):\n        for hook in self._forward_hook_list:\n            hook.remove()\n        self._forward_hook_list = []\n\n    def clean_acitvation_list(self):\n        self.input_activation = {}\n        self.output_activation = {}\n",
        "angelslim/compressor/quant/core/fp8_analyse_tools.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom safetensors.torch import load_file\n\n\ndef draw_sub_lines(weight_dict, ax, opname):\n    np_l = []\n    for v in weight_dict[opname]:\n        np_l.append(v)\n    np_l.sort(key=lambda x: x[0])\n    x_layer = [int(v[0]) for v in np_l]\n    y_scale = [float(v[1]) for v in np_l]\n    ax.plot(x_layer, y_scale, \"b-\")\n    ax.set_title(opname)\n    ax.set_xlabel(\"Layers\")\n    ax.set_ylabel(\"Scales\")\n    ax.grid(True)\n\n\ndef draw_fp8_scale_fig(model_path, save_path):\n    g = os.walk(model_path)\n    st_file_list = []\n    for path, _, file_list in g:\n        print(path)\n        for file_name in file_list:\n            if \"safetensors\" in file_name and \"index\" not in file_name:\n                st_file_list.append(file_name)\n    print(st_file_list)\n\n    weight_dict = {}  # {\"OP\": (layer, data)}\n\n    for file in st_file_list:\n        model_weight = load_file(os.path.join(model_path, file), device=\"cpu\")\n        for k in model_weight.keys():\n            if \"layers\" in k and \"scale\" in k:\n                k_spllit = k.split(\"layers\", 1)\n                num_layer = int(k_spllit[-1].split(\".\", 2)[1])\n                OP = k_spllit[-1].split(\".\", 2)[-1]\n                if OP not in weight_dict.keys():\n                    weight_dict[OP] = [(num_layer, model_weight[k].data.float())]\n                else:\n                    weight_dict[OP].append((num_layer, model_weight[k].data.float()))\n\n    weight_op = weight_dict.keys()\n    assert weight_op is not None, \"fp8 weight does not exist.\"\n    print(f\"weight scale op {weight_op}\")\n\n    # all fp8 scale\n    np_l = []\n    for opname in weight_op:\n        for v in weight_dict[opname]:\n            np_l.append(v[-1])\n            if v[-1].data > 1.5:\n                print(\n                    f\"[AngelSlim Warning] \"\n                    f\"layer_{v[0]}_{opname} The weight is too high:{v[-1]}. \"\n                    f\"It is recommended to clip it to 1.5 \"\n                )\n    a = np.array(np_l)\n    if len(a.shape) == 2:\n        a = a[:, 0]\n    elif len(a.shape) <= 1:\n        pass\n    else:\n        print(\"[AngelSlim Error] scale dim error \")\n    s = pd.Series(a)\n    plt.hist(s)\n    plt.savefig(os.path.join(save_path, \"all_quant_scale_histogram.jpg\"))\n    plt.cla()\n    plt.clf()\n    plt.close()\n\n    # per layer scale line\n    list_weight_op = list(weight_op)\n\n    list_weight_op.sort()\n    is_dynamic = True\n    for k in list_weight_op:\n        if \"input_scale\" in k:\n            is_dynamic = False\n            break\n    group_weight_op = []\n    if not is_dynamic:\n        while len(list_weight_op) > 0:\n            assert len(list_weight_op) % 2 == 0, \"Some fp8 scale are missing. \"\n            group_weight_op.append((list_weight_op.pop(0), list_weight_op.pop(0)))\n        print(f\"all scale sum {sum(np_l)}\")\n        if sum(np_l) < 3:\n            print(\n                \"[AngelSlim Warning] This model's scale is too small overall, \"\n                \"which is not conducive to the expression of FP8 precision.\"\n            )\n        for g_opname in group_weight_op:\n            plt.cla()\n            plt.clf()\n            plt.close()\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n            draw_sub_lines(weight_dict, ax1, g_opname[0])\n            draw_sub_lines(weight_dict, ax2, g_opname[1])\n            plt.savefig(\n                os.path.join(save_path, f\"./OP_{g_opname}_quant_scale_line.jpg\")\n            )\n    else:\n        print(\"dynamic fp8 analyse\")\n        for opname in list_weight_op:\n            plt.cla()\n            plt.clf()\n            plt.close()\n            fig, (ax1) = plt.subplots(1, 1, figsize=(6, 5))\n            draw_sub_lines(weight_dict, ax1, opname)\n            plt.savefig(os.path.join(save_path, f\"./OP_{opname}_quant_scale_line.jpg\"))\n\n\ndef get_weight_dict(model_path):\n    g = os.walk(model_path)\n    st_file_list = []\n\n    for path, _, file_list in g:\n        if model_path != path:\n            break\n        for file_name in file_list:\n            if \"safetensors\" in file_name and \"index\" not in file_name:\n                st_file_list.append(file_name)\n    weight_dict = {}  # {\"layer\": {op: data}\n    for file in st_file_list:\n        model_weight = load_file(os.path.join(model_path, file), device=\"cpu\")\n        for k in model_weight.keys():\n            if \"layers\" in k and \".weight\" in k and \"scale\" not in k:\n                k_spllit = k.split(\"layers\", 1)\n                num_layer = str(int(k_spllit[-1].split(\".\", 2)[1]))\n                op = k_spllit[-1].split(\".\", 2)[-1]\n                if num_layer not in weight_dict.keys():\n                    weight_dict[num_layer] = {}\n                    weight_dict[num_layer][op] = model_weight[k].data\n                else:\n                    weight_dict[num_layer][op] = model_weight[k].data\n\n    return weight_dict\n\n\ndef draw_hist(uniform_data, ax, name):\n    uniform_data.sort()\n    s = pd.Series(uniform_data)\n    ax.hist(s, bins=50, rwidth=1)\n    ax.set_title(name + \"_histgram\")\n    ax.grid(True)\n\n\ndef draw_bf16_fp8_weight_fig(bf16_path, fp8_path, save_path, layer_index):\n    bf16_weight_dict = get_weight_dict(bf16_path)\n    fp8_weight_dict = get_weight_dict(fp8_path)\n\n    for op_name in bf16_weight_dict[str(layer_index)].keys():\n        plt.cla()\n        plt.clf()\n        plt.close()\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n        tensor_data = bf16_weight_dict[str(layer_index)][op_name].float().view(-1)\n\n        bf16w = np.array(tensor_data)\n\n        draw_hist(bf16w, ax1, f\"BF16_{op_name}\")\n\n        fp8w = fp8_weight_dict[str(layer_index)][op_name].float().view(-1)\n\n        uniform_data = np.array(fp8w)\n        draw_hist(uniform_data, ax2, f\"FP8_{op_name}\")\n\n        plt.savefig(\n            os.path.join(save_path, f\"./layer_{layer_index}_op_{op_name}_histogram.jpg\")\n        )\n",
        "angelslim/compressor/quant/core/hook.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nimport torch\n\nfrom ..observers import ParentObserver, PTQObserver\nfrom .quant_func import get_fp_maxval, get_fp_search_maxval\n\n__all__ = [\"PTQHook\", \"DiTHook\"]\n\n\nclass PTQHook:\n    def __init__(self, model):\n        self.quant_model = model\n        self._forward_hook_list = []\n        # {name: layer}\n        self.quant_layers_dict = {}\n        # {layer: observer}\n        self.observer_dict = {}\n        self.kv_names = []\n\n    def apply_hook(self):\n        self.quant_layers_dict = self.quant_model.get_observer_layers()\n        self.kv_names = self.quant_model.get_kvcache_observer_layers_names(\n            self.quant_layers_dict.keys()\n        )\n        act_observer = self.quant_model.quant_algo_dict[\"act_observer\"]\n        weight_observer = self.quant_model.quant_algo_dict[\"weight_observer\"]\n        kv_cache_observer = self.quant_model.quant_algo_dict[\"kv_cache_observer\"]\n\n        quant_parent_dict = self.quant_model.get_parent_dict(self.quant_layers_dict)\n        parent_observers = {\n            v: ParentObserver() for v in set(quant_parent_dict.values())\n        }\n\n        # apply observers\n        for name, sub_layer in self.quant_layers_dict.items():\n            extra_kwargs = (\n                {\"parent_observer\": parent_observers[quant_parent_dict[name]]}\n                if name in quant_parent_dict\n                else {}\n            )\n            observer = PTQObserver(\n                sub_layer,\n                act_observer,\n                weight_observer,\n                kv_cache_observer if name in self.kv_names else None,\n                self.quant_model.quant_algo_dict,\n                **extra_kwargs\n            )\n            forward_hook_handle = sub_layer.register_forward_hook(self._forward_hook)\n            self.observer_dict[sub_layer] = observer\n            self._forward_hook_list.append(forward_hook_handle)\n\n    def apply_smooth_hook(self, smooth_mapping_layers, smooth_observer):\n        for smooth_layer, _ in smooth_mapping_layers.values():\n            observer = PTQObserver(\n                smooth_layer,\n                act_observer=None,\n                weight_observer=None,\n                kv_cache_observer=None,\n                quant_algo_dict=self.quant_model.quant_algo_dict,\n                smooth_act_observer=smooth_observer,\n            )\n            forward_hook_handle = smooth_layer.register_forward_hook(self._forward_hook)\n            self.observer_dict[smooth_layer] = observer\n            self._forward_hook_list.append(forward_hook_handle)\n\n    def _forward_hook(self, layer, input, output):\n        x = input[0].clone() if isinstance(input, tuple) else input.clone()\n        y = output[0].clone() if isinstance(output, tuple) else output.clone()\n        if hasattr(self.quant_model, \"apply_layer_norm_list\"):\n            if layer in self.quant_model.apply_layer_norm_list:\n                x = self.quant_model.apply_layer_norm(layer, x)\n        self.observer_dict[layer](x, y)\n        return output\n\n    def remove_hook(self):\n        for hook in self._forward_hook_list:\n            hook.remove()\n        self._forward_hook_list = []\n\n    def post_process(self):\n        maxval = get_fp_maxval(bits=8)\n        if self.quant_model.quant_algo_dict[\"w_quant_algo\"] == \"fp8\":\n            for k, v in self.quant_model.weight_scales_dict.items():\n                self.quant_model.weight_scales_dict[k] = v / maxval.type(v.dtype)\n        if self.quant_model.quant_algo_dict[\"a_quant_algo\"] == \"fp8\":\n            for name, sub_layer in self.quant_layers_dict.items():\n                if sub_layer in self.observer_dict:\n                    if name in self.quant_model.act_scales_dict.keys():\n                        act_dtype = self.quant_model.act_scales_dict[name].dtype\n                        if \"Search\" in str(self.observer_dict[sub_layer]):\n                            tmp_maxval = get_fp_search_maxval(\n                                self.observer_dict[sub_layer].sampled_input\n                            )\n                            self.quant_model.act_scales_dict[name] = (\n                                self.quant_model.act_scales_dict[name]\n                                / tmp_maxval.type(act_dtype)\n                            )\n                        else:\n                            self.quant_model.act_scales_dict[name] = (\n                                self.quant_model.act_scales_dict[name]\n                                / maxval.type(act_dtype)\n                            )\n        if self.quant_model.quant_algo_dict[\"c_quant_algo\"] == \"fp8\":\n            for k, v in self.quant_model.kv_cache_scales_dict.items():\n                self.quant_model.kv_cache_scales_dict[k] = v / maxval.type(v.dtype)\n\n\ndef _filter_func(name):\n    pattern = re.compile(\n        r\".*(mlp_t5|pooler|style_embedder|x_embedder|t_embedder|extra_embedder).*\"\n    )\n    return pattern.match(name) is not None\n\n\nclass DiTHook:\n    def __init__(self, model):\n        \"\"\"\n        Args:\n            model(nn.Moudle, required): the model to be quant\n        \"\"\"\n        self.model = model\n        self.input_activation = []\n        self.output_activation = []\n\n        self._apply_hook()\n\n    def _apply_hook(self):\n        self._forward_hook_list = []\n        for name, sub_layer in self.model.named_modules():\n            if _filter_func(name):\n                continue\n            if isinstance(sub_layer, (torch.nn.Conv2d, torch.nn.Linear)):\n                if \"blocks\" in name:\n                    # handle\n                    forward_pre_hook_handle = sub_layer.register_forward_hook(\n                        self._forward_pre_hook\n                    )\n                    self._forward_hook_list.append(forward_pre_hook_handle)\n\n    def _forward_pre_hook(self, layer, input, output):\n        layer_name = \"\"\n        for name, module in self.model.named_modules():\n            if _filter_func(name):\n                continue\n            if module == layer:\n                layer_name = name\n                break\n        x = (\n            output[0].detach().cpu()\n            if isinstance(output, tuple)\n            else output.detach().cpu()\n        )\n        self.output_activation.append((layer_name, x))\n        y = (\n            input[0].detach().cpu()\n            if isinstance(input, tuple)\n            else input.detach().cpu()\n        )\n        self.input_activation.append((layer_name, y))\n\n    def remove_hook(self):\n        for hook in self._forward_hook_list:\n            hook.remove()\n        self._forward_hook_list = []\n\n    def clean_acitvation_list(self):\n        self.input_activation = []\n        self.output_activation = []\n",
        "angelslim/compressor/quant/core/metrics.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n\ndef mse_loss(\n    y_pred: torch.Tensor, y_real: torch.Tensor, reduction: str = \"mean\"\n) -> torch.Tensor:\n    if y_pred.shape != y_real.shape:\n        raise ValueError(\n            f\"Can not compute mse loss for tensors with different shape. \"\n            f\"({y_pred.shape} and {y_real.shape})\"\n        )\n\n    mse = (y_pred - y_real) ** 2\n\n    if reduction == \"mean\":\n        return torch.mean(mse)\n    elif reduction == \"sum\":\n        return torch.sum(mse)\n    elif reduction == \"none\":\n        return mse\n    else:\n        raise ValueError(\"Unsupported reduction method.\")\n\n\ndef snr_loss(\n    y_pred: torch.Tensor, y_real: torch.Tensor, reduction: str = \"mean\"\n) -> torch.Tensor:\n    \"\"\"\n    Compute SNR between y_pred(tensor) and y_real(tensor)\n\n    SNR can be calcualted as following equation:\n\n        SNR(pred, real) = (pred - real) ^ 2 / (real) ^ 2\n\n    if x and y are matrixs, SNR error over matrix should be the\n    mean value of SNR error over all elements.\n\n        SNR(pred, real) = mean((pred - real) ^ 2 / (real) ^ 2)\n    Args:\n        y_pred (torch.Tensor): _description_\n        y_real (torch.Tensor): _description_\n        reduction (str, optional): _description_. Defaults to 'mean'.\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    if y_pred.shape != y_real.shape:\n        raise ValueError(\n            f\"Can not compute snr loss for tensors with different shape. \"\n            f\"({y_pred.shape} and {y_real.shape})\"\n        )\n    reduction = str(reduction).lower()\n\n    if y_pred.ndim == 1:\n        y_pred = y_pred.unsqueeze(0)\n        y_real = y_real.unsqueeze(0)\n\n    y_pred = y_pred.flatten(start_dim=1)\n    y_real = y_real.flatten(start_dim=1)\n\n    noise_power = torch.pow(y_pred - y_real, 2).sum(dim=-1)\n    signal_power = torch.pow(y_real, 2).sum(dim=-1)\n    snr = (noise_power) / (signal_power + 1e-7)\n\n    if reduction == \"mean\":\n        return torch.mean(snr)\n    elif reduction == \"sum\":\n        return torch.sum(snr)\n    elif reduction == \"none\":\n        return snr\n    else:\n        raise ValueError(\"Unsupported reduction method.\")\n",
        "angelslim/compressor/quant/core/packing_utils.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\n\nAWQ_ORDER = [0, 2, 4, 6, 1, 3, 5, 7]\nAWQ_REVERSE_ORDER = [0, 4, 1, 5, 2, 6, 3, 7]\n\n\ndef unpack_awq(qweight: torch.Tensor, qzeros: torch.Tensor, bits: int):\n    shifts = torch.arange(0, 32, bits, device=qzeros.device)\n\n    # unpacking columnwise\n    iweights = torch.bitwise_right_shift(qweight[:, :, None], shifts[None, None, :]).to(\n        torch.int8  # smallest dtype available\n    )\n    iweights = iweights.view(iweights.shape[0], -1)\n\n    # unpacking columnwise\n    if qzeros is not None:\n        izeros = torch.bitwise_right_shift(\n            qzeros[:, :, None], shifts[None, None, :]\n        ).to(\n            torch.int8  # smallest dtype available\n        )\n        izeros = izeros.view(izeros.shape[0], -1)\n    else:\n        izeros = qzeros\n\n    return iweights, izeros\n\n\ndef reverse_awq_order(iweights: torch.Tensor, izeros: torch.Tensor, bits: int):\n    reverse_order_tensor = torch.arange(\n        iweights.shape[-1],\n        dtype=torch.int32,\n        device=izeros.device,\n    )\n    reverse_order_tensor = reverse_order_tensor.view(-1, 32 // bits)\n    reverse_order_tensor = reverse_order_tensor[:, AWQ_REVERSE_ORDER]\n    reverse_order_tensor = reverse_order_tensor.view(-1)\n\n    if izeros is not None:\n        izeros = izeros[:, reverse_order_tensor]\n    iweights = iweights[:, reverse_order_tensor]\n\n    return iweights, izeros\n\n\ndef pack_exllama(iweights: torch.Tensor, izeros: torch.Tensor, bits: int):\n    shifts = torch.arange(0, 32, bits, device=iweights.device)\n\n    # packing rowwise\n    iweights = iweights.view(iweights.shape[0] // (32 // bits), 32 // bits, -1)\n    qweight = (\n        torch.bitwise_left_shift(iweights, shifts[None, :, None])\n        .sum(dim=1)\n        .to(torch.int32)\n    )\n\n    # packing columnwise\n    izeros = izeros.view(-1, izeros.shape[1] // (32 // bits), 32 // bits)\n    qzeros = (\n        torch.bitwise_left_shift(izeros, shifts[None, None, :])\n        .sum(dim=-1)\n        .to(torch.int32)\n    )\n\n    return qweight, qzeros\n\n\ndef unpack_reorder_pack(qweight, qzeros, bits):\n    # Unpack the qweight and qzeros tensors\n    iweight, izeros = unpack_awq(qweight, qzeros, bits)\n    # Reverse the order of the iweight and izeros tensors\n    iweight, izeros = reverse_awq_order(iweight, izeros, bits)\n\n    # overflow checks\n    iweight = torch.bitwise_and(iweight, (2**bits) - 1)\n    izeros = torch.bitwise_and(izeros, (2**bits) - 1)\n\n    # Subtract 1 from the izeros tensor (exllama adds 1 during inference)\n    # We can remove it if we remove the +1 in the exllama code\n    izeros = izeros - 1\n    # Pack the qweight and qzeros tensors\n    qweight, qzeros = pack_exllama(iweight, izeros, bits)\n\n    return qweight, qzeros\n\n\ndef dequantize_gemm(qweight, qzeros, scales, bits, group_size):\n    # Unpack the qweight and qzeros tensors\n    iweight, izeros = unpack_awq(qweight, qzeros, bits)\n    # Reverse the order of the iweight and izeros tensors\n    iweight, izeros = reverse_awq_order(iweight, izeros, bits)\n\n    # overflow checks\n    iweight = torch.bitwise_and(iweight, (2**bits) - 1)\n    izeros = torch.bitwise_and(izeros, (2**bits) - 1)\n\n    # fp16 weights\n    scales = scales.repeat_interleave(group_size, dim=0)\n    izeros = izeros.repeat_interleave(group_size, dim=0)\n    iweight = (iweight - izeros) * scales\n\n    return iweight\n\n\ndef pack_weight_to_int8(weight):\n    weight = weight.t().contiguous().cpu()\n    weight = weight.to(torch.float32).numpy().astype(np.int8)\n\n    i = 0\n    row = 0\n    packed_weight = np.zeros((weight.shape[0] // 2, weight.shape[1]), dtype=np.int8)\n    while row < packed_weight.shape[0]:\n        for j in range(i, i + (8 // 4)):\n            packed_weight[row] |= (weight[j] & 0x0F) << (4 * (j - i))\n        i += 8 // 4\n        row += 1\n\n    packed_weight = packed_weight.astype(np.int8)\n    packed_weight = torch.from_numpy(packed_weight).t().contiguous()\n    return packed_weight\n",
        "angelslim/compressor/quant/core/quant_func.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Tuple\n\nimport torch\nimport triton\nimport triton.language as tl\n\nfrom .metrics import mse_loss\n\n\n@torch.no_grad()\ndef pseudo_quantize_tensor(\n    w, w_bit=4, zero_point=True, q_group_size=-1, inplace=False, get_scale_zp=False\n):\n    org_w_shape = w.shape\n    if q_group_size > 0:\n        assert org_w_shape[-1] % q_group_size == 0\n        w = w.reshape(-1, q_group_size)\n    assert w.dim() == 2\n    if zero_point:\n        max_val = w.amax(dim=1, keepdim=True)\n        min_val = w.amin(dim=1, keepdim=True)\n        max_int = 2**w_bit - 1\n        min_int = 0\n        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n    else:  # we actually never used this\n        assert min_val is None\n        max_val = w.abs().amax(dim=1, keepdim=True)\n        max_val = max_val.clamp(min=1e-5)\n        max_int = 2 ** (w_bit - 1) - 1\n        min_int = -(2 ** (w_bit - 1))\n        scales = max_val / max_int\n        zeros = 0\n\n    assert torch.isnan(scales).sum() == 0\n    assert torch.isnan(w).sum() == 0\n\n    if inplace:\n        (\n            (w.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)\n        ).mul_(scales)\n    else:\n        w = (\n            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros\n        ) * scales\n    assert torch.isnan(w).sum() == 0\n\n    w = w.reshape(org_w_shape)\n\n    if get_scale_zp:\n        return w, scales.view(w.shape[0], -1), zeros.view(w.shape[0], -1)\n    else:\n        return w\n\n\ndef quantize_weight_per_tensor_fp8(\n    tensor: torch.Tensor, scale: torch.Tensor\n) -> Tuple[torch.Tensor, float]:\n    finfo = torch.finfo(torch.float8_e4m3fn)\n\n    qweight = (tensor / scale).clamp(min=finfo.min, max=finfo.max)\n    # Return both float8 data and the inverse scale (as float),\n    # as both required as inputs to torch._scaled_mm\n    qweight = qweight.to(torch.float8_e4m3fn)\n    scale = scale.float()\n    return qweight, scale\n\n\ndef quantize_activation_per_tensor_fp8(\n    tensor: torch.Tensor, scale: float\n) -> torch.Tensor:\n    finfo = torch.finfo(torch.float8_e4m3fn)\n    qweight = (tensor / scale).clamp(min=finfo.min, max=finfo.max)\n    return qweight.to(torch.float8_e4m3fn)\n\n\ndef gemm_fp8(act, act_scale, weight, weight_scale, bias, out_dtype):\n    if act.numel() == 0:\n        # Deal with empty tensors (triggeted by empty MoE experts)\n        return torch.empty(\n            size=(0, weight.shape[0]), dtype=out_dtype, device=act.device\n        )\n\n    # TODO: Disable native fp8 gemm for now, always just dequantize\n    # native_fp8_support = (\n    #     torch.cuda.is_available() and torch.cuda.get_device_capability() >= (8, 9)\n    # )\n    native_fp8_support = False\n    if native_fp8_support:\n        need_reshape = act.dim() == 3\n        if need_reshape:\n            batch_size = act.shape[0]\n            act_input = act.reshape(-1, act.shape[-1])\n        else:\n            batch_size = None\n            act_input = act\n        output, _ = torch._scaled_mm(\n            act_input,\n            weight.t(),\n            out_dtype=out_dtype,\n            scale_a=act_scale,\n            scale_b=weight_scale,\n            bias=bias,\n        )\n        if need_reshape:\n            output = output.reshape(\n                batch_size, output.shape[0] // batch_size, output.shape[1]\n            )\n    else:\n        output = torch.nn.functional.linear(\n            act.to(out_dtype) * act_scale.to(out_dtype),\n            weight.to(out_dtype) * weight_scale.to(out_dtype),\n            bias=bias,\n        )\n    return output\n\n\n@torch.no_grad()\ndef quantize_weight_int(\n    x: torch.Tensor, scales: torch.Tensor, bits=8\n) -> Tuple[torch.Tensor, float]:\n    if scales.ndim == 2:  # weight group-wise\n        scales = torch.repeat_interleave(scales, x.shape[1] // scales.shape[1], dim=-1)\n    bnt = (1 << (bits - 1)) - 1\n\n    while scales.ndim < x.ndim:\n        scales = scales.unsqueeze(-1)\n    scales.div_(bnt)\n    x.div_(scales).round_().clamp_(-bnt - 1, bnt)\n    return x, scales\n\n\ndef tensor_quant_dequant_int(x, scales, bits=8):\n    if scales.ndim == 2:  # weight group-wise\n        scales = torch.repeat_interleave(scales, x.shape[1] // scales.shape[1], dim=-1)\n    bnt = (1 << (bits - 1)) - 1\n\n    while scales.ndim < x.ndim:\n        scales = scales.unsqueeze(-1)\n    scales = scales.div(bnt)\n    quant_x = torch.clamp(torch.round(x / scales), -bnt - 1, bnt)\n    quant_dequant_x = quant_x * scales\n    return quant_dequant_x\n\n\ndef tensor_quant(x, scales, bits=8):\n    if len(scales.shape) == 2:  # weight group-wise\n        scales = torch.repeat_interleave(scales, x.shape[1] // scales.shape[1], dim=-1)\n    bnt = (1 << (bits - 1)) - 1\n    quant_scale = scales\n    for _ in range(len(x.shape) - len(scales.shape)):\n        quant_scale = quant_scale.unsqueeze(-1)\n    quant_x = torch.clamp(torch.round(x / quant_scale * bnt), -bnt - 1, bnt)\n    return quant_x\n\n\ndef compute_scales(x, method=\"abs_max\", group_size=-1):\n    if method == \"abs_max\":\n        quant_scale = float(torch.max(torch.abs(x.flatten())))\n        quant_scale = 1e-8 if quant_scale == 0.0 else quant_scale\n    elif method == \"avg\":\n        quant_scale, _ = x.view(x.shape[0], -1).abs().max(dim=1)\n        quant_scale = torch.mean(quant_scale)\n    elif method == \"abs_max_channel_wise\":\n        # abs_max_values, _ = x.abs().max(dim=-1, keepdim=True)\n        # quant_scale = abs_max_values.squeeze()\n        if len(x.shape) > 2:\n            x = x.flatten(1)\n        quant_scale, _ = x.abs().max(dim=-1)\n    elif method == \"groupwise\":\n        input_processed = x.view([x.shape[0], x.shape[1] // group_size, group_size])\n        abs_max_values, _ = input_processed.abs().max(dim=-1, keepdim=True)\n        quant_scale = abs_max_values.squeeze()\n    elif method == \"dynamic_per_token\":\n        init_shape = x.shape\n        reshaped_x = x.reshape((-1, x.shape[-1]))\n        tmp = torch.zeros(reshaped_x.shape[0], device=x.device)\n        xmin = torch.minimum(reshaped_x.min(1)[0], tmp)\n        xmax = torch.maximum(reshaped_x.max(1)[0], tmp)\n        xmax = torch.maximum(torch.abs(xmin), xmax)\n        tmp = xmax == 0\n        quant_scale = xmax.unsqueeze(1).repeat(1, reshaped_x.shape[-1])\n        quant_scale[tmp] = 1\n        quant_scale = quant_scale.reshape(init_shape)\n    return quant_scale\n\n\ndef fake_quant_dequant(x, method=\"abs_max\", bits=8, group_size=-1):\n    bnt = (1 << (bits - 1)) - 1\n    quant_scale = compute_scales(x, method=method, group_size=group_size)\n    if method == \"groupwise\":\n        quant_scale = torch.repeat_interleave(quant_scale, group_size, dim=-1)\n        quant_scale = quant_scale.reshape(x.shape)\n    for _ in range(len(x.shape) - len(quant_scale.shape)):\n        quant_scale = quant_scale.unsqueeze(-1)\n    quant_value = torch.clamp(torch.round(x / quant_scale * bnt), -bnt - 1, bnt)\n    quant_dequant_value = quant_value / bnt * quant_scale\n    return quant_dequant_value\n\n\ndef compute_scales_with_zero(x, bits=8, sym=False, perchannel=True):\n    maxq = torch.tensor(2**bits - 1)\n    shape = x.shape\n\n    if perchannel:\n        x = x.flatten(1)\n    else:\n        x = x.flatten().unsqueeze(0)\n    tmp = torch.zeros(x.shape[0], device=x.device)\n    xmin = torch.minimum(x.min(1)[0], tmp)\n    xmax = torch.maximum(x.max(1)[0], tmp)\n\n    if sym:\n        xmax = torch.maximum(torch.abs(xmin), xmax)\n        tmp = xmin < 0\n        if torch.any(tmp):\n            xmin[tmp] = -xmax[tmp]\n    tmp = (xmin == 0) & (xmax == 0)\n    xmin[tmp] = -1\n    xmax[tmp] = +1\n\n    if maxq < 0:\n        scale = xmax\n        zero = xmin\n    else:\n        scale = (xmax - xmin) / maxq\n        if sym:\n            zero = torch.full_like(scale, (maxq + 1) / 2)\n        else:\n            zero = torch.round(-xmin / scale)\n    shape = [-1] + [1] * (len(shape) - 1)\n    scale = scale.reshape(shape)\n    zero = zero.reshape(shape)\n    return scale, zero\n\n\ndef get_fp_maxval(bits=8, mantissa_bit=3, sign_bits=1):\n    _bits = torch.tensor(bits)\n    _mantissa_bit = torch.tensor(mantissa_bit)\n    _sign_bits = torch.tensor(sign_bits)\n    m = torch.clamp(torch.round(_mantissa_bit), 1, _bits - _sign_bits)\n    e = _bits - _sign_bits - m\n    bias = 2 ** (e - 1) - 1\n    mantissa = 1\n    for i in range(mantissa_bit - 1):\n        mantissa += 1 / (2 ** (i + 1))\n    maxval = mantissa * 2 ** (2**e - 1 - bias)\n    return maxval\n\n\ndef get_fp_search_maxval(x, bits=8, mantissa_bit=3, sign_bits=1):\n    best_scale = None\n    calibration_loss = float(\"inf\")\n    # for scale in search_scales:\n    for scale in range(1, 1000, 1):\n        scale /= 100\n        new_x = x / scale\n        quant_dequant_x = quantize_to_fp8(\n            new_x, bits=bits, mantissa_bit=mantissa_bit, sign_bits=sign_bits\n        )\n        quant_dequant_x *= scale\n        cur_loss = mse_loss(x, quant_dequant_x)\n        if cur_loss <= calibration_loss:\n            calibration_loss = cur_loss\n            best_scale = scale\n    return best_scale\n\n\ndef quantize_to_fp8(x, bits=8, mantissa_bit=3, sign_bits=1):\n    \"\"\"\n    Default is E4M3.\n    \"\"\"\n    bits = torch.tensor(bits)\n    mantissa_bit = torch.tensor(mantissa_bit)\n    sign_bits = torch.tensor(sign_bits)\n    m = torch.clamp(torch.round(mantissa_bit), 1, bits - sign_bits)\n    e = bits - sign_bits - m\n    bias = 2 ** (e - 1) - 1\n    mantissa = 1\n    for i in range(mantissa_bit - 1):\n        mantissa += 1 / (2 ** (i + 1))\n    maxval = mantissa * 2 ** (2**e - 1 - bias)\n    minval = -maxval\n    minval = -maxval if sign_bits == 1 else torch.zeros_like(maxval)\n    input_clamp = torch.min(torch.max(x, minval), maxval)\n    log_scales = torch.clamp(\n        (torch.floor(torch.log2(torch.abs(input_clamp)) + bias)).detach(), 1.0\n    )\n    log_scales = 2.0 ** (log_scales - m - bias.float())\n    # dequant\n    qdq_out = torch.round(input_clamp / log_scales) * log_scales\n    qdq_out = qdq_out.type(x.dtype)\n    return qdq_out\n\n\ndef tensor_quant_dequant_fp8(x, scale, bits=8, mantissa_bit=3, sign_bits=1):\n    for _ in range(len(x.shape) - 1):\n        scale = scale.unsqueeze(-1)\n    new_x = x / scale\n    quant_dequant_x = quantize_to_fp8(\n        new_x, bits=bits, mantissa_bit=mantissa_bit, sign_bits=sign_bits\n    )\n    quant_dequant_x *= scale\n    return quant_dequant_x\n\n\n# This function is copied from DeepSeek-V3 (MIT License):\n# Copyright (c) 2023 DeepSeek-AI\n# Original source: https://github.com/deepseek-ai/DeepSeek-V3\n@triton.jit\ndef weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Dequantizes weights using the provided scaling factors and stores the result.\n\n    Args:\n        x_ptr (tl.pointer): Pointer to the quantized weights.\n        s_ptr (tl.pointer): Pointer to the scaling factors.\n        y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.\n        M (int): Number of rows in the weight matrix.\n        N (int): Number of columns in the weight matrix.\n        BLOCK_SIZE (tl.constexpr): Size of the block for tiling.\n\n    Returns:\n        None\n    \"\"\"\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    n = tl.cdiv(N, BLOCK_SIZE)\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs = offs_m[:, None] * N + offs_n[None, :]\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    x = tl.load(x_ptr + offs, mask=mask).to(tl.float32)\n    s = tl.load(s_ptr + pid_m * n + pid_n)\n    y = x * s\n    tl.store(y_ptr + offs, y, mask=mask)\n\n\n# This function is copied from DeepSeek-V3 (MIT License):\n# Copyright (c) 2023 DeepSeek-AI\n# Original source: https://github.com/deepseek-ai/DeepSeek-V3\ndef weight_dequant(\n    x: torch.Tensor, s: torch.Tensor, block_size: int = 128\n) -> torch.Tensor:\n    \"\"\"\n    Dequantizes the given weight tensor using the provided scale tensor.\n\n    Args:\n        x (torch.Tensor): The quantized weight tensor of shape (M, N).\n        s (torch.Tensor): The scale tensor of shape (M, N).\n        block_size (int, optional): The block size to use for dequantization. Defaults to 128. # noqa: E501\n\n    Returns:\n        torch.Tensor: The dequantized weight tensor of the same shape as `x`.\n\n    Raises:\n        AssertionError: If `x` or `s` are not contiguous or if their dimensions are not 2. # noqa: E501\n    \"\"\"\n    assert x.is_contiguous() and s.is_contiguous(), \"Input tensors must be contiguous\"\n    assert x.dim() == 2 and s.dim() == 2, \"Input tensors must have 2 dimensions\"\n    M, N = x.size()\n    y = torch.empty_like(x, dtype=torch.get_default_dtype())\n    grid = lambda meta: (  # noqa: E731\n        triton.cdiv(M, meta[\"BLOCK_SIZE\"]),\n        triton.cdiv(N, meta[\"BLOCK_SIZE\"]),\n    )\n    weight_dequant_kernel[grid](x, s, y, M, N, BLOCK_SIZE=block_size)\n    return y\n",
        "angelslim/compressor/quant/core/sample_func.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\n\n\nclass MultiStepSampler:\n    def __init__(self):\n        pass\n\n    def sample(self, x, sampled_x=None, layer_name=None):\n        return torch.cat([x, sampled_x], dim=1)\n\n\nclass EMASampler:\n    def __init__(self):\n        self.ema_beta = 0.98\n        self.ema_step = {}\n        self.sampled = {}\n\n    def sample(self, x, sampled_x=None, layer_name=None):\n        if layer_name not in self.ema_step:\n            self.sampled[layer_name] = (1 - self.ema_beta) * x\n            self.ema_step[layer_name] = 0\n            return self.sampled[layer_name]\n        else:\n            v_ema = self.ema_beta * self.sampled[layer_name] + (1.0 - self.ema_beta) * x\n            self.sampled[layer_name] = v_ema\n            v_ema_corr = v_ema / float(\n                (1.0 - np.power(self.ema_beta, self.ema_step[layer_name] + 1.0))\n            )\n            self.ema_step[layer_name] += 1\n            return v_ema_corr\n",
        "angelslim/compressor/quant/core/save.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport re\nimport shutil\nfrom abc import ABCMeta, abstractmethod\nfrom copy import deepcopy\nfrom glob import glob\n\nimport torch\nimport torch.distributed as dist\nfrom safetensors.torch import load_file, safe_open\nfrom safetensors.torch import save_file as safe_save\nfrom safetensors.torch import save_model\nfrom tqdm import tqdm\nfrom transformers.models.deepseek_v3 import DeepseekV3Config\n\nfrom ....utils import print_info\nfrom ..modules import QDQModule, QDQSingleModule\nfrom .packing_utils import pack_weight_to_int8\nfrom .quant_func import fake_quant_dequant, tensor_quant, weight_dequant\n\n__all__ = [\"PTQvLLMSaveHF\"]\n\n\nclass PTQSaveBase(metaclass=ABCMeta):\n    def __init__(self, quant_model):\n        self.quant_model = quant_model\n\n    @abstractmethod\n    def save(self, save_path):\n        pass\n\n\nclass PTQvLLMSaveHF(PTQSaveBase):\n    def __init__(self, quant_model):\n        super(PTQvLLMSaveHF, self).__init__(quant_model=quant_model.model)\n\n    def save(self, save_path):\n        \"\"\"save quantized model and configs to local disk\"\"\"\n        os.makedirs(save_path, exist_ok=True)\n\n        state_dict = self.quant_model.state_dict()\n        for name in list(state_dict.keys()):\n            if \"qweight\" in name:\n                pop_name = name.replace(\"qweight\", \"layer.weight\")\n                if pop_name in state_dict.keys():\n                    state_dict.pop(pop_name)\n                pop_name = name.replace(\"qweight\", \"layer.bias\")\n                if pop_name in state_dict.keys():\n                    state_dict[name.replace(\"qweight\", \"bias\")] = state_dict[pop_name]\n                    state_dict.pop(pop_name)\n        print_info(\"state_dict:{}\".format(state_dict.keys()))\n        model_base_name = \"quant_model\"\n        model_save_name = model_base_name + \".safetensors\"\n        safetensors_metadata = {}\n        safetensors_metadata[\"format\"] = \"pt\"\n        safe_save(\n            state_dict, os.path.join(save_path, model_save_name), safetensors_metadata\n        )\n        self.quant_model.config.save_pretrained(save_path)\n\n\nclass PTQVLMSaveVllmHF(PTQSaveBase):\n    def __init__(self, quant_model):\n        super().__init__(quant_model=quant_model)\n\n    def save(self, save_path):\n        a_quant_algo = self.quant_model.quant_config.quant_algo_info[\"a\"]\n        ignored_layers = self.quant_model.skip_layer_names()\n\n        static_q_dict = {\n            \"quantization_config\": {\n                \"quant_method\": \"fp8\",\n                \"activation_scheme\": (\n                    \"dynamic\" if \"dynamic\" in a_quant_algo else \"static\"\n                ),\n                \"ignored_layers\": ignored_layers,\n            }\n        }\n        self.quant_model.get_model().config.update(static_q_dict)\n\n        os.makedirs(save_path, exist_ok=True)\n\n        if self.quant_model.quant_config.quant_algo == \"int8\":\n            for _, sub_layer in self.quant_model.quant_config.quant_layers_dict.items():\n                if isinstance(sub_layer, QDQSingleModule):\n                    sub_layer.weight = tensor_quant(\n                        sub_layer.weight, sub_layer.weight_scales\n                    )\n\n        self.quant_model.get_model().save_pretrained(save_path)\n        self.quant_model.processor.save_pretrained(save_path)\n        self.quant_model.tokenizer.save_pretrained(save_path)\n\n\nclass PTQSaveVllmHF(PTQSaveBase):\n    def __init__(self, quant_model):\n        super().__init__(quant_model=quant_model)\n\n    def save(self, save_path):\n        deploy_backend = self.quant_model.deploy_backend\n        ignore_field = \"ignored_layers\" if deploy_backend == \"vllm\" else \"ignore\"\n        w_quant_algo = self.quant_model.quant_config.quant_algo_info[\"w\"]\n        a_quant_algo = self.quant_model.quant_config.quant_algo_info[\"a\"]\n        ignored_layers = self.quant_model.skip_layer_names()\n        trtllm_config = {\n            \"quantization\": {\n                \"exclude_modules\": ignored_layers,\n                \"kv_cache_quant_algo\": None,\n            }\n        }\n\n        if \"fp8\" in self.quant_model.quant_config.quant_algo:\n            quant_format = \"naive-quantized\"\n            trtllm_config[\"quantization\"][\"quant_algo\"] = \"FP8\"\n            act_config = {\n                \"num_bits\": 8,\n                \"strategy\": re.search(r\"per-([a-zA-Z]+)\", a_quant_algo).group(1),\n                \"dynamic\": \"dynamic\" in a_quant_algo,\n                \"type\": \"float\",\n            }\n            weight_config = {\n                \"num_bits\": 8,\n                \"strategy\": re.search(r\"per-([a-zA-Z]+)\", w_quant_algo).group(1),\n                \"dynamic\": False,\n                \"type\": \"float\",\n            }\n        elif \"int8\" in self.quant_model.quant_config.quant_algo:\n            quant_format = \"int-quantized\"\n            trtllm_config[\"quantization\"][\"quant_algo\"] = \"INT8\"\n            act_config = {\n                \"num_bits\": 8,\n                \"strategy\": re.search(r\"per-([a-zA-Z]+)\", a_quant_algo).group(1),\n                \"dynamic\": \"dynamic\" in a_quant_algo,\n                \"type\": \"int\",\n            }\n            weight_config = {\n                \"num_bits\": 8,\n                \"strategy\": re.search(r\"per-([a-zA-Z]+)\", w_quant_algo).group(1),\n                \"dynamic\": False,\n                \"type\": \"int\",\n            }\n        else:\n            raise ValueError(\n                f\"{self.quant_model.quant_config.quant_algo} not supported\"\n            )\n\n        quant_dict = {\n            \"quantization_config\": {\n                \"config_groups\": {\n                    \"group_0\": {\n                        \"weights\": weight_config,\n                        \"input_activations\": act_config,\n                        \"output_activations\": None,\n                        \"targets\": [\"Linear\"],\n                    }\n                },\n                \"kv_cache_scheme\": None,\n                \"format\": quant_format,\n                ignore_field: ignored_layers,\n                \"quantization_status\": \"compressed\",\n                \"quant_method\": \"compressed-tensors\",\n            }\n        }\n        self.quant_model.get_model().config.update(quant_dict)\n        print_info(\"Save quantization_config: {}\".format(quant_dict))\n\n        os.makedirs(save_path, exist_ok=True)\n        self.quant_model.get_model().save_pretrained(save_path)\n\n        with open(os.path.join(save_path, \"hf_quant_config.json\"), \"w\") as f:\n            json.dump(trtllm_config, f, indent=4)\n\n        self.quant_model.tokenizer.save_pretrained(save_path)\n\n\nclass PTQTorchSave(PTQSaveBase):\n    def __init__(self, quant_model):\n        super(PTQTorchSave, self).__init__(quant_model=quant_model)\n\n    def save(self, save_path):\n        \"\"\"save quantized model and configs to local disk\"\"\"\n        os.makedirs(save_path, exist_ok=True)\n\n        if self.quant_model.act_scales_dict:\n            for k, v in self.quant_model.act_scales_dict.items():\n                _save_path = os.path.join(save_path, \"{}.act_scales.pt\".format(k))\n                torch.save(v, _save_path)\n            print_info(\"save act scales done.\")\n        else:\n            print_info(\"no act scales found.\")\n\n        if self.quant_model.weight_scales_dict:\n            for k, v in self.quant_model.weight_scales_dict.items():\n                _save_path = os.path.join(save_path, \"{}.weight_scales.pt\".format(k))\n                torch.save(v, _save_path)\n            print_info(\"save weight scales done.\")\n        else:\n            print_info(\"no act scales found.\")\n\n\nclass PTQPTMSave(PTQSaveBase):\n    def __init__(self, quant_model):\n        super(PTQPTMSave, self).__init__(quant_model=quant_model)\n\n    def save(self, save_path):\n        \"\"\"save quantized model and configs to local disk\"\"\"\n        os.makedirs(save_path, exist_ok=True)\n        _index = torch.distributed.get_rank()\n        if self.quant_model.act_scales_dict:\n            for k, v in self.quant_model.act_scales_dict.items():\n                _save_path = os.path.join(save_path, \"{}.act_scales.pt\".format(k))\n                torch.distributed.all_reduce(v, op=torch.distributed.ReduceOp.MAX)\n                if _index == 0:\n                    torch.save(v, _save_path)\n            print_info(\"save act scales done.\")\n\n        if self.quant_model.weight_scales_dict:\n            for k, v in self.quant_model.weight_scales_dict.items():\n                torch.distributed.all_reduce(v, op=torch.distributed.ReduceOp.MAX)\n                _save_path = os.path.join(save_path, \"{}.weight_scales.pt\".format(k))\n                if _index == 0:\n                    torch.save(v, _save_path)\n            print_info(\"save weight scales done.\")\n\n\nclass DeepseekV3HfPTQSave(PTQSaveBase):\n    def __init__(self, quant_model, check_scales=False):\n        super().__init__(quant_model=quant_model)\n        self.moe_act_scales_dict = {}\n        self.moe_weight_scales_dict = {}\n        self.check_scales = check_scales\n        self.rank = dist.get_rank() if dist.is_initialized() else 0\n        self.no_mp_key = [\n            \"input_layernorm\",\n            \"post_attention_layernorm\",\n            \".q_a_proj.\",\n            \"q_a_layernorm\",\n            \".kv_a_proj_with_mqa.\",\n            \"kv_a_layernorm\",\n            \".gate.\",\n            \"norm\",\n        ]\n        self.mp_key = [\n            \".q_proj.\",\n            \".q_b_proj.\",\n            \".kv_b_proj.\",\n            \".o_proj.\",\n            \".mlp.gate_proj\",\n            \".mlp.down_proj\",\n            \".mlp.up_proj\",\n            \".mlp.shared_experts.gate_proj\",\n            \".mlp.shared_experts.down_proj\",\n            \".mlp.shared_experts.up_proj\",\n        ]\n        self.dim0_mp_key = [\n            \".q_proj.\",\n            \".q_b_proj.\",\n            \".kv_b_proj.\",\n            \".mlp.gate_proj\",\n            \".mlp.up_proj\",\n            \".mlp.shared_experts.gate_proj\",\n            \".mlp.shared_experts.up_proj\",\n        ]\n        self.dim1_mp_key = [\n            \".o_proj.\",\n            \".mlp.down_proj\",\n            \".mlp.shared_experts.down_proj\",\n        ]\n        self.exclude_key = [\"*head*\", \"*kv_b*\"]\n\n    def save(self, save_path):\n        save_path = os.path.join(save_path, \"scales\")\n        os.makedirs(save_path, exist_ok=True)\n        _index = torch.cuda.current_device()\n\n        # fuse scale\n        fused_act_scale_dict = self._fuse_scale(self.quant_model.act_scales_dict)\n        weight_scale_dict = deepcopy(self.quant_model.weight_scales_dict)\n        fused_weight_fp8_scale_dict = self._fuse_scale(weight_scale_dict)\n\n        if fused_act_scale_dict:\n            for k, v in fused_act_scale_dict.items():\n                torch.distributed.all_reduce(v, op=torch.distributed.ReduceOp.MAX)\n                _save_path = os.path.join(\n                    save_path, \"{}.input_scale.{}.pt\".format(k, _index)\n                )\n                if \"experts\" in k and \"shared_experts\" not in k:\n                    # handle Deepseek EP, do not all reduce\n                    _save_path = os.path.join(\n                        save_path, \"{}.input_scale.{}.pt\".format(k, self.rank)\n                    )\n                    torch.save(v, _save_path)\n                else:\n                    if self.rank == 0:\n                        torch.save(v, _save_path)\n            print_info(\"save act scales done.\")\n\n        if self.quant_model.weight_scales_dict:\n            for k, v in self.quant_model.weight_scales_dict.items():\n                max_value_group_wise = v\n                # if weight quant is int4 and act quant is fp8, extra save int4 absmax\n                if (\n                    self.quant_model.quant_algo_dict[\"w_quant_algo\"] == \"int4\"\n                    and self.quant_model.quant_algo_dict[\"a_quant_algo\"] == \"fp8\"\n                ):\n                    _save_path = os.path.join(\n                        save_path, \"{}.weight_scale.{}.{}.pt\".format(k, \"int4\", _index)\n                    )\n                    scale_int4 = max_value_group_wise / 7\n\n                    # save weigth-int4-pergroup scale\n                    if \"experts\" in k and \"shared_experts\" not in k:\n                        _save_path = os.path.join(\n                            save_path,\n                            \"{}.weight_scale.{}.{}.pt\".format(k, \"int4\", self.rank),\n                        )\n                        torch.save(scale_int4, _save_path)\n                    else:\n                        self._save_ckpt(\n                            scale_int4,\n                            _save_path,\n                            self.quant_model.quant_algo_dict[\"all_reduce\"],\n                        )\n\n                # fp8 pertensor scale\n                fused_max_value = fused_weight_fp8_scale_dict[k]\n                scale = (fused_max_value.max() / 448.0).to(fused_max_value.dtype)\n                assert scale.numel() == 1\n                print_info(f\"before all reduce scale = {scale}\")\n                torch.distributed.all_reduce(scale, op=torch.distributed.ReduceOp.MAX)\n                print_info(f\"after all reduce scale = {scale}\")\n\n                if \"experts\" in k and \"shared_experts\" not in k:\n                    _save_path = os.path.join(\n                        save_path, \"{}.weight_scale.{}.pt\".format(k, self.rank)\n                    )\n                    torch.save(scale, _save_path)\n                else:\n                    _save_path = os.path.join(\n                        save_path, \"{}.weight_scale.{}.pt\".format(k, _index)\n                    )\n                    self._save_ckpt(\n                        scale,\n                        _save_path,\n                        self.quant_model.quant_algo_dict[\"all_reduce\"],\n                    )\n\n            print_info(\"save weight scales done.\")\n\n        tmp_path = os.path.join(\"/\".join(save_path.split(\"/\")[:-1]), \"tmp\")\n        os.makedirs(tmp_path, exist_ok=True)\n        self._update_and_save_weight(tmp_path, fused_weight_fp8_scale_dict)\n        dist.barrier()\n        dist.destroy_process_group()\n\n        if self.rank == 0:\n            save_model_path = os.path.join(\n                \"/\".join(save_path.split(\"/\")[:-1]), \"checkpoint\"\n            )\n            os.makedirs(save_model_path, exist_ok=True)\n\n            self.convert_scales_to_safetensors(save_path, save_model_path)\n            print_info(\"convert scales to safetensors done.\")\n\n            file_name = self.merge_model(\n                tmp_path, save_model_path, mp=self.quant_model.model.world_size\n            )\n            print_info(\"merge model done.\")\n\n            self.add_mtp_weight(save_path=save_model_path, file_name=file_name)\n\n    def _save_ckpt(self, scale, save_path, all_reduce=True):\n        if all_reduce:\n            if self.rank == 0:\n                torch.save(scale, save_path)\n        else:\n            torch.save(scale, save_path)\n\n    def _fuse_scale(self, scale_dict):\n        \"\"\"\n        1. fuse q_a_proj and kv_a_proj scale\n        2. fuse gate_proj(w1) and up_proj(w3) scale\n        \"\"\"\n        if not scale_dict:\n            return\n\n        for layer_name in scale_dict:\n            if \"q_a_proj\" in layer_name:\n                q_a_scale = scale_dict[layer_name]\n                kv_a_layer_name = layer_name.replace(\"q_a_proj\", \"kv_a_proj_with_mqa\")\n                kv_a_scale = scale_dict[kv_a_layer_name]\n                fused_scale = torch.max(q_a_scale.max(), kv_a_scale.max())\n                scale_dict[layer_name] = fused_scale\n                scale_dict[kv_a_layer_name] = fused_scale\n\n            if \"gate_proj\" in layer_name:\n                w1_scale = scale_dict[layer_name]\n                w3_layer_name = layer_name.replace(\"gate_proj\", \"up_proj\")\n                w3_scale = scale_dict[w3_layer_name]\n                fused_scale = torch.max(w1_scale.max(), w3_scale.max())\n                scale_dict[layer_name] = fused_scale\n                scale_dict[w3_layer_name] = fused_scale\n\n            scale_dict[layer_name] = scale_dict[layer_name].max()\n        return scale_dict\n\n    def _update_and_save_weight(self, save_model_path, fused_weight_fp8_scale_dict):\n        self._update_fp8_weights(fused_weight_fp8_scale_dict)\n        self._save_model(save_model_path)\n\n    def _update_fp8_weights(self, fused_weight_fp8_scale_dict):\n        # We always update fp8 weight in deepseek model,\n        # cause we delivery fp8 per tensor scale.\n        # It would change the distribution of origin bf16 weigth.\n        if not self.quant_model.weight_scales_dict:\n            return\n\n        quant_layers_dict = self.quant_model.get_observer_layers()\n        for name, layer in quant_layers_dict.items():\n            print_info(f\"*** update {name} weights...\")\n            assert hasattr(layer, \"weight\")\n            if layer.weight.dtype == torch.float8_e4m3fn:\n                weight_bf16 = weight_dequant(layer.weight, layer.weight_scale_inv)\n                ori_bf16_weight = weight_bf16\n                fused_max_value = fused_weight_fp8_scale_dict[name]\n                if \"w4a8\" in self.quant_model.quant_config.quant_algo:\n                    tensor_wise_scale = fused_max_value.max() / 448.0\n                else:\n                    tensor_wise_scale = fused_max_value.max()\n                torch.distributed.all_reduce(\n                    tensor_wise_scale, op=torch.distributed.ReduceOp.MAX\n                )\n                weight_fp8 = (\n                    (weight_bf16 / tensor_wise_scale)\n                    .clamp(-448, 448)\n                    .to(torch.float8_e4m3fn)\n                )\n                weight_fp8 = weight_fp8.to(layer.weight.device)\n\n                if \"fp8\" in self.quant_model.quant_config.quant_algo:\n                    if \"w4a8\" in self.quant_model.quant_config.quant_algo:\n                        new_weight_bf16 = (\n                            weight_fp8.to(torch.bfloat16) * tensor_wise_scale\n                        )\n                        new_weight_bf16_qdq = fake_quant_dequant(\n                            new_weight_bf16,\n                            method=\"groupwise\",\n                            bits=4,\n                            group_size=self.quant_model.quant_config.quant_algo_info[\n                                \"w_group_size\"\n                            ],\n                        )\n                        new_weight_fp8 = (\n                            (new_weight_bf16_qdq / tensor_wise_scale)\n                            .clamp(-448, 448)\n                            .to(torch.float8_e4m3fn)\n                        )\n                        new_weight_fp8 = new_weight_fp8.to(layer.weight.device)\n                        layer.weight.data = new_weight_fp8\n                        bf16_weight_qdq = (\n                            new_weight_fp8.to(torch.bfloat16) * tensor_wise_scale\n                        )\n                    else:\n                        layer.weight.data = weight_fp8\n                        bf16_weight_qdq = (\n                            weight_fp8.to(torch.bfloat16) * tensor_wise_scale\n                        )\n\n                cos_sim = torch.cosine_similarity(\n                    bf16_weight_qdq, ori_bf16_weight\n                ).mean()\n                print_info(f\"qdq weight cos sim :{cos_sim}\")\n                if cos_sim < 0.95:\n                    print_info(\"*** cos sim < 0.95 !!!\")\n\n    def _save_model(self, save_model_path):\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        save_model(\n            self.quant_model.model,\n            os.path.join(\n                save_model_path, f\"model{self.rank}-mp{world_size}.safetensors\"\n            ),\n        )\n        print_info(f\"save model{self.rank}-mp{world_size}.safetensors done.\")\n\n    def convert_scales_to_safetensors(self, input_path, save_path):\n        state_dict = {}\n        pt_files = list(glob(os.path.join(input_path, \"*.pt\")))\n        pt_files.sort()\n        for pt_file in tqdm(pt_files):\n            scale_name = \".\".join(pt_file.split(\"/\")[-1].split(\".\")[:-2])\n            scale = torch.load(pt_file)\n            state_dict[scale_name] = scale.cpu()\n        safetensor_file = os.path.join(save_path, \"model-scales.safetensors\")\n        safe_save(state_dict, safetensor_file)\n\n    def merge_model(self, input_path, save_model_path, mp=16):\n        ori_state_dicts = [{} for _ in range(mp)]\n        model_save_ind = 0\n        localind = 0\n\n        scale_path = os.path.join(save_model_path, \"model-scales.safetensors\")\n        scales_dict = load_file(scale_path)\n\n        for mpind in range(mp):\n            file_path = os.path.join(input_path, f\"model{mpind}-mp{mp}.safetensors\")\n            ori_state_dicts[localind] = safe_open(\n                file_path, framework=\"pt\", device=\"cpu\"\n            )\n            localind += 1\n\n        # process no_mp_key\n        print_info(\"##no_mp_key##\")\n        num_layers = 61\n        index_dict = {\"weight_map\": {}}\n\n        # process model.norm.weight\n        new_save_dict = {}\n        filename = \"model-\" + \"{:0>{}}\".format(model_save_ind, 5) + \".safetensors\"\n        model_save_ind += 1\n        for _, k in enumerate(ori_state_dicts[0].keys()):\n            if \"norm.weight\" in k:\n                param: torch.Tensor = ori_state_dicts[0].get_tensor(k)\n                new_k = \"model.norm.weight\"\n                new_save_dict[new_k] = param\n                index_dict[\"weight_map\"][new_k] = str(filename)\n        safe_save(new_save_dict, os.path.join(save_model_path, filename))\n\n        # process model.encoder\n        for nl in range(num_layers):\n            new_save_dict = {}\n            filename = \"model-\" + \"{:0>{}}\".format(model_save_ind, 5) + \".safetensors\"\n            model_save_ind += 1\n            for _, k in enumerate(ori_state_dicts[0].keys()):\n                if (\n                    any(word if word in k else False for word in self.no_mp_key)\n                    and \"layers.\" + str(nl) + \".\" in k\n                ):\n                    param: torch.Tensor = ori_state_dicts[0].get_tensor(k)\n                    self._transform_keys(\n                        k,\n                        param,\n                        scales_dict,\n                        new_save_dict,\n                        index_dict,\n                        filename,\n                    )\n            # process expert merge\n            for mp_index in range(mp):\n                for _, k in enumerate(ori_state_dicts[mp_index].keys()):\n                    if \"mlp.experts\" in k and \"layers.\" + str(nl) + \".\" in k:\n                        param: torch.Tensor = ori_state_dicts[mp_index].get_tensor(k)\n                        self._transform_keys(\n                            k,\n                            param,\n                            scales_dict,\n                            new_save_dict,\n                            index_dict,\n                            filename,\n                        )\n            safe_save(new_save_dict, os.path.join(save_model_path, filename))\n\n        print_info(\"##mp_key##\")\n        # process mp_key\n        filename = None\n        new_save_dict = None\n\n        # process embed_tokens, lm_head\n        new_save_dict = {}\n        filename = \"model-\" + \"{:0>{}}\".format(model_save_ind, 5) + \".safetensors\"\n        model_save_ind += 1\n        for _, k in enumerate(ori_state_dicts[0].keys()):\n            if any(\n                word if word in k else False for word in [\"embed_tokens\", \"lm_head\"]\n            ):\n                param_list = []\n                for i in range(mp):\n                    param: torch.Tensor = ori_state_dicts[i].get_tensor(k)\n                    param_list.append(param)\n                newparam = torch.cat(param_list, dim=0)\n                new_save_dict[k] = newparam\n                print(f\"shape of {k}: {new_save_dict[k].shape}\")\n                index_dict[\"weight_map\"][k] = str(filename)\n        safe_save(new_save_dict, os.path.join(save_model_path, filename))\n        # process others\n        for nl in range(num_layers):\n            new_save_dict = {}\n            filename = \"model-\" + \"{:0>{}}\".format(model_save_ind, 5) + \".safetensors\"\n            model_save_ind += 1\n            for _, k in enumerate(ori_state_dicts[0].keys()):\n                if (\n                    any(word if word in k else False for word in self.mp_key)\n                    and \"layers.\" + str(nl) + \".\" in k\n                ):\n                    param_list = []\n                    for i in range(mp):\n                        param: torch.Tensor = ori_state_dicts[i].get_tensor(k)\n                        param_list.append(param)\n                    if any(word if word in k else False for word in self.dim0_mp_key):\n                        newparam = torch.cat(param_list, dim=0)\n                    elif any(word if word in k else False for word in self.dim1_mp_key):\n                        newparam = torch.cat(param_list, dim=1)\n                    else:\n                        raise AssertionError(\"Key should not in mp key!\")\n                    self._transform_keys(\n                        k,\n                        newparam,\n                        scales_dict,\n                        new_save_dict,\n                        index_dict,\n                        filename,\n                    )\n                    print(f\"shape of {k}: {new_save_dict[k].shape}\")\n            safe_save(new_save_dict, os.path.join(save_model_path, filename))\n\n        # update scales map\n        for k, _ in scales_dict.items():\n            index_dict[\"weight_map\"][k] = \"model-scales.safetensors\"\n\n        path = self.quant_model.model.ori_model_path\n        for file_path in glob(os.path.join(path, \"*token*\")):\n            new_file_path = os.path.join(save_model_path, os.path.basename(file_path))\n            shutil.copyfile(file_path, new_file_path)\n        for file_path in glob(os.path.join(path, \"*conf*\")):\n            new_file_path = os.path.join(save_model_path, os.path.basename(file_path))\n            try:\n                shutil.copyfile(file_path, new_file_path)\n            except IsADirectoryError:\n                shutil.copytree(file_path, new_file_path)\n        for file_path in glob(os.path.join(path, \"*modeling_deepseek.py*\")):\n            new_file_path = os.path.join(save_model_path, os.path.basename(file_path))\n            shutil.copyfile(file_path, new_file_path)\n\n        if self.quant_model.model.config.model_type == \"kimi_k2\":\n            file_path = os.path.join(input_path, \"tiktoken.model\")\n            new_file_path = os.path.join(save_model_path, \"tiktoken.model\")\n            shutil.copyfile(file_path, new_file_path)\n\n        with open(\n            os.path.join(save_model_path, \"model.safetensors.index.json\"), \"w\"\n        ) as f:\n            json.dump(index_dict, f, indent=4)\n\n        # setting quantization config\n        a_quant_algo = self.quant_model.quant_config.quant_algo_info[\"a\"]\n        if \"fp8\" in self.quant_model.quant_config.quant_algo:\n            if \"w4a8\" in self.quant_model.quant_config.quant_algo:\n                if self.quant_model.deploy_backend == \"trtllm\":\n                    quant_dict = {\n                        \"quantization_config\": {\n                            \"quant_method\": \"w4a8_awq\",\n                            \"kv_cache_quant_method\": \"fp8\",\n                            \"activation_scheme\": (\n                                \"dynamic\" if \"dynamic\" in a_quant_algo else \"static\"\n                            ),\n                            \"ignored_modules\": [\n                                \"*self_attn*\",\n                                \"*gate_up_proj\",\n                                \"*down_proj\",\n                                \"*layers.61*\",\n                            ],\n                            \"ignored_quantization_config\": {\n                                \"quant_method\": \"fp8_block_scales\",\n                                \"kv_cache_quant_method\": \"fp8\",\n                            },\n                        },\n                    }\n                else:\n                    raise NotImplementedError(\n                        f\"deploy_backend {self.quant_model.deploy_backend} \\\n                            is not supported for w4a8_fp8.\"\n                    )\n            else:\n                ignore_layers = self.quant_model.quant_config.quant_algo_info[\n                    \"ignore_layers\"\n                ]\n                if self.quant_model.deploy_backend == \"vllm\":\n                    quant_dict = {\n                        \"quantization_config\": {\n                            \"quant_method\": \"fp8\",\n                            \"activation_scheme\": (\n                                \"dynamic\" if \"dynamic\" in a_quant_algo else \"static\"\n                            ),\n                            \"ignored_layers\": ignore_layers,\n                        }\n                    }\n                else:\n                    raise NotImplementedError(\n                        f\"deploy_backend {self.quant_model.deploy_backend} \\\n                            is not supported for fp8_static.\"\n                    )\n\n        config = DeepseekV3Config.from_pretrained(self.quant_model.model.ori_model_path)\n        if hasattr(config, \"quantization_config\"):\n            delattr(config, \"quantization_config\")\n        config.update(quant_dict)\n        config.save_pretrained(save_model_path)\n        print_info(\"Save quantization_config: {}\".format(quant_dict))\n        return \"model-\" + \"{:0>{}}\".format(model_save_ind, 5) + \".safetensors\"\n\n    def _transform_keys(\n        self,\n        param_name,\n        param,\n        scales_dict,\n        new_save_dict,\n        index_dict,\n        filename,\n    ):\n        if \"fp8\" in self.quant_model.quant_config.quant_algo:\n            if \"w4a8\" in self.quant_model.quant_config.quant_algo:\n                if self._is_packed(param_name, param, scales_dict):\n                    param = self._packed_weight(\n                        param_name,\n                        param,\n                        self.quant_model.quant_config.quant_algo_info[\"w_group_size\"],\n                        scales_dict,\n                    )\n                    param_name = param_name.replace(\"weight\", \"qweight\")\n            else:\n                if param_name.endswith(\"weight_scale_inv\"):\n                    return\n\n        new_save_dict[param_name] = param\n        index_dict[\"weight_map\"][param_name] = str(filename)\n\n    def _is_packed(self, weight_name, weight, scales_dict):\n        if weight_name.endswith(\"weight_scale_inv\") or self._is_exclude(weight_name):\n            return False\n        elif weight.element_size() == 1:\n            if f\"{weight_name}_scale.int4\" in scales_dict.keys():\n                return True\n            return False\n        else:\n            return False\n\n    def _is_exclude(self, tensor_name):\n        for pattern in self.exclude_key:\n            # Convert fnmatch-style pattern to regex\n            regex_pattern = pattern.replace(\"*\", \".*\").replace(\"?\", \".\")\n            if re.fullmatch(regex_pattern, tensor_name):\n                return True\n        return False\n\n    def _packed_weight(self, weight_name, weight, block_wise, scales_dict):\n        target_shape = (weight.shape[0] // block_wise, weight.shape[1] // block_wise)\n        scale_inv = scales_dict[f\"{weight_name}_scale\"]\n        scale_inv_padded = torch.full(target_shape, scale_inv.item()).float()\n\n        weight = weight.cuda()\n        scale_inv_padded = scale_inv_padded.cuda()\n        bf16_weight = weight_dequant(weight, scale_inv_padded)\n        weight = weight.cpu()\n        scale_inv_padded = scale_inv_padded.cpu()\n        bf16_weight = bf16_weight.cpu()\n\n        int4_scale = scales_dict[f\"{weight_name}_scale.int4\"]\n        int4_scale = torch.repeat_interleave(int4_scale, block_wise, dim=-1)\n        assert int4_scale.shape == weight.shape\n        quant_weight = torch.clamp(torch.round(bf16_weight / int4_scale), -8, 7)\n        packed_weight = pack_weight_to_int8(quant_weight)\n        print_info(\n            f\"Packing {weight_name}, packed weight dtype = {packed_weight.dtype}\"\n        )\n        del bf16_weight\n        return packed_weight\n\n    def _read_weight_map(self, input_path):\n        model_index_file = os.path.join(input_path, \"model.safetensors.index.json\")\n        with open(model_index_file, \"r\") as f:\n            model_index = json.load(f)\n        weight_map = model_index[\"weight_map\"]\n        return weight_map\n\n    def _get_tensor_from_safetensor(\n        self, input_path, weight_name, safetensor_file, loaded_files\n    ):\n        if safetensor_file not in loaded_files:\n            current_state_dict = load_file(\n                os.path.join(input_path, safetensor_file), device=\"cpu\"\n            )\n            loaded_files[safetensor_file] = current_state_dict\n        weight = loaded_files[safetensor_file][weight_name]\n        if len(loaded_files) > 4:\n            oldest_file = next(iter(loaded_files))\n            del loaded_files[oldest_file]\n        return weight\n\n    def add_mtp_weight(self, input_path=None, save_path=None, file_name=None):\n        if input_path is None:\n            input_path = self.quant_model.model.ori_model_path\n        weight_map = self._read_weight_map(input_path)\n\n        state_dict = {}\n        add_weight_map = {}\n        loaded_files = {}\n        for weight_name in weight_map:\n            if \"layers.61\" in weight_name or \"rotary_emb.inv_freq\" in weight_name:\n                print_info(f\"- Add {weight_name}\")\n                safetensor_file = weight_map[weight_name]\n                weight = self._get_tensor_from_safetensor(\n                    input_path, weight_name, safetensor_file, loaded_files\n                )\n                state_dict[weight_name] = weight\n                add_weight_map[weight_name] = file_name\n\n        safe_save(state_dict, os.path.join(save_path, file_name))\n\n        # update model index json\n        new_model_index_file = os.path.join(save_path, \"model.safetensors.index.json\")\n        with open(new_model_index_file, \"r\") as f:\n            new_model_index = json.load(f)\n        new_model_index[\"weight_map\"].update(add_weight_map)\n        with open(new_model_index_file, \"w\") as f:\n            json.dump(new_model_index, f, indent=2)\n\n\nclass DeepseekV3PTQSaveTRTLLM(DeepseekV3HfPTQSave):\n    def __init__(self, quant_model):\n        super().__init__(quant_model=quant_model)\n\n    def save(self, save_path):\n        a_quant_algo = self.quant_model.quant_config.quant_algo_info[\"a\"]\n        if \"w4a8\" in self.quant_model.quant_config.quant_algo:\n            quant_dict = {\n                \"quantization_config\": {\n                    \"quant_method\": \"w4a8_awq\",\n                    \"kv_cache_quant_method\": \"fp8\",\n                    \"activation_scheme\": (\n                        \"dynamic\" if \"dynamic\" in a_quant_algo else \"static\"\n                    ),\n                    \"ignored_modules\": [\n                        \"*self_attn*\",\n                        \"*gate_up_proj\",\n                        \"*down_proj\",\n                        \"*layers.61*\",\n                    ],\n                    \"ignored_quantization_config\": {\n                        \"quant_method\": \"fp8_block_scales\",\n                        \"kv_cache_quant_method\": \"fp8\",\n                    },\n                },\n            }\n            int4_scales = {}\n            for name, sub_layer in self.quant_model.model.named_modules():\n                if isinstance(sub_layer, QDQModule):\n                    max_value_group_wise = sub_layer.weight_scale.data.clone()\n                    int4_scales[f\"{name}.weight_scale.int4\"] = max_value_group_wise / 8\n                    sub_layer.weight_scale = None\n                    sub_layer.weight_scale = torch.nn.Parameter(\n                        (max_value_group_wise.max() / 448.0).to(\n                            max_value_group_wise.dtype\n                        ),\n                        requires_grad=False,\n                    )\n\n            os.makedirs(save_path, exist_ok=True)\n            safetensor_file = os.path.join(save_path, \"model-scales.safetensors\")\n            safe_save(int4_scales, safetensor_file)\n            print_info(f\"Save int4 scales to {safetensor_file}\")\n\n            self.quant_model.get_model().config.update(quant_dict)\n            print_info(\"Save quantization_config: {}\".format(quant_dict))\n\n            self.quant_model.get_model().save_pretrained(save_path)\n\n            new_model_index_file = os.path.join(\n                save_path, \"model.safetensors.index.json\"\n            )\n            with open(new_model_index_file, \"r\") as f:\n                new_model_index = json.load(f)\n            for key in int4_scales.keys():\n                new_model_index[\"weight_map\"][key] = \"model-scales.safetensors\"\n            with open(new_model_index_file, \"w\") as f:\n                json.dump(new_model_index, f, indent=2)\n\n            self.add_mtp_weight(save_path=save_path)\n        else:\n            raise ValueError(\n                f\"{self.quant_model.quant_config.quant_algo} not supported\"\n            )\n",
        "angelslim/compressor/quant/modules/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .awq.awq import AWQ  # noqa: F401\nfrom .fp8.fp8 import FP8  # noqa: F401\nfrom .gptq.gptq import GPTQ  # noqa: F401\nfrom .gptq.gptq_module import GPTQModule  # noqa: F401\nfrom .helper_layer import GPTQQuantLinear  # noqa: F401\nfrom .helper_layer import QDQModule  # noqa: F401\nfrom .helper_layer import QDQSingleModule  # noqa: F401\nfrom .helper_layer import SmoothHelpModule  # noqa: F401\nfrom .helper_layer import WQLinearGEMM  # noqa: F401\nfrom .int8.int8 import INT8  # noqa: F401\nfrom .smooth.smooth import SmoothQuant  # noqa: F401\n",
        "angelslim/compressor/quant/modules/awq/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/modules/awq/auto_clip.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\n\nimport torch\nimport torch.nn as nn\n\nfrom .....utils import get_op_by_name, print_info\nfrom ...core import EMASampler, MultiStepSampler, fake_quant_dequant, mse_loss\n\n\nclass AutoClip:\n    \"\"\"\n    AutoClip from AWQ[https://arxiv.org/abs/2306.00978]\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        weight_bits=4,\n        weight_quant_method=\"groupwise\",\n        loss_function=mse_loss,\n        sample_function=None,\n        n_grid=20,\n        max_shrink=0.5,\n        n_sample_token=512,\n        group_size=64,\n    ):\n        super(AutoClip, self).__init__()\n        self.model = model\n        self.weight_bits = weight_bits\n        self.weight_method = weight_quant_method\n        self.loss_function = loss_function\n        self.n_grid = n_grid\n        self.max_shrink = max_shrink\n        self.n_sample_token = n_sample_token\n        self.bnt = (1 << (self.weight_bits - 1)) - 1\n        self.sampled_inputs = {}\n        self.sample_function = sample_function\n        self.group_size = group_size\n        self.linear_name_dict = {}\n        assert sample_function in [None, \"cat\", \"ema\"]\n        if sample_function == \"cat\":\n            self.sample_function = MultiStepSampler()\n        elif sample_function == \"ema\":\n            self.sample_function = EMASampler()\n        else:\n            self.sample_function = EMASampler()\n\n        self._apply_hook()\n\n    def _apply_hook(self):\n        self._forward_hook_list = []\n        for name, sub_layer in self.model.named_modules():\n            if isinstance(sub_layer, (nn.Linear)):\n                forward_pre_hook_handle = sub_layer.register_forward_pre_hook(\n                    self._forward_pre_hook\n                )\n                self._forward_hook_list.append(forward_pre_hook_handle)\n                self.linear_name_dict[sub_layer] = name\n\n    def _forward_pre_hook(self, layer, input):\n        self._sample_scale(input, self.linear_name_dict[layer])\n        return input\n\n    def _sample_scale(self, input, name):\n        input = input[0] if isinstance(input, tuple) else input\n        if name not in self.sampled_inputs:\n            self.sampled_inputs[name] = input\n        else:\n            if self.sample_function is not None:\n                self.sampled_inputs[name] = self.sample_function.sample(\n                    input, self.sampled_inputs[name], name\n                )\n            else:\n                self.sampled_inputs[name] = input\n\n    def auto_clip(self, group_size=128, oc_batch_size=256):\n        \"\"\"\n        search clip scale for each layer and update the layer's weight\n        \"\"\"\n        for name, sub_layer in self.model.named_modules():\n            # TODO: or 'out_linear' in name\n            if name not in self.sampled_inputs:\n                continue\n            sub_layer.weight.all_gather()\n            w = sub_layer.weight\n            x = self.sampled_inputs[name]\n            print_info(\n                \"AutoClipping {}, x shape: {}, weight shape: {}\".format(\n                    name, x.shape, w.shape\n                )\n            )\n            x = x.view(-1, x.shape[-1])\n            x = x.reshape(1, x.shape[0], -1, group_size)\n            x = x[:, 0 :: x.shape[1] // self.n_sample_token]\n            w = w.reshape([w.shape[0], 1, -1, group_size])\n            oc_batch_size = (\n                oc_batch_size if w.shape[0] % oc_batch_size == 0 else 128\n            )  # prevent OOM\n            assert w.shape[0] % oc_batch_size == 0\n\n            w_all = w\n            best_max_val_all = []\n\n            for i_b in range(w.shape[0] // oc_batch_size):\n                w = w_all[i_b * oc_batch_size : (i_b + 1) * oc_batch_size]\n\n                org_max_val = w.abs().amax(dim=-1, keepdim=True)  # co, 1, n_group, 1\n                best_max_val = org_max_val.clone()\n                min_errs = torch.ones_like(org_max_val) * 1e9\n                x = x.to(w.device)\n                org_out = (x * w).sum(dim=-1)  # co, n_token, n_group\n                for i_s in range(int(self.max_shrink * self.n_grid)):\n                    max_val = org_max_val * (1 - i_s / self.n_grid)\n                    min_val = -max_val\n                    cur_w = torch.clamp(w, min_val, max_val)\n                    org_w_shape = cur_w.shape\n                    cur_w = cur_w.reshape([-1, self.group_size])\n                    quant_dequant_weight = fake_quant_dequant(\n                        cur_w, method=\"abs_max_channel_wise\", bits=self.weight_bits\n                    )\n                    quant_dequant_weight = quant_dequant_weight.reshape(org_w_shape)\n                    cur_out = (x * quant_dequant_weight).sum(dim=-1)\n                    # co, 1, n_group, 1\n                    err = (cur_out - org_out).pow(2).mean(dim=1).view(min_errs.shape)\n                    print_info(\n                        \"block {} search s {} err {}\".format(\n                            i_b, i_s, err.mean().item()\n                        )\n                    )\n                    del cur_w, cur_out, quant_dequant_weight\n                    cur_best_idx = err < min_errs\n                    min_errs[cur_best_idx] = err[cur_best_idx]\n                    best_max_val[cur_best_idx] = max_val[cur_best_idx]\n                best_max_val_all.append(best_max_val)\n\n            best_max_val = torch.cat(best_max_val_all, dim=0).squeeze(1)\n            del org_out, x, w\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            for name, param in sub_layer.named_parameters():\n                if \"weight\" in name:\n                    param.all_gather()\n                    org_shape = param.shape\n                    param_tmp = param.reshape(*best_max_val.shape[:2], -1)\n                    param_tmp = torch.clamp(param_tmp, -max_val, max_val)\n                    param_tmp = param_tmp.reshape(org_shape)\n                    param.data.copy_(param_tmp)\n                    param.partition(has_been_updated=True)\n                    del param_tmp\n\n            torch.cuda.empty_cache()\n\n\nclass AutoLayerClip:\n    def __init__(\n        self,\n        weight_bits=4,\n        weight_quant_method=\"groupwise\",\n        group_size=128,\n        n_grid=20,\n        max_shrink=0.5,\n        n_sample_token=1024,\n        loss_function=mse_loss,\n        merge_samples=True,\n    ):\n        \"\"\"\n        The implementation from AWQ(https://arxiv.org/pdf/2306.00978.pdf).\n        \"\"\"\n        self.weight_bits = weight_bits\n        self.weight_quant_method = weight_quant_method\n        self.bnt = (1 << (weight_bits - 1)) - 1\n        self.group_size = group_size\n        self.n_grid = n_grid\n        self.max_shrink = max_shrink\n        self.n_sample_token = n_sample_token\n        self.loss_function = loss_function\n        self.merge_samples = merge_samples\n\n    def apply_clip(self, module, clip_list):\n        print_info(\"[apply clip] start\")\n        for name, max_val in clip_list:\n            layer = get_op_by_name(module, name)\n            org_shape = layer.weight.shape\n            layer.weight.data = layer.weight.data.reshape(*max_val.shape[:2], -1)\n            layer.weight.data = torch.clamp(layer.weight.data, -max_val, max_val)\n            layer.weight.data = layer.weight.data.reshape(org_shape)\n\n        print_info(\"[apply clip] end\")\n\n    def auto_clip(self, module, input_feat):\n        print_info(\"[auto clip] start\")\n        named_linears = {\n            name: m for name, m in module.named_modules() if isinstance(m, (nn.Linear))\n        }\n\n        clip_list = []\n        for name in named_linears:\n            # # due to qk bmm, it is hard to clip precisely\n            # if 'attention.query_key_value' in name:\n            #     continue\n            print_info(f\"[auto clip] {name}\")\n            max_val = self._auto_clip_layer(\n                named_linears[name].weight, input_feat[name]\n            )\n            clip_list.append((name, max_val))\n\n        print_info(\"[auto clip] end\")\n        return clip_list\n\n    def _auto_clip_layer(self, w, inp, oc_batch_size=256):\n        assert inp.dim() in [3, 4]\n        inp = inp.permute(1, 0, 2).contiguous() if inp.dim() == 3 else inp.squeeze(0)\n        assert w.dim() == 2\n        torch.cuda.empty_cache()\n        # w           [co, ci]      -> [co, 1, n_group, group size]\n        # inp  [n_token, ci] -> [1, n_token, n_group, group size]\n        if self.merge_samples:\n            inp = inp.to(w.device)\n            inp = inp.view(-1, inp.shape[-1])\n            inp = inp.reshape(1, inp.shape[0], -1, self.group_size)\n            input_feat = inp[:, 0 :: inp.shape[1] // self.n_sample_token]\n        else:\n            inp = inp.to(w.device)\n            inp = inp.view(-1, inp.shape[-1])\n            inp = inp.reshape(1, inp.shape[0], -1, self.group_size)\n            input_feat = inp[\n                :, 0 :: inp.shape[1] // int(self.n_sample_token)\n            ].contiguous()\n            torch.cuda.empty_cache()\n\n        w = w.reshape([w.shape[0], 1, -1, self.group_size])\n        oc_batch_size = (\n            oc_batch_size if w.shape[0] % oc_batch_size == 0 else 128\n        )  # prevent OOM\n        assert w.shape[0] % oc_batch_size == 0\n\n        w_all = w\n        best_max_val_all = []\n\n        for i_b in range(w.shape[0] // oc_batch_size):\n            w = w_all[i_b * oc_batch_size : (i_b + 1) * oc_batch_size]\n\n            org_max_val = w.abs().amax(dim=-1, keepdim=True)  # co, 1, n_group, 1\n            best_max_val = org_max_val.clone()\n            min_errs = torch.ones_like(org_max_val) * 1e9\n            input_feat = input_feat.to(w.device)\n            org_out = (input_feat * w).sum(dim=-1)  # co, n_token, n_group\n\n            for i_s in range(int(self.max_shrink * self.n_grid)):\n                max_val = org_max_val * (1 - i_s / self.n_grid)\n                min_val = -max_val\n                cur_w = torch.clamp(w, min_val, max_val)\n                org_w_shape = cur_w.shape\n                cur_w = cur_w.reshape([-1, self.group_size])\n                quant_dequant_weight = fake_quant_dequant(\n                    cur_w, method=\"abs_max_channel_wise\", bits=self.weight_bits\n                )\n                quant_dequant_weight = quant_dequant_weight.reshape(org_w_shape)\n                cur_out = (input_feat * quant_dequant_weight).sum(dim=-1)\n                # co, 1, n_group, 1\n                err = (cur_out - org_out).pow(2).mean(dim=1).view(min_errs.shape)\n                del cur_w, cur_out, quant_dequant_weight\n                cur_best_idx = err < min_errs\n                min_errs[cur_best_idx] = err[cur_best_idx]\n                best_max_val[cur_best_idx] = max_val[cur_best_idx]\n            best_max_val_all.append(best_max_val)\n\n        best_max_val = torch.cat(best_max_val_all, dim=0).squeeze(1)  # co, n_group, 1\n        # print_info(f\"best_max_val: {best_max_val}\")\n        inp = inp.cpu()\n        del org_out, input_feat, w\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return best_max_val\n",
        "angelslim/compressor/quant/modules/awq/auto_scale.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .....utils import get_op_by_name, get_op_name, print_info, set_op_by_name\nfrom ...core import mse_loss\nfrom ...modules.helper_layer import SmoothHelpModule\nfrom .search import AWQSearch\n\n\nclass AutoLayerScale:\n    def __init__(\n        self,\n        weight_bits=4,\n        weight_quant_method=\"groupwise\",\n        group_size=64,\n        n_grid=20,\n        smooth_all_linears=False,\n        loss_function=mse_loss,\n        merge_samples=True,\n        model_type=\"dense\",\n        observer_layer_classes=None,\n        low_memory=False,\n    ):\n        \"\"\"\n        The implementation from AWQ(https://arxiv.org/pdf/2306.00978.pdf).\n        \"\"\"\n        self.weight_bits = weight_bits\n        self.weight_quant_method = weight_quant_method\n        self.group_size = group_size\n        self.n_grid = n_grid\n        self.smooth_all_linears = smooth_all_linears\n        self.loss_function = loss_function\n        self.merge_samples = merge_samples\n        self.model_type = model_type\n        self.layer_count = 0\n        self.observer_layer_classes = observer_layer_classes\n        self.low_memory = low_memory\n        self.search_function = AWQSearch(\n            n_grid=n_grid,\n            bits_length=weight_bits,\n            weight_quant_method=weight_quant_method,\n            group_size=group_size,\n            merge_samples=merge_samples,\n            observer_layer_classes=observer_layer_classes,\n            low_memory=low_memory,\n        )\n\n    def apply_scale(self, module, scales_list, input_feat_dict=None):\n        print_info(\"[apply scale] start\")\n        for prev_op_name, layer_names, scales in scales_list:\n            print_info(\n                f\"[apply scale] {prev_op_name} -> {layer_names},scales:{scales[0]}\"\n            )\n            prev_op = get_op_by_name(module, prev_op_name)\n            layers = [get_op_by_name(module, name) for name in layer_names]\n\n            if \"norm\" in prev_op_name:\n                scales = scales.to(prev_op.weight.device)\n                prev_op.weight.div_(scales)\n\n                for p in prev_op.parameters():\n                    assert torch.isnan(p).sum() == 0, f\"nan in {prev_op_name} weight\"\n\n                for layer in layers:\n                    layer.weight.mul_(scales.view(1, -1))\n                    for p in layer.parameters():\n                        assert torch.isnan(p).sum() == 0, f\"nan in {layer_names} weight\"\n\n            elif \"activation_func\" in prev_op_name:\n                scales = scales.to(prev_op.weight.device)\n                new_module = SmoothHelpModule(layers[0])\n                new_module.convert_weight(scales)\n                set_op_by_name(module, prev_op_name, new_module)\n\n            if input_feat_dict is not None:\n                for layer_name in layer_names:\n                    if layer_name not in input_feat_dict.keys():\n                        continue\n                    inp = input_feat_dict[layer_name].to(scales.device)\n                    if inp.dim() == 2:\n                        inp = inp.reshape(-1, inp.shape[0], scales.shape[-1])\n                    inp.div_(scales.view(1, -1))\n                    inp = inp.cpu()\n\n            scales = scales.cpu()\n        print_info(\"[apply scale] end\")\n\n    def auto_scale(self, module, input_feat, cache):\n        print_info(\"[auto scale] start\")\n\n        def _auto_get_scale(\n            layer_name, prev_op, layers, inp, module2inspect=None, cache=None\n        ):\n            if module2inspect is None:\n                assert len(layers) == 1\n                module2inspect = layers[0]\n            if not self.low_memory:\n                inp = inp.to(prev_op.weight.device)\n            if self.merge_samples:\n                act_abs_max = (\n                    inp.abs().reshape(-1, inp.shape[-1]).mean(0).reshape(1, -1)\n                )\n            else:\n                all_inp = inp\n                act_abs_max = (\n                    all_inp.abs().reshape(-1, all_inp.shape[-1]).mean(0).reshape(1, -1)\n                )\n                del all_inp\n\n            print_info(f\"[auto scale] {layer_name} act_abs_max: {act_abs_max}\")\n            scales = self.search_function.search_by_block(\n                layer_name,\n                inp,\n                act_abs_max,\n                layers,\n                module2inspect,\n                cache,\n                self.layer_count,\n            )\n            scales = scales.detach().cpu()\n            print_info(f\"[auto scale] {layer_name} scales: {scales}\")\n            inp = inp.cpu()\n            torch.cuda.empty_cache()\n\n            # prev_op_name, [layer_name], scale\n            return (\n                get_op_name(module, prev_op),\n                tuple([get_op_name(module, m) for m in layers]),\n                scales,\n            )\n\n        scales_list = []\n        print_info(input_feat.keys())\n        scales_list.append(\n            _auto_get_scale(\n                layer_name=\"attn.qkv\",\n                prev_op=module.input_layernorm,\n                layers=[\n                    module.self_attn.q_proj,\n                    module.self_attn.k_proj,\n                    module.self_attn.v_proj,\n                ],\n                inp=input_feat[\"self_attn.q_proj\"],\n                module2inspect=module.self_attn,\n                cache=cache,\n            )\n        )\n\n        # attention output\n        if module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape:\n            scales_list.append(\n                _auto_get_scale(\n                    layer_name=\"attn.o\",\n                    prev_op=module.self_attn.v_proj,\n                    layers=[module.self_attn.o_proj],\n                    inp=input_feat[\"self_attn.o_proj\"],\n                )\n            )\n\n        if hasattr(module.mlp, \"gate\"):\n            print_info(\"auto scale -> MoeAWQ\")\n            if self.model_type == \"hunyuan_v1_moe\":\n                # share_mlp fc1\n                scales_list.append(\n                    _auto_get_scale(\n                        layer_name=\"shared_mlp.gate_proj\",\n                        prev_op=module.post_attention_layernorm,\n                        layers=[\n                            module.mlp.shared_mlp.gate_proj,\n                            module.mlp.shared_mlp.up_proj,\n                        ],\n                        inp=input_feat[\"mlp\"],\n                        module2inspect=module.mlp,\n                        cache=cache,\n                    )\n                )\n                # share_mlp fc2\n                scales_list.append(\n                    _auto_get_scale(\n                        layer_name=\"shared_mlp.down_proj\",\n                        prev_op=module.mlp.shared_mlp.up_proj,\n                        layers=[module.mlp.shared_mlp.down_proj],\n                        inp=input_feat[\"mlp.shared_mlp.down_proj\"],\n                    )\n                )\n                # fc1\n                scales_list.append(\n                    _auto_get_scale(\n                        layer_name=\"expert.gate_proj\",\n                        prev_op=module.post_attention_layernorm,\n                        layers=[\n                            w\n                            for expert in module.mlp.experts\n                            for w in [expert.gate_proj, expert.up_proj]\n                        ],\n                        inp=input_feat[\"mlp\"],\n                        module2inspect=module.mlp,\n                        cache=cache,\n                    )\n                )\n                # fc2\n                for i, expert in enumerate(module.mlp.experts):\n                    scales_list.append(\n                        _auto_get_scale(\n                            layer_name=\"mlp.down_proj\",\n                            prev_op=expert.up_proj,\n                            layers=[expert.down_proj],\n                            inp=input_feat[f\"mlp.experts.{i}.down_proj\"],\n                        )\n                    )\n            else:\n                # fc1\n                scales_list.append(\n                    _auto_get_scale(\n                        layer_name=\"expert.gate_proj\",\n                        prev_op=module.post_attention_layernorm,\n                        layers=[\n                            w\n                            for expert in module.mlp.experts\n                            for w in [expert.gate_proj, expert.up_proj]\n                        ],\n                        inp=input_feat[\"mlp\"],\n                        module2inspect=module.mlp,\n                        cache=cache,\n                    )\n                )\n                # fc2\n                for i, expert in enumerate(module.mlp.experts):\n                    scales_list.append(\n                        _auto_get_scale(\n                            layer_name=\"mlp.down_proj\",\n                            prev_op=expert.up_proj,\n                            layers=[expert.down_proj],\n                            inp=input_feat[f\"mlp.experts.{i}.down_proj\"].unsqueeze(0),\n                        )\n                    )\n        else:\n            print_info(\"auto scale -> DenseAWQ\")\n            # fc1\n            scales_list.append(\n                _auto_get_scale(\n                    layer_name=\"mlp.gate_proj\",\n                    prev_op=module.post_attention_layernorm,\n                    layers=[module.mlp.gate_proj, module.mlp.up_proj],\n                    inp=input_feat[\"mlp.gate_proj\"],\n                    module2inspect=module.mlp,\n                    cache=cache,\n                )\n            )\n            # fc2\n            scales_list.append(\n                _auto_get_scale(\n                    layer_name=\"mlp.down_proj\",\n                    prev_op=module.mlp.up_proj,\n                    layers=[module.mlp.down_proj],\n                    inp=input_feat[\"mlp.down_proj\"],\n                )\n            )\n\n        self.layer_count += 1\n        print_info(\"[auto scale] end\")\n        return scales_list\n",
        "angelslim/compressor/quant/modules/awq/awq.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport os\nfrom collections import defaultdict\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nfrom huggingface_hub import save_torch_state_dict\nfrom tqdm import tqdm\n\nfrom .....utils import get_best_device, print_info, set_op_by_name\nfrom ...core import pseudo_quantize_tensor\nfrom ...modules.catcher import Catcher\nfrom ...modules.helper_layer import WQLinearGEMM\nfrom .auto_clip import AutoLayerClip\nfrom .auto_scale import AutoLayerScale\n\n__all__ = [\"AWQ\"]\n\n\nclass AWQ:\n    def __init__(\n        self,\n        model,\n        seq_length=2048,\n        hidden_size=2560,\n        mse_range=False,\n        model_arch_type=None,\n        observer_layer_classes=None,\n        smooth_all_linears=False,\n        merge_samples=False,\n        low_memory=False,\n    ):\n        \"\"\"\n        Args:\n            model(nn.Module, required): The model to be smoothed.\n            nsamples(int, optional): The number of samples to be used for AWQ.\n            seq_length(int, optional): The length of the sequence. Default: 2048.\n            hidden_size(int, optional): The size of the hidden layer. Default: 2560.\n            mse_range(bool, optional): Whether to use mse_range.\n            model_arch_type(str, optional): model arch type.\n            observer_layer_classes(list, optional): The layer need to observer.\n            smooth_all_linears(bool, optional): Whether to smooth all linears.\n            merge_samples(bool, optional): Whether to merge samples. Default: False.\n        \"\"\"\n        super(AWQ, self).__init__()\n        self.model = model\n        self.modal_type = self.model.modal_type\n        if self.modal_type == \"VLM\":\n            self.layers = self.model.model.model.language_model.layers\n        else:\n            self.layers = self.model.model.model.layers\n        self.quant_bits = self.model.quant_config.quant_bit\n        self.group_size = self.model.quant_config.quant_algo_info[\"group_size\"]\n        self.zero_point = self.model.quant_config.quant_algo_info[\"zero_point\"]\n        self.seq_length = seq_length\n        self.hidden_size = hidden_size\n        self.model_arch_type = model_arch_type\n        self.mse_range = mse_range\n        self.observer_layer_classes = observer_layer_classes\n        self.smooth_all_linears = smooth_all_linears\n        self.merge_samples = merge_samples\n        self.low_memory = low_memory\n        self._init_linear_list_and_model_type()\n        self.scale_function = AutoLayerScale(\n            weight_bits=self.quant_bits,\n            group_size=self.group_size,\n            smooth_all_linears=smooth_all_linears,\n            merge_samples=merge_samples,\n            model_type=self.model_arch_type,\n            observer_layer_classes=observer_layer_classes,\n            low_memory=low_memory,\n        )\n        self.clip_function = AutoLayerClip(\n            weight_bits=self.quant_bits,\n            group_size=self.group_size,\n            merge_samples=merge_samples,\n        )\n        self.dtype = torch.bfloat16\n        self.scales_dict = {}\n        self.inps = None\n\n    def _init_linear_list_and_model_type(self):\n        self.use_transformer_engine = False\n\n        self.isinstance_list = self.observer_layer_classes\n\n    def move_embed(self, model, device: str):\n        print_info(model)\n        model.model.model.embed_tokens = model.model.model.embed_tokens.to(device)\n        model.model.model.rotary_emb = model.model.model.rotary_emb.to(device)\n\n    @torch.no_grad()\n    def run(self, dataloader):\n        for model_module in self.layers:\n            model_module.eval()\n        layers = self.layers\n        dev = get_best_device()\n        nsamples = len(dataloader)\n        self.inps = torch.zeros(\n            (int(nsamples), self.seq_length, self.hidden_size),\n            device=dev,\n            dtype=self.dtype,\n        )\n        cache = {\"i\": 0}\n        layers[0] = layers[0].to(dev)\n        pre_transformer_modules_dict = self.model.get_pre_transformer_modules()\n        for _, module in pre_transformer_modules_dict.items():\n            module.to(dev)\n        layers[0] = Catcher(layers[0], self.inps, cache)\n        self.model.model_forward(dataloader)\n        for _, module in pre_transformer_modules_dict.items():\n            module.cpu()\n        layer_kwargs = layers[0].layer_kwargs\n        for k, v in layer_kwargs.items():\n            # position embeddings\n            if isinstance(v, tuple):\n                layer_kwargs[k] = tuple(\n                    (\n                        item.to(dev)\n                        if isinstance(item, (torch.Tensor, nn.Module))\n                        else item\n                    )\n                    for item in v\n                )\n\n        print_info(\"cache['i']:{}\".format(cache[\"i\"]))\n        print_info(len(layers))\n        layers[0] = layers[0].module\n        print_info(self.inps.shape)\n        outs = torch.zeros_like(self.inps)\n        # begin the awq process\n        print_info(\"Ready.\")\n        layers = layers.cpu()\n        torch.cuda.empty_cache()\n\n        outs = outs.to(\"cpu\")\n        self.inps = self.inps.to(\"cpu\")\n\n        for i in range(len(layers)):\n            if torch.cuda.is_available():\n                print_info(\n                    f\"GPU Memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\"\n                )\n\n            layer = layers[i].to(dev)\n            if not self.low_memory:\n                outs = outs.to(dev)\n                self.inps = self.inps.to(dev)\n            subset = self._find_layers(layer)\n\n            if self.model_arch_type in [\"qwen3_moe\", \"hunyuan_v1_moe\"]:\n                subset = {\n                    **subset,\n                    \"mlp\": layer.mlp,\n                }\n\n            # firstly, get input features of all linear layers\n            def cache_input_hook(m, x, y, name, feat_dict, layer):\n                x = x[0]\n                x = x.detach().cpu()\n                feat_dict[name].append(x)\n\n            input_feat = defaultdict(list)\n            handles = []\n            for name in subset:\n                handles.append(\n                    subset[name].register_forward_hook(\n                        functools.partial(\n                            cache_input_hook,\n                            name=name,\n                            feat_dict=input_feat,\n                            layer=subset[name],\n                        )\n                    )\n                )\n            # being hook\n            for j in range(min(self.inps.shape[0], nsamples)):\n                with torch.no_grad():\n                    outs[j, :, :] = (\n                        layer(\n                            hidden_states=self.inps[j, :, :].unsqueeze(0).to(dev),\n                            **layer_kwargs,\n                        )[0]\n                        .squeeze(1)\n                        .to(self.inps.device)\n                    )\n\n            # remove duplicate\n            def deduplicate_tensors(tensor_list):\n                unique_tensors = []\n                assert len(tensor_list) % 2 == 0\n                for i in range(int(len(tensor_list) / 2)):\n                    if torch.equal(tensor_list[i * 2], tensor_list[i * 2 + 1]):\n                        unique_tensors.append(tensor_list[i * 2])\n                    else:\n                        raise ValueError\n                for tensor in tensor_list:\n                    if not any(torch.equal(tensor, t) for t in unique_tensors):\n                        unique_tensors.append(tensor)\n                return unique_tensors\n\n            for k, v in input_feat.items():\n                if len(v) > nsamples:\n                    print_info(f\"Warning: repetition hook {k}\")\n                    input_feat[k] = deduplicate_tensors(v)\n\n            print_info(\"HOOK Step{}\".format(j))\n            for h in handles:\n                h.remove()\n\n            # now solve for scaling and clipping\n            input_feat = {k: torch.cat(v, dim=0) for k, v in input_feat.items()}\n            # Clear GPU memory\n            torch.cuda.empty_cache()\n\n            scales_list = self.scale_function.auto_scale(\n                layer, input_feat, layer_kwargs\n            )\n\n            self.scale_function.apply_scale(layer, scales_list, input_feat)\n            for scales in scales_list:\n                name = \"language_model.encoder.layers.{}.{}.scale\".format(i, scales[0])\n                self.scales_dict[name] = scales[2]\n\n            if self.mse_range:\n                clip_list = self.clip_function.auto_clip(layer, input_feat)\n                self.clip_function.apply_clip(layer, clip_list)\n\n                for j in range(min(self.inps.shape[1], nsamples)):\n                    with torch.no_grad():\n                        outs[j, :, :] = layer(\n                            self.inps[j, :, :].unsqueeze(0), **layer_kwargs\n                        )[0].squeeze(1)\n            layers[i] = layers[i].cpu()\n            layer = layer.cpu()\n            torch.cuda.empty_cache()\n            self.inps, outs = outs, self.inps\n            print_info(\"AWQ end layer {}\\n\".format(i))\n\n    def save(self, save_dir, shard_size=\"5GB\", safetensors=True):\n        save_dir = save_dir[:-1] if save_dir[-1] == \"/\" else save_dir\n\n        # Save model\n        class EmptyModule(nn.Module):\n            def __init__(self):\n                super(EmptyModule, self).__init__()\n\n            def forward(self, x):\n                return x\n\n        # Save model and config files with empty state dict\n        self.model.model.config.quantization_config = {\n            \"quant_method\": \"awq\",\n            \"zero_point\": self.zero_point,\n            \"group_size\": self.group_size,\n            \"bits\": self.quant_bits,\n            \"version\": \"gemm\",\n            \"modules_to_not_convert\": [\"visual\"] if self.modal_type == \"VLM\" else None,\n        }\n        self.model.model.config.save_pretrained(\n            save_dir, state_dict=EmptyModule().state_dict()\n        )\n\n        # Remove empty state dict\n        default_paths = [\n            f\"{save_dir}/model.safetensors\",\n            f\"{save_dir}/pytorch_model.bin\",\n        ]\n        for path in default_paths:\n            if os.path.exists(path):\n                os.remove(path)\n\n        save_torch_state_dict(\n            state_dict=self.model.model.state_dict(),\n            save_directory=save_dir,\n            max_shard_size=shard_size,\n            safe_serialization=safetensors,\n            force_contiguous=True,\n            shared_tensors_to_discard=self.model.model._tied_weights_keys,\n        )\n        self.model.model.config.torch_dtype = \"float16\"\n        self.model.model.config.to_json_file(os.path.join(save_dir, \"config.json\"))\n\n        # save processor and tokenizer\n        if self.modal_type == \"VLM\" and self.model.processor is not None:\n            self.model.processor.save_pretrained(save_dir)\n        if self.modal_type in [\"LLM\", \"VLM\"]:\n            self.model.tokenizer.save_pretrained(save_dir)\n\n    def _find_layers(self, module, layers=None, name=\"\"):\n        if not layers:\n            layers = self.isinstance_list\n        if type(module) in layers:\n            return {name: module}\n        res = {}\n        for name1, child in module.named_children():\n            res.update(\n                self._find_layers(\n                    child,\n                    layers=layers,\n                    name=name + \".\" + name1 if name != \"\" else name1,\n                )\n            )\n        return res\n\n    def _apply_quant(self, module, named_linears: Dict[str, nn.Linear]):\n        for name, linear_layer in named_linears.items():\n            if \"mlp.gate.\" in name:\n                continue\n            # NOTE: small regression in perplexity if linear layer uses .cpu().float()\n            linear_layer = linear_layer.to(get_best_device()).half()\n\n            linear_layer.weight.data, scales, zeros = pseudo_quantize_tensor(\n                linear_layer.weight.data,\n                w_bit=self.quant_bits,\n                zero_point=self.zero_point,\n                q_group_size=self.group_size,\n                get_scale_zp=True,\n            )\n\n            scales = scales.t().contiguous()\n            if zeros is not None:\n                zeros = zeros.t().contiguous()\n            q_linear_module = WQLinearGEMM\n\n            q_linear = q_linear_module.from_linear(\n                linear=linear_layer,\n                w_bit=self.quant_bits,\n                group_size=self.group_size,\n                init_only=False,\n                scales=scales,\n                zeros=zeros,\n            )\n\n            linear_layer.cpu()\n            q_linear.to(next(module.parameters()).device)\n            set_op_by_name(module, name, q_linear)\n\n    def _convert_llm(self):\n        for i in tqdm(range(len(self.layers)), desc=\"AWQ\"):\n            subset = self._find_layers(self.layers[i])\n            self._apply_quant(self.layers[i], subset)\n\n    def convert(self):\n        \"\"\"\n        Saves scales and inserts QDQ modules.\n        \"\"\"\n        print_info(\"Start convert model...\")\n        if self.modal_type in [\"LLM\", \"VLM\"]:\n            self._convert_llm()\n        elif self.modal_type == \"AIGC\":\n            pass\n        else:\n            print_info(\"current {} modal type not support\".format(self.modal_type))\n            raise NotImplementedError\n        print_info(\"convert model done.\")\n",
        "angelslim/compressor/quant/modules/awq/search.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch.nn import Linear\n\nfrom .....utils import get_best_device, print_info\nfrom ...core import mse_loss, pseudo_quantize_tensor\n\nprint_func = print_info\n\n\nclass AWQSearch:\n    def __init__(\n        self,\n        n_grid=20,\n        bits_length=4,\n        weight_quant_method=\"groupwise\",\n        group_size=128,\n        loss_function=mse_loss,\n        merge_samples=True,\n        observer_layer_classes=None,\n        low_memory=False,\n    ):\n        \"\"\"\n        The implementation of AutoScale from AWQ(https://arxiv.org/pdf/2306.00978.pdf).\n        \"\"\"\n        self.n_grid = n_grid\n        self.bits_length = bits_length\n        self.weight_quant_method = weight_quant_method\n        self.bnt = (1 << (bits_length - 1)) - 1\n        self.group_size = group_size\n        self.loss_function = loss_function\n        self.merge_samples = merge_samples\n        self.observer_layer_classes = observer_layer_classes\n        self.low_memory = low_memory\n\n    def _get_out(self, layer_name, act, block, cache):\n        if \"qkv\" in layer_name:\n            return block(act, **cache)[0].squeeze(1)\n        else:\n            return block(act)[0].squeeze(1)\n\n    # search by block\n    @torch.no_grad()\n    def search_by_block(\n        self, layer_name, act_input, act_abs_max, layers, block, cache, layer_count\n    ):\n        act = act_input\n        print_func(\"[awq search] act device: %s\" % act.device)\n        print_func(\"[awq search] search input of %s\" % layer_name)\n        best_error = float(\"inf\")\n        best_ratio = -1\n        best_scales = None\n        dev = get_best_device()\n        with torch.no_grad():\n            if cache is not None:\n                origin_out = torch.ones_like(act)\n                new_out = torch.ones_like(act)\n            else:\n                origin_out = torch.ones(\n                    (act.shape[0], act.shape[1], layers[0].weight.shape[0]),\n                    dtype=act.dtype,\n                    device=act.device,\n                )\n                new_out = torch.ones(\n                    (act.shape[0], act.shape[1], layers[0].weight.shape[0]),\n                    dtype=act.dtype,\n                    device=act.device,\n                )\n\n            for j in range(act.shape[0]):\n                origin_out[j, :, :] = self._get_out(\n                    layer_name, act[j, :, :].unsqueeze(0).to(dev), block, cache\n                ).to(act.device)\n\n            org_w = []\n            for layer in layers:\n                org_w.append(layer.weight.clone().cpu())\n\n            for ratio in range(self.n_grid):\n                ratio = ratio * 1 / self.n_grid\n                act_abs_max_tmp = act_abs_max.detach().clone().to(dev)\n                scales = act_abs_max_tmp.pow(ratio).clamp(min=1e-4).view(-1)\n                scales = scales / (scales.max() * scales.min()).sqrt()\n                for layer in layers:\n                    layer.weight.mul_(scales.view(1, -1))\n                    if type(layer) in [Linear]:\n                        quant_dequant_weight = pseudo_quantize_tensor(\n                            layer.weight,\n                            w_bit=self.bits_length,\n                            q_group_size=self.group_size,\n                        )\n                        layer.weight.data.copy_(quant_dequant_weight)\n\n                for j in range(act.shape[0]):\n                    new_act = act[j, :, :].unsqueeze(0).to(dev) / scales\n                    new_out[j, :, :] = self._get_out(\n                        layer_name, new_act, block, cache\n                    ).to(act.device)\n\n                loss = self.loss_function(origin_out, new_out).to(torch.float32)\n\n                if loss < best_error:\n                    print_func(\"find better ratio: {}, loss: {}\".format(ratio, loss))\n                    best_error = loss\n                    best_ratio = ratio\n                    best_scales = scales\n\n                for layer, w in zip(layers, org_w):\n                    layer.weight.data.copy_(w)\n\n        origin_out = origin_out.detach().cpu()\n        new_out = w.detach().cpu()\n        del origin_out\n        del new_out\n        for w in org_w:\n            w = w.detach().cpu()\n            del w\n\n        if best_scales is None:\n            best_scales = torch.ones(scales.shape, dtype=act.dtype)\n            print_func(\"Cannot find better ratio.\")\n        else:\n            print_func(\n                \"Best ratio :{}, minimal loss : {}.\".format(best_ratio, best_error)\n            )\n        return best_scales.detach().cpu()\n",
        "angelslim/compressor/quant/modules/catcher.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n__all__ = [\"Catcher\"]\n\n\nclass Catcher(torch.nn.Module):\n    def __init__(self, module, inps, cache):\n        super().__init__()\n        self.module = module\n        self.inps = inps\n        self.cache = cache\n        self.layer_kwargs = {}\n\n    def forward(self, inp, **kwargs):\n        for i in range(inp.shape[0]):\n            self.inps[self.cache[\"i\"], :, :] = inp[i, :, :]\n            self.cache[\"i\"] += 1\n        self.layer_kwargs.update(kwargs)\n        raise ValueError\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n",
        "angelslim/compressor/quant/modules/fp8/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/modules/fp8/fp8.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport gc\n\nimport torch\nimport torch.nn as nn\n\nfrom .....utils import get_best_device, print_info\nfrom ...modules.catcher import Catcher\n\n__all__ = [\"FP8\"]\n\n\nclass FP8:\n    def __init__(\n        self,\n        model,\n        seq_length=2048,\n        hidden_size=2560,\n        model_arch_type=None,\n        low_memory=False,\n    ):\n        \"\"\"\n        Args:\n            model(nn.Module, required): The model to be smoothed.\n            seq_length(int, optional): The length of the sequence. Default: 2048.\n            hidden_size(int, optional): The size of the hidden layer. Default: 2560.\n            model_arch_type(str, optional): model arch type.Default: None.\n            low_memory(boll, optional): using low memory .Default: None.\n        \"\"\"\n        super(FP8, self).__init__()\n        self.model = model\n        self.modal_type = self.model.modal_type\n        if self.modal_type == \"VLM\":\n            self.layers = self.model.model.model.language_model.layers\n        else:\n            self.layers = self.model.model.model.layers\n        self.quant_bits = self.model.quant_config.quant_bit\n        self.seq_length = seq_length\n        self.hidden_size = hidden_size\n        self.model_arch_type = model_arch_type\n        self.low_memory = low_memory\n        self.dtype = torch.bfloat16\n        torch.set_default_dtype(self.dtype)\n        self.scales_dict = {}\n        self.inps = None\n\n    def move_embed(self, model, device: str):\n        print_info(model)\n        model.model.model.embed_tokens = model.model.model.embed_tokens.to(device)\n        model.model.model.rotary_emb = model.model.model.rotary_emb.to(device)\n\n    @torch.no_grad()\n    def run(self, dataloader):\n        if self.low_memory:\n            print_info(\"Use FP8 low memory run\")\n            assert (\n                str(next(self.model.model.parameters()).device) == \"cpu\"\n            ), \"[AngelSlim Error] FP8 low memory mode need model in cpu\"\n            self.low_memory_run(dataloader)\n        else:\n            print_info(\"Use FP8 fast forward\")\n            self.model.model_forward(dataloader)\n\n    def low_memory_run(self, dataloader):\n        for model_module in self.layers:\n            model_module.eval()\n        layers = self.layers\n        dev = \"cpu\"\n        nsamples = len(dataloader) * dataloader.batch_size\n        print_info(f\"nsamples:{nsamples}\")\n        self.inps = torch.zeros(\n            (int(nsamples), self.seq_length, self.hidden_size),\n            device=dev,\n            dtype=self.dtype,\n        )\n        cache = {\"i\": 0}\n        layers[0] = layers[0].to(dev)\n        self.model.model.model.embed_tokens = self.model.model.model.embed_tokens.to(\n            dev\n        )\n        layers[0] = Catcher(layers[0], self.inps, cache)\n        self.model.model_forward(dataloader)\n        layer_kwargs = layers[0].layer_kwargs\n        dev = get_best_device()\n        for k, v in layer_kwargs.items():\n            # position embeddings\n            if isinstance(v, tuple):\n                layer_kwargs[k] = tuple(\n                    (\n                        item.to(dev)\n                        if isinstance(item, (torch.Tensor, nn.Module))\n                        else item\n                    )\n                    for item in v\n                )\n            if isinstance(v, torch.Tensor):\n                layer_kwargs[k] = v.to(dev)\n\n        print_info(\"cache['i']:{}\".format(cache[\"i\"]))\n        print_info(len(layers))\n        layers[0] = layers[0].module\n        print_info(self.inps.shape)\n        # begin the FP8 process\n        print_info(\"Ready.\")\n        layers = layers.cpu()\n        self.inps = self.inps.to(\"cpu\")\n        bs = dataloader.batch_size\n\n        for i in range(0, nsamples, bs):\n            inps = self.inps[i : i + bs, :, :].to(dev)\n            outs = torch.zeros_like(inps).to(dev)\n            for j in range(len(layers)):\n                layer = layers[j].to(dev)\n                with torch.no_grad():\n                    outs = layer(hidden_states=inps, **layer_kwargs)\n                    if isinstance(outs, tuple):\n                        outs = outs[0].squeeze(1)\n\n                if torch.cuda.is_available():\n                    print_info(\n                        f\"GPU Memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\"\n                    )\n                # Clear GPU memory\n                torch.cuda.empty_cache()\n\n                layers[j] = layers[j].cpu()\n                inps, outs = outs, inps\n                print_info(\"FP8 end layer {}\\n\".format(j))\n            print_info(\"FP8 end batch {}\\n\".format(i // bs))\n        del inps, outs\n        self.inps = None\n        gc.collect()\n        torch.cuda.empty_cache()\n",
        "angelslim/compressor/quant/modules/gptq/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/modules/gptq/gptq.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport threadpoolctl as tctl\nimport torch\nimport torch.nn as nn\nfrom huggingface_hub import save_torch_state_dict\nfrom tqdm import tqdm\n\nfrom .....utils import print_info\nfrom ...modules.catcher import Catcher\nfrom ...modules.helper_layer import GPTQQuantLinear\nfrom .gptq_module import GPTQModule\n\n__all__ = [\"GPTQ\"]\n\n\nclass GPTQ:\n    def __init__(\n        self, model, seq_length=2048, hidden_size=2560, sym=True, actorder=True\n    ):\n        super(GPTQ, self).__init__()\n        self.model = model\n        self.modal_type = self.model.modal_type\n        if self.modal_type == \"VLM\":\n            self.layers = self.model.model.model.language_model.layers\n        else:\n            self.layers = self.model.model.model.layers\n        self.layers_block_name = self.model.block_name\n        self.quant_bits = self.model.quant_config.quant_bit\n        self.group_size = self.model.quant_config.quant_algo_info[\"group_size\"]\n        self.ignore_layers = self.model.quant_config.quant_algo_info[\"ignore_layers\"]\n        self.percdamp = 0.01\n        self.sym = sym\n        self.actorder = actorder\n        self.seq_length = seq_length\n        self.hidden_size = hidden_size\n        self.dtype = next(iter(self.layers.parameters())).dtype\n        self.quantizers = {}\n        self.gptq = {}\n\n    @torch.no_grad()\n    def run(self, dataloader):\n        for model_module in self.layers:\n            model_module.eval()\n\n        layers = self.layers\n        dev = \"cuda:0\"\n\n        print_info(\"dev = :{}\".format(dev))\n\n        nsamples = len(dataloader)\n        inps = torch.zeros(\n            (nsamples, self.seq_length, self.hidden_size), device=dev, dtype=self.dtype\n        )\n        cache = {\"i\": 0}\n\n        pre_transformer_modules_dict = self.model.get_pre_transformer_modules()\n        for _, module in pre_transformer_modules_dict.items():\n            module.to(dev)\n        layers[0] = layers[0].to(dev)\n        layers[0] = Catcher(layers[0], inps, cache)\n        # get modle input in dataloader\n        self.model.model_forward(dataloader)\n        layer_kwargs = layers[0].layer_kwargs\n\n        print_info(\"cache['i']:{}\".format(cache[\"i\"]))\n\n        layers[0] = layers[0].module\n        for _, module in pre_transformer_modules_dict.items():\n            module.cpu()\n        layers[0].cpu()\n        torch.cuda.empty_cache()\n\n        outs = torch.zeros_like(inps)\n        # begin the gptq process\n        print_info(\"Ready.\")\n\n        layers = layers.cpu()\n\n        for i in range(len(layers)):\n            layer = layers[i].to(inps.device)\n            subset = self._find_layers(layer)\n            print_info(\"subset:{}\".format(subset))\n            self.gptq = {}\n            print_info(\"GPTQMoe start layer {}\".format(i))\n            for name in subset:\n                if name in self.ignore_layers:\n                    continue\n                self.gptq[name] = GPTQModule(subset[name], quant_bits=self.quant_bits)\n\n            def add_batch(layer_name):\n                def tmp(_, inp, out):\n                    self.gptq[layer_name].add_batch(inp[0].data, out.data)\n\n                return tmp\n\n            handles = []\n            for name in self.gptq:\n                handles.append(subset[name].register_forward_hook(add_batch(name)))\n\n            # being hook\n            for j in range(nsamples):\n                with torch.no_grad():\n                    outs[j, :, :] = layer(\n                        hidden_states=inps[j, :, :].unsqueeze(0), **layer_kwargs\n                    )[0].squeeze(1)\n\n            print_info(\"HOOK Step{}\".format(j))\n            for h in handles:\n                h.remove()\n\n            for name in subset:\n                if name in self.ignore_layers:\n                    continue\n                print_info(\"Quant {} ...\".format(name))\n                scale, zero, g_idx = self.gptq[name].fasterquant(\n                    percdamp=self.percdamp,\n                    group_size=self.group_size,\n                    actorder=self.actorder,\n                    sym=self.sym,\n                )\n                self.quantizers[f\"{self.layers_block_name}.{i}.{name}\"] = (\n                    scale,\n                    zero,\n                    g_idx,\n                )\n                self.gptq[name].free()\n\n            for j in range(nsamples):\n                with torch.no_grad():\n                    outs[j, :, :] = layer(\n                        hidden_states=inps[j, :, :].unsqueeze(0), **layer_kwargs\n                    )[0].squeeze(1)\n\n            for name in self.gptq:\n                del self.gptq[name].layer\n\n            layers[i] = layer.cpu()\n            del layer\n            # del gptq\n            torch.cuda.empty_cache()\n            inps, outs = outs, inps\n            print_info(\"GPTQ end layer {}\\n\".format(i))\n\n        # inps = inps.cpu()\n        # outs = outs.cpu()\n        del inps, outs\n        torch.cuda.empty_cache()\n        print_info(\"GPTQ done.\")\n\n    def _make_quant(\n        self,\n        module,\n        names,\n        bits,\n        group_size,\n    ):\n        if isinstance(module, GPTQQuantLinear):\n            return\n\n        for name, submodule in module.named_modules():\n            if name in names:\n                ori_layer_device = next(submodule.parameters()).device\n\n                if isinstance(submodule, nn.Linear):\n                    in_features = submodule.in_features\n                    out_features = submodule.out_features\n                bias = submodule.bias is not None\n                new_layer = GPTQQuantLinear(\n                    bits,\n                    group_size,\n                    in_features,\n                    out_features,\n                    bias,\n                    weight_dtype=submodule.weight.dtype,\n                )\n                new_layer.device = ori_layer_device\n                self._recurse_setattr(module, name, new_layer.to(ori_layer_device))\n\n    def _pack_model(\n        self, model, quantizers, bits, group_size, force_layer_back_to_cpu: bool = False\n    ):\n        if force_layer_back_to_cpu:\n            model.cpu()\n\n        print_info(\"Packing model...\")\n        layers = self._find_layers(model)\n        layers = {n: layers[n] for n in quantizers}\n\n        self._make_quant(model, quantizers, bits, group_size)\n\n        qlayers = self._find_layers(model, [GPTQQuantLinear])\n\n        with tctl.threadpool_limits(limits=1):\n            pbar = tqdm(qlayers.keys(), leave=True)\n            for name in pbar:\n                pbar.set_description(f\"Packing {name}...\", refresh=True)\n\n                scale, zero, g_idx = quantizers[name]\n                # so far can only pack layer on CPU\n                layer_device = qlayers[name].device\n                qlayers[name].cpu()\n                layers[name], scale, zero, g_idx = (\n                    layers[name].cpu(),\n                    scale.cpu(),\n                    zero.cpu(),\n                    g_idx.cpu(),\n                )\n                qlayers[name].pack(layers[name], scale, zero, g_idx)\n                qlayers[name].to(layer_device)\n        print_info(\"Model packed.\")\n\n    def _convert_llm(self):\n        self._pack_model(\n            model=self.model.model,\n            quantizers=self.quantizers,\n            bits=self.quant_bits,\n            group_size=self.group_size,\n            force_layer_back_to_cpu=True,\n        )\n\n    def convert(self):\n        \"\"\"\n        Saves scales and inserts QDQ modules.\n        \"\"\"\n        print_info(\"Start convert model...\")\n        if self.modal_type in [\"LLM\", \"VLM\"]:\n            self._convert_llm()\n        elif self.modal_type == \"AIGC\":\n            pass\n        else:\n            print_info(\"current {} modal type not support\".format(self.modal_type))\n            raise NotImplementedError\n        print_info(\"convert model done.\")\n\n    def save(self, save_dir: str, shard_size=\"5GB\", safetensors=True):\n        \"\"\"save quantized model and configs to local disk\"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n\n        self.model.model.cpu()\n\n        # Save model\n        class EmptyModule(nn.Module):\n            def __init__(self):\n                super(EmptyModule, self).__init__()\n\n            def forward(self, x):\n                return x\n\n        # Save model and config files with empty state dict\n        self.model.model.config.quantization_config = {\n            \"bits\": self.quant_bits,\n            \"checkpoint_format\": \"gptq\",\n            \"desc_act\": True,\n            \"group_size\": self.group_size,\n            \"quant_method\": \"gptq\",\n            \"static_groups\": True,\n            \"sym\": True,\n            \"true_sequential\": True,\n        }\n        self.model.model.config.save_pretrained(\n            save_dir, state_dict=EmptyModule().state_dict()\n        )\n\n        # Remove empty state dict\n        default_paths = [\n            f\"{save_dir}/model.safetensors\",\n            f\"{save_dir}/pytorch_model.bin\",\n        ]\n        for path in default_paths:\n            if os.path.exists(path):\n                os.remove(path)\n\n        save_torch_state_dict(\n            state_dict=self.model.model.state_dict(),\n            save_directory=save_dir,\n            max_shard_size=shard_size,\n            safe_serialization=safetensors,\n            force_contiguous=True,\n            shared_tensors_to_discard=self.model.model._tied_weights_keys,\n        )\n        # self.model.model.config.torch_dtype = \"float16\"\n        self.model.model.config.to_json_file(os.path.join(save_dir, \"config.json\"))\n\n        # save processor and tokenizer\n        if self.modal_type == \"VLM\" and self.model.processor is not None:\n            self.model.processor.save_pretrained(save_dir)\n        if self.modal_type in [\"LLM\", \"VLM\"]:\n            self.model.tokenizer.save_pretrained(save_dir)\n\n    def _recurse_setattr(self, module, name, value):\n        \"\"\"A function to recursively set attributes to a module.\"\"\"\n        if \".\" not in name:\n            setattr(module, name, value)\n        else:\n            name, rest = name.split(\".\", 1)\n            self._recurse_setattr(getattr(module, name), rest, value)\n\n    def _find_layers(self, module, layers=None, name=\"\"):\n        if not layers:\n            layers = [torch.nn.Linear]\n        if type(module) in layers:\n            return {name: module}\n        res = {}\n        for name1, child in module.named_children():\n            res.update(\n                self._find_layers(\n                    child,\n                    layers=layers,\n                    name=name + \".\" + name1 if name != \"\" else name1,\n                )\n            )\n        return res\n",
        "angelslim/compressor/quant/modules/gptq/gptq_module.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport time\n\nimport torch\n\nfrom .....utils import get_tensor_item, print_info\nfrom ...core import compute_scales_with_zero\n\n__all__ = [\"GPTQModule\"]\n\n\nclass GPTQModule:\n    def __init__(self, layer, quant_bits=4):\n        \"\"\"\n        GPTQ quantization wrapper for neural network layers.\n\n        Args:\n            layer: Full-precision torch.nn.Module to quantize (Linear)\n            quant_bits: Quantization bitwidth (2-8 bits, default=4)\n        \"\"\"\n        super(GPTQModule, self).__init__()\n        self.layer = layer\n        self.dev = self.layer.weight.device\n        self.w = layer.weight.data.clone()\n        self.rows = self.w.shape[0]\n        self.columns = self.w.shape[1]\n        self.h = torch.zeros((self.columns, self.columns), device=self.dev)\n        self.nsamples = 0\n        self.quant_bits = quant_bits\n\n    def add_batch(self, inp, out):\n        if len(inp.shape) == 4:\n            inp = inp[0, 0, :, :]\n        inp = inp.squeeze()\n        if len(inp.shape) == 2:\n            inp = inp.unsqueeze(0)\n        tmp = inp.shape[0]\n        if len(inp.shape) == 3:\n            inp = inp.reshape((-1, inp.shape[-1]))\n        inp = inp.t()\n        self.h *= self.nsamples / (self.nsamples + tmp)\n        self.nsamples += tmp\n        inp = math.sqrt(2 / self.nsamples) * inp.float()\n        self.h += inp.matmul(inp.t())\n\n    def fasterquant(\n        self,\n        blocksize=128,\n        percdamp=0.01,\n        group_size=-1,\n        actorder=True,\n        sym=True,\n    ):\n        w_weight = self.w.float()\n\n        tick = time.time()\n\n        hessian = self.h\n        if torch.isnan(hessian).any():\n            print_info(\"[error] Hessian contains nan!\")\n            exit()\n        self.h.detach().cpu()\n        del self.h\n        dead = torch.diag(hessian) == 0\n        hessian[dead, dead] = 1\n        w_weight[:, dead] = 0\n\n        g_idx = []\n        scale = []\n        zero = []\n        now_idx = 1\n        static_groups = True\n\n        if static_groups:\n            for i in range(0, self.columns, group_size):\n                weight_scale, weight_zero = compute_scales_with_zero(\n                    w_weight[:, i : (i + group_size)], bits=self.quant_bits, sym=sym\n                )\n                scale.append(weight_scale)\n                zero.append(weight_zero)\n\n        if actorder:\n            perm = torch.argsort(torch.diag(hessian), descending=True)\n            w_weight = w_weight[:, perm]\n            hessian = hessian[perm][:, perm]\n            invperm = torch.argsort(perm)\n\n        losses = torch.zeros_like(w_weight)\n        q_weight = torch.zeros_like(w_weight)\n\n        while 1 > percdamp > 0:\n            try:\n                damp = percdamp * torch.mean(torch.diag(hessian))\n                diag = torch.arange(self.columns, device=self.dev)\n                hessian[diag, diag] += damp\n                hessian = torch.linalg.cholesky(hessian)\n                hessian = torch.cholesky_inverse(hessian)\n                hessian = torch.linalg.cholesky(hessian, upper=True)\n                hinv = hessian\n                break\n            except torch._C._LinAlgError as e:\n                print_info(e)\n                print_info(f\"Cholesky failed with percdamp={percdamp:.5f}\")\n                percdamp += 0.01\n\n        for i1 in range(0, self.columns, blocksize):\n            i2 = min(i1 + blocksize, self.columns)\n            count = i2 - i1\n\n            w1 = w_weight[:, i1:i2].clone()\n            q1 = torch.zeros_like(w1)\n            err1 = torch.zeros_like(w1)\n            losses1 = torch.zeros_like(w1)\n            hinv1 = hinv[i1:i2, i1:i2]\n\n            for i in range(count):\n                w = w1[:, i]\n                d = hinv1[i, i]\n\n                if group_size != -1:\n                    if not static_groups:\n                        if (i1 + i) % group_size == 0:\n                            weight_scale, weight_zero = compute_scales_with_zero(\n                                w_weight[:, (i1 + i) : (i1 + i + group_size)],\n                                bits=self.quant_bits,\n                                sym=sym,\n                            )\n\n                        if ((i1 + i) // group_size) - now_idx == -1:\n                            scale.append(weight_scale)\n                            zero.append(weight_zero)\n                            now_idx += 1\n                    else:\n                        idx = i1 + i\n                        if actorder:\n                            idx = perm[idx]\n                        weight_scale = scale[idx // group_size]\n                        weight_zero = zero[idx // group_size]\n\n                maxq = torch.tensor(2**self.quant_bits - 1)\n                q = torch.clamp(\n                    torch.round(w.unsqueeze(1) / weight_scale) + weight_zero, 0, maxq\n                )\n                q = weight_scale * (q - weight_zero)\n                q = q.flatten()\n                q1[:, i] = q\n                losses1[:, i] = (w - q) ** 2 / d**2\n\n                err = (w - q) / d\n                w1[:, i:] -= err.unsqueeze(1).matmul(hinv1[i, i:].unsqueeze(0))\n                err1[:, i] = err\n\n            q_weight[:, i1:i2] = q1\n            losses[:, i1:i2] = losses1 / 2\n\n            w_weight[:, i2:] -= err1.matmul(hinv[i1:i2, i2:])\n\n        torch.cuda.synchronize()\n        print_info(f\" duration: {(time.time() - tick)}\")\n        print_info(f\" avg loss: {torch.sum(losses).item() / self.nsamples}\")\n\n        group_size = group_size if group_size != -1 else self.columns\n        if static_groups and actorder:\n            g_idx = [perm[i] // group_size for i in range(self.columns)]\n        else:\n            g_idx = [i // group_size for i in range(self.columns)]\n        g_idx = torch.tensor(g_idx, dtype=torch.int32, device=q_weight.device)\n        if actorder:\n            q_weight = q_weight[:, invperm]\n            g_idx = g_idx[invperm]\n\n        norm_loss = torch.norm(\n            q_weight.reshape(self.layer.weight.shape).type_as(self.layer.weight.data)\n            - self.layer.weight.data\n        )\n        all_norm_loss = [norm_loss]\n\n        print_info(\" self.layer.weight: {}, {}\".format(q_weight.shape, q_weight.sum()))\n        print_info(f\" norm loss: {list(map(get_tensor_item, all_norm_loss))}\")\n\n        self.layer.weight.data.copy_(\n            q_weight.reshape(self.layer.weight.shape).type_as(self.layer.weight.data)\n        )\n\n        if scale == []:\n            scale = weight_scale\n            zero = torch.zeros_like(weight_scale)\n        scale = torch.cat(scale, dim=1)\n        zero = torch.cat(zero, dim=1)\n        losses = losses.cpu()\n        q_weight = q_weight.cpu()\n        w_weight = w_weight.cpu()\n        hessian = hessian.cpu()\n        hinv = hinv.cpu()\n        del losses, q_weight, w_weight, hessian, hinv\n        self.w = self.w.cpu()\n        del self.w\n        torch.cuda.empty_cache()\n        return scale, zero, g_idx\n\n    def free(self):\n        self.h = None\n        self.w = None\n        self.losses = None\n        torch.cuda.empty_cache()\n",
        "angelslim/compressor/quant/modules/helper_layer.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport math\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\n\nfrom ....utils import get_best_device\nfrom ..core import (\n    QuantConfig,\n    dequantize_gemm,\n    fake_quant_dequant,\n    gemm_fp8,\n    pack_weight_to_int8,\n    quantize_activation_per_tensor_fp8,\n    quantize_weight_int,\n    quantize_weight_per_tensor_fp8,\n    tensor_quant_dequant_fp8,\n    tensor_quant_dequant_int,\n)\n\n\ndef flush():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\n# Adapted from https://github.com/compressa-ai/AutoAWQ/tree/dev\nclass WQLinearMMFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(\n        ctx,\n        x,\n        qweight,\n        qzeros,\n        scales,\n        w_bit=4,\n        group_size=128,\n        bias=None,\n        out_features=0,\n    ):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(x, qweight, qzeros, scales, bias)\n        ctx.out_features = out_features\n\n        out_shape = x.shape[:-1] + (out_features,)\n        x = x.to(torch.float16)\n        if x.shape[0] == 0:\n            return torch.zeros(out_shape, dtype=x.dtype, device=x.device)\n\n        # global user_has_been_warned\n        # if not user_has_been_warned:\n        #     warnings.warn(\"Using naive (slow) implementation.\" + msg)\n        #     user_has_been_warned = True\n        out = dequantize_gemm(qweight, qzeros, scales, w_bit, group_size)\n        out = torch.matmul(x, out)\n\n        out = out + bias if bias is not None else out\n        out = out.reshape(out_shape)\n\n        # always want 3D tensor if tensor is 2D\n        if len(out.shape) == 2:\n            out = out.unsqueeze(0)\n\n        return out\n\n\nclass WQLinearGEMM(nn.Module):\n    def __init__(\n        self, w_bit, group_size, in_features, out_features, bias, dev, training=False\n    ):\n        super().__init__()\n\n        if w_bit not in [4]:\n            raise NotImplementedError(\"Only 4-bit are supported for now.\")\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.w_bit = w_bit\n        self.group_size = group_size if group_size != -1 else in_features\n        self.training = training\n\n        # quick sanity check (make sure aligment)\n        assert self.in_features % self.group_size == 0\n        assert out_features % (32 // self.w_bit) == 0\n\n        self.register_buffer(\n            \"qweight\",\n            torch.zeros(\n                (in_features, out_features // (32 // self.w_bit)),\n                dtype=torch.int32,\n                device=dev,\n            ),\n        )\n        self.register_buffer(\n            \"qzeros\",\n            torch.zeros(\n                (in_features // self.group_size, out_features // (32 // self.w_bit)),\n                dtype=torch.int32,\n                device=dev,\n            ),\n        )\n        self.register_buffer(\n            \"scales\",\n            torch.zeros(\n                (in_features // self.group_size, out_features),\n                dtype=torch.float16,\n                device=dev,\n            ),\n        )\n        if bias:\n            self.register_buffer(\n                \"bias\",\n                torch.zeros(\n                    (out_features),\n                    dtype=torch.float16,\n                    device=dev,\n                ),\n            )\n        else:\n            self.bias = None\n\n    @classmethod\n    def from_linear(\n        cls, linear, w_bit, group_size, init_only=False, scales=None, zeros=None\n    ):\n        awq_linear = cls(\n            w_bit,\n            group_size,\n            linear.in_features,\n            linear.out_features,\n            linear.bias is not None,\n            linear.weight.device,\n        )\n        if init_only:  # just prepare for loading sd\n            return awq_linear\n\n        # need scales and zeros info for real quantization\n        assert scales is not None and zeros is not None\n        scale_zeros = zeros * scales\n\n        awq_linear.scales = scales.clone().half()\n        if linear.bias is not None:\n            awq_linear.bias = linear.bias.clone().half()\n\n        pack_num = 32 // awq_linear.w_bit\n\n        intweight = []\n        for idx in range(awq_linear.in_features):\n            intweight.append(\n                torch.round(\n                    (linear.weight.data[:, idx] + scale_zeros[idx // group_size])\n                    / awq_linear.scales[idx // group_size]\n                ).to(torch.int)[:, None]\n            )\n        intweight = torch.cat(intweight, dim=1)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.to(dtype=torch.int32)\n\n        best_device = get_best_device()\n\n        if \"mps\" in best_device:\n            intweight = intweight.to(\"cpu\")\n\n        qweight = torch.zeros(\n            (intweight.shape[0], intweight.shape[1] // 32 * awq_linear.w_bit),\n            dtype=torch.int32,\n            device=intweight.device,\n        )\n\n        for col in range(intweight.shape[1] // pack_num):\n            if awq_linear.w_bit == 4:\n                order_map = [0, 2, 4, 6, 1, 3, 5, 7]\n            else:\n                raise NotImplementedError(\"Only 4-bit are supported for now.\")\n            for i in range(pack_num):\n                qweight_col = intweight[:, col * pack_num + order_map[i]]\n                qweight[:, col] |= qweight_col << (i * awq_linear.w_bit)\n        awq_linear.qweight = qweight\n\n        zeros = zeros.to(dtype=torch.int32, device=best_device)\n\n        if \"mps\" in best_device:\n            zeros = zeros.to(\"cpu\")\n\n        qzeros = torch.zeros(\n            (zeros.shape[0], zeros.shape[1] // 32 * awq_linear.w_bit),\n            dtype=torch.int32,\n            device=zeros.device,\n        )\n\n        for col in range(zeros.shape[1] // pack_num):\n            if awq_linear.w_bit == 4:\n                order_map = [0, 2, 4, 6, 1, 3, 5, 7]\n            else:\n                raise NotImplementedError(\"Only 4-bit are supported for now.\")\n            for i in range(pack_num):\n                qzero_col = zeros[:, col * pack_num + order_map[i]]\n                qzeros[:, col] |= qzero_col << (i * awq_linear.w_bit)\n        awq_linear.qzeros = qzeros\n\n        return awq_linear\n\n    def forward(self, x):\n        out_shape = x.shape[:-1] + (self.out_features,)\n\n        input_dtype = x.dtype\n        if input_dtype != torch.float16:\n            x = x.half()\n\n        with torch.no_grad():\n            out = WQLinearMMFunction.apply(\n                x,\n                self.qweight,\n                self.qzeros,\n                self.scales,\n                self.w_bit,\n                self.group_size,\n                self.bias,\n                self.out_features,\n            )\n\n        if input_dtype != torch.float16:\n            out = out.to(dtype=input_dtype)\n\n        return out.reshape(out_shape)\n\n    def extra_repr(self) -> str:\n        return (\n            \"in_features={}, out_features={}, bias={}, w_bit={}, group_size={}\".format(\n                self.in_features,\n                self.out_features,\n                self.bias is not None,\n                self.w_bit,\n                self.group_size,\n            )\n        )\n\n\nclass GPTQQuantLinear(nn.Module):\n    QUANT_TYPE = \"cuda\"\n\n    def __init__(\n        self,\n        bits,\n        group_size,\n        infeatures,\n        outfeatures,\n        bias,\n        weight_dtype=torch.float16,\n    ):\n        super().__init__()\n        if bits not in [2, 3, 4, 8]:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n\n        self.infeatures = infeatures\n        self.outfeatures = outfeatures\n        self.bits = bits\n        self.group_size = group_size if group_size != -1 else infeatures\n        self.maxq = 2**self.bits - 1\n\n        self.register_buffer(\n            \"qweight\",\n            torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32),\n        )\n        self.register_buffer(\n            \"qzeros\",\n            torch.zeros(\n                (\n                    math.ceil(infeatures / self.group_size),\n                    outfeatures // 32 * self.bits,\n                ),\n                dtype=torch.int32,\n            ),\n        )\n        self.register_buffer(\n            \"scales\",\n            torch.zeros(\n                (math.ceil(infeatures / self.group_size), outfeatures),\n                dtype=weight_dtype,\n            ),\n        )\n        self.register_buffer(\n            \"g_idx\",\n            torch.tensor(\n                [i // self.group_size for i in range(infeatures)], dtype=torch.int32\n            ),\n        )\n        if bias:\n            self.register_buffer(\"bias\", torch.zeros((outfeatures), dtype=weight_dtype))\n        else:\n            self.bias = None\n\n        # is performed by unpacking the weights and using torch.matmul\n        if self.bits in [2, 4, 8]:\n            self.wf = torch.tensor(\n                list(range(0, 32, self.bits)), dtype=torch.int32\n            ).unsqueeze(0)\n        elif self.bits == 3:\n            self.wf = torch.tensor(\n                [\n                    [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 0],\n                    [0, 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31],\n                    [0, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 0],\n                ],\n                dtype=torch.int32,\n            ).reshape(1, 3, 12)\n\n    def post_init(self):\n        pass\n\n    def pack(self, linear, scales, zeros, g_idx=None):\n        w = linear.weight.data.clone()\n\n        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx\n\n        scales = scales.t().contiguous()\n        zeros = zeros.t().contiguous()\n        scale_zeros = zeros * scales\n        self.scales = scales.clone().to(dtype=linear.weight.dtype)\n        if linear.bias is not None:\n            self.bias = linear.bias.clone().to(dtype=linear.weight.dtype)\n\n        intweight = []\n        for idx in range(self.infeatures):\n            intweight.append(\n                torch.round(\n                    (w[:, idx] + scale_zeros[self.g_idx[idx]])\n                    / self.scales[self.g_idx[idx]]\n                ).to(torch.int)[:, None]\n            )\n        intweight = torch.cat(intweight, dim=1)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.numpy().astype(np.uint32)\n\n        i = 0\n        row = 0\n        qweight = np.zeros(\n            (intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32\n        )\n        while row < qweight.shape[0]:\n            if self.bits in [2, 4, 8]:\n                for j in range(i, i + (32 // self.bits)):\n                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n                i += 32 // self.bits\n                row += 1\n            elif self.bits == 3:\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i))\n                i += 10\n                qweight[row] |= intweight[i] << 30\n                row += 1\n                qweight[row] |= (intweight[i] >> 2) & 1\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 1)\n                i += 10\n                qweight[row] |= intweight[i] << 31\n                row += 1\n                qweight[row] |= (intweight[i] >> 1) & 0x3\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 2)\n                i += 10\n                row += 1\n            else:\n                raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n\n        qweight = qweight.astype(np.int32)\n        self.qweight = torch.from_numpy(qweight)\n\n        zeros -= 1\n        zeros = zeros.numpy().astype(np.uint32)\n        qzeros = np.zeros(\n            (zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32\n        )\n        i = 0\n        col = 0\n        while col < qzeros.shape[1]:\n            if self.bits in [2, 4, 8]:\n                for j in range(i, i + (32 // self.bits)):\n                    qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))\n                i += 32 // self.bits\n                col += 1\n            elif self.bits == 3:\n                for j in range(i, i + 10):\n                    qzeros[:, col] |= zeros[:, j] << (3 * (j - i))\n                i += 10\n                qzeros[:, col] |= zeros[:, i] << 30\n                col += 1\n                qzeros[:, col] |= (zeros[:, i] >> 2) & 1\n                i += 1\n                for j in range(i, i + 10):\n                    qzeros[:, col] |= zeros[:, j] << (3 * (j - i) + 1)\n                i += 10\n                qzeros[:, col] |= zeros[:, i] << 31\n                col += 1\n                qzeros[:, col] |= (zeros[:, i] >> 1) & 0x3\n                i += 1\n                for j in range(i, i + 10):\n                    qzeros[:, col] |= zeros[:, j] << (3 * (j - i) + 2)\n                i += 10\n                col += 1\n            else:\n                raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n\n        qzeros = qzeros.astype(np.int32)\n        self.qzeros = torch.from_numpy(qzeros)\n\n    def forward(self, x: torch.Tensor):\n        out_shape = x.shape[:-1] + (self.outfeatures,)\n        x = x.reshape(-1, x.shape[-1])\n        x_dtype = x.dtype\n\n        if self.wf.device != self.qzeros.device:\n            self.wf = self.wf.to(self.qzeros.device)\n\n        if self.bits in [2, 4, 8]:\n            zeros = torch.bitwise_right_shift(\n                torch.unsqueeze(self.qzeros, 2).expand(-1, -1, 32 // self.bits),\n                self.wf.unsqueeze(0),\n            ).to(torch.int16 if self.bits == 8 else torch.int8)\n            zeros = torch.bitwise_and(zeros, (2**self.bits) - 1)\n\n            zeros = zeros + 1\n            zeros = zeros.reshape(self.scales.shape)\n\n            weight = torch.bitwise_right_shift(\n                torch.unsqueeze(self.qweight, 1).expand(-1, 32 // self.bits, -1),\n                self.wf.unsqueeze(-1),\n            ).to(torch.int16 if self.bits == 8 else torch.int8)\n            weight = torch.bitwise_and(weight, (2**self.bits) - 1)\n        elif self.bits == 3:\n            zeros = self.qzeros.reshape(\n                self.qzeros.shape[0], self.qzeros.shape[1] // 3, 3, 1\n            ).expand(-1, -1, -1, 12)\n            zeros = zeros >> self.wf.unsqueeze(0)\n            zeros[:, :, 0, 10] = (zeros[:, :, 0, 10] & 0x3) | (\n                (zeros[:, :, 1, 0] << 2) & 0x4\n            )\n            zeros[:, :, 1, 11] = (zeros[:, :, 1, 11] & 0x1) | (\n                (zeros[:, :, 2, 0] << 1) & 0x6\n            )\n            zeros = zeros & 0x7\n            zeros = torch.cat(\n                [zeros[:, :, 0, :11], zeros[:, :, 1, 1:12], zeros[:, :, 2, 1:11]],\n                dim=2,\n            )\n\n            zeros = zeros + 1\n            zeros = zeros.reshape(self.scales.shape)\n\n            weight = self.qweight.reshape(\n                self.qweight.shape[0] // 3, 3, 1, self.qweight.shape[1]\n            ).expand(-1, -1, 12, -1)\n            weight = (weight >> self.wf.unsqueeze(-1)) & 0x7\n            weight[:, 0, 10] = (weight[:, 0, 10] & 0x3) | ((weight[:, 1, 0] << 2) & 0x4)\n            weight[:, 1, 11] = (weight[:, 1, 11] & 0x1) | ((weight[:, 2, 0] << 1) & 0x6)\n            weight = weight & 0x7\n            weight = torch.cat(\n                [weight[:, 0, :11], weight[:, 1, 1:12], weight[:, 2, 1:11]], dim=1\n            )\n        else:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n\n        weight = weight.reshape(weight.shape[0] * weight.shape[1], weight.shape[2])\n        num_itr = self.g_idx.shape[0] // x.shape[-1]\n        if num_itr == 1:\n            weights = self.scales[self.g_idx.long()] * (\n                weight - zeros[self.g_idx.long()]\n            )\n        else:\n            num_dim = self.g_idx.shape[0] // num_itr\n            weights = []\n            for i in range(num_itr):\n                scale_i = self.scales[:, i * num_dim : (i + 1) * num_dim]\n                weight_i = weight[:, i * num_dim : (i + 1) * num_dim]\n                zeros_i = zeros[:, i * num_dim : (i + 1) * num_dim]\n                g_idx_i = self.g_idx[i * num_dim : (i + 1) * num_dim]\n                weights.append(\n                    scale_i[g_idx_i.long()] * (weight_i - zeros_i[g_idx_i.long()])\n                )\n            weights = torch.cat(weights, dim=1)\n        out = torch.matmul(x, weights)\n        out = out.to(x_dtype)\n        out = out.reshape(out_shape)\n        out = out + self.bias if self.bias is not None else out\n        return out\n\n\nclass SmoothHelpModule(nn.Module):\n    def __init__(self, layer):\n        super(SmoothHelpModule, self).__init__()\n        self.weight = layer.weight\n        self.weight.all_gather()\n        self.layer = layer\n        module_shape = self.weight.shape[-1]\n        smooth_data = torch.ones(module_shape, dtype=self.weight.dtype).to(\n            self.weight.device\n        )\n        self.smooth_weight = nn.Parameter(smooth_data)\n        self.register_parameter(\"smooth_weight\", self.smooth_weight)\n\n    def forward(self, input):\n        new_input = input\n        # multiply\n        smooth_input = torch.mul(new_input, self.smooth_weight)\n        return self.layer(smooth_input)\n\n    def convert_weight(self, smooth_weight):\n        self.smooth_weight.data.copy_(1 / smooth_weight.squeeze())\n        self.weight.all_gather()\n        self.weight.data.copy_(self.weight * smooth_weight)\n        self.weight.partition(has_been_updated=True)\n\n\nclass QDQSingleModule(nn.Module):\n    def __init__(self, layer, act_scales, weight_scales, quant_algo=\"int8\"):\n        super(QDQSingleModule, self).__init__()\n        assert act_scales is not None and weight_scales is not None\n        self.layer = layer\n        self.weight = layer.weight\n        self.act_scales = act_scales\n        self.weight_scales = weight_scales\n        self.quant_algo = quant_algo\n        for param_name, params in layer.named_parameters():\n            if \"weight\" in param_name:\n                if self.quant_algo == \"int8\":\n                    qdq_weight = tensor_quant_dequant_int(\n                        params, self.weight_scales, bits=8\n                    )\n                elif self.quant_algo == \"fp8\":\n                    qdq_weight = tensor_quant_dequant_fp8(\n                        params, self.weight_scales, bits=8\n                    )\n                params.data.copy_(qdq_weight)\n\n    def forward(self, input):\n        new_input = input\n        if self.quant_algo == \"int8\":\n            qdq_inp = tensor_quant_dequant_int(new_input, self.act_scales)\n        elif self.quant_algo == \"fp8\":\n            qdq_inp = tensor_quant_dequant_fp8(new_input, self.act_scales)\n        return self.layer(qdq_inp)\n\n\nclass QDQModule(torch.nn.Module):\n    def __init__(\n        self,\n        quant_algo: QuantConfig,\n        weight: torch.nn.Parameter,\n        weight_scale: torch.nn.Parameter,\n        bias: torch.nn.Parameter,\n        group_size: int = 128,\n        input_scale: Optional[torch.nn.Parameter] = None,\n        output_scale: Optional[torch.nn.Parameter] = None,\n    ):\n        super().__init__()\n        self.quant_algo = quant_algo\n        if \"fp8\" in quant_algo:\n            if \"w4a8\" in self.quant_algo:\n                tensor_max_value = weight_scale.clone()\n                tensor_wise_scale = tensor_max_value.max() / 448.0\n                quant_weight, _ = quantize_weight_per_tensor_fp8(\n                    weight, tensor_wise_scale\n                )\n                new_weight_bf16 = quant_weight.to(torch.bfloat16) * tensor_wise_scale\n\n                new_weight_bf16_qdq = fake_quant_dequant(\n                    new_weight_bf16, method=\"groupwise\", bits=4, group_size=group_size\n                )\n                quant_weight, _ = quantize_weight_int(\n                    new_weight_bf16_qdq, tensor_max_value, bits=4\n                )\n                quant_weight = pack_weight_to_int8(quant_weight)\n                del new_weight_bf16_qdq, new_weight_bf16\n            else:\n                quant_weight, weight_scale = quantize_weight_per_tensor_fp8(\n                    weight, weight_scale\n                )\n        elif \"int8\" in self.quant_algo:\n            quant_weight, weight_scale = quantize_weight_int(\n                weight, weight_scale, bits=8\n            )\n        else:\n            raise ValueError(f\"Unsupported quantization algorithm: {self.quant_algo}\")\n\n        if \"w4a8\" in self.quant_algo:\n            self.qweight = torch.nn.Parameter(quant_weight, requires_grad=False)\n        else:\n            self.weight = torch.nn.Parameter(quant_weight, requires_grad=False)\n        weight_scale = weight_scale.view(-1) if weight_scale.ndim == 0 else weight_scale\n        self.weight_scale = torch.nn.Parameter(weight_scale, requires_grad=False)\n        self.bias = bias\n        self.output_scale = output_scale\n        if input_scale is not None:\n            input_scale = input_scale.view(-1) if input_scale.ndim == 0 else input_scale\n            self.input_scale = torch.nn.Parameter(input_scale, requires_grad=False)\n        else:\n            self.input_scale = None\n        if self.output_scale:\n            self.output_scale = torch.nn.Parameter(\n                self.output_scale, requires_grad=False\n            )\n\n    def forward(self, x):\n        if self.input_scale:\n            if \"fp8\" in self.quant_algo:\n                qinput = quantize_activation_per_tensor_fp8(x, self.input_scale)\n            elif \"int8\" in self.quant_algo:\n                qinput = tensor_quant_dequant_int(x, self.input_scale, bits=8)\n            else:\n                raise ValueError(\n                    f\"Unsupported quantization algorithm: {self.quant_algo}\"\n                )\n\n        if \"fp8\" in self.quant_algo:\n            output = gemm_fp8(\n                act=qinput,\n                act_scale=self.input_scale,\n                weight=self.weight,\n                weight_scale=self.weight_scale,\n                bias=self.bias,\n                out_dtype=x.dtype,\n            )\n        elif \"int8\" in self.quant_algo:\n            output = torch.nn.functional.linear(\n                x, self.weight * self.weight_scale, bias=self.bias\n            )\n        else:\n            raise ValueError(f\"Unsupported quantization algorithm: {self.quant_algo}\")\n\n        if self.output_scale:\n            if \"fp8\" in self.quant_algo:\n                qoutput = quantize_activation_per_tensor_fp8(output, self.output_scale)\n                output = qoutput.to(output.dtype) * self.output_scale\n        return output\n",
        "angelslim/compressor/quant/modules/int8/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/modules/int8/int8.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nimport torch.nn as nn\n\nfrom .....utils import get_best_device, print_info\nfrom ...modules.catcher import Catcher\n\n__all__ = [\"INT8\"]\n\n\nclass INT8:\n    def __init__(\n        self,\n        model,\n        seq_length=2048,\n        hidden_size=2560,\n        model_arch_type=None,\n        low_memory=False,\n    ):\n        \"\"\"\n        Args:\n            model(nn.Module, required): The model to be quanted.\n            seq_length(int, optional): The length of the sequence. Default: 2048.\n            hidden_size(int, optional): The size of the hidden layer. Default: 2560.\n            model_arch_type(str, optional): model arch type.Default: None.\n            low_memory(boll, optional): using low memory .Default: None.\n        \"\"\"\n        super(INT8, self).__init__()\n        self.model = model\n        self.modal_type = self.model.modal_type\n        self.layers = self.model.model.model.layers\n        self.quant_bits = self.model.quant_config.quant_bit\n        self.seq_length = seq_length\n        self.hidden_size = hidden_size\n        self.model_arch_type = model_arch_type\n        self.low_memory = low_memory\n        self.dtype = torch.bfloat16\n        self.scales_dict = {}\n        self.inps = None\n\n    def move_embed(self, model, device: str):\n        print_info(model)\n        model.model.model.embed_tokens = model.model.model.embed_tokens.to(device)\n        model.model.model.rotary_emb = model.model.model.rotary_emb.to(device)\n\n    @torch.no_grad()\n    def run(self, dataloader):\n        if self.low_memory:\n            print_info(\"Use INT8 low memory run\")\n            assert (\n                str(next(self.model.model.parameters()).device) == \"cpu\"\n            ), \"[AngelSlim Error] INT8 low memory mode need model in cpu\"\n            self.low_memory_run(dataloader)\n        else:\n            print_info(\"[AngelSlim] Use INT8 fast forward\")\n            self.model.model_forward(dataloader)\n\n    def low_memory_run(self, dataloader):\n        for model_module in self.layers:\n            model_module.eval()\n        layers = self.layers\n        dev = \"cpu\"\n        nsamples = len(dataloader)\n        print_info(f\"nsamples:{nsamples}\")\n        self.inps = torch.zeros(\n            (int(nsamples), self.seq_length, self.hidden_size),\n            device=dev,\n            dtype=self.dtype,\n        )\n        cache = {\"i\": 0}\n        layers[0] = layers[0].to(dev)\n        self.model.model.model.embed_tokens = self.model.model.model.embed_tokens.to(\n            dev\n        )\n        layers[0] = Catcher(layers[0], self.inps, cache)\n        self.model.model_forward(dataloader)\n        layer_kwargs = layers[0].layer_kwargs\n        dev = get_best_device()\n        for k, v in layer_kwargs.items():\n            # position embeddings\n            if isinstance(v, tuple):\n                layer_kwargs[k] = tuple(\n                    (\n                        item.to(dev)\n                        if isinstance(item, (torch.Tensor, nn.Module))\n                        else item\n                    )\n                    for item in v\n                )\n\n        print_info(\"cache['i']:{}\".format(cache[\"i\"]))\n        print_info(len(layers))\n        layers[0] = layers[0].module\n        print_info(self.inps.shape)\n        outs = torch.zeros_like(self.inps)\n        # begin the INT8 process\n        print_info(\"Ready.\")\n        layers = layers.cpu()\n        torch.cuda.empty_cache()\n\n        outs = outs.to(\"cpu\")\n        self.inps = self.inps.to(\"cpu\")\n        for i in range(len(layers)):\n            if torch.cuda.is_available():\n                print_info(\n                    f\"GPU Memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\"\n                )\n\n            layer = layers[i].to(dev)\n            outs = outs.to(dev)\n            self.inps = self.inps.to(dev)\n            # being hook\n            for j in range(min(self.inps.shape[0], nsamples)):\n                with torch.no_grad():\n                    outs[j, :, :] = layer(\n                        hidden_states=self.inps[j, :, :].unsqueeze(0), **layer_kwargs\n                    )[0].squeeze(1)\n\n            print_info(\"HOOK Step{}\".format(j))\n\n            # Clear GPU memory\n            torch.cuda.empty_cache()\n\n            layers[i] = layers[i].cpu()\n            layer = layer.cpu()\n            torch.cuda.empty_cache()\n            self.inps, outs = outs, self.inps\n            print_info(\"INT8 end layer {}\\n\".format(i))\n",
        "angelslim/compressor/quant/modules/smooth/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/quant/modules/smooth/smooth.py": "#! /usr/bin/env python\n# -*- encoding: utf-8 -*-\n#\n# Copyright 2024 the tencent authors.\n#\n\nfrom dataclasses import dataclass\n\nimport torch\n\n__all__ = [\"SmoothQuant\"]\n\n\n@dataclass\nclass SmoothConfig:\n    alpha: float = 0.5\n    smooth_first_linears: bool = True\n    smooth_second_linears: bool = False\n\n\nclass SmoothQuant:\n    def __init__(\n        self,\n        quant_model,\n        ptq_hook,\n        alpha=0.5,\n        smooth_first_linears=True,\n        smooth_second_linears=False,\n    ):\n        \"\"\"\n        SmoothConfig:\n            alpha: smoothing parameter. Default: 0.5\n            smooth_first_linears: whether to smooth qkv and ffn1 linears.\n            smooth_second_linears: whether to smooth out and ffn2 linears.\n        \"\"\"\n        self.quant_model = quant_model\n        self.ptq_hook = ptq_hook\n        self.smooth_config = SmoothConfig(\n            alpha, smooth_first_linears, smooth_second_linears\n        )\n        self.apply_hook()\n\n    def apply_hook(self):\n        # smooth_mapping_layers: smooth_name: (smooth_layer, balance_layers)\n        self.smooth_mapping_layers = self.quant_model.get_smooth_mapping_layers(\n            self.smooth_config\n        )\n        smooth_observer = self.quant_model.quant_algo_dict[\"smooth_observer\"]\n        self.ptq_hook.apply_smooth_hook(self.smooth_mapping_layers, smooth_observer)\n\n    @torch.no_grad()\n    def convert(self):\n        for smooth_name, (\n            smooth_layer,\n            balance_layers,\n        ) in self.smooth_mapping_layers.items():\n            assert hasattr(\n                self.ptq_hook.observer_dict[smooth_layer], \"smooth_act_observer\"\n            )\n            smooth_scale = self.ptq_hook.observer_dict[\n                smooth_layer\n            ].smooth_act_observer.scales()\n\n            balance_scales = []\n            for _, balance_layer in balance_layers:\n                balance_scale = balance_layer.weight.abs().max(dim=0, keepdim=True)[0]\n                balance_scales.append(balance_scale)\n            balance_scales = torch.cat(balance_scales, dim=0).max(dim=0)[0]\n\n            # s_j = max(|X_j|)^alpha / max(|W_j|)^(1-alpha)\n            smooth_scale = smooth_scale.to(balance_scales.device)\n            scales = smooth_scale.pow(self.smooth_config.alpha) / balance_scales.pow(\n                1 - self.smooth_config.alpha\n            )\n\n            for _, balance_layer in balance_layers:\n                balance_layer.weight.mul_(scales.view(1, -1))\n\n            if smooth_layer.weight.ndim == 1:\n                smooth_layer.weight.div_(scales)\n            elif smooth_layer.weight.ndim == 2:\n                smooth_layer.weight.div_(scales.view(-1, 1))\n            else:\n                raise ValueError(\n                    f\"{smooth_name} {smooth_layer.weight.shape} not supported\"\n                )\n            if hasattr(smooth_layer, \"bias\") and smooth_layer.bias is not None:\n                smooth_layer.bias.div_(scales)\n",
        "angelslim/compressor/quant/observers/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .abs_max_activation import AbsmaxPerchannelObserver  # noqa: F401\nfrom .abs_max_activation import AbsmaxPertensorObserver  # noqa: F401\nfrom .abs_max_activation import AbsMaxTokenWiseActObserver  # noqa: F401; noqa: F401\nfrom .abs_max_weight import AbsMaxChannelWiseWeightObserver  # noqa: F401\nfrom .base_observer import BaseObserver, ParentObserver  # noqa: F401\nfrom .ema_activation import EMAObserver  # noqa: F401\nfrom .groupwise_weight import AbsMaxGroupWiseWeightObserver  # noqa: F401\nfrom .hist_activation import HistObserver  # noqa: F401\nfrom .observer import PTQObserver  # noqa: F401\n",
        "angelslim/compressor/quant/observers/abs_max_activation.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .base_observer import BaseObserver\n\n__all__ = [\n    \"AbsmaxPertensorObserver\",\n    \"AbsMaxTokenWiseActObserver\",\n    \"AbsmaxPerchannelObserver\",\n]\n\n\nclass AbsmaxPertensorObserver(BaseObserver):\n    def __init__(self, layer=None, quant_bits=8, **kwargs):\n        super(AbsmaxPertensorObserver, self).__init__(quant_bits=quant_bits)\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = torch.tensor(1e-7, dtype=torch.float32)\n        self.step = 0\n        self.dtype = None\n        self.parent_observer = (\n            kwargs[\"parent_observer\"]\n            if kwargs and \"parent_observer\" in kwargs\n            else None\n        )\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n\n        self.step += 1\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        if inputs.numel() > 0:\n            self._min, self._max = self._cal_min_max(inputs)\n            if self.parent_observer is not None:\n                self.parent_observer.update(self._min, self._max, self.step)\n        else:\n            assert self.parent_observer is not None\n            self._update_min_max(self.parent_observer.min, self.parent_observer.max)\n        return inputs\n\n    def _cal_min_max(self, inputs):\n        abs_max_val = torch.max(torch.abs(inputs))\n        # abs_max_val = torch.maximum(abs_max_val, self._max)\n        if abs_max_val.data < self._max.data:\n            abs_max_val = self._max\n        # torch.cuda.empty_cache()\n        return 0, abs_max_val.to(inputs.device)\n\n    def _update_min_max(self, min, max):\n        if min is not None and max is not None:\n            if self._min is None or min < self._min:\n                self._min = min\n            if self._max is None or max > self._max:\n                self._max = max\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        if self._scale is None:\n            self._scale = self._max\n        self._zero_point = torch.zeros_like(self._scale)\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self.step == 0 and self.parent_observer is not None:\n            self._update_min_max(self.parent_observer.min, self.parent_observer.max)\n            self.step = self.parent_observer.step\n        if self.step == 0:\n            raise ValueError(\n                \"AbsmaxPertensorObserver scales must calibrate data first!\"\n            )\n        if self._scale is None:\n            self.cal_thresholds()\n        if self.dtype:\n            self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n\n\nclass AbsMaxTokenWiseActObserver(BaseObserver):\n    def __init__(\n        self,\n        layer=None,\n        quant_bits=8,\n    ):\n        super(AbsMaxTokenWiseActObserver, self).__init__(quant_bits=quant_bits)\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = torch.tensor(1e-7, dtype=torch.float32)\n        self.step = 0\n        self.dtype = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        self._min, self._max = self._cal_min_max(inputs)\n        return inputs\n\n    def _cal_min_max(self, inputs):\n        new_inp = inputs\n        if len(new_inp.shape) == 3:\n            n_token = new_inp.shape[2]\n            new_inp = new_inp.permute([2, 0, 1]).reshape([n_token, -1])\n            abs_max_values, _ = new_inp.abs().max(dim=self.quant_axis())\n        else:\n            abs_max_val = torch.max(torch.abs(new_inp))\n            abs_max_values = torch.maximum(abs_max_val, self._max)\n        return 0, abs_max_values\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        if self._scale is None:\n            self._scale = self._max\n        self._zero_point = torch.zeros_like(self._scale)\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self._scale is None:\n            self.cal_thresholds()\n        self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n\n\nclass AbsmaxPerchannelObserver(BaseObserver):\n    def __init__(self, layer, quant_bits=8, **kwargs):\n        super(AbsmaxPerchannelObserver, self).__init__(quant_bits=quant_bits)\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = (\n            torch.zeros((self._layer.weight.shape[0])) - torch.inf\n        )  # per-outchannel\n        self.step = 0\n        self.dtype = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        x = inputs.clone()\n        self._min, self._max = self._cal_min_max(x)\n        del x\n        self.step += 1\n        return inputs\n\n    def _cal_min_max(self, tensor, dim=0):\n        hidden_dim = tensor.shape[-1]\n        tensor_x = tensor.view(-1, hidden_dim).abs().detach()\n        comming_max_x = torch.max(tensor_x, dim=dim)[0].float().cpu()\n        return 0, torch.max(self._max, comming_max_x)\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        if self._scale is None:\n            self._scale = self._max\n        self._zero_point = torch.zeros_like(self._scale)\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self.step == 0:\n            raise ValueError(\n                \"AbsmaxPerchannelObserver scales must calibrate data first!\"\n            )\n        if self._scale is None:\n            self.cal_thresholds()\n        if self.dtype:\n            self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n",
        "angelslim/compressor/quant/observers/abs_max_weight.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .base_observer import BaseObserver\n\n\nclass AbsMaxChannelWiseWeightObserver(BaseObserver):\n    def __init__(self, layer=None, quant_bits=8, group_size=-1):\n        super(AbsMaxChannelWiseWeightObserver, self).__init__(quant_bits=quant_bits)\n        assert group_size < 0, \"ChannelWise quantization, group_size cannot > 0.\"\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = torch.tensor(1e-7, dtype=torch.float32)\n        self.step = 0\n        self.dtype = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        self._min, self._max = self._cal_min_max(inputs)\n        self.step += 1\n        return inputs\n\n    def _cal_min_max(self, inputs):\n        new_inp = inputs\n        # abs_max_values, _ = new_inp.abs().max(dim=self.quant_axis(), keepdim=True)\n        # abs_max_values, _ = abs_max_values.max(dim=0)\n        # abs_max_values = abs_max_values.squeeze()\n        if len(new_inp.shape) > 2:\n            new_inp = new_inp.flatten(1)\n        abs_max_values, _ = new_inp.abs().max(dim=self.quant_axis())\n        return 0, abs_max_values\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        if self._scale is None:\n            self._scale = self._max\n        self._zero_point = torch.zeros_like(self._scale)\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self.step == 0:\n            raise ValueError(\n                \"AbsMaxChannelWiseWeightObserver scales must calibrate data first!\"\n            )\n        if self._scale is None:\n            self.cal_thresholds()\n        if self.dtype:\n            self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n",
        "angelslim/compressor/quant/observers/base_observer.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom ....utils import print_info\n\n\nclass BaseObserver(nn.Module, metaclass=abc.ABCMeta):\n    \"\"\"\n    Customized observers should extend this base observer\n    and implement abstract methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        quant_bits=8,\n        sign=True,\n        symmetric=True,\n    ):\n        super(BaseObserver, self).__init__()\n        self._quant_bits = quant_bits\n        self._sign = sign\n        self._symmetric = symmetric\n\n        self._min = None\n        self._max = None\n        self._qmin = None\n        self._qmax = None\n\n        self._scale = None\n        self._zero_point = None\n\n    @abc.abstractmethod\n    def cal_thresholds(self):\n        pass\n\n    @abc.abstractmethod\n    def scales(self):\n        pass\n\n    def min_value(self):\n        return self._min\n\n    def max_value(self):\n        return self._max\n\n    def bit_length(self):\n        \"\"\"Return the bit length of quantized data.\"\"\"\n        return self._quant_bits\n\n    @property\n    def qmin_qmax(self):\n        \"\"\"Calculate the range of the quantized integer based on the specified\n        quant_bits, sign, and symmetric properties.\"\"\"\n        if self._sign:\n            self._qmin = -(2 ** (self.bit_length() - 1))\n            self._qmax = 2 ** (self.bit_length() - 1) - 1\n        else:\n            self._qmin = 0\n            self._qmax = 2 ** self.bit_length()\n        return self._qmin, self._qmax\n\n    def cal_scales_zero_points(self):\n        \"\"\"Calculate the scales and zero points based on the min_value and max_value.\"\"\"\n        # assert self.min_value() is not None and self.max_value() is not None\n        if self.min_value() is None or self.max_value() is None:\n            print_info(\"[Warning] scales is None, Please check calibration and model.\")\n            return None, None\n        _qmin, _qmax = self.qmin_qmax\n        if self._min == 0:\n            self._min = torch.tensor(0.0)\n        _min = torch.minimum(self.min_value(), torch.tensor(0.0))\n        _max = torch.maximum(self.max_value(), torch.tensor(0.0))\n\n        if self._symmetric:\n            self._scale = torch.maximum(-_min, _max)\n            if self._sign:\n                self._zero_point = 0\n            else:\n                self._zero_point = (_qmax + _qmin) / 2\n        else:\n            self._scale = (_max - _min) / float(_qmax - _qmin)\n            self._zero_point = _qmin - round(_min / self._scale)\n            self._zero_point = np.clip(self._zero_point, _qmin, _qmax)\n        return self._scale, self._zero_point\n\n\n@dataclass\nclass ParentObserver:\n    min = None\n    max = None\n    step = 0\n\n    def update(self, min, max, step=None):\n        if min is not None and max is not None:\n            if self.min is None or min < self.min:\n                self.min = min\n            if self.max is None or max > self.max:\n                self.max = max\n        if step is not None:\n            self.step = step\n",
        "angelslim/compressor/quant/observers/ema_activation.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\n\nfrom .abs_max_activation import AbsmaxPertensorObserver\n\n\nclass EMAObserver(AbsmaxPertensorObserver):\n    def __init__(\n        self,\n        layer=None,\n        quant_bits=8,\n    ):\n        super(EMAObserver, self).__init__(quant_bits=quant_bits)\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = torch.tensor(1e-7, dtype=torch.float32)\n        self.step = 0\n        self.dtype = None\n        self.ema_beta = 0.98\n        self.ema_step = 0\n        self.sampled = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        x = inputs.clone()\n        self._min, self._max = self._cal_min_max(x)\n        del x\n        self.step += 1\n        torch.cuda.empty_cache()\n        return inputs\n\n    def _cal_min_max(self, inputs):\n        abs_max_val = torch.max(torch.abs(inputs)).item()\n        if not self.ema_step:\n            self.sampled = (1 - self.ema_beta) * abs_max_val\n            v_ema_corr = self.sampled\n        else:\n            v_ema = self.ema_beta * self.sampled + (1.0 - self.ema_beta) * abs_max_val\n            self.sampled = v_ema\n            v_ema_corr = v_ema / float(\n                (1.0 - np.power(self.ema_beta, self.ema_step + 1.0))\n            )\n        self.ema_step += 1\n        return 0, torch.tensor(v_ema_corr, dtype=self.dtype).to(inputs.device)\n",
        "angelslim/compressor/quant/observers/groupwise_weight.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nfrom .base_observer import BaseObserver\n\n\nclass AbsMaxGroupWiseWeightObserver(BaseObserver):\n    def __init__(self, layer=None, quant_bits=8, group_size=128):\n        super(AbsMaxGroupWiseWeightObserver, self).__init__(quant_bits=quant_bits)\n        self._layer = layer\n        self._quant_bits = quant_bits\n        self._group_size = group_size\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = torch.tensor(1e-7, dtype=torch.float32)\n        self.step = 0\n        self.dtype = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n        self._min, self._max = self._cal_abs_max(inputs)\n        self.step += 1\n        return inputs\n\n    def _cal_abs_max(self, inputs):\n        assert (\n            self._group_size == 64 or self._group_size == 128\n        ), \"group_size only support 64 or 128\"\n        assert (\n            inputs.shape[1] % self._group_size == 0\n        ), \"group_size must be a factor of input channels\"\n        assert len(inputs.shape) == 2, \"Currently only support 2D tensor\"\n        new_inp = inputs\n        input_processed = new_inp.view(\n            [new_inp.shape[0], new_inp.shape[1] // self._group_size, self._group_size]\n        )\n        abs_max_values, _ = input_processed.abs().max(\n            dim=self.quant_axis(), keepdim=True\n        )\n        abs_max_values = abs_max_values.squeeze(-1)\n        return 0, abs_max_values\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        if self._scale is None:\n            self._scale = self._max\n        self._zero_point = torch.zeros_like(self._scale)\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self.step == 0:\n            raise ValueError(\n                \"AbsMaxGroupWiseWeightObserver scales must calibrate data first!\"\n            )\n        if self._scale is None:\n            self.cal_thresholds()\n        if self.dtype:\n            self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n",
        "angelslim/compressor/quant/observers/hist_activation.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\n\nfrom .base_observer import BaseObserver\n\n\nclass HistObserver(BaseObserver):\n    def __init__(\n        self,\n        layer=None,\n        quant_bits=8,\n        bins_count=2048,\n        percent=0.9999,\n        sign=True,\n        symmetric=True,\n    ):\n        super(HistObserver, self).__init__(\n            quant_bits=quant_bits, sign=sign, symmetric=symmetric\n        )\n        self._layer = layer\n        self._bins_count = bins_count\n        self._percent = percent\n        self._upsample_bin_count = 64\n\n        self._hist_min = None\n        self._hist_max = None\n        self._hist = None\n\n        self.step = 0\n        self.dev = None\n        self.dtype = None\n\n    def forward(self, inputs):\n        \"\"\"Calculate forward pass.\"\"\"\n        if not self.dtype:\n            self.dtype = inputs.dtype\n            self.dev = inputs.device\n\n        self._scale = None\n        self._zero_point = None\n        self._min = None\n        self._max = None\n\n        inputs_np = inputs.to(torch.float32).cpu()\n        if self._hist_min is None or self._hist_max is None:\n            self._hist_min, self._hist_max = self._min_max(inputs_np)\n            self._hist = self._init_hists(inputs_np)\n        else:\n            new_min, new_max, new_hist = self._update_min_max_and_hist(\n                inputs_np,\n                self._hist_min,\n                self._hist_max,\n                self._hist,\n                self._bins_count,\n                self._upsample_bin_count,\n            )\n            self._hist_min, self._hist_max = new_min, new_max\n            self._hist = new_hist\n\n        return inputs\n\n    def cal_min_max(self):\n        return self._cal_min_max_by_percent()\n\n    def _cal_min_max_by_percent(self):\n        \"\"\"Calculate the min and max value of the tensor based on the histogram.\"\"\"\n        hist = self._hist / np.sum(self._hist, dtype=np.float64)\n        cumsumed_hist = np.cumsum(hist)\n        max_idx = np.argwhere(cumsumed_hist >= min(self._percent, cumsumed_hist[-1]))[0]\n        min_idx = np.argwhere(\n            cumsumed_hist >= max(1 - self._percent, cumsumed_hist[0])\n        )[0]\n        bin_width = (self._hist_max - self._hist_min) / hist.shape[0]\n        _max = self._hist_min + float((max_idx - 0.5) * bin_width)\n        _min = self._hist_min + float((min_idx - 0.5) * bin_width)\n\n        _max = torch.tensor(_max, dtype=self.dtype, device=self.dev)\n        _min = torch.tensor(_min, dtype=self.dtype, device=self.dev)\n\n        return _min, _max\n\n    def _init_hists(self, inputs):\n        \"\"\"Initialize the histogram instance based on a tensor.\"\"\"\n        _min, _max = self._min_max(inputs)\n        hist = None\n        if _max > _min:\n            hist, _ = np.histogram(\n                inputs.numpy(), bins=self._bins_count, range=(_min, _max)\n            )\n            hist = hist.astype(np.float32)\n\n        return hist\n\n    def _min_max(self, inputs):\n        \"\"\"Get the min and max value of a tensor.\"\"\"\n        return float(torch.min(inputs).numpy()), float(torch.max(inputs).numpy())\n\n    def _update_min_max_and_hist(\n        self,\n        tensor,\n        origin_min,\n        origin_max,\n        origin_hist,\n        bins_count,\n        upsample_bins_count,\n    ):\n        \"\"\"Update the histogram and its range based on\n            the values of the target tensor.\n        Args:\n            tensor: The tensor used to update the histogram.\n            origin_min(float): The minimum of the original histogram's range.\n            origin_max(float): The max of the original histogram's range.\n            origin_hist: The original histogram.\n            bins_count(int): The number of histogram bins.\n            upsample_bins_count(int):\n                The number of upsampled bins used to extend the histogram.\n        \"\"\"\n        _origin_min, _origin_max = origin_min, origin_max\n        _new_min, _new_max = self._min_max(tensor)\n\n        if (_new_max - _new_min) == 0.0:\n            return _origin_min, _origin_max, origin_hist\n        elif _origin_max - _origin_min == 0.0:\n            new_hist, _ = np.histogram(\n                tensor.numpy(), range=(_new_min, _new_max), bins=bins_count\n            )\n            new_hist = new_hist.astype(np.float32)\n            return _new_min, _new_max, new_hist\n        elif _new_max <= _origin_max and _new_min >= _origin_min:\n            new_hist, _ = np.histogram(\n                tensor.numpy(), range=(_origin_min, _origin_max), bins=bins_count\n            )\n            new_hist = new_hist.astype(np.float32)\n            new_hist += origin_hist\n            return _origin_min, _origin_max, new_hist\n        else:\n            _new_min = min(_new_min, _origin_min)\n            _new_max = max(_new_max, _origin_max)\n            (\n                _new_min,\n                _new_max,\n                downsample_bins_count,\n                start_bin_idx,\n            ) = self._relax_min_max(\n                _new_min,\n                _new_max,\n                _origin_min,\n                _origin_max,\n                bins_count,\n                upsample_bins_count,\n            )\n\n            new_hist, _ = np.histogram(\n                tensor.numpy(), range=(_new_min, _new_max), bins=bins_count\n            )\n\n            merged_histogram = self._merge_histograms(\n                new_hist,\n                origin_hist,\n                upsample_bins_count,\n                downsample_bins_count,\n                start_bin_idx,\n                bins_count,\n            )\n            return _new_min, _new_max, merged_histogram\n\n    def _merge_histograms(\n        self,\n        new_hist: np.ndarray,\n        origin_hist: np.ndarray,\n        upsample_bins_count: int,\n        downsample_bins_count: int,\n        start_bin_idx: int,\n        bins_count: int,\n    ):\n        \"\"\"Merge two histograms.\"\"\"\n        upsampled_histogram = np.repeat(origin_hist, upsample_bins_count)\n        expanded_hist = np.zeros((bins_count * downsample_bins_count), dtype=np.float32)\n        expanded_hist[\n            start_bin_idx : bins_count * upsample_bins_count + start_bin_idx\n        ] = upsampled_histogram\n\n        cumsumed_hist = np.cumsum(expanded_hist, dtype=np.float64)[\n            downsample_bins_count - 1 :: downsample_bins_count\n        ]\n        shift_cumsumed_hist = np.zeros((bins_count), dtype=np.float64)\n        shift_cumsumed_hist[1:] = cumsumed_hist[0:-1]\n        sampled_hist = (cumsumed_hist - shift_cumsumed_hist) / upsample_bins_count\n        new_hist = new_hist.astype(np.float32)\n        new_hist += sampled_hist.astype(np.float32)\n        return new_hist\n\n    def _relax_min_max(\n        self, new_min, new_max, origin_min, origin_max, bins_count, upsample_bins_count\n    ) -> Tuple[float, float, int, int]:\n        \"\"\"Relax the min and max values of the histogram.\"\"\"\n        _bin_width = (origin_max - origin_min) / (bins_count * upsample_bins_count)\n        downsample_bins_count = int(\n            np.ceil((new_max - new_min) / (bins_count * _bin_width))\n        )\n        error = downsample_bins_count * bins_count * _bin_width - (new_max - new_min)\n        new_max += error\n        start_bin_idx = round((origin_min - new_min) / _bin_width)\n        return new_min, new_max, downsample_bins_count, start_bin_idx\n\n    def cal_thresholds(self):\n        \"\"\"Compute thresholds for MAX function.\"\"\"\n        assert self._hist is not None\n        self._min, self._max = self.cal_min_max()\n        self._scale, self._zero_point = self.cal_scales_zero_points()\n\n    def quant_axis(self):\n        \"\"\"Return quantization axis.\"\"\"\n        return -1\n\n    def scales(self):\n        \"\"\"Return output scales.\"\"\"\n        if self._scale is None:\n            self.cal_thresholds()\n        if self.dtype:\n            self._scale = self._scale.type(self.dtype)\n        return self._scale\n\n    def zero_points(self):\n        \"\"\"Return output zero points.\"\"\"\n        if self._zero_point is None:\n            self.cal_thresholds()\n        return self._zero_point\n",
        "angelslim/compressor/quant/observers/observer.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\n\nimport torch.nn as nn\n\n\nclass PTQObserver(nn.Module, metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        layer,\n        act_observer,\n        weight_observer,\n        kv_cache_observer,\n        quant_algo_dict,\n        smooth_act_observer=None,\n        parent_observer=None,\n        **kwargs,\n    ):\n        super(PTQObserver, self).__init__()\n        self.layer = layer\n        self.quant_algo_dict = quant_algo_dict\n        self.act_observer = None\n        self.smooth_act_observer = None\n        self.weight_observer = None\n        self.kv_cache_observer = None\n        if act_observer:\n            if parent_observer is not None:\n                kwargs[\"parent_observer\"] = parent_observer\n            self.act_observer = act_observer(\n                self.layer, quant_bits=quant_algo_dict[\"a_quant_bits\"], **kwargs\n            )\n        if smooth_act_observer:\n            if parent_observer is not None:\n                kwargs[\"parent_observer\"] = parent_observer\n            self.smooth_act_observer = smooth_act_observer(\n                self.layer, quant_bits=quant_algo_dict[\"a_quant_bits\"], **kwargs\n            )\n        if kv_cache_observer:\n            self.kv_cache_observer = kv_cache_observer(\n                self.layer,\n                quant_bits=quant_algo_dict[\"c_quant_bits\"],\n                quantization_wise=quant_algo_dict[\"c_quant_algo\"],\n            )\n        if weight_observer:\n            self.weight_observer = weight_observer(\n                self.layer,\n                quant_bits=quant_algo_dict[\"w_quant_bits\"],\n                group_size=quant_algo_dict.get(\"w_group_size\", -1),\n            )\n\n    def forward(self, input, output):\n        if self.act_observer is not None:\n            self.act_observer(input)\n        if self.kv_cache_observer is not None:\n            self.kv_cache_observer(output)\n        if self.smooth_act_observer is not None:\n            self.smooth_act_observer(output)\n",
        "angelslim/compressor/quant/ptq.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\n\nfrom ...utils import find_parent_layer_and_sub_name, print_info\nfrom ..compressor_factory import CompressorFactory\nfrom .core import PTQHook\nfrom .modules import AWQ, FP8, GPTQ, INT8, SmoothQuant\n\n__all__ = [\"PTQ\"]\n\n\n@CompressorFactory.register\nclass PTQ:\n    def __init__(self, model, slim_config=None):\n        \"\"\"\n        Args:\n            model(nn.Moudle, required): the model to be quant.\n            slim_config(dict, required): the configuration for quantization.\n                - compress_config: the configuration for compression.\n                - global_config: the global configuration for the model.\n        \"\"\"\n        self.quant_model = model\n        # init ptq config of model\n        self.quant_model.init_ptq(slim_config)\n        self.modal_type = self.quant_model.modal_type\n        self.layers = self.quant_model.get_model()\n        self.quant_algo = self.quant_model.quant_config.quant_algo\n        self.quant_helpers = self.quant_model.quant_config.quant_helpers\n        if self.modal_type in [\"LLM\", \"VLM\"]:\n            # Add ptq observer hook\n            self.ptq_hook = PTQHook(self.quant_model)\n            self.ptq_hook.apply_hook()\n\n        if \"gptq\" in self.quant_algo:\n            max_seq_length = self.quant_model.quant_config.max_seq_length\n            hidden_size = self.quant_model.quant_config.hidden_size\n            self.gptq = GPTQ(\n                self.quant_model, seq_length=max_seq_length, hidden_size=hidden_size\n            )\n\n        if \"awq\" in self.quant_algo:\n            max_seq_length = self.quant_model.quant_config.max_seq_length\n            hidden_size = self.quant_model.quant_config.hidden_size\n            model_arch_type = self.quant_model.quant_config.model_arch_type\n            self.awq = AWQ(\n                self.quant_model,\n                seq_length=max_seq_length,\n                hidden_size=hidden_size,\n                model_arch_type=model_arch_type,\n                mse_range=self.quant_model.quant_config.quant_algo_info[\"mse_range\"],\n                observer_layer_classes=[nn.Linear],\n                low_memory=self.quant_model.quant_config.low_memory,\n            )\n        if \"fp8\" in self.quant_algo:\n            max_seq_length = self.quant_model.quant_config.max_seq_length\n            hidden_size = self.quant_model.quant_config.hidden_size\n            model_arch_type = self.quant_model.quant_config.model_arch_type\n            self.fp8 = FP8(\n                self.quant_model,\n                seq_length=max_seq_length,\n                hidden_size=hidden_size,\n                model_arch_type=model_arch_type,\n                low_memory=self.quant_model.quant_config.low_memory,\n            )\n        if \"int8\" in self.quant_algo:\n            max_seq_length = self.quant_model.quant_config.max_seq_length\n            hidden_size = self.quant_model.quant_config.hidden_size\n            model_arch_type = self.quant_model.quant_config.model_arch_type\n            self.int8 = INT8(\n                self.quant_model,\n                seq_length=max_seq_length,\n                hidden_size=hidden_size,\n                model_arch_type=model_arch_type,\n                low_memory=self.quant_model.quant_config.low_memory,\n            )\n        if \"smooth\" in self.quant_helpers:\n            self.smooth = SmoothQuant(\n                self.quant_model,\n                self.ptq_hook,\n                alpha=self.quant_model.quant_config.smooth_alpha,\n            )\n\n    def calibrate(self, dataloader):\n        if \"gptq\" in self.quant_algo:\n            self.gptq.run(dataloader)\n        elif \"awq\" in self.quant_algo:\n            self.awq.run(dataloader)\n        elif \"fp8\" in self.quant_algo:\n            self.fp8.run(dataloader)\n        elif \"int8\" in self.quant_algo:\n            self.int8.run(dataloader)\n        else:\n            raise AssertionError(\n                f\"[AngelSlim Error] algo {self.quant_algo} is not support calibrate\"\n            )\n\n    def convert(self):\n        \"\"\"\n        Saves scales and inserts QDQ modules.\n        \"\"\"\n        print_info(\"Start convert model...\")\n        if \"gptq\" in self.quant_algo:\n            self.gptq.convert()\n        elif \"awq\" in self.quant_algo:\n            self.awq.convert()\n        else:\n            if self.modal_type in [\"LLM\", \"VLM\"]:\n                if \"smooth\" in self.quant_helpers:\n                    self.smooth.convert()\n                self._convert_llm()\n            else:\n                print_info(\"current {} modal type not support\".format(self.modal_type))\n                raise NotImplementedError\n        print_info(\"convert model done.\")\n\n    def save(self, save_path: str):\n        \"\"\"\n        Save PTQ scales or ckpt.\n        \"\"\"\n        if (\n            hasattr(self.quant_model.quant_config, \"quant_analyse\")\n            and self.quant_model.quant_config.quant_analyse\n        ):\n            # scale analyse\n            for k in self.quant_model.act_scales_dict.keys():\n                act_scales_data = self.quant_model.act_scales_dict[k].data\n                if act_scales_data > 1.5:\n                    print(\n                        f\"[AngelSlim Warning] Act_scales {k}: \"\n                        f\"The weight is too high:{act_scales_data}. \"\n                        f\"It is recommended to clip it to 1.5 \"\n                    )\n            for k in self.quant_model.weight_scales_dict.keys():\n                weight_scales_data = self.quant_model.weight_scales_dict[k].data\n                if weight_scales_data > 1.5:\n                    print(\n                        f\"[AngelSlim Warning] Weight_scales {k}: \"\n                        f\"The weight is too high:{weight_scales_data}. \"\n                        f\"It is recommended to clip it to 1.5 \"\n                    )\n\n        print_info(\"Start save PTQ ckpt to: {}\".format(save_path))\n        if \"gptq\" in self.quant_algo:\n            self.gptq.save(save_path)\n        elif \"awq\" in self.quant_algo:\n            self.awq.save(save_path)\n        else:\n            save_func = self.quant_model.get_save_func()(self.quant_model)\n            save_func.save(save_path)\n\n    def _convert_llm(self):\n        # 1. get act, weight and kv-cache scale\n        for name, sub_layer in self.ptq_hook.quant_layers_dict.items():\n            if (\n                getattr(  # noqa: B009\n                    self.ptq_hook.observer_dict[sub_layer], \"act_observer\"\n                )\n                is not None\n            ):\n                self.quant_model.act_scales_dict[name] = self.ptq_hook.observer_dict[\n                    sub_layer\n                ].act_observer.scales()\n            if (\n                getattr(  # noqa: B009\n                    self.ptq_hook.observer_dict[sub_layer], \"kv_cache_observer\"\n                )\n                is not None\n            ):\n                self.quant_model.kv_cache_scales_dict[name] = (\n                    self.ptq_hook.observer_dict[sub_layer].kv_cache_observer.scales()\n                )\n            if (\n                getattr(  # noqa: B009\n                    self.ptq_hook.observer_dict[sub_layer], \"weight_observer\"\n                )\n                is not None\n            ):\n                weight_scales = self.quant_model.get_weight_scales(\n                    sub_layer, self.ptq_hook.observer_dict[sub_layer].weight_observer\n                )\n                self.quant_model.weight_scales_dict[name] = weight_scales\n\n        self.ptq_hook.remove_hook()\n        torch.cuda.empty_cache()\n\n        self.ptq_hook.post_process()\n\n        # 2. insert qdq module\n        for name, sub_layer in self.ptq_hook.quant_layers_dict.items():\n            parent_layer, sub_name = find_parent_layer_and_sub_name(self.layers, name)\n\n            qdq_module = self.quant_model.get_qdq_module(sub_layer, name)\n            setattr(parent_layer, sub_name, qdq_module)\n        self.quant_model.quantized = True\n\n    def __getattr__(self, item):\n        return super().__getattr__(item)\n",
        "angelslim/compressor/sparsity/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/compressor/speculative_decoding/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n",
        "angelslim/data/__init__.py": "#! /usr/bin/env python\n# -*- encoding: utf-8 -*-\n#\n# Copyright 2024 the tencent authors.\n#\n\nfrom .dataloader import DataLoaderFactory  # noqa: F401\nfrom .multimodal_dataset import MultiModalDataset  # noqa: F401\nfrom .text_dataset import TextDataset  # noqa: F401\n",
        "angelslim/data/base_dataset.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Dict, List\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import ProcessorMixin\n\n\nclass BaseDataset(Dataset):\n    \"\"\"Base class for all datasets\"\"\"\n\n    def __init__(\n        self, processor: ProcessorMixin, device: str = \"cpu\", max_length: int = 4096\n    ):\n        self.processor = processor\n        self.device = device\n        self.max_length = max_length\n        self.data = []\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -> Dict:\n        return self.data[idx]\n\n    @staticmethod\n    def collate_fn(batch: List[Dict]) -> Dict:\n        \"\"\"Custom collate function to batch dictionary items\"\"\"\n        collated = {}\n        for key in batch[0].keys():\n            # Skip non-tensor items\n            if not isinstance(batch[0][key], torch.Tensor):\n                continue\n\n            # Stack tensors of the same type\n            tensors = [item[key] for item in batch]\n            if all(t.dim() == 0 for t in tensors):  # Handle scalar tensors\n                collated[key] = torch.stack(tensors)\n            else:\n                collated[key] = torch.cat(tensors, dim=0)\n        return collated\n",
        "angelslim/data/dataloader.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import Dict, Union\n\nfrom torch.utils.data import DataLoader\nfrom transformers import ProcessorMixin\n\nfrom .base_dataset import BaseDataset\nfrom .multimodal_dataset import MultiModalDataset\nfrom .text_dataset import TextDataset\n\n\nclass DataLoaderFactory:\n    \"\"\"Factory for creating PyTorch DataLoaders from various data sources\"\"\"\n\n    @staticmethod\n    def create_data_loader(\n        processor: ProcessorMixin,\n        device: str = \"cpu\",\n        max_length: int = 4096,\n        batch_size: int = 1,\n        shuffle: bool = True,\n        num_samples: int = -1,\n        data_source: Union[str, Dict] = None,\n        data_type: str = \"auto\",\n        num_workers: int = 0,\n    ) -> DataLoader:\n        \"\"\"\n        Create appropriate DataLoader based on data source\n\n        Args:\n            processor: Text/vision processor\n            device: Target device for tensors\n            max_length: Maximum sequence length\n            batch_size: DataLoader batch size\n            shuffle: Whether to shuffle data\n            num_samples: Limit number of samples (-1 for all)\n            data_source: File path or HF dataset dict\n            data_type: \"text\", \"multimodal\" or \"auto\"\n            num_workers: Number of workers for DataLoader\n\n        Returns:\n            PyTorch DataLoader ready for use\n        \"\"\"\n        # Auto detect data type if not specified\n        if data_type == \"auto\":\n            if isinstance(data_source, str) and (\n                \".parquet\" in data_source.lower() or \".json\" in data_source\n            ):\n                data_type = \"text\"\n            else:\n                data_type = \"multimodal\"\n\n        # Create appropriate dataset\n        if data_type == \"TextDataset\":\n            dataset = TextDataset(\n                processor=processor,\n                device=device,\n                max_length=max_length,\n                data_path=data_source,\n                num_samples=num_samples,\n            )\n        elif data_type == \"MultiModalDataset\":\n            dataset = MultiModalDataset(\n                processor=processor,\n                device=device,\n                max_length=max_length,\n                num_samples=num_samples,\n                data_source=data_source,\n                is_hf_dataset=not os.path.isfile(data_source),\n            )\n        else:\n            raise ValueError(f\"Unsupported data type: {data_type}\")\n\n        # Create DataLoader\n        return DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            collate_fn=BaseDataset.collate_fn,\n            num_workers=num_workers,\n            # pin_memory = device.type == 'cuda'\n        )\n",
        "angelslim/data/multimodal_dataset.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nfrom typing import Dict, List, Union\n\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom qwen_vl_utils import process_vision_info\nfrom tqdm import tqdm\nfrom transformers import ProcessorMixin\n\nfrom .base_dataset import BaseDataset\n\n\nclass MultiModalDataset(BaseDataset):\n    \"\"\"Dataset for multimodal (text + image) data\"\"\"\n\n    def __init__(\n        self,\n        processor: ProcessorMixin,\n        device: str = \"cpu\",\n        max_length: int = 4096,\n        num_samples: int = -1,\n        data_source: Union[str, Dict] = None,\n        is_hf_dataset: bool = False,\n    ):\n        super().__init__(processor, device, max_length)\n        self.is_hf_dataset = is_hf_dataset\n\n        if is_hf_dataset:\n            self._load_hf_dataset(data_source, num_samples)\n        else:\n            self._load_file_based_dataset(data_source, num_samples)\n\n    def _load_file_based_dataset(self, data_path: str, num_samples: int):\n        \"\"\"Load dataset from local file system\"\"\"\n        image_dir = os.path.join(os.path.dirname(data_path), \"images\")\n        line_count = 0\n\n        with open(data_path, \"r\") as f:\n            for line in f:\n                if num_samples > 0 and line_count >= num_samples:\n                    break\n\n                data = json.loads(line.strip())\n                image_path = os.path.join(image_dir, data[\"img_path\"])\n\n                # Prepare chat messages with image\n                messages = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"image\", \"image\": image_path},\n                            {\n                                \"type\": \"text\",\n                                \"text\": data[\"question\"].replace(\"<image>\", \"\"),\n                            },\n                        ],\n                    },\n                    {\"role\": \"assistant\", \"content\": data[\"answer\"]},\n                ]\n\n                self._process_and_append(messages)\n                line_count += 1\n\n    def _load_hf_dataset(self, dataset: str, num_samples: int):\n        \"\"\"Load dataset from Hugging Face format\"\"\"\n        dataset = load_dataset(dataset, split=\"test\")\n        total_samples = (\n            min(num_samples, len(dataset[\"query\"]))\n            if num_samples > 0\n            else len(dataset[\"query\"])\n        )\n\n        for i in tqdm(range(total_samples), desc=\"Processing HF Dataset\"):\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\", \"image\": dataset[\"image\"][i]},\n                        {\"type\": \"text\", \"text\": dataset[\"query\"][i]},\n                    ],\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [{\"type\": \"text\", \"text\": dataset[\"label\"][i][0]}],\n                },\n            ]\n            self._process_and_append(messages)\n\n    def _process_and_append(self, messages: List[Dict]):\n        \"\"\"Process messages and append to dataset\"\"\"\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        # Extract vision info\n        image_inputs, video_inputs = process_vision_info(messages)\n\n        # Process inputs\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n        )\n\n        inputs[\"labels\"] = inputs[\"input_ids\"]\n\n        self.data.append(inputs)\n\n    def _extract_vision_info(self, messages: List[Dict]) -> tuple:\n        \"\"\"Extract image and video paths from messages\"\"\"\n        image_paths = []\n        video_paths = []\n\n        for message in messages:\n            content = message.get(\"content\", [])\n            if not isinstance(content, list):\n                continue\n\n            for item in content:\n                if item.get(\"type\") == \"image\":\n                    # Handle both file paths and PIL images\n                    if isinstance(item[\"image\"], str):\n                        try:\n                            img = Image.open(item[\"image\"])\n                            image_paths.append(img)\n                        except ValueError as e:\n                            raise ValueError(\n                                f\"Could not open image file: {item['image']}, {e}\"\n                            )\n                    elif isinstance(item[\"image\"], Image.Image):\n                        image_paths.append(item[\"image\"])\n                elif item.get(\"type\") == \"video\":\n                    video_paths.append(item[\"video\"])\n\n        return image_paths, video_paths\n",
        "angelslim/data/text_dataset.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom typing import Dict, List\n\nimport pyarrow.parquet as pq\nimport torch\nfrom transformers import ProcessorMixin\n\nfrom .base_dataset import BaseDataset\n\n\nclass TextDataset(BaseDataset):\n    \"\"\"Dataset for text-only data in Parquet or JSONL formats\"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        processor: ProcessorMixin,\n        device: str = \"cpu\",\n        max_length: int = 4096,\n        num_samples: int = -1,\n    ):\n        super().__init__(processor, device, max_length)\n        self._load_data(data_path, num_samples)\n\n    def _load_data(self, data_path: str, num_samples: int):\n        if \".parquet\" in data_path.lower():\n            self._load_parquet_data(data_path, num_samples)\n        else:\n            self._load_jsonl_data(data_path, num_samples)\n\n    def _load_parquet_data(self, data_path: str, num_samples: int):\n        table = pq.read_table(data_path)\n        df = table.to_pandas()\n\n        # Handle sample limits\n        total_samples = min(num_samples, len(df)) if num_samples > 0 else len(df)\n\n        for i in range(total_samples):\n            text = df[\"text\"].iloc[i]\n            model_inputs = self.processor(\n                [text],\n                return_tensors=\"pt\",\n                max_length=self.max_length,\n                truncation=True,\n                padding=\"max_length\",\n            )\n\n            # Handle potential labels\n            if \"labels\" in df.columns:\n                labels = torch.tensor(df[\"labels\"].iloc[i]).unsqueeze(0)\n            else:\n                labels = model_inputs[\"input_ids\"].roll(shifts=-1, dims=-1)\n                labels[:, -1] = -100\n\n            data_item = {\n                \"input_ids\": model_inputs[\"input_ids\"].to(self.device),\n                \"attention_mask\": model_inputs[\"attention_mask\"].to(self.device),\n                \"labels\": labels.to(self.device),\n            }\n            self.data.append(data_item)\n\n    def _load_jsonl_data(self, data_path: str, num_samples: int):\n        line_count = 0\n        with open(data_path, \"r\") as f:\n            for line in f:\n                if num_samples > 0 and line_count >= num_samples:\n                    break\n\n                data = json.loads(line)\n\n                # Validate format\n                assert (\n                    \"messages\" in data or \"input\" in data or \"conversations\" in data\n                ), \"JSON format error\"\n\n                # Prepare messages\n                messages = self._prepare_messages(data)\n\n                # Apply chat template\n                text = self.processor.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True\n                )\n\n                model_inputs = self.processor(\n                    [text],\n                    return_tensors=\"pt\",\n                    max_length=self.max_length,\n                    truncation=True,\n                    padding=\"max_length\",\n                )\n\n                labels = model_inputs[\"input_ids\"].roll(shifts=-1, dims=-1)\n                labels[:, -1] = -100\n\n                self.data.append(\n                    {\n                        \"input_ids\": model_inputs[\"input_ids\"].to(self.device),\n                        \"attention_mask\": model_inputs[\"attention_mask\"].to(\n                            self.device\n                        ),\n                        \"labels\": labels.to(self.device),\n                    }\n                )\n\n                line_count += 1\n\n    def _prepare_messages(self, data: Dict) -> List[Dict]:\n        \"\"\"Prepare chat messages from data entry\"\"\"\n        if \"messages\" in data:\n            messages = data[\"messages\"]\n            # Add system prompt if available\n            if (\n                \"system_prompt\" in data\n                and data[\"system_prompt\"]\n                and messages[0][\"role\"] != \"system\"\n            ):\n                messages = [\n                    {\"role\": \"system\", \"content\": data[\"system_prompt\"]}\n                ] + messages\n        elif \"conversations\" in data:\n            share_gpt_data = data[\"conversations\"]\n            messages = [\n                {\"role\": \"user\", \"content\": share_gpt_data[0][\"value\"]},\n                {\"role\": \"assistant\", \"content\": share_gpt_data[1][\"value\"]},\n            ]\n            if \"system\" in data and data[\"system\"]:\n                messages = [\n                    {\"role\": \"system\", \"content\": data[\"system_prompt\"]}\n                ] + messages\n        else:\n            messages = [\n                {\"role\": \"user\", \"content\": data[\"input\"]},\n                {\"role\": \"assistant\", \"content\": data[\"output\"]},\n            ]\n            if \"system_prompt\" in data and data[\"system_prompt\"]:\n                messages = [\n                    {\"role\": \"system\", \"content\": data[\"system_prompt\"]}\n                ] + messages\n\n        # Normalize role names\n        for item in messages:\n            if \"role\" not in item and \"from\" in item:\n                item[\"role\"] = item[\"from\"]\n            if \"content\" not in item and \"value\" in item:\n                item[\"content\"] = item[\"value\"]\n            role = item[\"role\"]\n            if \"human\" in role:\n                item[\"role\"] = \"user\"\n            elif \"gpt\" in role:\n                item[\"role\"] = \"assistant\"\n\n        return messages\n",
        "angelslim/engine.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport sys\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, Optional\n\nimport torch\n\nfrom .compressor import CompressorFactory\nfrom .data.dataloader import DataLoaderFactory\nfrom .models import SlimModelFactory\nfrom .utils import default_compress_config, get_package_info, print_info\n\nDEFAULT_COMPRESSION_CONFIG = {\n    \"fp8_static\": default_compress_config.default_fp8_static_config(),\n    \"fp8_dynamic\": default_compress_config.default_fp8_dynamic_config(),\n    \"int8_dynamic\": default_compress_config.default_int8_dynamic_config(),\n    \"int4_awq\": default_compress_config.default_int4_awq_config(),\n    \"int4_gptq\": default_compress_config.default_int4_gptq_config(),\n    \"w4a8_fp8\": default_compress_config.default_w4a8_fp8_static_config(),\n}\n\n\ndef get_supported_compress_method():\n    return DEFAULT_COMPRESSION_CONFIG.keys()\n\n\nclass Engine:\n    def __init__(self):\n        \"\"\"\n        Initialize engine configuration\n        \"\"\"\n        self.slim_model = None\n        self.tokenizer = None\n        self.dataloader = None\n        self.compressor = None\n        self.compress_type = None\n        self.model_path = None\n        self.max_seq_length = None\n\n    def prepare_model(\n        self,\n        model_name=\"Qwen\",\n        model=None,\n        tokenizer=None,\n        model_path=None,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        use_cache=False,\n        cache_dir=None,\n        deploy_backend=\"vllm\",\n        using_multi_nodes=False,\n    ) -> Any:\n        \"\"\"Load pretrained model and tokenizer\n        Args:\n            model_name (str): Name of the model to load.\n            model (Any, optional): Preloaded model instance.\n                If provided, `model_path` is ignored.\n            tokenizer (Any, optional): Preloaded tokenizer instance.\n                If model is set, tokenizer must be also set in LLM and VLM.\n            model_path (str, optional): Path to the pretrained model.\n            torch_dtype (str): Data type for the model weights.\n            device_map (str): Device map for the model.\n            trust_remote_code (bool): Whether to trust remote code.\n            low_cpu_mem_usage (bool): Whether to use low CPU memory usage mode.\n            use_cache (bool): Whether to use cache during loading.\n            cache_dir (str, optional): Directory to cache the model.\n            deploy_backend (str): Backend for deployment, e.g., \"torch\", \"vllm\".\n            using_multi_nodes (bool): Whether to use multi-nodes for calibration.\n        \"\"\"\n        assert model_name, \"model_name must be specified.\"\n        assert model_path, \"model_path must be specified.\"\n\n        # Initialize slim model by ModelFactory\n        self.slim_model = SlimModelFactory.create(\n            model_name, model=model, deploy_backend=deploy_backend\n        )\n\n        self.series = SlimModelFactory.get_series_by_models(model_name)\n\n        if self.series in [\"LLM\", \"VLM\"]:\n            if model:\n                assert tokenizer, \" If model is set, tokenizer must be also set.\"\n                self.slim_model.tokenizer = tokenizer\n            else:\n                self.slim_model.from_pretrained(\n                    model_path,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    trust_remote_code=trust_remote_code,\n                    low_cpu_mem_usage=low_cpu_mem_usage,\n                    use_cache=use_cache,\n                    using_multi_nodes=using_multi_nodes,\n                )\n                self.model_path = model_path\n        elif self.series == \"Diffusion\":\n            if not model:\n                self.slim_model.from_pretrained(\n                    model_path,\n                    torch_dtype=torch_dtype,\n                    cache_dir=cache_dir,\n                )\n        else:\n            raise ValueError(f\"Unsupported series: {self.series}\")\n\n        return self.slim_model\n\n    def prepare_data(\n        self,\n        data_path=None,\n        data_type=\"TextDataset\",\n        custom_dataloader=None,\n        max_length=2048,\n        batch_size=1,\n        num_samples=128,\n        shuffle=True,\n    ) -> Optional[Any]:\n        \"\"\"Prepare compression dataset\"\"\"\n        if custom_dataloader is not None:\n            print_info(\"Using custom provided dataloader...\")\n            self.dataloader = custom_dataloader\n            return self.dataloader\n\n        assert data_path, \"data_path must be specified.\"\n        # Dynamically create dataloader by DataLoaderFactory\n        self.dataloader = DataLoaderFactory.create_data_loader(\n            data_type=data_type,\n            processor=(\n                self.slim_model.processor\n                if self.series == \"VLM\"\n                else self.slim_model.tokenizer\n            ),\n            device=self.slim_model.model.device,\n            max_length=max_length,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_samples=num_samples,\n            data_source=data_path,\n        )\n        self.max_seq_length = max_length\n\n        return self.dataloader\n\n    def prepare_compressor(\n        self,\n        compress_name=\"PTQ\",\n        global_config=None,\n        compress_config=None,\n        default_method=None,\n    ) -> Any:\n        \"\"\"\n        Initialize compression components.\n        Args:\n            compress_name (str): Name of the compression method to use.\n            global_config (dict, optional): Global configuration for the model.\n            compress_config (dict, optional): Configuration for the compression method.\n            default_method (str, optional): Default compression method if not specified.\n               If set default_method, compress_config and global_config will be ignored.\n        \"\"\"\n        if compress_name not in CompressorFactory.get_available_compressor():\n            raise ValueError(\n                f\"Compression method '{compress_name}' not registered. \"\n                f\"Available methods: {CompressorFactory.get_available_compressor()}\"\n            )\n        if self.series in [\"LLM\", \"VLM\"]:\n            global_config.update(self.model_path, self.max_seq_length)\n\n        if default_method:\n            assert (\n                default_method in DEFAULT_COMPRESSION_CONFIG\n            ), f\"`default_method` not found in : {DEFAULT_COMPRESSION_CONFIG.keys()}.\"\n            slim_config = DEFAULT_COMPRESSION_CONFIG[default_method]\n        else:\n            slim_config = {\n                \"global_config\": global_config,\n                \"compress_config\": compress_config,\n            }\n        self.compress_type = compress_name\n        # Create compressor by CompressorFactory\n        self.compressor = CompressorFactory.create(\n            compress_name, self.slim_model, slim_config=slim_config\n        )\n        return self.compressor\n\n    def run(self) -> Any:\n        \"\"\"Execute compression pipeline\"\"\"\n        if not self.compressor:\n            raise RuntimeError(\n                \"Compressor not initialized. Call prepare_compressor() first\"\n            )\n\n        if self.compress_type == \"PTQ\":\n            self.compressor.calibrate(self.dataloader)\n        else:\n            raise NotImplementedError(\n                f\"Compression type {self.compress_type} is not implemented\"\n            )\n\n    def save(\n        self, save_path: Optional[str] = None, config: Optional[dataclass] = None\n    ) -> None:\n        \"\"\"Save compressed model and tokenizer\n        Args:\n            save_path (str, optional): Path to save the compressed model and tokenizer.\n        \"\"\"\n        assert save_path, \"Save path must be provided in model_config or as an argument\"\n        if self.compress_type == \"PTQ\":\n            # Execute model conversion\n            self.compressor.convert()\n\n        # Save quantized model\n        self.compressor.save(save_path)\n\n        # Save all config\n        if config is not None:\n            config_dict = asdict(config)\n            config_dict[\"debug_info\"] = {\n                \"python\": sys.version,\n                \"angelslim\": get_package_info(\"angelslim\"),\n                \"torch\": get_package_info(\"torch\"),\n                \"transformers\": get_package_info(\"transformers\"),\n                \"torch_cuda_version\": (\n                    torch.version.cuda if torch.cuda.is_available() else None\n                ),\n            }\n            config_dict[\"model_config\"][\"model_path\"] = \"Base Model Path\"\n            config_dict[\"global_config\"][\"save_path\"] = \"Save Model Path\"\n            with open(os.path.join(save_path, \"angelslim_config.json\"), \"w\") as f:\n                json.dump(config_dict, f, indent=4)\n\n        print_info(f\"Compressed model saved to {save_path}\")\n\n    def infer(self, input_prompt: str, **kwargs) -> Any:\n        \"\"\"Run inference with the compressed model\n        Args:\n            input_prompt (str): Input prompt for the model.\n        \"\"\"\n        if not self.slim_model or not self.slim_model.model:\n            raise RuntimeError(\"Model not initialized. Call prepare_model() first\")\n\n        if self.series in [\"LLM\", \"VLM\"]:\n            return self.slim_model.generate(\n                input_ids=self.slim_model.tokenizer(\n                    input_prompt, return_tensors=\"pt\"\n                ).input_ids,\n                **kwargs,\n            )\n        elif self.series == \"Diffusion\":\n            return self.slim_model.generate(input_prompt, **kwargs)\n        else:\n            raise NotImplementedError(\n                f\"Series {self.series} is not implemented for inference\"\n            )\n",
        "angelslim/models/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .diffusion import *  # noqa: F401 F403\nfrom .llm import *  # noqa: F401 F403\nfrom .model_factory import SlimModelFactory  # noqa: F401\nfrom .vlm import *  # noqa: F401 F403\n",
        "angelslim/models/base_model.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom ..compressor.quant.core import QuantConfig\nfrom ..compressor.quant.modules import QDQModule\nfrom ..utils import common_prefix, print_info\n\n__all__ = [\"BaseLLMModel\", \"BaseDiffusionModel\"]\n\n\nclass BaseLLMModel(metaclass=ABCMeta):\n    \"\"\"\n    Base class for model compression, providing common functionalities\n    such as initialization, quantization configuration, and model handling.\n    Args:\n        model (torch.nn.Module, optional): the model to be compressed.\n            If not provided, the model will be built from `model_path`.\n        deploy_backend (str, optional): deploy_backend for model compression,\n            currently only supports \"vllm\".\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[torch.nn.Module] = None,\n        deploy_backend: Optional[str] = \"vllm\",\n    ):\n        assert deploy_backend in [\n            \"vllm\",\n            \"huggingface\",\n            \"trtllm\",\n        ], f\"Unsupported deploy backend {deploy_backend}\"\n        self.deploy_backend = deploy_backend\n        self.model = model\n        self.tokenizer = None\n        self.modal_type = \"LLM\"\n        self.pre_transformer_module_names = [\"model.embed_tokens\"]\n\n    def from_pretrained(\n        self,\n        model_path,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        use_cache=False,\n        using_multi_nodes=False,\n    ):\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            trust_remote_code=trust_remote_code,\n            low_cpu_mem_usage=low_cpu_mem_usage,\n            use_cache=use_cache,\n        )\n\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=trust_remote_code\n        )\n\n    def init_ptq(self, slim_config):\n        \"\"\"\n        Initialize the model for post-training quantization (PTQ).\n        Args:\n            slim_config(dict, required): the configuration for quantization.\n                - compress_config: the configuration for compression.\n                - global_config: the global configuration for the model.\n        \"\"\"\n        quant_config = QuantConfig(\n            slim_config[\"compress_config\"], slim_config[\"global_config\"]\n        )\n        self.quant_config = quant_config\n        self.act_scales_dict = {}\n        self.weight_scales_dict = {}\n        self.kv_cache_scales_dict = {}\n        if hasattr(self.quant_config, \"weight_observer\"):\n            self.quant_algo_dict = self.get_quant_config()\n        else:\n            self.quant_algo_dict = None\n        self.quantized = False\n\n    @abstractmethod\n    def get_observer_layers(self):\n        pass\n\n    @abstractmethod\n    def get_save_func(self):\n        pass\n\n    def skip_layer_names(self):\n        return self.quant_config.quant_algo_info.get(\"ignore_layers\", [])\n\n    def get_model(self):\n        return self.model\n\n    def get_qdq_module(self, sub_layer, name):\n        act_scale, weight_scale = None, None\n        if name in self.act_scales_dict:\n            act_scale = self.act_scales_dict[name]\n        if name in self.weight_scales_dict:\n            weight_scale = self.weight_scales_dict[name]\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            q_linear = QDQModule(\n                quant_algo=self.quant_config.quant_algo,\n                weight=sub_layer.weight,\n                weight_scale=weight_scale,\n                bias=sub_layer.bias,\n                input_scale=act_scale,\n            )\n        else:\n            print_info(\n                \"[Slim] current {} deploy_backend not support\".format(\n                    self.deploy_backend\n                )\n            )\n            raise NotImplementedError\n        return q_linear\n\n    def get_kvcache_observer_layers_names(self, observe_names):\n        names = [\"self_attn.k_proj\", \"self_attn.v_proj\"]\n        return [\n            k\n            for k in observe_names\n            if k.startswith(self.block_name)\n            and k.split(\".\")[-2] + \".\" + k.split(\".\")[-1] in names\n        ]\n\n    def get_quant_config(self):\n        assert self.quant_config is not None\n        kv_cache_observer = self.quant_config.kv_cache_observer\n        act_observer = self.quant_config.act_observer\n        weight_observer = self.quant_config.weight_observer\n\n        if hasattr(self.quant_config, \"smooth_observer\"):\n            smooth_observer = self.quant_config.smooth_observer\n        else:\n            smooth_observer = None\n\n        # assert isinstance(self.quant_config.quant_algo, dict)\n        w = self.quant_config.quant_algo_info.get(\"w\", None)\n        a = self.quant_config.quant_algo_info.get(\"a\", None)\n        c = self.quant_config.quant_algo_info.get(\"c\", None)\n\n        w_group_size = self.quant_config.quant_algo_info.get(\"w_group_size\", -1)\n\n        a_quant_algo = a.split(\"_\")[0] if a is not None else None\n        w_quant_algo = w.split(\"_\")[0] if w is not None else None\n        c_quant_algo = c.split(\"_\")[0] if c is not None else None\n        a_quant_bits = (\n            int(re.search(r\"\\d+\", a_quant_algo).group())\n            if a_quant_algo is not None\n            else None\n        )\n        w_quant_bits = (\n            int(re.search(r\"\\d+\", w_quant_algo).group())\n            if w_quant_algo is not None\n            else None\n        )\n        c_quant_bits = (\n            int(re.search(r\"\\d+\", c_quant_algo).group())\n            if c_quant_algo is not None\n            else None\n        )\n        a_quant_method = a.split(\"_\")[1] if a is not None else None\n        w_quant_method = w.split(\"_\")[1] if w is not None else None\n        c_quant_method = c.split(\"_\")[1] if c is not None else None\n\n        custom_observe_layers_names = self.quant_config.custom_observe_layers_names\n\n        quant_algo_dict = {\n            \"act_observer\": act_observer,\n            \"weight_observer\": weight_observer,\n            \"kv_cache_observer\": kv_cache_observer,\n            \"smooth_observer\": smooth_observer,\n            \"a_quant_algo\": a_quant_algo,\n            \"w_quant_algo\": w_quant_algo,\n            \"c_quant_algo\": c_quant_algo,\n            \"a_quant_bits\": a_quant_bits,\n            \"w_quant_bits\": w_quant_bits,\n            \"c_quant_bits\": c_quant_bits,\n            \"w_group_size\": w_group_size,\n            \"a_quant_method\": a_quant_method,\n            \"w_quant_method\": w_quant_method,\n            \"c_quant_method\": c_quant_method,\n            \"all_reduce\": self.is_all_reduce(),\n            \"custom_observe_layers_names\": custom_observe_layers_names,\n        }\n        return quant_algo_dict\n\n    def is_all_reduce(self):\n        return False\n\n    def build_hf_model(self, model_path):\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=\"auto\",\n            device_map=\"auto\",\n            use_flash_attention_2=True,\n            trust_remote_code=True,\n        )\n        return model\n\n    def find_layers(self, module, layers=None, name=\"\"):\n        if type(module) in layers and name not in self.skip_layer_names():\n            return {name: module}\n        res = {}\n        for name1, child in module.named_children():\n            res.update(\n                self.find_layers(\n                    child,\n                    layers=layers,\n                    name=name + \".\" + name1 if name != \"\" else name1,\n                )\n            )\n        return res\n\n    def get_pre_transformer_modules(self):\n        pre_transformer_modules_dict = {}\n        for full_name in self.pre_transformer_module_names:\n            current_module = self.model\n            parts = full_name.split(\".\")\n            for part in parts:\n                if not hasattr(current_module, part):\n                    current_module = None\n                    break\n                current_module = getattr(current_module, part)\n            if current_module is not None:\n                pre_transformer_modules_dict[full_name] = current_module\n        return pre_transformer_modules_dict\n\n    def model_forward(self, dataloader, **kwargs):\n        self.model.use_cache = False\n\n        calibrated_cnt = 0\n        if (\n            \"gptq\" in self.quant_config.quant_algo\n            or \"awq\" in self.quant_config.quant_algo\n        ):\n            device = \"cuda:0\"\n        else:\n            device = self.model.device\n\n        if dataloader is not None:\n            with torch.no_grad():\n                for batch in tqdm(\n                    dataloader, desc=\"calibrating...\", total=len(dataloader)\n                ):\n                    inputs = batch[\"input_ids\"].to(device)\n                    labels = batch[\"labels\"].to(device)\n                    attention_mask = batch[\"attention_mask\"].to(device)\n                    try:\n                        outputs = self.model(inputs)\n                        logits = outputs.logits.float()\n\n                        loss = F.cross_entropy(\n                            logits.view(-1, logits.size(-1)),\n                            labels.view(-1),\n                            reduction=\"none\",\n                        )\n\n                        attention_mask = (\n                            attention_mask.view(-1).to(logits.device).float()\n                        )\n                        loss = loss * attention_mask\n                        avg_loss = loss.mean()\n                        ppl = torch.exp(avg_loss)\n\n                        print_info(f\"ppl is : {ppl:.4f}\")\n\n                        calibrated_cnt += 1\n                    except ValueError:\n                        calibrated_cnt += 1\n                        pass\n\n    def get_smooth_mapping_layers(self, smooth_config, mappings=None):\n        assert mappings is not None, \"mappings must be provided\"\n        smooth_mapping_layers = {}\n\n        for to_balance_list, to_smooth in mappings:\n            for smooth_name, smooth_layer in self.model.named_modules():\n                if smooth_name.split(\".\")[-1] != to_smooth:\n                    continue\n                balance_layers_list = []\n\n                for to_balance in to_balance_list:\n                    longest_prefix = 0\n                    balance_layers = []\n\n                    # use common_prefix to support moe experts\n                    for name, layer in self.model.named_modules():\n                        if name.split(\".\")[-1] != to_balance:\n                            continue\n                        prefix = common_prefix(name, smooth_name)\n                        if prefix.count(\".\") < longest_prefix:\n                            continue\n                        elif prefix.count(\".\") == longest_prefix:\n                            balance_layers.append((name, layer))\n                        else:\n                            longest_prefix = prefix.count(\".\")\n                            balance_layers = [(name, layer)]\n\n                    if balance_layers:\n                        balance_layers_list.extend(balance_layers)\n\n                if balance_layers_list:\n                    smooth_mapping_layers[smooth_name] = (\n                        smooth_layer,\n                        balance_layers_list,\n                    )\n\n        return smooth_mapping_layers\n\n    def get_parent_dict(self, observer_layers_dict):\n        return {}\n\n    def get_weight_scales(self, layer, weight_observer):\n        weight = layer.weight.clone().detach()\n        weight_observer(weight)\n        return weight_observer.scales()\n\n    def __getattr__(self, item):\n        return super().__getattr__(item)\n\n\nclass BaseDiffusionModel(metaclass=ABCMeta):\n    \"\"\"\n    Base class for diffusion model compression, providing common functionalities\n    such as initialization, quantization configuration, and model handling.\n    Args:\n        model (torch.nn.Module, optional): the model to be compressed.\n            If not provided, the model will be built from `model_path`.\n        deploy_backend (str, optional): deploy_backend for model compression.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[torch.nn.Module] = None,\n        deploy_backend: Optional[str] = \"torch\",\n    ):\n        assert deploy_backend in [\n            \"torch\",\n            \"tensorrt\",\n        ], f\"Unsupported deploy backend {deploy_backend}\"\n        self.deploy_backend = deploy_backend\n        self.model = model\n        self.modal_type = \"Diffusion\"\n\n    @staticmethod\n    def from_pretrained(self, model_path, **kwargs):\n        \"\"\"\n        Load a pretrained diffusion model.\n        Args:\n            model_path (str): Path to the pretrained model.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented in subclasses.\")\n\n    @abstractmethod\n    def get_observer_layers(self):\n        pass\n\n    @abstractmethod\n    def get_save_func(self):\n        pass\n",
        "angelslim/models/diffusion/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom .flux import FLUX  # noqa: F401\n",
        "angelslim/models/diffusion/flux.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom diffusers import FluxPipeline\n\nfrom ..base_model import BaseDiffusionModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass FLUX(BaseDiffusionModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"torch\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.model_type = \"flux\"\n        self.cache_helper = None\n\n    def from_pretrained(\n        self,\n        model_path,\n        torch_dtype=\"auto\",\n        cache_dir=None,\n        use_cache_helper=False,\n    ):\n        \"\"\"\n        Load a pretrained FLUX model.\n        Args:\n            model_path (str): Path to the pretrained model.\n            torch_dtype (str): Data type for the model weights.\n            cache_dir (str): Directory to cache the model.\n        \"\"\"\n        self.model = FluxPipeline.from_pretrained(\n            model_path,\n            torch_dtype=torch_dtype,\n            cache_dir=cache_dir,\n        )\n        if use_cache_helper:\n            self.model.cache_helper = self.cache_helper\n\n    def generate(\n        self,\n        prompt,\n        height=1024,\n        width=1024,\n        guidance_scale=3.5,\n        num_inference_steps=50,\n        max_sequence_length=512,\n        seed=42,\n    ):\n        \"\"\"\n        Generate images using the FLUX model.\n        Args:\n            prompt (list): List of text prompt for image generation.\n            height (int): Height of the generated images.\n            width (int): Width of the generated images.\n            guidance_scale (float): Guidance scale for the generation.\n            num_inference_steps (int): Number of inference steps.\n            max_sequence_length (int): Maximum sequence length for the model.\n            seed (int): Random number torch.Generator for reproducibility.\n        Returns:\n            Generated image tensor.\n        \"\"\"\n        generator = torch.Generator().manual_seed(seed)\n        return self.model(\n            prompt=prompt,\n            height=height,\n            width=width,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            max_sequence_length=max_sequence_length,\n            generator=generator,\n        ).images[0]\n\n    def get_observer_layers(self):\n        pass\n\n    def get_save_func(self):\n        pass\n",
        "angelslim/models/llm/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .deepseek import DeepSeek  # noqa: F401\nfrom .hunyuan_dense import HunyuanDense  # noqa: F401\nfrom .hunyuan_moe import HunyuanMoE  # noqa: F401\nfrom .kimi_k2 import KimiK2  # noqa: F401\nfrom .llama import Llama  # noqa: F401\nfrom .qwen import Qwen  # noqa: F401\n",
        "angelslim/models/llm/deepseek.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.models.deepseek_v3 import DeepseekV3Config\n\nfrom ...compressor.quant.core import (\n    DeepseekV3HfPTQSave,\n    DeepseekV3PTQSaveTRTLLM,\n    PTQSaveVllmHF,\n    weight_dequant,\n)\nfrom ...compressor.quant.modules import QDQModule\nfrom ...utils import print_info\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\nfrom .modeling_deepseek import (\n    ColumnParallelLinear,\n    DeepseekV3ForCausalLM,\n    Linear,\n    RowParallelLinear,\n)\n\n\n@SlimModelFactory.register\nclass DeepSeek(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.block_name = \"model.layers\"\n        self.column_parallel_linear_class = ColumnParallelLinear\n        self.row_parallel_linear_class = RowParallelLinear\n\n    def from_pretrained(\n        self,\n        model_path,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        use_cache=False,\n        using_multi_nodes=False,\n    ):\n        if torch_dtype == \"fp8\":\n            print_info(\"[Slim] Loading DeepSeek with fp8\")\n            config = DeepseekV3Config.from_pretrained(model_path)\n            if hasattr(config, \"quantization_config\"):\n                delattr(config, \"quantization_config\")\n            if hasattr(config, \"use_cache\"):\n                config.use_cache = use_cache\n            self.model = DeepseekV3ForCausalLM.from_pretrained(\n                model_path,\n                config=config,\n                torch_dtype=\"auto\",\n                device_map=device_map,\n                trust_remote_code=trust_remote_code,\n                low_cpu_mem_usage=low_cpu_mem_usage,\n                using_multi_nodes=using_multi_nodes,\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=\"auto\",\n                device_map=device_map,\n                trust_remote_code=trust_remote_code,\n                low_cpu_mem_usage=low_cpu_mem_usage,\n                use_cache=use_cache,\n            )\n\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=trust_remote_code\n        )\n\n    def get_observer_layers(self):\n        names = self.quant_config.quant_algo_info[\"ignore_layers\"]\n        obs_layers = [nn.Linear, Linear]\n        observer_layers_dict = self.find_layers(self.model, layers=obs_layers)\n        observer_layers_dict = {\n            k: v\n            for k, v in observer_layers_dict.items()\n            if k.startswith(self.block_name) and not any(name in k for name in names)\n        }\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def get_weight_scales(self, layer, weight_observer):\n        assert layer.weight.dtype in [torch.bfloat16, torch.float8_e4m3fn]\n        if layer.weight.dtype == torch.bfloat16:\n            weight = layer.weight.clone().detach()\n            weight_observer(weight)\n            return weight_observer.scales()\n        else:\n            if not self.model.using_multi_nodes:\n                layer = layer.to(\"cuda\")\n                weight = weight_dequant(layer.weight, layer.weight_scale_inv)\n                weight = weight.to(\"cpu\")\n                layer = layer.to(\"cpu\")\n                weight_observer(weight)\n                del weight\n                return weight_observer.scales()\n            else:\n                weight_bf16 = weight_dequant(layer.weight, layer.weight_scale_inv)\n                weight_observer(weight_bf16)\n                scales = weight_observer.scales()\n                world_size = dist.get_world_size() if dist.is_initialized() else 1\n                if isinstance(layer, self.column_parallel_linear_class):\n                    all_scales = [torch.empty_like(scales) for _ in range(world_size)]\n                    dist.all_gather(all_scales, scales)\n                    merged_scales = torch.cat(all_scales, dim=0)\n                    return merged_scales\n                elif isinstance(layer, self.row_parallel_linear_class):\n                    all_scales = [torch.empty_like(scales) for _ in range(world_size)]\n                    dist.all_gather(all_scales, scales)\n                    merged_scales = torch.cat(all_scales, dim=-1)\n                    return merged_scales\n                else:\n                    return scales\n\n    def get_qdq_module(self, layer, name):\n        if self.model.using_multi_nodes:\n            return layer\n        act_scale, weight_scale = None, None\n        if name in self.act_scales_dict:\n            act_scale = self.act_scales_dict[name]\n        if name in self.weight_scales_dict:\n            weight_scale = self.weight_scales_dict[name]\n\n        if layer.weight.dtype == torch.bfloat16:\n            weight = layer.weight.clone().detach()\n        elif layer.weight.dtype == torch.float8_e4m3fn:\n            layer = layer.to(\"cuda\")\n            weight = weight_dequant(layer.weight, layer.weight_scale_inv)\n            weight = weight.to(\"cpu\")\n            layer = layer.to(\"cpu\")\n        q_linear = QDQModule(\n            quant_algo=self.quant_config.quant_algo,\n            weight=weight,\n            weight_scale=weight_scale,\n            group_size=self.quant_config.quant_algo_info[\"w_group_size\"],\n            bias=layer.bias,\n            input_scale=act_scale,\n        )\n        del weight\n        return q_linear\n\n    def model_forward(self, dataloader, **kwargs):\n        if self.quant_config.low_memory:\n            device = \"cpu\"\n        else:\n            device = torch.cuda.current_device()\n        calibrated_cnt = 0\n        if dataloader is not None:\n            with torch.no_grad():\n                for batch in tqdm(\n                    dataloader, desc=\"calibrating...\", total=len(dataloader)\n                ):\n                    inputs = batch[\"input_ids\"].to(device)\n                    labels = batch[\"labels\"].to(device)\n                    attention_mask = batch[\"attention_mask\"].to(device)\n                    try:\n                        _, logits = self.model(inputs)\n                        loss = F.cross_entropy(\n                            logits.view(-1, logits.size(-1)),\n                            labels.view(-1),\n                            reduction=\"none\",\n                        )\n                        attention_mask = (\n                            attention_mask.view(-1).to(logits.device).float()\n                        )\n                        loss = loss * attention_mask\n                        avg_loss = loss.mean()\n                        ppl = torch.exp(avg_loss)\n\n                        print_info(f\"ppl is : {ppl:.4f}\")\n\n                        calibrated_cnt += 1\n                    except ValueError:\n                        calibrated_cnt += 1\n                        pass\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            if self.model.using_multi_nodes:\n                return DeepseekV3HfPTQSave\n            return PTQSaveVllmHF\n        elif self.deploy_backend == \"trtllm\":\n            if self.quant_config.low_memory:\n                return DeepseekV3PTQSaveTRTLLM\n            return DeepseekV3HfPTQSave\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/models/llm/hunyuan_dense.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch.nn as nn\n\nfrom ...compressor.quant.core import PTQSaveVllmHF\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass HunyuanDense(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.block_name = \"model.layers\"\n\n    def get_observer_layers(self):\n        names = [\n            \"self_attn.k_proj\",\n            \"self_attn.v_proj\",\n            \"self_attn.q_proj\",\n            \"self_attn.o_proj\",\n            \"self_attn.qkv_proj\",\n            \"mlp.up_proj\",\n            \"mlp.gate_proj\",\n            \"mlp.down_proj\",\n            \"mlp.gate_and_up_proj\",\n        ]\n        obs_layers = [nn.Linear]\n        observer_layers_dict = self.find_layers(self.model, layers=obs_layers)\n\n        observer_layers_dict = {\n            k: v\n            for k, v in observer_layers_dict.items()\n            if k.startswith(self.block_name)\n            and k.split(\".\")[-2] + \".\" + k.split(\".\")[-1] in names\n        }\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            return PTQSaveVllmHF\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/models/llm/hunyuan_moe.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nimport torch.nn as nn\n\nfrom ...compressor.quant.core import PTQSaveVllmHF\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass HunyuanMoE(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.block_name = \"model.layers\"\n\n    def get_observer_layers(self):\n        names = [\n            \"self_attn.q_proj\",\n            \"self_attn.k_proj\",\n            \"self_attn.v_proj\",\n            \"self_attn.o_proj\",\n            \"shared_mlp.gate_proj\",\n            \"shared_mlp.up_proj\",\n            \"shared_mlp.down_proj\",\n        ]\n        expert_pattern = [\n            r\"model\\.layers\\.\\d+\\.mlp\\.experts\\.\\d+\\.gate_proj\",\n            r\"model\\.layers\\.\\d+\\.mlp\\.experts\\.\\d+\\.up_proj\",\n            r\"model\\.layers\\.\\d+\\.mlp\\.experts\\.\\d+\\.down_proj\",\n        ]\n\n        obs_layers = [nn.Linear]\n        observer_layers_dict = self.find_layers(self.model, layers=obs_layers)\n\n        compiled_patterns = [re.compile(pattern) for pattern in expert_pattern]\n\n        observer_layers_dict = {\n            k: v\n            for k, v in observer_layers_dict.items()\n            if k.startswith(self.block_name)\n            and (\n                any(name in k for name in names)\n                or any(pattern.search(k) for pattern in compiled_patterns)\n            )\n        }\n\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def get_parent_dict(self, observer_layers_dict):\n        parent_mapping = {r\"experts\\.\\d+\": \"experts\"}\n        parent_dict = {}\n        for layer_name in observer_layers_dict.keys():\n            parent_name = layer_name\n            for k, v in parent_mapping.items():\n                parent_name = re.sub(k, v, layer_name)\n            if parent_name != layer_name:\n                parent_dict[layer_name] = parent_name\n        return parent_dict\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            return PTQSaveVllmHF\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/models/llm/kimi_k2.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom transformers import AutoModelForCausalLM\nfrom transformers.models.deepseek_v3 import DeepseekV3Config\n\nfrom ...tokenizer import TikTokenTokenizer\nfrom ...utils import print_info\nfrom ..model_factory import SlimModelFactory\nfrom .deepseek import DeepSeek\nfrom .modeling_deepseek import DeepseekV3ForCausalLM\n\n\n@SlimModelFactory.register\nclass KimiK2(DeepSeek):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n\n    def from_pretrained(\n        self,\n        model_path,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        use_cache=False,\n        using_multi_nodes=False,\n    ):\n        if torch_dtype == \"fp8\":\n            print_info(\"[Slim] Loading KimiK2 with fp8\")\n            config = DeepseekV3Config.from_pretrained(model_path)\n            if hasattr(config, \"quantization_config\"):\n                delattr(config, \"quantization_config\")\n            if hasattr(config, \"use_cache\"):\n                config.use_cache = use_cache\n            self.model = DeepseekV3ForCausalLM.from_pretrained(\n                model_path,\n                config=config,\n                torch_dtype=\"auto\",\n                device_map=device_map,\n                trust_remote_code=trust_remote_code,\n                low_cpu_mem_usage=low_cpu_mem_usage,\n                using_multi_nodes=using_multi_nodes,\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=\"auto\",\n                device_map=device_map,\n                trust_remote_code=trust_remote_code,\n                low_cpu_mem_usage=low_cpu_mem_usage,\n                use_cache=use_cache,\n            )\n\n        # Load tokenizer\n        self.tokenizer = TikTokenTokenizer.from_pretrained(model_path)\n",
        "angelslim/models/llm/llama.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch.nn as nn\n\nfrom ...compressor.quant.core import PTQSaveVllmHF\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass Llama(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.block_name = \"model.layers\"\n\n    def get_observer_layers(self):\n        names = [\n            \"self_attn.k_proj\",\n            \"self_attn.v_proj\",\n            \"self_attn.q_proj\",\n            \"self_attn.o_proj\",\n            \"mlp.up_proj\",\n            \"mlp.gate_proj\",\n            \"mlp.down_proj\",\n        ]\n        obs_layers = [nn.Linear]\n        observer_layers_dict = self.find_layers(self.model, layers=obs_layers)\n        observer_layers_dict = {\n            k: v\n            for k, v in observer_layers_dict.items()\n            if k.startswith(self.block_name)\n            and k.split(\".\")[-2] + \".\" + k.split(\".\")[-1] in names\n        }\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            return PTQSaveVllmHF\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/models/llm/modeling_deepseek.py": "# flake8: noqa: E501\n# coding=utf-8\n# Copyright 2023 DeepSeek-AI and The HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport os\nfrom glob import glob\nfrom typing import Literal, Optional, Tuple\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom safetensors.torch import load_model, safe_open, save_file\nfrom torch import nn\nfrom tqdm import tqdm, trange\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.models.deepseek_v3 import DeepseekV3Config\n\nfrom angelslim.compressor.quant import weight_dequant\nfrom angelslim.utils import print_info\n\nworld_size = 1\nrank = 0\nblock_size = 128\nmax_batch_size = 8\ngemm_impl = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n\n\ndef convert_ckpt(input_path, save_path, n_experts, mp):\n    \"\"\"\n    Converts and saves model checkpoint files into a specified format.\n\n    Args:\n        hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n        save_path (str): Path to the directory where the converted checkpoint files will be saved.\n        n_experts (int): Total number of experts in the model.\n        mp (int): Model parallelism factor.\n\n    Returns:\n        None\n    \"\"\"\n    mapping = {\n        \"embed_tokens\": 0,\n        \"input_layernorm\": None,\n        \"post_attention_layernorm\": None,\n        \"q_proj\": 0,\n        \"q_a_proj\": None,\n        \"q_a_layernorm\": None,\n        \"q_b_proj\": 0,\n        \"kv_a_proj_with_mqa\": None,\n        \"kv_a_layernorm\": None,\n        \"kv_b_proj\": 0,\n        \"o_proj\": 1,\n        \"gate\": None,\n        \"gate_proj\": 0,\n        \"down_proj\": 1,\n        \"up_proj\": 0,\n        \"norm\": None,\n        \"lm_head\": 0,\n        \"weight_scale_inv\": None,\n    }\n    torch.set_num_threads(8)\n    n_local_experts = n_experts // mp\n    state_dicts = [{} for _ in range(mp)]\n\n    for file_path in tqdm(glob(os.path.join(input_path, \"*.safetensors\"))):\n        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n            for name in f.keys():\n                if \"model.layers.61\" in name:\n                    continue\n                if \"rotary_emb\" in name:\n                    continue\n                param: torch.Tensor = f.get_tensor(name)\n                key = name.split(\".\")[-2]\n                dim = mapping[key]\n                for i in range(mp):\n                    new_param = param\n                    if \"experts\" in name and \"shared_experts\" not in name:\n                        idx = int(name.split(\".\")[-3])\n                        if (\n                            idx < i * n_local_experts\n                            or idx >= (i + 1) * n_local_experts\n                        ):\n                            continue\n                    elif dim is not None:\n                        assert (\n                            param.size(dim) % mp == 0\n                        ), f\"Dimension {dim} must be divisible by {mp}\"\n                        shard_size = param.size(dim) // mp\n                        new_param = param.narrow(\n                            dim, i * shard_size, shard_size\n                        ).contiguous()\n                    state_dicts[i][name] = new_param\n\n    os.makedirs(save_path, exist_ok=True)\n\n    for i in trange(mp):\n        save_file(\n            state_dicts[i], os.path.join(save_path, f\"model{i}-mp{mp}.safetensors\")\n        )\n\n\nclass ParallelEmbedding(nn.Module):\n    \"\"\"\n    Embedding layer with parallelism support across distributed processes.\n\n    Args:\n        vocab_size (int): Vocabulary size.\n        dim (int): Embedding dimension.\n    \"\"\"\n\n    def __init__(self, vocab_size: int, dim: int):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.dim = dim\n        assert (\n            vocab_size % world_size == 0\n        ), f\"Vocabulary size must be divisible by world size (world_size={world_size})\"\n        self.part_vocab_size = vocab_size // world_size\n        self.vocab_start_idx = rank * self.part_vocab_size\n        self.vocab_end_idx = self.vocab_start_idx + self.part_vocab_size\n        self.weight = nn.Parameter(torch.empty(self.part_vocab_size, self.dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for parallel embedding layer.\n\n        Args:\n            x (torch.Tensor): Input tensor containing token indices.\n\n        Returns:\n            torch.Tensor: Embedded representations.\n\n        Raises:\n            ValueError: If `world_size` is not defined.\n        \"\"\"\n        if world_size > 1:\n            mask = (x < self.vocab_start_idx) | (x >= self.vocab_end_idx)\n            x = x - self.vocab_start_idx\n            x[mask] = 0\n        y = F.embedding(x, self.weight)\n        if world_size > 1:\n            y[mask] = 0\n            dist.all_reduce(y)\n        return y\n\n\ndef linear(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    bias: Optional[torch.Tensor] = None,\n    weight_scale_inv: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Applies a linear transformation to the incoming data: y = xA^T + b.\n    This function supports specialized implementations based on quantization\n    and tensor formats.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        weight (torch.Tensor): The weight tensor. It may be quantized and\n            requires dequantization for certain cases.\n        bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.\n\n    Returns:\n        torch.Tensor: The result of the linear transformation, which may involve\n        quantization-aware computations depending on the input parameters.\n\n    Notes:\n        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version\n          is used for computation.\n        - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n    \"\"\"\n    if weight.element_size() > 1:\n        return F.linear(x, weight, bias)\n    elif gemm_impl == \"bf16\":\n        weight = weight_dequant(weight, weight_scale_inv)\n        return F.linear(x, weight, bias)\n\n\nclass Linear(nn.Module):\n    \"\"\"\n    Custom linear layer with support for quantized weights and optional bias.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n\n    dtype = torch.bfloat16\n\n    def __init__(\n        self, in_features: int, out_features: int, bias: bool = False, dtype=None\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(\n            torch.empty(out_features, in_features, dtype=dtype or Linear.dtype)\n        )\n        if self.weight.element_size() == 1:\n            scale_out_features = (out_features + block_size - 1) // block_size\n            scale_in_features = (in_features + block_size - 1) // block_size\n            self.weight_scale_inv = nn.Parameter(\n                torch.empty(scale_out_features, scale_in_features, dtype=torch.float32)\n            )\n        else:\n            self.register_parameter(\"weight_scale_inv\", None)\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the custom linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor after linear computation.\n        \"\"\"\n        return linear(x, self.weight, self.bias, self.weight_scale_inv)\n\n\nclass ColumnParallelLinear(Linear):\n    \"\"\"\n    Linear layer with column parallelism, splitting output features across distributed processes.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Total number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n\n    def __init__(\n        self, in_features: int, out_features: int, bias: bool = False, dtype=None\n    ):\n        assert (\n            out_features % world_size == 0\n        ), f\"Output features must be divisible by world size (world_size={world_size})\"\n        self.part_out_features = out_features // world_size\n        super().__init__(in_features, self.part_out_features, bias, dtype)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for column parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with column-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight, self.bias, self.weight_scale_inv)\n        return y\n\n\nclass RowParallelLinear(Linear):\n    \"\"\"\n    Linear layer with row parallelism, splitting input features across distributed processes.\n\n    Args:\n        in_features (int): Total number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n\n    def __init__(\n        self, in_features: int, out_features: int, bias: bool = False, dtype=None\n    ):\n        assert (\n            in_features % world_size == 0\n        ), f\"Input features must be divisible by world size (world_size={world_size})\"\n        self.part_in_features = in_features // world_size\n        super().__init__(self.part_in_features, out_features, bias, dtype)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for row parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with row-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight, self.bias, self.weight_scale_inv)\n        if world_size > 1:\n            dist.all_reduce(y)\n        if self.bias is not None:\n            y += self.bias\n        return y\n\n\nclass RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    Args:\n        dim (int): Dimension of the input tensor.\n        eps (float): Epsilon value for numerical stability. Defaults to 1e-6.\n    \"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.dim = dim\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Forward pass for RMSNorm.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Normalized tensor with the same shape as input.\n        \"\"\"\n        return F.rms_norm(x, (self.dim,), self.weight, self.eps)\n\n\ndef precompute_freqs_cis(config) -> torch.Tensor:\n    \"\"\"\n    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n\n    Args:\n        args (ModelArgs): Model arguments containing positional embedding parameters.\n\n    Returns:\n        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n    \"\"\"\n    dim = config.qk_rope_head_dim\n    beta_fast = config.rope_scaling[\"beta_fast\"]\n    beta_slow = config.rope_scaling[\"beta_slow\"]\n    base = config.rope_theta\n    factor = config.rope_scaling[\"factor\"]\n    max_seq_len = config.max_position_embeddings\n    original_seq_len = config.rope_scaling[\"original_max_position_embeddings\"]\n\n    def find_correction_dim(num_rotations, dim, base, max_seq_len):\n        \"\"\"\n        Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n\n        Args:\n            num_rotations (float): Number of rotations to compute the correction for.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            float: The correction dimension based on the input parameters.\n        \"\"\"\n        return (\n            dim\n            * math.log(max_seq_len / (num_rotations * 2 * math.pi))\n            / (2 * math.log(base))\n        )\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n        \"\"\"\n        Computes the range of correction dimensions for rotary positional embeddings.\n\n        Args:\n            low_rot (float): Lower bound for the number of rotations.\n            high_rot (float): Upper bound for the number of rotations.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n        \"\"\"\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        \"\"\"\n        Computes a linear ramp function used to smooth values between a minimum and maximum range.\n\n        Args:\n            min (float): Minimum value for the ramp function.\n            max (float): Maximum value for the ramp function.\n            dim (int): Dimensionality of the ramp tensor.\n\n        Returns:\n            torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n                clamped to the range [0, 1].\n        \"\"\"\n        if min == max:\n            max += 0.001\n        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n        ramp_func = torch.clamp(linear_func, 0, 1)\n        return ramp_func\n\n    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n    if max_seq_len > original_seq_len:\n        low, high = find_correction_range(\n            beta_fast, beta_slow, dim, base, original_seq_len\n        )\n        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n        freqs = freqs / factor * (1 - smooth) + freqs * smooth\n\n    t = torch.arange(max_seq_len)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    return freqs_cis\n\n\ndef apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Applies rotary positional embeddings to the input tensor.\n\n    Args:\n        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n\n    Returns:\n        torch.Tensor: Tensor with rotary embeddings applied.\n    \"\"\"\n    dtype = x.dtype\n    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n    y = torch.view_as_real(x * freqs_cis).flatten(3)\n    return y.to(dtype)\n\n\nclass MLA(nn.Module):\n    \"\"\"\n    Multi-Headed Attention Layer (MLA).\n\n    Attributes:\n        dim (int): Dimensionality of the input features.\n        n_heads (int): Number of attention heads.\n        n_local_heads (int): Number of local attention heads for distributed systems.\n        q_lora_rank (int): Rank for low-rank query projection.\n        kv_lora_rank (int): Rank for low-rank key/value projection.\n        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n        qk_head_dim (int): Total dimensionality of query/key projections.\n        v_head_dim (int): Dimensionality of value projections.\n        softmax_scale (float): Scaling factor for softmax in attention computation.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dim = config.hidden_size\n        self.n_heads = config.num_attention_heads\n        self.n_local_heads = config.num_attention_heads // world_size\n        self.q_lora_rank = config.q_lora_rank\n        self.kv_lora_rank = config.kv_lora_rank\n        self.qk_nope_head_dim = config.qk_nope_head_dim\n        self.qk_rope_head_dim = config.qk_rope_head_dim\n        self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n        self.v_head_dim = config.v_head_dim\n        self.mscale = config.rope_scaling[\"mscale\"]\n\n        if self.q_lora_rank == 0:\n            self.q_proj = ColumnParallelLinear(\n                self.dim, self.n_heads * self.qk_head_dim\n            )\n        else:\n            self.q_a_proj = Linear(self.dim, self.q_lora_rank)\n            self.q_a_layernorm = RMSNorm(self.q_lora_rank)\n            self.q_b_proj = ColumnParallelLinear(\n                self.q_lora_rank, self.n_heads * self.qk_head_dim\n            )\n        self.kv_a_proj_with_mqa = Linear(\n            self.dim, self.kv_lora_rank + self.qk_rope_head_dim\n        )\n        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank)\n        self.kv_b_proj = ColumnParallelLinear(\n            self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)\n        )\n        self.o_proj = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)\n        self.softmax_scale = self.qk_head_dim**-0.5\n        self.original_seq_len = config.rope_scaling[\"original_max_position_embeddings\"]\n        self.max_seq_len = config.max_position_embeddings\n        if self.max_seq_len > self.original_seq_len:\n            self.mscale = (\n                0.1 * self.mscale * math.log(config.rope_scaling[\"factor\"]) + 1.0\n            )\n            self.softmax_scale = self.softmax_scale * self.mscale * self.mscale\n\n        if config.use_cache:\n            if attn_impl == \"naive\":\n                self.register_buffer(\n                    \"k_cache\",\n                    torch.zeros(\n                        max_batch_size,\n                        self.max_seq_len,\n                        self.n_local_heads,\n                        self.qk_head_dim,\n                    ),\n                    persistent=False,\n                )\n                self.register_buffer(\n                    \"v_cache\",\n                    torch.zeros(\n                        max_batch_size,\n                        self.max_seq_len,\n                        self.n_local_heads,\n                        self.v_head_dim,\n                    ),\n                    persistent=False,\n                )\n            else:\n                self.register_buffer(\n                    \"kv_cache\",\n                    torch.zeros(max_batch_size, self.max_seq_len, self.kv_lora_rank),\n                    persistent=False,\n                )\n                self.register_buffer(\n                    \"pe_cache\",\n                    torch.zeros(\n                        max_batch_size, self.max_seq_len, self.qk_rope_head_dim\n                    ),\n                    persistent=False,\n                )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        start_pos: int,\n        freqs_cis: torch.Tensor,\n        mask: Optional[torch.Tensor],\n        use_cache: bool = False,\n    ):\n        \"\"\"\n        Forward pass for the Multi-Headed Attention Layer (MLA).\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n            start_pos (int): Starting position in the sequence for caching.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor with the same shape as the input.\n        \"\"\"\n        bsz, seqlen, _ = x.size()\n        end_pos = start_pos + seqlen\n        if self.q_lora_rank == 0:\n            q = self.q_proj(x)\n        else:\n            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(x)))\n        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n        q_nope, q_pe = torch.split(\n            q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1\n        )\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n        kv = self.kv_a_proj_with_mqa(x)\n        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n        if attn_impl == \"naive\":\n            q = torch.cat([q_nope, q_pe], dim=-1)\n            kv = self.kv_b_proj(self.kv_a_layernorm(kv))\n            kv = kv.view(\n                bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim\n            )\n            k_nope, v = torch.split(\n                kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1\n            )\n            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)\n            if use_cache:\n                self.k_cache[:bsz, start_pos:end_pos] = k\n                self.v_cache[:bsz, start_pos:end_pos] = v\n                k_to_use = self.k_cache[:bsz, :end_pos]\n                v_to_use = self.v_cache[:bsz, :end_pos]\n            else:\n                k_to_use = k\n                v_to_use = v\n            scores = torch.einsum(\"bshd,bthd->bsht\", q, k_to_use) * self.softmax_scale\n        else:\n            wkv_b = (\n                self.kv_b_proj.weight\n                if self.kv_b_proj.weight_scale_inv is None\n                else weight_dequant(\n                    self.kv_b_proj.weight, self.kv_b_proj.weight_scale_inv, block_size\n                )\n            )\n            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n            q_nope = torch.einsum(\n                \"bshd,hdc->bshc\", q_nope, wkv_b[:, : self.qk_nope_head_dim]\n            )\n            if use_cache:\n                self.kv_cache[:bsz, start_pos:end_pos] = self.kv_a_layernorm(kv)\n                self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n                kv_to_use = self.kv_cache[:bsz, :end_pos]\n                pe_to_use = self.pe_cache[:bsz, :end_pos]\n            else:\n                kv_to_use = self.kv_a_layernorm(kv)\n                pe_to_use = k_pe.squeeze(2)\n            scores = (\n                torch.einsum(\"bshc,btc->bsht\", q_nope, kv_to_use)\n                + torch.einsum(\"bshr,btr->bsht\", q_pe, pe_to_use)\n            ) * self.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = torch.einsum(\"bsht,bthd->bshd\", scores, v_to_use)\n        else:\n            x = torch.einsum(\"bsht,btc->bshc\", scores, kv_to_use)\n            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim :])\n        x = self.o_proj(x.flatten(2))\n        return x\n\n\nclass MLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n\n    Attributes:\n        gate_proj (nn.Module): Linear layer for input-to-hidden transformation.\n        down_proj (nn.Module): Linear layer for hidden-to-output transformation.\n        up_proj (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the MLP layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.gate_proj = ColumnParallelLinear(dim, inter_dim)\n        self.down_proj = RowParallelLinear(inter_dim, dim)\n        self.up_proj = ColumnParallelLinear(dim, inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MLP layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after MLP computation.\n        \"\"\"\n        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Gate(nn.Module):\n    \"\"\"\n    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n\n    Attributes:\n        dim (int): Dimensionality of input features.\n        topk (int): Number of top experts activated for each input.\n        n_groups (int): Number of groups for routing.\n        topk_groups (int): Number of groups to route inputs to.\n        score_func (str): Scoring function ('softmax' or 'sigmoid').\n        route_scale (float): Scaling factor for routing weights.\n        weight (torch.nn.Parameter): Learnable weights for the gate.\n        bias (Optional[torch.nn.Parameter]): Optional bias term for the gate.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initializes the Gate module.\n\n        Args:\n            args (ModelArgs): Model arguments containing gating parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = config.hidden_size\n        self.topk = config.num_experts_per_tok\n        self.n_groups = config.n_group\n        self.topk_groups = config.topk_group\n        self.score_func = config.scoring_func\n        self.route_scale = config.routed_scaling_factor\n        self.weight = nn.Parameter(\n            torch.empty(config.n_routed_experts, config.hidden_size)\n        )\n        self.register_buffer(\n            \"e_score_correction_bias\", torch.zeros(config.n_routed_experts)\n        )\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for the gating mechanism.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices.\n        \"\"\"\n\n        scores = linear(x, self.weight)\n        if self.score_func == \"softmax\":\n            scores = scores.softmax(dim=-1, dtype=torch.float32)\n        else:\n            scores = scores.sigmoid()\n        original_scores = scores\n        if self.e_score_correction_bias is not None:\n            scores = scores + self.e_score_correction_bias\n        if self.n_groups > 1:\n            scores = scores.view(x.size(0), self.n_groups, -1)\n            if self.e_score_correction_bias is None:\n                group_scores = scores.amax(dim=-1)\n            else:\n                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n            indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(\n                1, indices, False\n            )\n            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n        indices = torch.topk(scores, self.topk, dim=-1)[1]\n        weights = original_scores.gather(1, indices)\n        if self.score_func == \"sigmoid\":\n            weights /= weights.sum(dim=-1, keepdim=True)\n        weights *= self.route_scale\n        return weights.type_as(x), indices\n\n\nclass Expert(nn.Module):\n    \"\"\"\n    Expert layer for Mixture-of-Experts (MoE) models.\n\n    Attributes:\n        gate_proj (nn.Module): Linear layer for input-to-hidden transformation.\n        down_proj (nn.Module): Linear layer for hidden-to-output transformation.\n        up_proj (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the Expert layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.gate_proj = Linear(dim, inter_dim)\n        self.down_proj = Linear(inter_dim, dim)\n        self.up_proj = Linear(dim, inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Expert layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert computation.\n        \"\"\"\n        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass MoE(nn.Module):\n    \"\"\"\n    Mixture-of-Experts (MoE) module.\n\n    Attributes:\n        dim (int): Dimensionality of input features.\n        n_routed_experts (int): Total number of experts in the model.\n        n_local_experts (int): Number of experts handled locally in distributed systems.\n        n_activated_experts (int): Number of experts activated for each input.\n        gate (nn.Module): Gating mechanism to route inputs to experts.\n        experts (nn.ModuleList): List of expert modules.\n        shared_experts (nn.Module): Shared experts applied to all inputs.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"\n        Initializes the MoE module.\n\n        Args:\n            args (ModelArgs): Model arguments containing MoE parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = config.hidden_size\n        assert (\n            config.n_routed_experts % world_size == 0\n        ), f\"Number of experts must be divisible by world size (world_size={world_size})\"\n        self.n_routed_experts = config.n_routed_experts\n        self.n_local_experts = config.n_routed_experts // world_size\n        self.n_activated_experts = config.num_experts_per_tok\n        self.experts_start_idx = rank * self.n_local_experts\n        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n        self.gate = Gate(config)\n        self.experts = nn.ModuleList(\n            [\n                (\n                    Expert(config.hidden_size, config.moe_intermediate_size)\n                    if self.experts_start_idx <= i < self.experts_end_idx\n                    else None\n                )\n                for i in range(self.n_routed_experts)\n            ]\n        )\n        self.shared_experts = MLP(\n            config.hidden_size, config.n_shared_experts * config.moe_intermediate_size\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MoE module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert routing and computation.\n        \"\"\"\n        shape = x.size()\n        x = x.view(-1, self.dim)\n        weights, indices = self.gate(x)\n        y = torch.zeros_like(x)\n        counts = torch.bincount(\n            indices.flatten(), minlength=self.n_routed_experts\n        ).tolist()\n        for i in range(self.experts_start_idx, self.experts_end_idx):\n            if counts[i] == 0:\n                continue\n            expert = self.experts[i]\n            idx, top = torch.where(indices == i)\n            y[idx] += expert(x[idx]) * weights[idx, top, None]\n        z = self.shared_experts(x)\n        if world_size > 1:\n            dist.all_reduce(y)\n        return (y + z).view(shape)\n\n\nclass Block(nn.Module):\n    \"\"\"\n    Transformer block combining attention and feed-forward layers.\n\n    Attributes:\n        attn (nn.Module): Attention layer (MLA).\n        ffn (nn.Module): Feed-forward network (MLP or MoE).\n        attn_norm (nn.Module): Layer normalization for attention.\n        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n    \"\"\"\n\n    def __init__(self, layer_id: int, config):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            layer_id (int): Layer index in the transformer.\n            args (ModelArgs): Model arguments containing block parameters.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MLA(config)\n        self.mlp = (\n            MLP(config.hidden_size, config.intermediate_size)\n            if layer_id < config.first_k_dense_replace\n            else MoE(config)\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        start_pos: int,\n        freqs_cis: torch.Tensor,\n        mask: Optional[torch.Tensor],\n        use_cache: bool,\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Transformer block.\n\n        Args:\n            hidden_states (torch.Tensor): Input tensor.\n            start_pos (int): Starting position in the sequence.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor after block computation.\n        \"\"\"\n        hidden_states = hidden_states + self.self_attn(\n            self.input_layernorm(hidden_states), start_pos, freqs_cis, mask, use_cache\n        )\n        hidden_states = hidden_states + self.mlp(\n            self.post_attention_layernorm(hidden_states)\n        )\n        return hidden_states\n\n\nclass DeepseekV3PreTrainedModel(PreTrainedModel):\n    config_class = DeepseekV3Config\n    base_model_prefix = \"model\"\n\n\nclass DeepseekV3Model(DeepseekV3PreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"model\\.layers\\.61.*\"]\n\n    def __init__(self, config: DeepseekV3Config):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = ParallelEmbedding(config.vocab_size, config.hidden_size)\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(config.num_hidden_layers):\n            self.layers.append(Block(layer_id, config))\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.register_buffer(\n            \"freqs_cis\", precompute_freqs_cis(config), persistent=False\n        )\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n        seqlen = tokens.size(1)\n        h = self.embed_tokens(tokens)\n        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n        mask = None\n        if seqlen > 1:\n            mask = torch.full(\n                (seqlen, seqlen), float(\"-inf\"), device=tokens.device\n            ).triu_(1)\n        for layer in self.layers:\n            h = layer(\n                h,\n                start_pos=start_pos,\n                freqs_cis=freqs_cis,\n                mask=mask,\n                use_cache=self.config.use_cache,\n            )\n        # h = self.norm(h)[:, -1]\n        # logits = self.head(h)\n        h = self.norm(h)\n        return h\n\n\nclass DeepseekV3ForCausalLM(DeepseekV3PreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n    using_multi_nodes = False\n\n    def __init__(self, config):\n        super().__init__(config)\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = torch.float8_e4m3fn\n\n        self.model = DeepseekV3Model(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = ColumnParallelLinear(\n            config.hidden_size, config.vocab_size, dtype=torch.get_default_dtype()\n        )\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_path,\n        config,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        using_multi_nodes=False,\n    ):\n        cls.ori_model_path = model_path\n        cls.world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n        if using_multi_nodes:\n            cls.using_multi_nodes = True\n            rank = int(os.getenv(\"RANK\", \"0\"))\n            parent_dir = os.path.dirname(model_path)\n            tp_model_path = os.path.join(parent_dir, f\"ds_ckpt_tp{cls.world_size}\")\n            os.makedirs(tp_model_path, exist_ok=True)\n\n            safetensor_files = glob(os.path.join(tp_model_path, \"*.safetensors\"))\n            if len(safetensor_files) >= cls.world_size:\n                print_info(\"safetensor files already exist\")\n            else:\n                if rank == 0:\n                    convert_ckpt(\n                        model_path,\n                        tp_model_path,\n                        config.n_routed_experts,\n                        cls.world_size,\n                    )\n                dist.barrier()\n            with torch.device(\"cuda\"):\n                model = cls(config)\n            load_model(\n                model,\n                os.path.join(\n                    tp_model_path, f\"model{rank}-mp{cls.world_size}.safetensors\"\n                ),\n            )\n            return model\n        return super().from_pretrained(\n            model_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            trust_remote_code=trust_remote_code,\n            low_cpu_mem_usage=low_cpu_mem_usage,\n        )\n\n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n        \"\"\"\n        Forward pass for the Transformer model.\n\n        Args:\n            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n\n        Returns:\n            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n        \"\"\"\n        h = self.model(tokens, start_pos)\n        logits = self.lm_head(h)\n        if world_size > 1:\n            all_logits = [torch.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = torch.cat(all_logits, dim=-1)\n        last_logit = logits[:, -1]\n        return last_logit, logits\n",
        "angelslim/models/llm/qwen.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nimport torch.nn as nn\n\nfrom ...compressor.quant.core import PTQSaveVllmHF\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass Qwen(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.block_name = \"model.layers\"\n\n    def get_observer_layers(self):\n        names = [\n            \"k_proj\",\n            \"v_proj\",\n            \"q_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"gate_proj\",\n            \"down_proj\",\n        ]\n        obs_layers = [nn.Linear]\n        observer_layers_dict = {}\n        layers_dict = self.find_layers(self.model, layers=obs_layers)\n\n        ignore_layers = self.skip_layer_names()\n        for name, module in layers_dict.items():\n            if name.startswith(self.block_name) and name.split(\".\")[-1] in names:\n                observer_layers_dict[name] = module\n            else:\n                ignore_layers.append(name)\n        self.quant_config.quant_algo_info[\"ignore_layers\"] = ignore_layers\n\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def get_smooth_mapping_layers(self, smooth_config, mappings=None):\n        if mappings is None:\n            mappings = [\n                ([\"q_proj\", \"k_proj\", \"v_proj\"], \"input_layernorm\"),\n                ([\"gate_proj\", \"up_proj\"], \"post_attention_layernorm\"),\n            ]\n        print(f\"smooth mappings={mappings}\")\n        assert len(mappings) == 2\n        assert smooth_config.smooth_first_linears or smooth_config.smooth_last_linears\n        # TODO: support smooth_last_linears\n        return super().get_smooth_mapping_layers(smooth_config, mappings)\n\n    def get_parent_dict(self, observer_layers_dict):\n        parent_mapping = {r\"experts\\.\\d+\": \"experts\"}\n        parent_dict = {}\n        for layer_name in observer_layers_dict.keys():\n            parent_name = layer_name\n            for k, v in parent_mapping.items():\n                parent_name = re.sub(k, v, layer_name)\n            if parent_name != layer_name:\n                parent_dict[layer_name] = parent_name\n        return parent_dict\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            return PTQSaveVllmHF\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/models/model_factory.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Dict, Optional, Type\n\n\nclass SlimModelFactory:\n    \"\"\"Factory for model creation using class decorator registration\"\"\"\n\n    # Registry to store mapping of class names to model classes\n    registry: Dict[str, Type] = {}\n    series_registry: Dict[str, str] = {}\n\n    ALLOWED_SERIES = (\"LLM\", \"VLM\", \"Diffusion\")\n\n    @classmethod\n    def register(cls, model_class: Type) -> Type:\n        \"\"\"Class decorator for automatic registration\"\"\"\n        class_name = model_class.__name__\n        if class_name in cls.registry:\n            raise ValueError(f\"Model class '{class_name}' is already registered\")\n        cls.registry[class_name] = model_class\n\n        module_path = model_class.__module__.lower()\n        if \"llm\" in module_path:\n            series = \"LLM\"\n        elif \"vlm\" in module_path:\n            series = \"VLM\"\n        elif \"diffusion\" in module_path:\n            series = \"Diffusion\"\n        else:\n            raise ValueError(\n                f\"model_class '{class_name}' is not in a valid series: {cls.ALLOWED_SERIES}\"  # noqa: E501\n            )\n\n        cls.series_registry[class_name] = series\n\n        return model_class\n\n    @classmethod\n    def create(\n        cls,\n        model_name: str,\n        model: Optional[Any] = None,\n        deploy_backend: str = \"vllm\",\n        **kwargs,\n    ) -> Any:\n        \"\"\"Create an instance of the specified model\"\"\"\n        if model_name not in cls.registry:\n            available = \", \".join(cls.registry.keys())\n            raise ValueError(\n                f\"Unknown model: '{model_name}'. Available models: {available}\"\n            )\n\n        return cls.registry[model_name](\n            model=model,\n            deploy_backend=deploy_backend,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_model_class(cls, model_name: str) -> Type:\n        \"\"\"Get the class for a registered model\"\"\"\n        if model_name not in cls.registry:\n            available = \", \".join(cls.registry.keys())\n            raise ValueError(\n                f\"Unknown model: '{model_name}'. Available models: {available}\"\n            )\n        return cls.registry[model_name]\n\n    @classmethod\n    def get_registered_models(cls) -> Dict[str, Type]:\n        \"\"\"Get all registered model classes\"\"\"\n        return cls.registry.copy()\n\n    @classmethod\n    def get_series_by_models(cls, model_name: str) -> str:\n        \"\"\"Get all model classes for a specific series\"\"\"\n        if model_name not in cls.registry:\n            available = \", \".join(cls.registry.keys())\n            raise ValueError(\n                f\"Unknown model: '{model_name}'. Available models: {available}\"\n            )\n        return cls.series_registry.get(model_name, [])\n",
        "angelslim/models/vlm/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom .qwen_vl import QwenVL  # noqa: F401\n",
        "angelslim/models/vlm/qwen_vl.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoProcessor,\n    AutoTokenizer,\n    Qwen2_5_VLForConditionalGeneration,\n)\n\nfrom ...compressor.quant.core import PTQVLMSaveVllmHF\nfrom ...utils import print_info\nfrom ..base_model import BaseLLMModel\nfrom ..model_factory import SlimModelFactory\n\n\n@SlimModelFactory.register\nclass QwenVL(BaseLLMModel):\n    def __init__(\n        self,\n        model=None,\n        deploy_backend=\"vllm\",\n    ):\n        super().__init__(\n            model=model,\n            deploy_backend=deploy_backend,\n        )\n        self.modal_type = \"VLM\"\n        self.block_name = \"model.language_model.layers\"\n        self.vit_block_name = \"model.visual.blocks\"\n        self.pre_transformer_module_names = [\n            \"visual\",\n            \"language_model.embed_tokens\",\n            \"language_model.norm\",\n            \"language_model.rotary_emb\",\n        ]\n\n    def from_pretrained(\n        self,\n        model_path,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        use_cache=False,\n        using_multi_nodes=False,\n    ):\n        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            model_path,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            trust_remote_code=trust_remote_code,\n            low_cpu_mem_usage=low_cpu_mem_usage,\n            use_cache=use_cache,\n        )\n\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=trust_remote_code\n        )\n\n        # Load processor\n        self.processor = AutoProcessor.from_pretrained(\n            model_path, trust_remote_code=trust_remote_code\n        )\n\n    def get_observer_layers(self):\n        names = [\n            \"k_proj\",\n            \"v_proj\",\n            \"q_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"gate_proj\",\n            \"down_proj\",\n        ]\n\n        if hasattr(self.quant_config, \"quant_vit\") and self.quant_config.quant_vit:\n            vit_names = [\"qkv\", \"proj\"]\n            names.extend(vit_names)\n\n        obs_layers = [nn.Linear]\n        observer_layers_dict = {}\n        layers_dict = self.find_layers(self.model, layers=obs_layers)\n\n        ignore_layers = self.skip_layer_names()\n        for name, module in layers_dict.items():\n            block_condition = name.startswith(self.block_name) or (\n                hasattr(self.quant_config, \"quant_vit\")\n                and self.quant_config.quant_vit\n                and name.startswith(self.vit_block_name)\n            )\n            if block_condition and name.split(\".\")[-1] in names:\n                observer_layers_dict[name] = module\n            else:\n                ignore_layers.append(name)\n        self.quant_config.quant_algo_info[\"ignore_layers\"] = ignore_layers\n\n        if self.quant_config.custom_observe_layers_names != \"default\":\n            for custom_observe_name in self.quant_config.custom_observe_layers_names:\n                for default_name in observer_layers_dict.keys():\n                    if custom_observe_name not in default_name:\n                        observer_layers_dict.pop(default_name)\n        return observer_layers_dict\n\n    def model_forward(self, dataloader, **kwargs):\n        self.model.use_cache = False\n\n        calibrated_cnt = 0\n        if (\n            \"gptq\" in self.quant_config.quant_algo\n            or \"awq\" in self.quant_config.quant_algo\n        ):\n            device = \"cuda:0\"\n        else:\n            device = self.model.device\n        print_info(f\"device is {device}\")\n        if dataloader is not None:\n            with torch.no_grad():\n                for batch in tqdm(\n                    dataloader, desc=\"calibrating...\", total=len(dataloader)\n                ):\n                    inputs = {\n                        \"input_ids\": batch[\"input_ids\"].to(device),\n                        \"attention_mask\": batch[\"attention_mask\"].to(device),\n                        \"pixel_values\": batch[\"pixel_values\"].to(device),\n                        \"image_grid_thw\": batch[\"image_grid_thw\"].to(device),\n                    }\n                    inputs = {k: v.to(device) for k, v in inputs.items()}\n                    inputs[\"use_cache\"] = False\n                    labels = batch[\"labels\"].to(device)\n                    attention_mask = batch[\"attention_mask\"].to(device)\n                    try:\n                        outputs = self.model(**inputs)\n                        logits = outputs.logits.float()\n\n                        loss = F.cross_entropy(\n                            logits.view(-1, logits.size(-1)),\n                            labels.view(-1),\n                            reduction=\"none\",\n                        )\n\n                        attention_mask = (\n                            attention_mask.view(-1).to(logits.device).float()\n                        )\n                        loss = loss * attention_mask\n                        avg_loss = loss.mean()\n                        ppl = torch.exp(avg_loss)\n\n                        print_info(f\"ppl is : {ppl:.4f}\")\n\n                        calibrated_cnt += 1\n                    except ValueError:\n                        calibrated_cnt += 1\n                        pass\n\n    def get_save_func(self):\n        if self.deploy_backend in [\"vllm\", \"huggingface\"]:\n            return PTQVLMSaveVllmHF\n        else:\n            raise NotImplementedError(\n                f\"deploy_backend {self.deploy_backend} is not supported for saving.\"\n            )\n",
        "angelslim/tokenizer/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .kimi_k2 import TikTokenTokenizer  # noqa: F401\n",
        "angelslim/tokenizer/kimi_k2.py": "# flake8: noqa: E501\nimport os\nfrom logging import getLogger\nfrom pathlib import Path\nfrom shutil import copyfile\nfrom typing import Dict, Iterator, List, Optional, Tuple, Union, cast\n\nimport tiktoken\nfrom tiktoken.load import load_tiktoken_bpe\nfrom tokenizers import AddedToken\nfrom transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nlogger = getLogger(__name__)\nVOCAB_FILES_NAMES = {\"vocab_file\": \"tiktoken.model\"}\n\n\nclass TikTokenTokenizer(PreTrainedTokenizer):\n    \"\"\"\n    Tokenizing and encoding/decoding text using the Tiktoken tokenizer. See megatron/tokenizer/tiktoken_tokenizer.py.\n\n    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n    this superclass for more information regarding those methods.\n\n    Args:\n        vocab_file (`str`):\n            The path to the Tiktoken model file.\n        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<|begin_of_text|>\",`):\n            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<|end_of_text|>\"`):\n            The end of sequence token.\n        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<|reserved_special_token_249|>\"`):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead. The second to last item in special_tokens.\n        pad_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<|reserved_special_token_250|>\"`):\n            The token used for padding, for example when batching sequences of different lengths.\n        additional_special_tokens (list of `str`, *optional*):\n            A tuple or a list of additional tokens, which will be marked as `special`, meaning that they will be\n            skipped when decoding if `skip_special_tokens` is set to `True`.\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    special_tokens: Dict[str, int]\n\n    num_reserved_special_tokens = 256\n\n    pat_str = \"|\".join(\n        [\n            r\"\"\"[\\p{Han}]+\"\"\",\n            r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}&&[^\\p{Han}]]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}&&[^\\p{Han}]]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n            r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}&&[^\\p{Han}]]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}&&[^\\p{Han}]]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?\"\"\",\n            r\"\"\"\\p{N}{1,3}\"\"\",\n            r\"\"\" ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*\"\"\",\n            r\"\"\"\\s*[\\r\\n]+\"\"\",\n            r\"\"\"\\s+(?!\\S)\"\"\",\n            r\"\"\"\\s+\"\"\",\n        ]\n    )\n\n    def __init__(\n        self,\n        vocab_file,\n        bos_token: Union[str, AddedToken] = \"[BOS]\",\n        eos_token: Union[str, AddedToken] = \"[EOS]\",\n        unk_token: Union[str, AddedToken, None] = None,\n        pad_token: Union[str, AddedToken, None] = None,\n        additional_special_tokens: List[str] = None,\n        added_tokens_decoder: Optional[dict] = None,\n        **kwargs,\n    ):\n        assert os.path.isfile(vocab_file), vocab_file\n\n        if additional_special_tokens is None:\n            additional_special_tokens = [\n                \"<|im_end|>\",\n                \"<|im_user|>\",\n                \"<|im_assistant|>\",\n                \"<|start_header_id|>\",\n                \"<|end_header_id|>\",\n                \"[EOT]\",\n                \"<|im_system|>\",\n                \"<|im_middle|>\",\n            ]\n\n        special_tokens_mapping = {\n            i: added_tokens_decoder[i].content for i in added_tokens_decoder\n        }\n\n        self.vocab_file = vocab_file\n        mergeable_ranks = load_tiktoken_bpe(vocab_file)\n        num_base_tokens = len(mergeable_ranks)\n        self.special_tokens = {\n            special_tokens_mapping.get(i, f\"<|reserved_token_{i}|>\"): i\n            for i in range(\n                num_base_tokens, num_base_tokens + self.num_reserved_special_tokens + 2\n            )\n        }\n\n        self.model = tiktoken.Encoding(\n            name=Path(vocab_file).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        logger.info(f\"Reloaded tiktoken model from {vocab_file}\")\n\n        self.n_words: int = self.model.n_vocab\n        # BOS / EOS token IDs\n        self.bos_id: int = self.special_tokens[str(bos_token)]\n        self.eos_id: int = self.special_tokens[str(eos_token)]\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n\n        self.pad_id: int = self.special_tokens[str(pad_token)]\n        self.unk_id: int = self.special_tokens[str(unk_token)]\n\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n\n        self.decoder = {}\n        for i in range(self.n_words):\n            # Taken from https://gist.github.com/xenova/a452a6474428de0182b17605a98631ee\n            decoding = \"\".join(\n                [\n                    self.byte_encoder[ord(char)]\n                    for char in self.model.decode_single_token_bytes(i).decode(\n                        \"latin-1\"\n                    )\n                ]\n            )\n            self.decoder[i] = decoding\n\n        self.encoder = {}\n        for i in range(self.n_words):\n            if i in self.decoder:\n                self.encoder[self.decoder[i]] = i\n\n        super().__init__(\n            bos_token=bos_token,\n            eos_token=eos_token,\n            unk_token=unk_token,\n            pad_token=pad_token,\n            additional_special_tokens=additional_special_tokens,\n            **kwargs,\n        )\n        self.all_special_ids_set = set(self.all_special_ids)\n\n    def encode(\n        self, text: str, allow_special_tokens: bool = True, **kwargs\n    ) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            text (str): The input string to be encoded.\n\n        Returns:\n            list[int]: A list of token IDs.\n        \"\"\"\n        # If there are other args, we should call super().encode because there are a lot of code\n        # to handle those args. supper().encode finally will call _tokenize and _convert_token_to_id.\n        # NOTE: our encode method is not compatible with the super().encode method,\n        #   e.g. split_special_tokens' default is True in our encode method.\n        if len(kwargs) > 0:\n            logger.warning(f\"Calling super().encode with {kwargs}\")\n            return super().encode(text, **kwargs)\n\n        assert type(text) is str\n\n        # The tiktoken tokenizer can handle <=400k chars without\n        # pyo3_runtime.PanicException.\n        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n\n        # https://github.com/openai/tiktoken/issues/195\n        # Here we iterate over subsequences and split if we exceed the limit\n        # of max consecutive non-whitespace or whitespace characters.\n        MAX_NO_WHITESPACES_CHARS = 25_000\n\n        texts = self.pre_tokenizer_process(text)\n\n        all_substrs = []\n        for text in texts:\n            substrs = (\n                substr\n                for i in range(0, len(text), TIKTOKEN_MAX_ENCODE_CHARS)\n                for substr in self._split_whitespaces_or_nonwhitespaces(\n                    text[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n                )\n            )\n            all_substrs.extend(substrs)\n\n        t: List[int] = []\n        for substr in all_substrs:\n            if allow_special_tokens:\n                t.extend(\n                    # we should consider special token as a common token\n                    self.model.encode(\n                        substr,\n                        allowed_special=\"all\",\n                    )\n                )\n            else:\n                t.extend(\n                    # we should consider special token as a common token\n                    self.model.encode(\n                        substr,\n                        disallowed_special=(),\n                    )\n                )\n\n        return t\n\n    def decode(self, token_ids: Union[int, List[int]], **kwargs) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            token_ids (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        # If there are other args, we should call super().decode because there are a lot of code\n        # to handle those args. supper().encode finally will call convert_tokens_to_string and _convert_id_to_token.\n        if len(kwargs) > 0:\n            return super().decode(token_ids, **kwargs)\n\n        if type(token_ids) is int:\n            token_ids = [token_ids]\n\n        return self.model.decode(cast(List[int], token_ids))\n\n    @staticmethod\n    def _split_whitespaces_or_nonwhitespaces(\n        s: str, max_consecutive_slice_len: int\n    ) -> Iterator[str]:\n        \"\"\"\n        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n        consecutive whitespaces or consecutive non-whitespaces.\n        \"\"\"\n        current_slice_len = 0\n        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n        slice_start = 0\n\n        for i in range(len(s)):\n            is_now_space = s[i].isspace()\n\n            if current_slice_is_space ^ is_now_space:\n                current_slice_len = 1\n                current_slice_is_space = is_now_space\n            else:\n                current_slice_len += 1\n                if current_slice_len > max_consecutive_slice_len:\n                    yield s[slice_start:i]\n                    slice_start = i\n                    current_slice_len = 1\n        yield s[slice_start:]\n\n    def pre_tokenizer_process(self, text: str) -> List[str]:\n        \"\"\"\n        pre-tokenizes the input text into a list of tokens.\n        This method is used to split the input text into smaller chunks for internal processing.\n        \"\"\"\n        return [text]\n\n    \"\"\" ----- Below are the abstract methods required by PreTrainedTokenizer ----- \"\"\"\n\n    @property\n    def vocab_size(self) -> int:\n        return self.n_words\n\n    def get_vocab(self) -> Dict[str, int]:\n        return self.encoder\n\n    def _tokenize(self, text: str, **kwargs) -> List[str]:\n        return [self.decoder[t] for t in self.encode(text)]\n\n    def _convert_token_to_id(self, token: str) -> int:\n        return self.encoder.get(token, self.unk_id)\n\n    def _convert_id_to_token(self, index: int) -> str:\n        return self.decoder.get(index)\n\n    @staticmethod\n    def clean_up_tokenization(out_string: str) -> str:\n        return out_string\n\n    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n        text = \"\".join(tokens)\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n            \"utf-8\", \"replace\"\n        )\n        return text\n\n    def save_vocabulary(\n        self, save_directory: str, filename_prefix: Optional[str] = None\n    ) -> Tuple[str]:\n        if not os.path.isdir(save_directory):\n            raise ValueError(\n                f\"vocabulary path ({save_directory}) should be a directory\"\n            )\n        out_vocab_file = os.path.join(\n            save_directory,\n            (filename_prefix + \"-\" if filename_prefix else \"\")\n            + VOCAB_FILES_NAMES[\"vocab_file\"],\n        )\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(\n            out_vocab_file\n        ) and os.path.isfile(self.vocab_file):\n            copyfile(self.vocab_file, out_vocab_file)\n\n        return (out_vocab_file,)\n",
        "angelslim/utils/__init__.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config_parser import SlimConfigParser  # noqa: F401\nfrom .default_compress_config import *  # noqa: F401 F403\nfrom .utils import common_prefix  # noqa: F401\nfrom .utils import find_parent_layer_and_sub_name  # noqa: F401\nfrom .utils import get_best_device  # noqa: F401\nfrom .utils import get_op_by_name  # noqa: F401\nfrom .utils import get_op_name  # noqa: F401\nfrom .utils import get_package_info  # noqa: F401\nfrom .utils import get_tensor_item  # noqa: F401\nfrom .utils import get_yaml_prefix_simple  # noqa: F401\nfrom .utils import print_info  # noqa: F401\nfrom .utils import set_op_by_name  # noqa: F401\n",
        "angelslim/utils/config_parser.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\nimport yaml\n\nfrom .utils import get_hf_config\n\n\n@dataclass\nclass GlobalConfig:\n    \"\"\"\n    Global configuration for LLM compression.\n    Attributes:\n        save_path: Directory to save compressed models\n        max_seq_length: Maximum sequence length for calibration data\n        hidden_size: Hidden size of the model\n        model_arch_type: Model architecture type\n        deploy_backend: Backend for deployment (e.g., \"vllm\", \"huggingface\")\n    \"\"\"\n\n    save_path: str = field(default=\"./output\")\n    # Shared max_seq_length configuration\n    max_seq_length: int = field(default=2048)\n    hidden_size: int = field(default=2048)\n    model_arch_type: str = field(default=None)\n    deploy_backend: str = field(default=\"vllm\")\n\n    def update(self, model_path: str = None, max_seq_length: int = None):\n        \"\"\"\n        Update global configuration with model and dataset properties.\n\n        Args:\n            model_path: Path to the model for extracting hidden size and architecture\n            max_seq_length: Maximum sequence length for the model\n\n        Returns:\n            Updated GlobalConfig object\n        \"\"\"\n        if model_path:\n            self.set_model_hidden_size(model_path)\n            self.set_model_arch_type(model_path)\n        if max_seq_length:\n            self.set_max_seq_length(max_seq_length)\n\n    def set_max_seq_length(self, value: int):\n        self.max_seq_length = value\n\n    def get_max_seq_length(self) -> int:\n        return self.max_seq_length\n\n    def set_model_hidden_size(self, model_path) -> int:\n        json_data = get_hf_config(model_path)\n        self.hidden_size = json_data[\"hidden_size\"]\n\n    def set_model_arch_type(self, model_path) -> str:\n        json_data = get_hf_config(model_path)\n        self.model_arch_type = json_data[\"model_type\"]\n\n\n@dataclass\nclass ModelConfig:\n    \"\"\"\n    Configuration for the LLM model to be compressed.\n\n    Attributes:\n        name: Model name (e.g., \"Qwen3-8B\")\n        model_path: Path to model weights/directory\n        trust_remote_code: Trust remote code for HuggingFace\n        torch_dtype: PyTorch dtype for loading (e.g., \"bfloat16\")\n        device_map: Strategy for device placement (e.g., \"auto\", \"cpu\", \"cuda\")\n        low_cpu_mem_usage: Use low memory loading for large models\n        use_cache: Whether to use cache during model loading\n        cache_dir: Directory for caching model files\n    \"\"\"\n\n    name: str\n    model_path: str\n    trust_remote_code: bool = field(default=True)\n    torch_dtype: str = field(default=\"auto\")\n    device_map: str = field(default=\"auto\")\n    low_cpu_mem_usage: bool = field(default=True)\n    use_cache: bool = field(default=False)\n    cache_dir: Optional[str] = field(default=None)\n\n\n@dataclass\nclass DatasetConfig:\n    \"\"\"\n    Configuration for LLM dataset used in compression.\n\n    Attributes:\n        name: Dataset identifier (e.g., \"wikitext\")\n        data_path: Directory path to dataset files\n        max_length: Context length for processing\n        max_samples: Maximum samples for calibration\n        batch_size: Batch size for processing\n        shuffle: whether to shuffle dataset\n    \"\"\"\n\n    name: str\n    data_path: str\n    max_seq_length: int = field(default=2048)\n    num_samples: int = field(default=256)\n    batch_size: int = field(default=1)\n    shuffle: bool = field(default=False)\n\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"\n    Quantization-specific configurations for LLMs.\n\n    Attributes:\n        name: Quantization method (e.g., \"awq\", \"gptq\")\n        bits: Quantization bit-width (4/8)\n        group_size: Group size for grouped quantization\n        quant_method: Algorithm used for quantization\n        modules_to_quantize: List of module types to quantize\n        ignore_layers: List of layer names to skip\n    \"\"\"\n\n    name: str = field(default=\"fp8_dynamic\")\n    bits: int = field(default=8)\n    quant_method: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"weight\": \"per-tensor\",\n            \"activation\": \"per-tensor\",\n            \"group_size\": -1,\n        }\n    )\n    quant_helpers: List[str] = field(default_factory=list)\n    smooth_alpha: float = field(default=0.5)\n    low_memory: bool = field(default=False)\n    modules_to_quantize: List[str] = field(default_factory=list)\n    zero_point: bool = field(default=True)\n    mse_range: bool = field(default=False)\n    ignore_layers: List[str] = field(default_factory=list)\n    quant_analyse: bool = field(default=False)\n    quant_vit: bool = field(default=False)\n\n\n@dataclass\nclass CacheConfig:\n    \"\"\"\n    Configuration for caching in LLM compression.\n\n    Attributes:\n        name: Cache method (e.g., \"DeepCache\")\n        no_cache_steps: List of steps where caching is disabled\n    \"\"\"\n\n    name: str = field(default=\"DeepCache\")\n    use_cache_helper: bool = field(default=False)\n    no_cache_steps: List[int] = field(default_factory=list)\n    no_cache_block_id: Dict[str, List[int]] = field(\n        default_factory=lambda: {\n            \"single\": [],\n            \"multi\": [],\n        }\n    )\n    cnt: int = field(default=0)\n    num_steps: int = field(default=50)\n    rel_l1_thresh: float = field(default=0.6)  # Threshold for relative L1 distance\n    # Accumulated distance for caching decisions\n    accumulated_rel_l1_distance: float = field(default=0.0)\n\n\n@dataclass\nclass CompressionConfig:\n    \"\"\"\n    Compression configurations container for LLM.\n\n    Attributes:\n        method: Selected compression method\n        quantization: Quantization configurations\n        speculative_decoding: Training settings for quantization-aware compression\n    \"\"\"\n\n    name: str\n    quantization: Optional[QuantizationConfig] = None\n    cache: Optional[CacheConfig] = None\n    # speculative_decoding: Optional[SpeculativeDecodingConfig] = None\n\n    @property\n    def need_dataset(self) -> bool:\n        return (\n            \"dynamic\" not in self.quantization.name\n            or \"smooth\" in self.quantization.quant_helpers\n        )\n\n\n@dataclass\nclass InferenceConfig:\n    \"\"\"Configuration for inference parameters.\n    Attributes:\n        height: Height of the generated image\n        width: Width of the generated image\n        guidance_scale: Guidance scale for inference\n        num_inference_steps: Number of inference steps\n        max_sequence_length: Maximum sequence length for the model\n        seed: Random seed for reproducibility\n    \"\"\"\n\n    height: Optional[int]\n    width: Optional[int]\n    guidance_scale: Optional[float]\n    num_inference_steps: Optional[int]\n    max_sequence_length: Optional[float]\n    seed: Optional[int]\n\n\n@dataclass\nclass FullConfig:\n    \"\"\"\n    Top-level configuration container for LLM compression.\n\n    Attributes:\n        model_config: Model configuration parameters\n        compression_config: Compression configuration parameters\n        dataset_config: Dataset configuration parameters\n    \"\"\"\n\n    model_config: ModelConfig\n    compression_config: CompressionConfig\n    dataset_config: DatasetConfig\n    global_config: GlobalConfig\n    infer_config: InferenceConfig\n\n\nclass SlimConfigParser:\n    \"\"\"\n    Parser for LLM compression YAML configurations.\n\n    Methods:\n        parse: Load and validate configuration from YAML\n    \"\"\"\n\n    def __init__(self):\n        # Supported compression methods\n        self.supported_methods = [\"PTQ\", \"QAT\", \"Cache\", \"speculative_decoding\"]\n        # Supported quantization methods\n        self.supported_quant_methods = [\n            \"fp8_static\",\n            \"fp8_dynamic\",\n            \"int4_awq\",\n            \"int4_gptq\",\n            \"int8_dynamic\",\n            \"w4a8_fp8\",\n        ]\n        # Supported speculative decoding methods\n        self.supported_speculative_decoding_methods = [\"EAGLE\", \"EAGLE2\", \"EAGLE3\"]\n\n    def parse(self, yaml_path: str) -> FullConfig:\n        \"\"\"\n        Load and parse YAML configuration file for LLM compression.\n\n        Args:\n            yaml_path: Path to YAML configuration file\n\n        Returns:\n            Fully populated FullConfig object\n\n        Raises:\n            ValueError: On invalid configuration or unsupported methods\n        \"\"\"\n        try:\n            with open(yaml_path, \"r\") as f:\n                config_dict = yaml.safe_load(f)\n        except FileNotFoundError:\n            print(f\"Warning: Config file '{yaml_path}' not found. Using defaults.\")\n            return self.get_default_config()\n\n        return self._get_configs(config_dict)\n\n    def _get_configs(self, config_dict: dict) -> FullConfig:\n        # Parse base configurations\n        model_dict = config_dict.get(\"model\", {})\n        if not model_dict:\n            raise ValueError(\"Missing 'model' section in configuration\")\n        # Initialize model config\n        model_conf = ModelConfig(**model_dict)\n\n        dataset_conf = None\n        if \"dataset\" in config_dict:\n            dataset_dict = config_dict[\"dataset\"]\n            dataset_conf = DatasetConfig(**dataset_dict)\n\n        # Get compression section\n        compression_dict = config_dict.get(\"compression\", {})\n        if not compression_dict:\n            raise ValueError(\"Missing 'compression' section in configuration\")\n\n        # Validate compression method\n        compress_name = compression_dict.get(\"name\")\n        if compress_name not in self.supported_methods:\n            raise ValueError(\n                f\"Unsupported compression method: {compress_name}. \"\n                f\"Supported methods: {self.supported_methods}\"\n            )\n\n        # Initialize compression config\n        compression_conf = CompressionConfig(name=compress_name)\n\n        # Parse method-specific configurations\n        if compress_name in [\"PTQ\", \"QAT\"]:\n            # Validate quantization type\n            quant_dict = compression_dict.get(\"quantization\", {})\n            quant_method = quant_dict.get(\"name\")\n            if quant_method not in self.supported_quant_methods:\n                raise ValueError(\n                    f\"Unsupported quantization method: {quant_method}. \"\n                    f\"Supported: {self.supported_quant_methods}\"\n                )\n\n            # Parse quantization config\n            compression_conf.quantization = QuantizationConfig(**quant_dict)\n        elif compress_name == \"Cache\":\n            # Parse cache configuration\n            cache_dict = compression_dict.get(\"cache\", {})\n            compression_conf.cache = CacheConfig(**cache_dict)\n        else:\n            raise ValueError(\n                f\"Unsupported compression method: {compress_name}. \"\n                f\"Supported methods: {self.supported_methods}\"\n            )\n\n        # Global properties\n        global_config = self._get_global_config(config_dict, model_conf, dataset_conf)\n\n        # Inference configuration\n        inference_conf = None\n        if \"inference\" in config_dict:\n            inference_dict = config_dict[\"inference\"]\n            inference_conf = InferenceConfig(**inference_dict)\n\n        return FullConfig(\n            model_config=model_conf,\n            compression_config=compression_conf,\n            dataset_config=dataset_conf,\n            global_config=global_config,\n            infer_config=inference_conf,\n        )\n\n    def _get_global_config(\n        self, config_dict, model_conf, dataset_conf=None\n    ) -> GlobalConfig:\n        \"\"\"\n        Extract global configuration settings from the provided dictionary.\n\n        Args:\n            config_dict: Dictionary containing configuration parameters\n\n        Returns:\n            GlobalConfig object with populated fields\n        \"\"\"\n        global_dict = config_dict.get(\"global\", {})\n        global_config = GlobalConfig(**global_dict)\n        return global_config\n\n    @staticmethod\n    def get_default_config() -> FullConfig:\n        \"\"\"Return a default configuration for Qwen model\"\"\"\n        model_config = ModelConfig(\n            name=\"Qwen\",\n            model_path=\"Qwen/Qwen2.5-7B-Instruct\",\n            trust_remote_code=True,\n        )\n        # Global properties\n        global_config = GlobalConfig()\n        global_config.set_model_hidden_size(model_config.model_path)\n        global_config.set_model_arch_type(model_config.model_path)\n        return FullConfig(\n            model_config=model_config,\n            compression_config=CompressionConfig(\n                name=\"PTQ\",\n                quantization=QuantizationConfig(\n                    name=\"fp8_dynamic\",\n                    bits=8,\n                    ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n                ),\n            ),\n            dataset_config=None,\n            global_config=global_config,\n        )\n\n\ndef print_config(config, indent=0):\n    \"\"\"\n    Print the configuration in a structured YAML-like format\n\n    Args:\n        config: Configuration object to print\n        indent: Current indentation level\n    \"\"\"\n    prefix = \" \" * indent\n    next_indent = indent + 2\n\n    # Special handling for FullLLMConfig\n    if (\n        hasattr(config, \"model_config\")\n        and hasattr(config, \"compression_config\")\n        and hasattr(config, \"dataset_config\")\n        and hasattr(config, \"global_config\")\n        and hasattr(config, \"infer_config\")\n    ):\n        print(f\"{prefix}model:\")\n        print_config(config.model_config, next_indent)\n\n        print(f\"{prefix}compression:\")\n        print_config(config.compression_config, next_indent)\n\n        print(f\"{prefix}dataset:\")\n        if config.dataset_config:\n            print_config(config.dataset_config, next_indent)\n        else:\n            print(f\"{prefix}None\")\n\n        print(f\"{prefix}Global:\")\n        if config.global_config:\n            print_config(config.global_config, next_indent)\n        else:\n            print(f\"{prefix}None\")\n\n        print(f\"{prefix}Inference:\")\n        if config.infer_config:\n            print_config(config.infer_config, next_indent)\n        else:\n            print(f\"{prefix}None\")\n        return\n\n    # Handle dataclass instances\n    if hasattr(config, \"__dataclass_fields__\"):\n        for _field in config.__dataclass_fields__:\n            value = getattr(config, _field)\n            # Skip uninteresting default values\n            if value is None or (isinstance(value, list) and len(value) == 0):\n                continue\n\n            # Special case for models with path in name\n            if _field == \"name\" and hasattr(config, \"path\") and config.path != \"\":\n                value = f\"{value} @ {config.path}\"\n\n            # Print field with appropriate formatting\n            if hasattr(value, \"__dataclass_fields__\"):\n                print(f\"{prefix}{_field}:\")\n                print_config(value, next_indent)\n            elif isinstance(value, list):\n                print(f\"{prefix}{_field}:\")\n                for item in value:\n                    print(f\"{prefix}- {item}\")\n            elif isinstance(value, bool):\n                print(f\"{prefix}{_field}: {'true' if value else 'false'}\")\n            else:\n                print(f\"{prefix}{_field}: {value}\")\n        return\n\n    # Fallback for other types\n    print(f\"{prefix}{str(config)}\")\n\n\n# Example usage\nif __name__ == \"__main__\":\n    parser = SlimConfigParser()\n    config = parser.parse(\"llm_compression_config.yaml\")\n\n    # Access configuration values\n    print(f\"Compressing model: {config.model_config.name}\")\n    print(f\"Device map: {config.model_config.device_map}\")\n\n    comp_conf = config.compression_config\n    print(f\"\\nCompression Method: {comp_conf.name}\")\n\n    if comp_conf.name == \"quantization\":\n        quant_conf = comp_conf.quantization\n        print(f\"Quantization Type: {quant_conf.name}\")\n        print(f\"Bit Width: {quant_conf.bits}-bit\")\n\n    _dataset_conf = config.dataset_config\n    if _dataset_conf:\n        print(f\"\\nDataset: {_dataset_conf.name}\")\n        print(f\"Max Context Length: {_dataset_conf.max_seq_length}\")\n",
        "angelslim/utils/default_compress_config.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config_parser import CompressionConfig, GlobalConfig, QuantizationConfig\n\n__all__ = [\n    \"default_fp8_dynamic_config\",\n    \"default_fp8_static_config\",\n    \"default_int8_dynamic_config\",\n    \"default_int4_gptq_config\",\n    \"default_int4_awq_config\",\n]\n\n\ndef default_fp8_dynamic_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"fp8_dynamic\",\n                bits=8,\n                quant_method={\"weight\": \"per-tensor\", \"activation\": \"per-tensor\"},\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n\n\ndef default_fp8_static_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"fp8_static\",\n                bits=8,\n                quant_method={\"weight\": \"per-tensor\", \"activation\": \"per-tensor\"},\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n\n\ndef default_int8_dynamic_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"int8_dynamic\",\n                bits=8,\n                quant_method={\"weight\": \"per-channel\", \"activation\": \"per-token\"},\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n\n\ndef default_int4_gptq_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"int4_gptq\",\n                bits=4,\n                quant_method={\"weight\": \"per-group\", \"group_size\": 128},\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n\n\ndef default_int4_awq_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"int4_awq\",\n                bits=4,\n                quant_method={\n                    \"weight\": \"per-group\",\n                    \"group_size\": 128,\n                    \"zero_point\": True,\n                    \"mse_range\": False,\n                },\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n\n\ndef default_w4a8_fp8_static_config() -> dict:\n    \"\"\"\n    Returns a default configuration dictionary for model compression.\n\n    This configuration includes global settings and specific compression parameters.\n    \"\"\"\n    return {\n        \"global_config\": GlobalConfig(),\n        \"compress_config\": CompressionConfig(\n            name=\"PTQ\",\n            quantization=QuantizationConfig(\n                name=\"w4a8_fp8\",\n                bits=4,\n                quant_method={\n                    \"weight\": \"per-group\",\n                    \"group_size\": 128,\n                    \"activation\": \"per-tensor\",\n                },\n                ignore_layers=[\"lm_head\", \"model.embed_tokens\"],\n            ),\n        ),\n    }\n",
        "angelslim/utils/utils.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport importlib.metadata\nimport json\nimport os\nimport subprocess\nfrom itertools import takewhile\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nfrom transformers.utils.hub import cached_file\n\n\ndef get_op_name(module, op):\n    # get the name of the op relative to the module\n    for name, m in module.named_modules():\n        if m is op:\n            return name\n    raise ValueError(f\"Cannot find op {op} in module {module}\")\n\n\ndef get_op_by_name(module, op_name):\n    # get the op by its name relative to the module\n    for name, m in module.named_modules():\n        if name == op_name:\n            return m\n    raise ValueError(f\"Cannot find op {op_name} in module {module}\")\n\n\ndef set_op_by_name(layer, name, new_module):\n    levels = name.split(\".\")\n    if len(levels) > 1:\n        mod_ = layer\n        for l_idx in range(len(levels) - 1):\n            if levels[l_idx].isdigit():\n                mod_ = mod_[int(levels[l_idx])]\n            else:\n                mod_ = getattr(mod_, levels[l_idx])\n        setattr(mod_, levels[-1], new_module)\n    else:\n        setattr(layer, name, new_module)\n\n\ndef find_parent_layer_and_sub_name(model, name):\n    last_idx = 0\n    idx = 0\n    parent_layer = model\n    while idx < len(name):\n        if name[idx] == \".\":\n            sub_name = name[last_idx:idx]\n            if hasattr(parent_layer, sub_name):\n                parent_layer = getattr(parent_layer, sub_name)\n                last_idx = idx + 1\n        idx += 1\n    sub_name = name[last_idx:idx]\n    return parent_layer, sub_name\n\n\ndef get_tensor_item(x):\n    return x.item()\n\n\ndef print_info(info):\n    time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    prefix = \"[AngelSlim]\"\n    try:\n        _index = torch.distributed.get_rank()\n    except ValueError:\n        _index = 0\n    if _index == 0:\n        print(\"[{}] {} {}\".format(time, prefix, info))\n\n\ndef get_best_device():\n    if torch.backends.mps.is_available():\n        return \"mps\"\n    elif torch.cuda.is_available():\n        return \"cuda:0\"\n    elif torch.xpu.is_available():\n        return \"xpu:0\"\n    else:\n        return \"cpu\"\n\n\ndef get_yaml_prefix_simple(file_path: str) -> Optional[str]:\n    \"\"\"\n    Simplified version using os.path\n    \"\"\"\n    if not file_path or not isinstance(file_path, str):\n        return None\n\n    filename = os.path.basename(file_path)\n\n    # Handle hidden files\n    if filename.startswith(\".\") and \".\" in filename[1:]:\n        parts = filename.split(\".\")\n        if parts[-1].lower() in [\"yaml\", \"yml\"]:\n            return \".\".join(parts[:-1])\n        return filename\n\n    # Process normal files\n    name, ext = os.path.splitext(filename)\n    if ext.lower() in [\".yaml\", \".yml\"]:\n        return name\n    return filename\n\n\ndef get_hf_config(model_path) -> dict:\n    \"When model_path does not exist, fetch the model.config from cached_file.\"\n    if os.path.isfile(model_path):\n        config_path = os.path.join(model_path, \"config.json\")\n    else:\n        config_path = cached_file(model_path, \"config.json\")\n\n    with open(config_path, \"r\", encoding=\"utf8\") as fp:\n        json_data = json.load(fp)\n        return json_data\n\n\ndef common_prefix(str1, str2):\n    return \"\".join(\n        x[0] for x in takewhile(lambda x: x[0] == x[1], zip(str1, str2))\n    ).rpartition(\".\")[0]\n\n\ndef get_package_info(package_name: str) -> dict:\n    info = {\"name\": package_name, \"version\": \"N/A\", \"source\": \"Unknown\"}\n    try:\n        version = importlib.metadata.version(package_name)\n        info[\"version\"] = version\n        info[\"source\"] = \"pip\"\n    except Exception:\n        try:\n            package = __import__(package_name)\n            path = Path(package.__path__[0]).parent\n            commit_hash = subprocess.check_output(\n                [\"git\", \"rev-parse\", \"HEAD\"], cwd=path, text=True\n            ).strip()\n            info[\"version\"] = commit_hash\n            info[\"source\"] = \"git\"\n        except Exception:\n            pass\n    return info\n",
        "deploy/offline.py": "import sys\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = sys.argv[1]\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    low_cpu_mem_usage=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nprompt = \"Hello, my name is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs)\nprint(tokenizer.decode(outputs[0]))\n",
        "docs/source/conf.py": "# SPDX-License-Identifier: Apache-2.0\n\n# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport datetime\nimport logging\nimport os\nimport re\nimport sys\nfrom pathlib import Path\n\nimport requests\n\nlogger = logging.getLogger(__name__)\nREPO_ROOT = Path(__file__).resolve().parent.parent.parent\nsys.path.append(os.path.abspath(REPO_ROOT))\n\n# -- Project information -----------------------------------------------------\n\nproject = \"AngelSlim\"\ncopyright = f\"{datetime.datetime.now().year}, AngelSlim Team\"\nauthor = \"the AngelSlim Team\"\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.linkcode\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx_tabs.tabs\",\n    \"sphinx_copybutton\",\n    \"myst_parser\",\n    \"sphinxarg.ext\",\n    \"sphinx_design\",\n    \"sphinx_togglebutton\",\n]\nmyst_enable_extensions = [\n    \"colon_fence\",\n    \"fieldlist\",\n]\n\nsphinx_tabs_disable_tab_closing = True\nsource_suffix = {\n    \".rst\": \"restructuredtext\",\n    \".md\": \"myst\",\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n\n# Exclude the prompt \"$\" when copying code\ncopybutton_prompt_text = r\"\\$ \"\ncopybutton_prompt_is_regexp = True\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_title = project\nhtml_theme = \"sphinx_book_theme\"\nhtml_logo = \"assets/logos/angelslim_logo.png\"\nhtml_favicon = \"assets/logos/angelslim_icon.png\"\nhtml_theme_options = {\n    \"path_to_docs\": \"docs/source\",\n    \"repository_url\": \"https://github.com/Tencent/AngelSlim\",\n    \"use_repository_button\": True,\n    \"use_edit_page_button\": True,\n    # Prevents the full API being added to the left sidebar of every page.\n    # Reduces build time by 2.5x and reduces build size from ~225MB to ~95MB.\n    \"collapse_navbar\": True,\n    # Makes API visible in the right sidebar on API reference pages.\n    \"show_toc_level\": 3,\n}\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n# html_static_path = [\"_static\"]\nhtml_js_files = [\"custom.js\"]\nhtml_css_files = [\"custom.css\"]\n\nmyst_heading_anchors = 2\nmyst_url_schemes = {\n    \"http\": None,\n    \"https\": None,\n    \"mailto\": None,\n    \"ftp\": None,\n    \"gh-issue\": {\n        \"url\": \"https://github.com/Tencent/AngelSlim/issues/{{path}}#{{fragment}}\",\n        \"title\": \"Issue #{{path}}\",\n        \"classes\": [\"github\"],\n    },\n    \"gh-pr\": {\n        \"url\": \"https://github.com/Tencent/AngelSlim/pull/{{path}}#{{fragment}}\",\n        \"title\": \"Pull Request #{{path}}\",\n        \"classes\": [\"github\"],\n    },\n    \"gh-dir\": {\n        \"url\": \"https://github.com/Tencent/AngelSlim/tree/main/{{path}}\",\n        \"title\": \"{{path}}\",\n        \"classes\": [\"github\"],\n    },\n    \"gh-file\": {\n        \"url\": \"https://github.com/Tencent/AngelSlim/blob/main/{{path}}\",\n        \"title\": \"{{path}}\",\n        \"classes\": [\"github\"],\n    },\n}\n\n# see https://docs.readthedocs.io/en/stable/reference/environment-variables.html # noqa\nREADTHEDOCS_VERSION_TYPE = os.environ.get(\"READTHEDOCS_VERSION_TYPE\")\nif READTHEDOCS_VERSION_TYPE == \"tag\":\n    # remove the warning banner if the version is a tagged release\n    header_file = os.path.join(\n        os.path.dirname(__file__), \"_templates/sections/header.html\"\n    )\n    # The file might be removed already if the build is triggered multiple times\n    # (readthedocs build both HTML and PDF versions separately)\n    if os.path.exists(header_file):\n        os.remove(header_file)\n\n\n_cached_base: str = \"\"\n_cached_branch: str = \"\"\n\n\ndef get_repo_base_and_branch(pr_number):\n    global _cached_base, _cached_branch\n    if _cached_base and _cached_branch:\n        return _cached_base, _cached_branch\n\n    url = f\"https://api.github.com/repos/Tencent/AngelSlim/pulls/{pr_number}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        _cached_base = data[\"head\"][\"repo\"][\"full_name\"]\n        _cached_branch = data[\"head\"][\"ref\"]\n        return _cached_base, _cached_branch\n    else:\n        logger.error(\"Failed to fetch PR details: %s\", response)\n        return None, None\n\n\ndef linkcode_resolve(domain, info):\n    if domain != \"py\":\n        return None\n    if not info[\"module\"]:\n        return None\n\n    # Get path from module name\n    file = Path(f\"{info['module'].replace('.', '/')}.py\")\n    path = REPO_ROOT / file\n    if not path.exists():\n        path = REPO_ROOT / file.with_suffix(\"\") / \"__init__.py\"\n    if not path.exists():\n        return None\n\n    # Get the line number of the object\n    with open(path) as f:\n        lines = f.readlines()\n    name = info[\"fullname\"].split(\".\")[-1]\n    pattern = rf\"^( {{4}})*((def|class) )?{name}\\b.*\"\n    for lineno, line in enumerate(lines, 1):  # noqa: B007\n        if not line or line.startswith(\"#\"):\n            continue\n        if re.match(pattern, line):\n            break\n\n    # If the line number is not found, return None\n    if lineno == len(lines):\n        return None\n\n    # If the line number is found, create the URL\n    filename = path.relative_to(REPO_ROOT)\n    if \"checkouts\" in path.parts:\n        # a PR build on readthedocs\n        pr_number = REPO_ROOT.name\n        base, branch = get_repo_base_and_branch(pr_number)\n        if base and branch:\n            return f\"https://github.com/{base}/blob/{branch}/{filename}#L{lineno}\"\n    # Otherwise, link to the source file on the main branch\n    return f\"https://github.com/Tencent/AngelSlim/blob/main/{filename}#L{lineno}\"\n\n\nintersphinx_mapping = {\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"typing_extensions\": (\"https://typing-extensions.readthedocs.io/en/latest\", None),\n    \"aiohttp\": (\"https://docs.aiohttp.org/en/stable\", None),\n    \"pillow\": (\"https://pillow.readthedocs.io/en/stable\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable\", None),\n    \"torch\": (\"https://pytorch.org/docs/stable\", None),\n    \"psutil\": (\"https://psutil.readthedocs.io/en/stable\", None),\n}\n\nnavigation_with_keys = False\n",
        "setup.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Setup for pip package.\"\"\"\nimport subprocess\n\nfrom setuptools import find_packages, setup\n\nTOOLS_VERSION = None\n\nif \"main\" in subprocess.getoutput(\"git branch\"):\n    TOOLS_VERSION = \"0.0.0_dev\"\nelse:\n    tag_list = subprocess.getoutput(\"git tag\").split(\"\\n\")\n    TOOLS_VERSION = tag_list[-1]\n\n\ndef get_requirements():\n    \"\"\"from requirements.txt load dependency package\"\"\"\n    with open(\"requirements.txt\") as f:\n        return [\n            line.strip()\n            for line in f.readlines()\n            if line.strip() and not line.startswith((\"#\", \"-\"))\n        ]\n\n\nsetup(\n    name=\"angelslim\",\n    version=TOOLS_VERSION,\n    description=(\"A toolkit for compress llm model.\"),\n    long_description=\"Tools for llm model compression\",\n    url=\"https://github.com/Tencent/AngelSlim\",\n    author=\"Tencent Author\",\n    install_requires=get_requirements(),\n    packages=find_packages(),\n    python_requires=\">=3.0\",\n    # PyPI package information.\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    license=\"License for AngelSlim\",\n    keywords=(\"Tencent large language model model-optimize compression toolkit.\"),\n)\n",
        "tools/fp8_quant_analyse.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\n\nfrom angelslim.compressor.quant.core.fp8_analyse_tools import (\n    draw_bf16_fp8_weight_fig,\n    draw_fp8_scale_fig,\n)\n\n\ndef quant_analyse(args):\n    if args.analyse_type == \"act\":\n        os.makedirs(args.save_path, exist_ok=True)\n        assert os.path.exists(args.model_path), f\"File {args.model_path} not exist\"\n        print(f\"[AngelSlim] Save all quant scale graph to {args.save_path}\")\n        draw_fp8_scale_fig(args.model_path, args.save_path)\n    elif args.analyse_type == \"weight\":\n        print(f\"[AngelSlim] Save weight analyse graph to {args.save_path}\")\n        os.makedirs(args.save_path, exist_ok=True)\n        assert os.path.exists(args.bf16_path), f\"File {args.bf16_path} not exist\"\n        assert os.path.exists(args.fp8_path), f\"File {args.fp8_path} not exist\"\n        bf16_path = args.bf16_path\n        fp8_path = args.fp8_path\n        layer_index = args.layer_index\n        draw_bf16_fp8_weight_fig(\n            bf16_path=bf16_path,\n            fp8_path=fp8_path,\n            save_path=args.save_path,\n            layer_index=layer_index,\n        )\n\n\nif __name__ == \"__main__\":\n\n    global_parser = argparse.ArgumentParser(description=\"全局参数\", add_help=True)\n    global_parser.add_argument(\n        \"--analyse-type\",\n        type=str,\n        required=True,\n        choices=[\"act\", \"weight\"],\n        help=\"choice 'activation', 'weight'\",\n    )\n    global_args, remaining_args = global_parser.parse_known_args()\n\n    parser = argparse.ArgumentParser(\n        description=f\"branch {global_args.analyse_type} args\"\n    )\n    if global_args.analyse_type == \"act\":\n        parser.add_argument(\"--model-path\", type=str, help=\"Fp8 path\", required=True)\n        parser.add_argument(\"--save-path\", type=str, default=\"./Quant_Scale_SavePath\")\n    elif global_args.analyse_type == \"weight\":\n        parser.add_argument(\"--bf16-path\", type=str, help=\"Bf16 path\", required=True)\n        parser.add_argument(\"--fp8-path\", type=str, help=\"Fp8 path\", required=True)\n        parser.add_argument(\"--save-path\", type=str, default=\"./Weight_analyse\")\n        parser.add_argument(\"--layer-index\", type=int, required=True)\n\n    args = parser.parse_args(remaining_args)\n    args.analyse_type = global_args.analyse_type\n    print(f\"Args:{args}\")\n    quant_analyse(args)\n",
        "tools/run.py": "# Copyright 2025 Tencent Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\n\nfrom angelslim.engine import Engine\nfrom angelslim.utils import get_yaml_prefix_simple\nfrom angelslim.utils.config_parser import SlimConfigParser, print_config\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\"AngelSlim\")\n    parser.add_argument(\"-c\", \"--config\", type=str, required=True)\n    parser.add_argument(\"--model-path\", type=str, default=None)\n    parser.add_argument(\"--save-path\", type=str, default=None)\n    parser.add_argument(\"--input-prompt\", type=str, default=None)\n    parser.add_argument(\"--multi-nodes\", action=\"store_true\")\n    args = parser.parse_args()\n    return args\n\n\ndef merge_config(config, args):\n    \"\"\"\n    Merge command line arguments into the configuration dictionary.\n\n    Args:\n        config (dict): Configuration dictionary to be updated.\n        args (argparse.Namespace): Parsed command line arguments.\n    \"\"\"\n    if args.save_path is not None:\n        config.global_config.save_path = args.save_path\n    if args.model_path is not None:\n        config.model_config.model_path = args.model_path\n    config.global_config.save_path = os.path.join(\n        config.global_config.save_path,\n        get_yaml_prefix_simple(args.config),\n    )\n\n\ndef multi_nodes_run(config):\n    \"\"\"\n    Run the LLM compression process based on the provided configuration\n    using multiple nodes.\n\n    Args:\n        config (dict): Configuration dictionary containing\n                       parameters for LLM compression.\n    \"\"\"\n    # Step 1: Initialize distributed environment\n    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n    if world_size > 1:\n        dist.init_process_group(\"nccl\", timeout=timedelta(minutes=60))\n    torch.cuda.set_device(local_rank)\n    torch.set_default_dtype(torch.bfloat16)\n    torch.set_num_threads(8)\n    torch.manual_seed(965)\n\n    # Step 2: Initialize configurations\n    model_config = config.model_config\n    dataset_config = config.dataset_config\n    compress_config = config.compression_config\n    global_config = config.global_config\n\n    # Step 3: Execute complete pipeline\n    slim_engine = Engine()\n\n    # Step 4: Prepare model\n    slim_engine.prepare_model(\n        model_name=model_config.name,\n        model_path=model_config.model_path,\n        torch_dtype=model_config.torch_dtype,\n        device_map=model_config.device_map,\n        trust_remote_code=model_config.trust_remote_code,\n        low_cpu_mem_usage=model_config.low_cpu_mem_usage,\n        use_cache=model_config.use_cache,\n        deploy_backend=global_config.deploy_backend,\n        using_multi_nodes=True,\n    )\n\n    # Step 5: Prepare data (optional custom dataloader)\n    if compress_config.need_dataset:\n        slim_engine.prepare_data(\n            data_path=dataset_config.data_path,\n            data_type=dataset_config.name,\n            custom_dataloader=None,\n            max_length=dataset_config.max_seq_length,\n            batch_size=dataset_config.batch_size,\n            num_samples=dataset_config.num_samples,\n            shuffle=dataset_config.shuffle,\n        )\n\n    # Step 6: Initialize compressor\n    slim_engine.prepare_compressor(\n        compress_name=compress_config.name,\n        compress_config=compress_config,\n        global_config=global_config,\n    )\n\n    # Step 7: Compress model\n    slim_engine.run()\n\n    # Step 8: Save compressed model\n    slim_engine.save(global_config.save_path, config)\n\n\ndef run(config):\n    \"\"\"\n    Run the LLM compression process based on the provided configuration.\n\n    Args:\n        config (dict): Configuration dictionary containing\n                       parameters for LLM compression.\n    \"\"\"\n    # Step 1: Initialize configurations\n    model_config = config.model_config\n    dataset_config = config.dataset_config\n    compress_config = config.compression_config\n    global_config = config.global_config\n\n    # Step 2: Execute complete pipeline\n    slim_engine = Engine()\n\n    # Step 3: Prepare model\n    slim_engine.prepare_model(\n        model_name=model_config.name,\n        model_path=model_config.model_path,\n        torch_dtype=model_config.torch_dtype,\n        device_map=model_config.device_map,\n        trust_remote_code=model_config.trust_remote_code,\n        low_cpu_mem_usage=model_config.low_cpu_mem_usage,\n        use_cache=model_config.use_cache,\n        deploy_backend=global_config.deploy_backend,\n    )\n\n    # Step 4: Prepare data (optional custom dataloader)\n    if compress_config.need_dataset:\n        slim_engine.prepare_data(\n            data_path=dataset_config.data_path,\n            data_type=dataset_config.name,\n            custom_dataloader=None,\n            max_length=dataset_config.max_seq_length,\n            batch_size=dataset_config.batch_size,\n            num_samples=dataset_config.num_samples,\n            shuffle=dataset_config.shuffle,\n        )\n\n    # Step 5: Initialize compressor\n    slim_engine.prepare_compressor(\n        compress_name=compress_config.name,\n        compress_config=compress_config,\n        global_config=global_config,\n    )\n\n    # Step 6: Compress model\n    slim_engine.run()\n\n    # Step 7: Save compressed model\n    slim_engine.save(global_config.save_path, config)\n\n\ndef infer(config, input_prompt):\n    \"\"\"\n    Evaluate the compression process.\n    This function is a placeholder for future evaluation logic.\n    \"\"\"\n    # Step 1: Initialize configurations\n    model_config = config.model_config\n    compress_config = config.compression_config\n    global_config = config.global_config\n    infer_config = config.infer_config\n\n    # Step 2: Execute complete pipeline\n    slim_engine = Engine()\n\n    # Step 3: Prepare model\n    slim_engine.prepare_model(\n        model_name=model_config.name,\n        model_path=model_config.model_path,\n        torch_dtype=model_config.torch_dtype,\n        device_map=model_config.device_map,\n        trust_remote_code=model_config.trust_remote_code,\n        low_cpu_mem_usage=model_config.low_cpu_mem_usage,\n        use_cache=model_config.use_cache,\n        cache_dir=model_config.cache_dir,\n        deploy_backend=global_config.deploy_backend,\n    )\n\n    # Step 4: Initialize compressor\n    slim_engine.prepare_compressor(\n        compress_name=compress_config.name,\n        compress_config=compress_config,\n        global_config=global_config,\n    )\n\n    # Step 5: Run inference\n    output = slim_engine.infer(input_prompt, **infer_config.__dict__)\n    if slim_engine.series == \"Diffusion\":\n        # Save the generated image\n        if global_config.save_path:\n            save_path = os.path.join(global_config.save_path, \"output_image.png\")\n        else:\n            save_path = \"output_image.png\"\n\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        output.save(save_path)\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    parser = SlimConfigParser()\n    config = parser.parse(args.config)\n    merge_config(config, args)\n    print_config(config)\n    if args.input_prompt:\n        infer(config, args.input_prompt)\n    elif args.multi_nodes:\n        multi_nodes_run(config)\n    else:\n        run(config)\n"
    }
}