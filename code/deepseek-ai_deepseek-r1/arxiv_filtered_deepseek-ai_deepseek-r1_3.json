{
  "2-3 (API)": "The provided quotes indicate that DeepSeek-R1 and its corresponding API have been open-sourced, making them publicly available. This open source release is expected to significantly aid the research community, enabling them to develop and distill smaller, more efficient models in the future.",
  "3-1 (Pre-training)": "",
  "3-2 (Fine-tuning)": "The quotes describe a comprehensive pipeline used to develop DeepSeek-R1. This pipeline includes two reinforcement learning (RL) stages focused on discovering improved reasoning patterns and aligning the model's behavior with human preferences, as well as two supervised fine-tuning (SFT) stages that seed the model’s reasoning and non-reasoning capabilities. Additionally, after the fine-tuning phase with new data, the checkpoint is subjected to a further RL process that integrates prompts from various scenarios, ultimately resulting in a model checkpoint named DeepSeek-R1, which performs on par with competitive benchmarks such as OpenAI-o1-1217.",
  "3-3 (Reinforcement Learning)": "The quotes provide detailed insights into the training methodology of DeepSeek-R1-Zero. This model variant was trained using large-scale reinforcement learning (RL) without an initial supervised fine-tuning stage, and as a result, it developed remarkable reasoning capabilities. Furthermore, throughout its training, DeepSeek-R1-Zero naturally developed a variety of powerful and interesting reasoning behaviors through thousands of RL steps, ultimately achieving super performance on reasoning benchmarks.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/Introduction - 1.1 Contributions]",
      "quote": "We open-source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "sections/1.1 Contributions",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Introduction - 1.1 Contributions]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "sections/1.1 Contributions",
      "quote": "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text Abstract]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "sections/Introduction",
      "quote": "During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks."
    }
  ]
}