{
  "model": "deepseek-ai/DeepSeek-R1",
  "scores": {
    "1-2 Code": {
      "score": 1.0,
      "reason": "Training pipeline code detected (files/README contain training scripts or instructions)."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is released under the MIT License and explicitly supports commercial use, modifications, and derivative works."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official paper for DeepSeek-R1 is provided (both via GitHub and a cited technical document), clearly linked to the target model."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "There is no disclosure of the training hardware type or quantity."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "No details of the full training software stack (framework versions, distributed orchestration, etc.) are provided."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "API available (strong evidence): You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details (objectives/schedules/hyperparameters/pipeline). Only high-level claims or unrelated info → Closed (0)."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Methodology appears fully reproducible across key categories (objective/optimizer/schedule/batching/duration)."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No concrete method details (objectives/schedules/hyperparameters/pipeline). Only high-level claims or unrelated info → Closed (0)."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "The model’s pre-training is described in terms of token count (14.8T tokens) and language composition, but full data sources, proportions, licensing, and access details are not disclosed."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "The fine-tuning data is partially described, mentioning a small, carefully collected set of long chain-of-thought (CoT) data and 800K samples for distillation, yet it lacks comprehensive dataset naming and complete availability details."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No explicit information or quotes are provided regarding the reinforcement learning data used for DeepSeek-R1."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "The documentation mentions that a readable pattern (including a summary for each response) is used to filter out unsuitable data, which is partial disclosure of the filtering criteria."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 1.0,
      "reason": "Training pipeline code detected (files/README contain training scripts or instructions)."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is released under the MIT License and explicitly supports commercial use, modifications, and derivative works."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official paper for DeepSeek-R1 is provided (both via GitHub and a cited technical document), clearly linked to the target model."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "There is no disclosure of the training hardware type or quantity."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "No details of the full training software stack (framework versions, distributed orchestration, etc.) are provided."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "API available (strong evidence): You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details (objectives/schedules/hyperparameters/pipeline). Only high-level claims or unrelated info → Closed (0)."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Methodology appears fully reproducible across key categories (objective/optimizer/schedule/batching/duration)."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No concrete method details (objectives/schedules/hyperparameters/pipeline). Only high-level claims or unrelated info → Closed (0)."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "The model’s pre-training is described in terms of token count (14.8T tokens) and language composition, but full data sources, proportions, licensing, and access details are not disclosed."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "The fine-tuning data is partially described, mentioning a small, carefully collected set of long chain-of-thought (CoT) data and 800K samples for distillation, yet it lacks comprehensive dataset naming and complete availability details."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No explicit information or quotes are provided regarding the reinforcement learning data used for DeepSeek-R1."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "The documentation mentions that a readable pattern (including a summary for each response) is used to filter out unsuitable data, which is partial disclosure of the filtering criteria."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 5.312,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 8.5,
    "scale": "10/16",
    "code_detection_reason": "Training pipeline code detected (files/README contain training scripts or instructions).",
    "pretrain_sources_used": true
  }
}