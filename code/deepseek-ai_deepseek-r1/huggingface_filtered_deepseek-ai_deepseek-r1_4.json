{
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The available information indicates that the DeepSeek-R1-Distill models are fine-tuned versions derived from existing open-source models, and their fine-tuning process leverages samples generated by DeepSeek-R1. Specifically, models such as DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, and DeepSeek-R1-Distill-Qwen-32B are based on the Qwen-2.5 series. These source models were originally released under the Apache 2.0 License, and the fine-tuning step involved curating a dataset of 800,000 samples utilizing DeepSeek-R1, ensuring a robust base for further model specialization.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    },
    {
      "source": "[readme]",
      "quote": "- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5) which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}