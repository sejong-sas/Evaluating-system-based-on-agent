{
  "1-1 (Weights)": "The provided quotes include two entries that explicitly reference DeepSeek-R1: \"DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)\" and \"DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)\". These entries show that the model weights for DeepSeek-R1 and its variant DeepSeek-R1-Zero are available publicly via HuggingFace, and they specify details such as numerical parameters (671B, 37B, and 128K).",
  "1-1 (Weights)__evidence": [
    {
      "source": "readme",
      "quote": "DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)"
    },
    {
      "source": "readme",
      "quote": "DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)"
    }
  ],
  "1-2 (Code)": "",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "The repository includes the statement: \"This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\" This confirms that both the code repository and the model weights for DeepSeek-R1 are distributed under the MIT License.",
  "1-3 (License)__evidence": [
    {
      "source": "readme",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE)."
    }
  ],
  "1-4 (Paper)": "An official paper link is provided with the quote: \"<a href=\\\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\\\"><b>Paper Link</b>üëÅÔ∏è</a>\". This directs users to the GitHub location of the DeepSeek-R1 paper.",
  "1-4 (Paper)__evidence": [
    {
      "source": "readme",
      "quote": "<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>"
    }
  ],
  "1-5 (Architecture)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The provided quote explicitly states, 'The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.' This sentence emphasizes that DeepSeek-R1‚Äôs open source nature and its API availability are seen as key contributions to the research community, paving the way for the creation and distillation of smaller, more efficient models.",
  "2-3 (API)__evidence": [
    {
      "source": "readme",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "readme",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    }
  ],
  "3-3 (Reinforcement Learning)": "The quote indicates, 'This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero.' In this statement, the role of reinforcement learning is highlighted in enabling the exploration of chain-of-thought strategies, which in turn has been instrumental in the development of a variant named DeepSeek-R1-Zero.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "readme",
      "quote": "This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero."
    }
  ],
  "4-1 (Pre-training Data)": "No quotes mentioning deepseek-ai/DeepSeek-R1 were provided regarding pre-training data.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "No quotes mentioning deepseek-ai/DeepSeek-R1 were provided regarding fine-tuning data.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "No quotes mentioning deepseek-ai/DeepSeek-R1 were provided regarding reinforcement learning data.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "No quotes mentioning deepseek-ai/DeepSeek-R1 were provided regarding data filtering.",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}