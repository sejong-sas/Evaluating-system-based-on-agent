{
  "2-3 (API)": "The available quotes highlight that DeepSeek-R1 is an open source model complemented by a publicly accessible API. This API is designed to support and benefit the research community by enabling the distillation of larger models’ reasoning patterns into smaller, more efficient versions. The emphasis is on the API’s role in promoting research that leads to better smaller models, suggesting that both the model and its interface are key resources for advancing model distillation techniques.",
  "3-1 (Pre-training)": "",
  "3-2 (Fine-tuning)": "The provided quotes detail a comprehensive fine-tuning process used in developing DeepSeek-R1. The model’s training pipeline incorporates a blend of reinforcement learning (RL) stages and supervised fine-tuning (SFT) stages. Initially, the fine-tuning pipeline includes two RL stages that help in discovering improved reasoning patterns and aligning the model with human preferences, along with two SFT stages which establish the foundational reasoning and non-reasoning capabilities. Moreover, fine-tuning is enhanced by a subsequent RL process that further refines the model using prompts from diverse scenarios, leading to a final checkpoint that performs comparably to other competitive models. Additionally, efforts are made to mitigate early RL training instabilities by fine-tuning with a limited set of long chain-of-thought data. DeepSeek-R1 also serves as a teacher model by generating 800K training samples to further fine-tune several smaller dense models, underscoring a robust, reproducible pipeline that differentiates it from other variants like DeepSeek-R1-Zero.",
  "3-3 (Reinforcement Learning)": "The quotes regarding reinforcement learning focus on the DeepSeek-R1-Zero variant, which is exclusively trained using a large-scale RL approach. This method avoids the preliminary use of supervised fine-tuning (SFT), relying entirely on RL to shape the model’s reasoning capabilities. The approach entails training the model without cold-start data, thereby ensuring that the model achieves strong performance across various tasks. The emphasis on a pure RL training strategy highlights the model’s capacity for remarkable reasoning without the conventional dependencies on SFT, reflecting an innovative method in the development of DeepSeek-R1 derivatives.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[pdf_text]",
      "quote": "We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks."
    }
  ]
}