{
  "model_id": "deepseek-ai/DeepSeek-V3-Base",
  "full_texts": [
    {
      "arxiv_id": "https://github.com/sgl-project/sglang/releases/tag/v0.4.1",
      "full_text": " Release Release v0.4.1 · sgl-project/sglang · GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} sgl-project / sglang Public Notifications You must be signed in to change notification settings Fork 2.8k Star 17.5k Code Issues 572 Pull requests 633 Discussions Actions Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Discussions Actions Security Insights Releases v0.4.1 Release v0.4.1 Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags zhyncs released this 25 Dec 23:27 &middot; 3458 commits to main since this release v0.4.1 efc52f8 This commit was created on GitHub.com and signed with GitHub’s verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Highlights We're excited to announce SGLang v0.4.1, which now supports DeepSeek V3 - currently the strongest open-source LLM, even surpassing GPT-4o. The SGLang and DeepSeek teams worked together to get DeepSeek V3 FP8 running on NVIDIA and AMD GPU from day one . We've also supported MLA optimization and DP attention before, making SGLang one of the best open-source LLM engines for running DeepSeek models. Special thanks to Meituan's Search &amp; Recommend Platform Team @ispobock @HandH1998 and Baseten's Model Performance Team @zhyncs for implementing the model, and DataCrunch for providing GPU resources. Various improvements to the cache-aware sglang router, torchao integration, server termination Added a standalone package sgl-kernel for supporting more custom kernels in the code base. What's Changed Adding SGLang FP8 Utils by @HaiShaw in #2348 docs: add SGLang v0.4 blog by @zhyncs in #2341 MLA prefill w/o weight absorption by @ispobock in #2349 Check gpu availability at server args creation by @MrAta in #2340 minor: limit the range of vllm versions by @zhyncs in #2350 Fix Docs CI When Compile Error by @zhaochenyang20 in #2323 Add Docs For SGLang Native Router by @zhaochenyang20 in #2308 Make torch TP composable with torch.compile by @kwen2501 in #2352 move apply_torchao_config_ to model_runner by @jerryzh168 in #2342 [Minor] Code style improvements by @merrymercy in #2355 Fix AWQ with enable MLA by @ispobock in #2364 MoE Expert Parallel by @xiaobochen123 in #2371 Move FP8 to SGLang by @zhyncs in #2370 optimize cuda graph max_bs_settings on low-end gpus by @BBuf in #2360 Add more support for intel Gaudi accelerators by @YangQun1 in #2357 [router] support /add_worker api by @ByronHsu in #2369 docs: update adoption (Meituan) by @zhyncs in #2373 Use proc.join instead of busy waiting by @merrymercy in #2374 docs: Improve instructions for supporting new models by @vchzls in #2363 Fix the overlap for xgrammar by @merrymercy in #2377 Release v0.4.0.post1 by @merrymercy in #2375 [Router] remove duplicate char count by @ByronHsu in #2378 [router] add remove tenant method in the radix tree by @ByronHsu in #2379 [router] Add remove worker api by @ByronHsu in #2380 fix: resolve fp8 moe issue by @zhyncs in #2387 fix: update xgrammar v0.1.6 by @zhyncs in #2390 Fp8 MoE optimizations on AMD by @HaiShaw in #2388 minor: update killall script by @zhyncs in #2391 [router] Health check on worker before added to the router by @ByronHsu in #2392 Fix shape error that occurred when loading lora weight of gemma2 model. by @upskyy in #2330 nit: Remove busy waiting on scheduler by @rkooo567 in #2382 Optimize Triton decoding kernel for long context by @ispobock in #2394 Update killall_sglang.sh by @merrymercy in #2397 Remove unused vars in the triton backend by @ispobock in #2401 Fix a bug with logprob streaming + chunked prefill by @merrymercy in #2403 fix: specify dtype with begin_forward aka plan by @zhyncs in #2404 Fix recv_requests by @merrymercy in #2405 minor: update correct measurement unit by @zhyncs in #2406 feat: support custom task runner by @zhyncs in #2407 minor: add random use case by @zhyncs in #2408 minor: add random flashinfer vs triton use case by @zhyncs in #2409 Simplify stream_output by @merrymercy in #2398 [router] Improve cleanup logic by @ByronHsu in #2411 [Router] fix interrupt from terminal by @ByronHsu in #2413 [router] defer health checking to router init by @ByronHsu in #2393 reduce watchdog interval to 5s by @ByronHsu in #2410 Add a unittest for fused_moe by @BBuf in #2416 [Minor] Improve code style by @merrymercy in #2419 [Minor] Improve code style by @merrymercy in #2422 [feat] Enable chunked prefill for llava-onevision by @Ying1123 in #2412 Typo fix in router.md by @adarshxs in #2424 feat: support sgl-kernel PyPI by @zhyncs in #2433 fix: use manylinux2014_x86_64 tag by @zhyncs in #2434 fix: compatible with PEP 440 by @zhyncs in #2435 [router] Refactor: decouple select and send stage by @ByronHsu in #2440 [router] Use borrow if possible to save cost by @ByronHsu in #2441 Make torch TP composable with torchao by @kwen2501 in #2436 chore: update ao v0.7.0 by @zhyncs in #2447 decoding attention kernel benchmark by @bjmsong in #2425 Fix model loader for more quantization formats by @merrymercy in #2448 Fix warmup in bench_offline_throughput.py by @merrymercy in #2449 Add support for IBM Granite 3.x models by @frreiss in #2437 [router] Add retries based fault tolerance by @ByronHsu in #2452 [router] remove main.rs because only lib.rs is used for py binding by @ByronHsu in #2453 [Core] in batch prefix caching by delay scheduling by @rkooo567 in #2442 [router] Update doc for dynamic scaling and fault tolerance by @ByronHsu in #2454 [router] Release router 0.1.0 with dynamic scaling and fault tolerance by @ByronHsu in #2455 Make request payload size configurable by @MrAta in #2444 Include version info into the router package by @MrAta in #2456 Bump sglang-router to 0.1.1 by @MrAta in #2459 chore: bump v0.0.2 for sgl-kernel by @zhyncs in #2462 minor: update pypi tag by @zhyncs in #2463 fix: set runtime path by @zhyncs in #2466 Rename rust folder to sgl-router by @MrAta in #2464 feat: support dev image by @zhyncs in #2469 [Minor] Fix grok model loader by @merrymercy in #2473 Fix correctness issue for triton decoding kernel by @ispobock in #2479 format: add clang-format for sgl-kernel by @zhyncs in #2483 Remove cuda graph batch size adjustment for dp attention by @ispobock in #2484 hotfix: checking for HIP by @zhyncs in #2485 sgl-kernel adapt tensorrt llm custom allreduce by @yizhang2077 in #2481 fix typo by @zhyncs in #2487 [Benchmark] add a benchmark for hf/vllm/sglang rmsnorm by @BBuf in #2486 fix moe-ep accuracy issue for fp8 by @xiaobochen123 in #2489 minor: update flashinfer nightly by @zhyncs in #2490 Small fixes for torchao quant by @jerryzh168 in #2476 Simplify pytorch sampling kernel and logit processor by @merrymercy in #2491 Temporarily disable unit test of torch native attention backend by @merrymercy in #2492 Revert \"Small fixes for torchao quant\" by @merrymercy in #2493 Add a benchmark script for in-batch prefix caching by @merrymercy in #2494 Small fix for the order of apply_torchao_config by @merrymercy in #2495 benchmark decoding attention kernel with cudnn by @bjmsong in #2467 Clean up GPU memory after killing sglang processes by @MrAta in #2457 ROCm support for sglang.check_env by @hliuca in #2426 Add lora_path to chat completion by @ccchow in #2438 Fix openai protocols and pass top_k, min_p by @merrymercy in #2499 Update readme by @merrymercy in #2500 feat: add llama3 eval by @zhyncs in #2515 docs: update README by @zhyncs in #2516 fix: continue to use flashinfer 0.1.6 temporarily by @zhyncs in #2517 fix followup #2517 by @zhyncs in #2524 Add integration with gemlite weight only quant by @jerryzh168 in #2528 chore: bump v0.4.0.post2 by @zhyncs in #2525 fix #2528 by @zhyncs in #2541 Add lora_paths to v1_chat_generate_request by @ccchow in #2529 docs: update sponsorship (DataCrunch) by @zhyncs in #2523 [kernel optimize] benchmark write_req_to_token_pool_triton and optimize kernel by @BBuf in #2509 A better aio rwlock that guarantees the order by @merrymercy in #2547 Updated documentation for Grammar Backend by @shuaills in #2545 Fix gemlite import by @merrymercy in #2553 Reorg moe code by @ispobock in #2563 [Bench] Flush cache before benchmarking by @Ying1123 in #2566 Refactor MoE by @HandH1998 in #2575 fix moe_align_block_size_kernel for shared memory issue by @zhyncs in #2579 chore: bump 0.0.2.post8 for sgl-kernel by @zhyncs in #2580 use sgl-kernel moe_align_block_size by @zhyncs in #2581 chore: bump v0.4.1 by @zhyncs in #2582 New Contributors @vchzls made their first contribution in #2363 @upskyy made their first contribution in #2330 @rkooo567 made their first contribution in #2382 @adarshxs made their first contribution in #2424 @frreiss made their first contribution in #2437 @ccchow made their first contribution in #2438 @shuaills made their first contribution in #2545 Full Changelog : v0.4.0...v0.4.1 Contributors ccchow, MrAta, and 22 other contributors Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> 👍 1 firengate reacted with thumbs up emoji 🎉 13 zhyncs, kevin85421, dsingal0, 651961, Ying1123, feitianxue, upskyy, ispobock, ColumbusAI, adarshxs, and 3 more reacted with hooray emoji 🚀 14 zhyncs, ispobock, HandH1998, kevin85421, dsingal0, 651961, jiangguochaoGG, mssongit, Ying1123, feitianxue, and 4 more reacted with rocket emoji 👀 4 651961, Ying1123, merrymercy, and firengate reacted with eyes emoji All reactions 👍 1 reaction 🎉 13 reactions 🚀 14 reactions 👀 4 reactions 17 people reacted Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time. "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2412.19437",
      "full_text": " [2412.19437] DeepSeek-V3 Technical Report Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2412.19437 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2412.19437 (cs) [Submitted on 27 Dec 2024 ( v1 ), last revised 18 Feb 2025 (this version, v2)] Title: DeepSeek-V3 Technical Report Authors: DeepSeek-AI , Aixin Liu , Bei Feng , Bing Xue , Bingxuan Wang , Bochao Wu , Chengda Lu , Chenggang Zhao , Chengqi Deng , Chenyu Zhang , Chong Ruan , Damai Dai , Daya Guo , Dejian Yang , Deli Chen , Dongjie Ji , Erhang Li , Fangyun Lin , Fucong Dai , Fuli Luo , Guangbo Hao , Guanting Chen , Guowei Li , H. Zhang , Han Bao , Hanwei Xu , Haocheng Wang , Haowei Zhang , Honghui Ding , Huajian Xin , Huazuo Gao , Hui Li , Hui Qu , J.L. Cai , Jian Liang , Jianzhong Guo , Jiaqi Ni , Jiashi Li , Jiawei Wang , Jin Chen , Jingchang Chen , Jingyang Yuan , Junjie Qiu , Junlong Li , Junxiao Song , Kai Dong , Kai Hu , Kaige Gao , Kang Guan , Kexin Huang , Kuai Yu , Lean Wang , Lecong Zhang , Lei Xu , Leyi Xia , Liang Zhao , Litong Wang , Liyue Zhang , Meng Li , Miaojun Wang , Mingchuan Zhang , Minghua Zhang , Minghui Tang , Mingming Li , Ning Tian , Panpan Huang , Peiyi Wang , Peng Zhang , Qiancheng Wang , Qihao Zhu , Qinyu Chen , Qiushi Du , R.J. Chen , R.L. Jin , Ruiqi Ge , Ruisong Zhang , Ruizhe Pan , Runji Wang , Runxin Xu , Ruoyu Zhang , Ruyi Chen , S.S. Li , Shanghao Lu , Shangyan Zhou , Shanhuang Chen , Shaoqing Wu , Shengfeng Ye , Shengfeng Ye , Shirong Ma , Shiyu Wang , Shuang Zhou , Shuiping Yu , Shunfeng Zhou , Shuting Pan , T. Wang , Tao Yun , Tian Pei , Tianyu Sun , W.L. Xiao , Wangding Zeng , Wanjia Zhao , Wei An , Wen Liu , Wenfeng Liang , Wenjun Gao , Wenqin Yu , Wentao Zhang , X.Q. Li , Xiangyue Jin , Xianzu Wang , Xiao Bi , Xiaodong Liu , Xiaohan Wang , Xiaojin Shen , Xiaokang Chen , Xiaokang Zhang , Xiaosha Chen , Xiaotao Nie , Xiaowen Sun , Xiaoxiang Wang , Xin Cheng , Xin Liu , Xin Xie , Xingchao Liu , Xingkai Yu , Xinnan Song , Xinxia Shan , Xinyi Zhou , Xinyu Yang , Xinyuan Li , Xuecheng Su , Xuheng Lin , Y.K. Li , Y.Q. Wang , Y.X. Wei , Y.X. Zhu , Yang Zhang , Yanhong Xu , Yanhong Xu , Yanping Huang , Yao Li , Yao Zhao , Yaofeng Sun , Yaohui Li , Yaohui Wang , Yi Yu , Yi Zheng , Yichao Zhang , Yifan Shi , Yiliang Xiong , Ying He , Ying Tang , Yishi Piao , Yisong Wang , Yixuan Tan , Yiyang Ma , Yiyuan Liu , Yongqiang Guo , Yu Wu , Yuan Ou , Yuchen Zhu , Yuduan Wang , Yue Gong , Yuheng Zou , Yujia He , Yukun Zha , Yunfan Xiong , Yunxian Ma , Yuting Yan , Yuxiang Luo , Yuxiang You , Yuxuan Liu , Yuyang Zhou , Z.F. Wu , Z.Z. Ren , Zehui Ren , Zhangli Sha , Zhe Fu , Zhean Xu , Zhen Huang , Zhen Zhang , Zhenda Xie , Zhengyan Zhang , Zhewen Hao , Zhibin Gou , Zhicheng Ma , Zhigang Yan , Zhihong Shao , Zhipeng Xu , Zhiyu Wu , Zhongyu Zhang , Zhuoshu Li , Zihui Gu , Zijia Zhu , Zijun Liu , Zilin Li , Ziwei Xie , Ziyang Song , Ziyi Gao , Zizheng Pan et al. (100 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled DeepSeek-V3 Technical Report, by DeepSeek-AI and 199 other authors View PDF HTML (experimental) Abstract: We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at this https URL . Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2412.19437 [cs.CL] &nbsp; (or arXiv:2412.19437v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2412.19437 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Wenfeng Liang [ view email ] [v1] Fri, 27 Dec 2024 04:03:16 UTC (1,114 KB) [v2] Tue, 18 Feb 2025 17:26:38 UTC (1,114 KB) Full-text links: Access Paper: View a PDF of the paper titled DeepSeek-V3 Technical Report, by DeepSeek-AI and 199 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-12 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}