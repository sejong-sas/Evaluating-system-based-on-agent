{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "For fine-tuning, the provided quotes describe a deliberate approach to preparing DeepSeek-R1 by collecting a small amount of extended chain-of-thought (CoT) data. This specialized data set is used to fine-tune DeepSeek-R1 in order to serve as the initial reinforcement learning (RL) actor, thereby avoiding the early unstable cold start phase that could arise from beginning RL training directly from the base model. The process is highlighted by its contrast with DeepSeek-R1-Zero, emphasizing the need for this fine-tuning step to stabilize training in DeepSeek-R1.",
  "4-3 (Reinforcement Learning Data)": "The reinforcement learning data, as illustrated in the quote, is connected to the training of DeepSeek-R1-Zeroâ€”a variant within the DeepSeek-R1 family. Although this variant is distinguished by the absence of an initial supervised fine-tuning (SFT) phase, it is noted for demonstrating remarkable reasoning capabilities after undergoing large-scale reinforcement learning. This detail underscores the effectiveness of reinforcement learning techniques even when prior fine-tuning is not applied, providing key insights into the design decisions for models related to DeepSeek-R1.",
  "4-4 (Data Filtering)": "In the data filtering process for DeepSeek-R1, the method involves constructing a cold-start data set with a clear and readable pattern. Each response in this dataset is formatted to include a summary at the end, and there is an active filtering mechanism in place to remove responses that are not reader-friendly. This approach ensures that the data used in training DeepSeek-R1 is both accessible and of high quality, contributing to the clarity and coherence of the model's outputs.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    },
    {
      "source": "[sections/2501.12948]",
      "quote": "In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    }
  ]
}