{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The fine-tuning data details specify that for DeepSeek-R1 the process involves the careful construction and collection of a small set of long chain-of-thought (CoT) data. This approach is implemented to counteract the early unstable cold start phase encountered during reinforcement learning training. In particular, the quotes distinguish DeepSeek-R1 from its variant DeepSeek-R1-Zero by highlighting that the collected data is used to fine-tune the base model. Several methods are described for obtaining this CoT data, including using few-shot prompting that employs a long CoT example, generating detailed answers with steps of reflection and verification from the models, processing outputs generated in a clear and readable format, and refining the results with post-processing by human annotators. This multifaceted method ensures that the fine-tuning data for DeepSeek-R1 is robust and tailored specifically to its reinforcement learning requirements.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/2.3.1]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "sections/2.3.1. Cold Start",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}