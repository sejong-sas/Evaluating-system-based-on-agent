{
  "2-3 (API)": "The material explicitly states that DeepSeek makes an \"OpenAI-Compatible\" interface available: \"We also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com.\" This indicates that the DeepSeek ecosystem (and, by inclusion, models such as DeepSeek-R1) can be accessed over the network through an HTTP-style programmatic endpoint that imitates the request / response format popularised by the OpenAI API. The sentence also reveals that the entry point is public and documented at the URL https://platform.deepseek.com/, implying hosted inference rather than a mere local library. No other access modes, rate limits, or authentication schemes are mentioned in the provided excerpts.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "The only direct pre-training detail provided is the statement: \"DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\" This establishes that both DeepSeek-R1 and its variant DeepSeek-R1-Zero inherit their initial weights or architectural foundation from a predecessor model named DeepSeek-V3-Base. The quote, however, does not supply further hyper-parameters, dataset composition, token counts, or training hardware; it simply identifies the initialization lineage and thus positions DeepSeek-R1 as a continued-pre-training (or further-pre-training) effort on top of the V3-Base checkpoint.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    }
  ],
  "3-2 (Fine-tuning)": "Two sentences outline the fine-tuning activities connected to DeepSeek-R1:\n1) \"Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community.\" This conveys that inference outputs produced by DeepSeek-R1 were collected as \"reasoning data,\" and those generated samples then served as supervision to further fine-tune multiple existing dense (i.e., standard, non-Mixture-of-Experts) language models.\n2) \"DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\" Here, the Distill-branded variants are specifically described as open-source model checkpoints that underwent an additional fine-tuning pass driven by data generated from DeepSeek-R1 itself.\nTogether these lines describe a knowledge-distillation or self-instruct style pipeline: DeepSeek-R1 first produces reasoning demonstrations; those demonstrations then become training data for other dense or open-source backbones, yielding \"DeepSeek-R1-Distill\" derivatives. No numeric hyper-parameters or dataset sizes are revealed in the quoted material.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    }
  ],
  "3-3 (Reinforcement Learning)": "Three supplied sentences map out the reinforcement-learning (RL) strategy for the DeepSeek-R1 family:\n• \"We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.\" This indicates that, unlike conventional RLHF pipelines that start with a supervised phase, the DeepSeek approach begins RL from the base model checkpoint.\n• \"The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\" Although the first quote downplayed SFT, this elaboration clarifies the overall 4-stage schedule: two SFT passes (seeding both reasoning and non-reasoning abilities) followed by two RL passes, each with distinct objectives—one to enhance reasoning quality and one to enforce human preference alignment.\n• \"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\" This emphasises the existence of a special variant, DeepSeek-R1-Zero, that pursued a ‘pure RL’ route (skipping SFT entirely) and achieved noteworthy reasoning skill.\nIn sum, the reinforcement-learning methodology for the DeepSeek-R1 line explores both hybrid (SFT + RL) and RL-only workflows, employs a multi-stage RL segment focused on reasoning improvement and alignment, and culminates in the release of DeepSeek-R1-Zero as proof of the effectiveness of large-scale RL without mandatory supervised warm-up.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step."
    },
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    }
  ]
}