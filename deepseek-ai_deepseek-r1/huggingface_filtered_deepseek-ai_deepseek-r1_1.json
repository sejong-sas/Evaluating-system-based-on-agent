{
  "1-1 (Weights)": "The quotes state that ‚Äúwe have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen.‚Äù  In other words, the full, original DeepSeek-R1 checkpoint, its ‚ÄòZero‚Äô variant, and a set of six distilled offspring models are all placed in the public domain for direct download.  Concretely, the project provides a HuggingFace model card: ‚Äú| DeepSeek-R1 | 671B | 37B | 128K | https://huggingface.co/deepseek-ai/DeepSeek-R1 |‚Äù.  The presence of the public HuggingFace URL indicates that anyone with a HuggingFace account can retrieve the weights through the standard HF interface (git-LFS or the HF web UI/Hub API).  No gated-access, request form, or license pre-approval flow is mentioned in the supplied text; the wording ‚Äúopen-sourced‚Äù and the direct HF link together imply immediate, no-cost availability.  In addition to the main model, the release explicitly bundles all six dense distilled variants that were ‚Äúdistilled from DeepSeek-R1 based on Llama and Qwen,‚Äù signalling that derivative checkpoints are also downloadable.  Overall, the quotes show that the entire family of DeepSeek-R1 models is publicly hosted, with the primary distribution channel being the deepseek-ai organisation page on HuggingFace.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen."
    },
    {
      "source": "[readme]",
      "quote": "| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |"
    }
  ],
  "1-2 (Code)": "Only a single sentence about code is provided: ‚ÄúPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally.‚Äù  This sentence indicates that inference or serving scripts for DeepSeek-R1 live in the DeepSeek-V3 GitHub repository and can be used to execute the model on local hardware.  The wording talks about ‚Äúrunning‚Äù rather than ‚Äútraining,‚Äù ‚Äúpre-training,‚Äù ‚Äúfine-tuning,‚Äù or ‚ÄúRL,‚Äù so the quote supports the conclusion that public code coverage is limited to deployment / inference.  There is no passage claiming that pre-training, supervised fine-tuning, or reinforcement learning pipelines are open-sourced; nor is there any mention of configuration files, datasets, or trainer scripts for those stages.  Hence, based on the provided evidence, the project publishes code that helps users load and run the already-trained checkpoints but does not share the end-to-end training stack.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally."
    }
  ],
  "1-3 (License)": "Multiple snippets repeatedly emphasise that both the repository and the model weights are governed by the MIT License: ‚ÄúThis code repository and the model weights are licensed under the MIT License.‚Äù  The README front-matter reinforces this with `license: mit`, and a badge visual (‚ÄúLicense-MIT‚Äù) plus the presence of a top-level `LICENSE` file further confirm the choice.  The license description is expansive: ‚ÄúDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs.‚Äù  Therefore users are explicitly granted the right to use the weights commercially, redistribute them, or build derivative models without additional permission.  The documentation is also transparent about the lineage-specific constraints of certain distilled releases: it lists that the Qwen-based variants are ‚Äúoriginally licensed under Apache 2.0,‚Äù while two Llama-based variants inherit their respective ‚Äúllama3.x license.‚Äù  These notes make it clear that, although DeepSeek-R1‚Äôs own contribution is MIT-licensed, downstream users must respect the original licenses of the base models for those particular checkpoints.  Taken together, the quotes depict a permissive, modification-friendly licensing posture, with explicit acknowledgement of upstream obligations where relevant.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs."
    },
    {
      "source": "[readme]",
      "quote": "- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1."
    },
    {
      "source": "[readme]",
      "quote": "- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE)."
    },
    {
      "source": "[readme]",
      "quote": "- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE)."
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplic"
    },
    {
      "source": "[readme]",
      "quote": "t: 1;\">\n <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n </a>\n</div>\n\n\n<p align=\"center\">\n <a href=\"https://github.com/deepseek-ai/Dee"
    },
    {
      "source": "[readme]",
      "quote": "forcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative w"
    },
    {
      "source": "[readme]",
      "quote": "n-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama"
    },
    {
      "source": "[readme]",
      "quote": "wen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://hugging"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE"
    }
  ],
  "1-4 (Paper)": "A single BibTeX entry is given: ‚Äútitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}.‚Äù  This confirms the existence of an official academic or technical document dedicated to the model, dated 2025 and authored under the `deepseekai` namespace.  The title reveals the core research contribution‚Äîusing reinforcement learning to ‚Äòincentivize reasoning capability‚Äô in large language models‚Äîthereby positioning the paper as the canonical reference for the algorithmic methods and evaluation results behind DeepSeek-R1.  No URL, venue, or page numbers are provided in the extract, but the BibTeX formatting and the `@misc` tag strongly suggest that the work is a pre-print (e.g., on arXiv) or a technical report hosted online.  Hence, an official written resource is available for readers who need theoretical background or experimental details about DeepSeek-R1.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},"
    }
  ]
}