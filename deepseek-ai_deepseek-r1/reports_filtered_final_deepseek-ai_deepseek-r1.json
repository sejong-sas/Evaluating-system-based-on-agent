{
  "1-1 (Weights)": "The quotes explicitly state that the weights for DeepSeek-R1 are being made publicly available. One sentence declares: “To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1…”. This indicates that the core DeepSeek-R1 checkpoint, its zero-shot variant, and multiple size-specific distilled derivatives are all released. A second sentence reinforces the open release by noting that “The open source DeepSeek-R1, as well as its API, will benefit the research community…”. Together, these quotes confirm that the pretrained weights are freely downloadable (i.e., “open-source”) and that an accompanying API endpoint is also exposed; however, no additional details about the precise download location, authentication method, or hosting platform are given in the provided text.",
  "1-2 (Code)": "Availability of training-related code is signalled by the line “deepseek-ai / DeepSeek-R1 Public”. The phrase strongly suggests that a public repository—titled exactly “DeepSeek-R1” under the “deepseek-ai” organization—is accessible. Because the repository is marked “Public,” the quote substantiates that at least some portion of code relevant to DeepSeek-R1 (potentially training, configuration, or utilities) is openly released. No further granularity (e.g., which stages of the training pipeline are included) is stated in the quoted material.",
  "1-3 (License)": "",
  "1-4 (Paper)": "The quotation set references an official write-up twice. First, it states: “We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1,” signaling that these models are formally presented in written form. Second, the explicit title of the work is provided: “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.” These lines collectively document the existence of a paper or technical report dedicated to explaining the design and training of DeepSeek-R1 (and its “-Zero” counterpart), with a focus on reinforcement-learning-based methods for reasoning performance.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    },
    {
      "source": "[pdf_text]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "deepseek-ai / DeepSeek-R1 Public"
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
    }
  ],
  "1-5 (Architecture)": "A benchmarking table explicitly labels the DeepSeek-R1 entry as a Mixture-of-Experts (MoE) model. For this variant the table lists “# Activated Params = 37 B” and “# Total Params = 671 B,” showing that only 37 billion of the 671 billion parameters are active on any given forward pass—i.e., DeepSeek-R1 is a sparse-activation MoE with a very large overall parameter pool. In a separate statement the authors add that they \"open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5 B, 7 B, 8 B, 14 B, 32 B, 70 B) distilled from DeepSeek-R1.\" This confirms that, besides the flagship MoE model, the release also includes a distilled family of fully-dense descendants spanning 1.5 billion to 70 billion parameters, with DeepSeek-R1 acting as the teacher model for those smaller checkpoints.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "For the reinforcement-learning stage the team \"use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\" Thus, GRPO is identified as the specific RL algorithm used, and DeepSeek-V3-Base is cited as the initialization checkpoint for that stage of training.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Benchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022\n0513\nV3\no1-mini o1-1217\nR1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B"
    },
    {
      "source": "[abstract]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."
    }
  ],
  "2-3 (API)": "The quotes indicate that DeepSeek-R1 is accompanied by an officially provided, publicly accessible API. The API is explicitly mentioned as being part of the same open-source release as the model (e.g., “The open source DeepSeek-R1, as well as its API…”). The stated purpose of making the API available is to “benefit the research community,” specifically by enabling researchers to perform knowledge-distillation experiments in which the reasoning patterns of larger systems are transferred into smaller models. In short, DeepSeek-R1 is not merely downloadable weights; it also offers an online interface (API) that outside users can call, with the declared goal of facilitating further research and model distillation work.",
  "3-1 (Pre-training)": "",
  "3-2 (Fine-tuning)": "All fine-tuning information revolves around a multi-stage pipeline designed for DeepSeek-R1. First, the developers perform two SFT (supervised fine-tuning) stages that are explicitly described as “the seed for the model’s reasoning and non-reasoning capabilities.” To avoid the “unstable cold start phase” of beginning RL from a raw base model, they “construct and collect a small amount of long Chain-of-Thought (CoT) data” and use it to fine-tune the model, thereby producing an initial RL actor. After an RL phase (see 3-3), they reuse the RL checkpoint to generate new SFT data via rejection sampling; this fresh data is merged with additional supervised data that already exists from DeepSeek-V3 in multiple domains (“writing, factual QA, and self-cognition”). The combined dataset is then employed to “retrain the DeepSeek-V3-Base model.” Altogether, DeepSeek-R1’s pipeline alternates between SFT and RL: SFT(1) → RL(1) → SFT(2) → RL(2), with each SFT stage either seeding capabilities or consolidating knowledge extracted from previous RL checkpoints.",
  "3-3 (Reinforcement Learning)": "DeepSeek-R1’s reinforcement-learning strategy is detailed through comparisons with its variant DeepSeek-R1-Zero. DeepSeek-R1-Zero is said to be “trained via large-scale reinforcement learning (RL) without supervised fine-tuning,” and this purely-RL approach already yields “remarkable reasoning capabilities” and “numerous powerful and intriguing reasoning behaviors.” Building on that, DeepSeek-R1 itself “incorporates multi-stage training and cold-start data before RL” to further enhance reasoning. Concretely, the quoted pipeline description states that there are “two RL stages” in DeepSeek-R1: one stage aims at “discovering improved reasoning patterns,” and a later stage focuses on “aligning with human preferences.” The project team also cites related reinforcement-learning work (e.g., “Deepseek-prover-v1.5” that uses proof-assistant feedback combined with Monte-Carlo Tree Search), underlining their broader experience with RL techniques. In summary, DeepSeek-R1 relies on a multi-phase RLHF-style procedure: (1) an initial RL block that surfaces better reasoning strategies, followed by (2) a second RL block that performs preference alignment, with both blocks embedded inside a larger SFT↔RL iterative pipeline.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors."
    },
    {
      "source": "[pdf_text]",
      "quote": "To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152."
    }
  ],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "Fine-tuning for the DeepSeek-R1 line is carried out in several clearly described stages. First, the team \"collect thousands of cold-start data\" and apply them to \"fine-tune the DeepSeek-V3-Base as the starting point for RL.\"  After this small cold-start pass, they run a more substantial supervised fine-tune in which \"We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.\"  The supervised corpus is heterogeneous.  For purely “non-reasoning data” – e.g., \"writing, factual QA, self-cognition, and translation\" – they \"adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3.\"  In contrast, to stabilise the model before reinforcement learning, the R1 variant explicitly augments the supervised set with reasoning traces: \"for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.\"  Together these statements reveal (1) at least two epochs of SFT, (2) a supervised set whose scale is on the order of 8 × 10^5 examples plus several thousand cold-start items, (3) domain composition that mixes reasoning-oriented chains-of-thought with non-reasoning text genres, and (4) partial reuse of earlier DeepSeek-V3 SFT material.",
  "4-3 (Reinforcement Learning Data)": "Once supervised fine-tuning is finished, the project moves to a “large-scale reinforcement learning training process” explicitly described as \"the same … as employed in DeepSeek-R1-Zero.\"  The RL phase \"focuses on enhancing … reasoning capabilities\" and is trained on tasks in \"coding, mathematics, science, and logic reasoning\" – domains that provide \"well-defined problems with clear solutions.\"  The authors \"adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards\".  Concretely, \"a rule-based reward system … mainly consists of two types of rewards: • Accuracy rewards …\" where correctness can be checked automatically (e.g., boxed final answers for math, or running LeetCode test cases through a compiler).  A published derivative, \"Deepseek-prover-v1.5,\" further underscores that some RL data include proof-assistant feedback and Monte-Carlo tree-search, although the citation is the only detail provided.  Overall, the RL data are automatically scored, domain-specific reasoning problems whose correctness can be verified by rules or external tools.",
  "4-4 (Data Filtering)": "Several filtering and data-cleaning mechanics are documented.  During expansion of the supervised/RL corpora the team \"incorporat[es] additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\"  Any resulting outputs that are hard to parse are removed: \"we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks.\"  A specific antidote for language mixing is applied in RL: \"we introduce a language consistency reward … calculated as the proportion of target language words in the CoT.\"  The same idea is repeated, emphasising that language-mixing problems arise \"particularly when RL prompts involve multiple languages.\"  Near the end of RL, additional supervised data are produced by \"rejection sampling on the RL checkpoint\"; only high-quality samples survive and are then \"combined with supervised data from DeepSeek-V3\" before a final retraining pass.  Altogether, the filtering criteria comprise: (1) generative-reward judgment filtering, (2) explicit removal of CoTs exhibiting mixed languages, excessive length, or embedded code, (3) a quantitative language-consistency reward (ratio of desired-language tokens), and (4) quality-based rejection sampling at convergence.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
      "source": "[pdf_text]",
      "quote": "Non-Reasoning data   For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains."
    },
    {
      "source": "[pdf_text]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: • Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases."
    },
    {
      "source": "[pdf_text]",
      "quote": "H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks."
    },
    {
      "source": "[pdf_text]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. ... To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    },
    {
      "source": "[pdf_text]",
      "quote": "…some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks."
    },
    {
      "source": "[pdf_text]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    },
    {
      "source": "[pdf_text]",
      "quote": "Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}