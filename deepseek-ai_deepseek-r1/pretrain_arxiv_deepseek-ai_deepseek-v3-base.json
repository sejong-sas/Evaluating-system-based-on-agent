{
  "pretrain_method": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open‐source base model. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. In addition, DeepSeek-V3 incorporates FP8 mixed-precision training, significantly lowering computational costs and making large-scale training more practical without compromising model quality.",
  "pretrain_data": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.",
  "__evidence": [
    {
      "source": "arxiv:2412.19437",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open‐source base model."
    },
    {
      "source": "arxiv:2505.09343",
      "quote": "DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale."
    },
    {
      "source": "arxiv:2505.09343",
      "quote": "In addition, DeepSeek-V3 incorporates FP8 mixed-precision training, significantly lowering computational costs and making large-scale training more practical without compromising model quality."
    },
    {
      "source": "arxiv:2412.19437",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ]
}