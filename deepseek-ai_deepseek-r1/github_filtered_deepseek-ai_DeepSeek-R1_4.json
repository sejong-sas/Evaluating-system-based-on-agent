{
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The fine-tuning procedure for DeepSeek-R1 is organised in multiple explicit stages. The authors state that \"the pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\"  Within this scheme, the two supervised-fine-tuning (SFT) stages constitute the core of the fine-tuning data process, providing the initial examples that endow the model with both general and reasoning-specific skills.  In a parallel branch, the team builds derivative \"DeepSeek-R1-Distill\" variants: \"DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\"  Consequently, the fine-tuning data for these distillation runs comes from a mixture of (i) parameters inherited from an open-source base model and (ii) training samples that the primary DeepSeek-R1 model has generated itself.  Together, the two SFT stages and the distillation scheme outline the composition and origin of the fine-tuning datasets used in the development of the DeepSeek-R1 series.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "\"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\"  This sentence indicates that the RL dataset for DeepSeek-R1-Zero is sufficiently extensive to train the model from scratch—no SFT warm-up is needed—yet it still delivers strong reasoning ability.  The statement \"We introduce our pipeline to develop DeepSeek-R1\" places this RL-centric training approach inside the broader, officially described development pipeline.  Together, these quotes clarify that a large-scale RL corpus forms a major component of the DeepSeek-R1 training workflow, and that at least one model variant (DeepSeek-R1-Zero) relies exclusively on this RL data to achieve high reasoning performance.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    },
    {
      "source": "[readme]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}