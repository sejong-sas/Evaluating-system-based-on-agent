{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The fine-tuning stage for the DeepSeek series begins with \"collect[ing] thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.\"  From this effort the authors obtain \"the above curated dataset of about 800k samples,\" and they \"fine-tune DeepSeek-V3-Base for two epochs\" on that corpus.  The overall methodology that ultimately yields DeepSeek-R1 is outlined as follows: \"We introduce our pipeline to develop DeepSeek-R1.  The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.\"  After DeepSeek-R1 itself has generated additional reasoning-focused material, the team extends the impact of that data: \"Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community.\"  In combination, these statements describe a fine-tuning regimen that (1) starts from a modest set of manually gathered cold-start examples, (2) scales to an 800 k-sample curated dataset, (3) applies two full epochs of supervised updates to DeepSeek-V3-Base, (4) interleaves those supervised runs with two dedicated RL stages inside a larger pipeline, and (5) eventually re-uses the resulting DeepSeek-R1 reasoning outputs to fine-tune other popular dense architectures.",
  "4-3 (Reinforcement Learning Data)": "The reinforcement-learning phase is repeatedly described as the same \"large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero.\"  It is launched only \"after fine-tuning DeepSeek-V3-Base on the cold start data,\" and its explicit goal is to \"enhanc[e] the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions.\"  The reward design is rule-based and centers on two primary signals: \"Accuracy rewards: The accuracy reward model evaluates whether the response is correct,\" and \"Format rewards: … we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.\"  A supplementary signal combats multilingual drift: \"To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.\"  Thus, the RL dataset is implicitly composed of task instances from coding, math, science, and logic; its evaluations depend on correctness, adherence to a designated thinking-tag format, and the measured fraction of target-language tokens, all of which guide policy updates within the DeepSeek-R1 pipeline.",
  "4-4 (Data Filtering)": "Data cleaning in the DeepSeek-R1 development flow is centered on readability and language consistency.  During supervised data construction, the team states: \"When creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.\"  They reiterate the motivation by contrasting with a prior variant: \"Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading.  In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.\"  Filtering therefore explicitly discards any items whose answers do not conform to the readable template.  During the RL stage, an additional automatic criterion is applied: \"To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.\"  Although expressed as a reward, this proportion acts as a continuous filter that penalizes—and effectively suppresses—examples containing excessive words from non-target languages.  Together, the read-time response pattern enforcement and the language-proportion reward constitute the documented filtering mechanisms used when building and refining datasets for DeepSeek-R1.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Approach §2.3.1 Cold Start]",
      "quote": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "[sections/Approach §2.3.3 Rejection Sampling and Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
      "source": "[sections/1.1 Contributions]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[sections/1.1 Contributions]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Approach §2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions."
    },
    {
      "source": "[sections/Approach §2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: • Accuracy rewards: The accuracy reward model evaluates whether the response is correct. • Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags."
    },
    {
      "source": "[sections/2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:"
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Approach §2.3.1 Cold Start]",
      "quote": "When creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    },
    {
      "source": "[sections/2.3.1 Cold Start]",
      "quote": "Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    }
  ]
}