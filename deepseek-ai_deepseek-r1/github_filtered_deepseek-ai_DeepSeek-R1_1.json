{
  "1-1 (Weights)": "The provided material explicitly states that the core DeepSeek-R1 weights have been released to the public. In the sentence ‚ÄúTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen,‚Äù the authors confirm that the complete base model as well as derivative dense checkpoints are freely downloadable. A second line presents a table row ‚Äî ‚Äú| DeepSeek-R1 | 671B | 37B | 128K | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1) |‚Äù ‚Äî which shows that the checkpoint is hosted on HuggingFace and gives core statistics (671 B total parameters, 37 B trainable, 128 K sequence length). Accessibility is therefore self-serve through the linked HF hub. Finally, the quote ‚ÄúThis code repository and the model weights are licensed under the [MIT License]‚Äù confirms that no gated request, email, or special agreement is required; anyone who accepts the MIT terms can obtain and redistribute the weights. In short, the weights for DeepSeek-R1 and its immediate family are fully published under MIT, placed on HuggingFace for direct download, and accompanied by similarly open sibling variants (DeepSeek-R1-Zero and six dense distilled versions).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen."
    },
    {
      "source": "[readme]",
      "quote": "| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |"
    },
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE)."
    }
  ],
  "1-2 (Code)": "The corpus indicates that the training-related code is available in the same repository that holds the weights. The statement ‚ÄúThis code repository and the model weights are licensed under the MIT License‚Äù shows that the repository is public and governed by an open license. The authors go beyond simple inference scripts, noting ‚ÄúWe introduce our pipeline to develop DeepSeek-R1,‚Äù which implies inclusion of development / training pipeline components (data preparation, configuration, or scheduling logic) rather than merely runtime utilities. A follow-up sentence ‚Äî ‚ÄúPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally‚Äù ‚Äî hints that certain deployment helpers might live in a companion repository (DeepSeek-V3), but the reference to ‚Äúour pipeline‚Äù in the current repo underscores that at least part of the end-to-end training workflow is documented or scripted here. Therefore, the available code base openly covers both the core DeepSeek-R1 training pipeline and any associated utilities, all released under MIT.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the MIT License."
    },
    {
      "source": "[readme]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1."
    },
    {
      "source": "[readme]",
      "quote": "Please visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally."
    }
  ],
  "1-3 (License)": "Every licensing excerpt centers on the permissive MIT grant. The headline sentence ‚Äî ‚ÄúThis code repository and the model weights are licensed under the [MIT License]‚Äù ‚Äî identifies the license type and scope (code + weights). The clause ‚ÄúDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs‚Äù explicitly asserts that (a) commercial exploitation and (b) derivative model creation are authorized. The longer legal extract ‚Äî ‚ÄúPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software‚Ä¶‚Äù ‚Äî enumerates the traditional MIT rights: unrestricted use, modification, redistribution, sublicense, and sale. No special carve-outs, non-commercial clauses, or evaluation-only constraints appear. Consequently, users are granted full (a) use, (b) modification, (c) redistribution, and (d) commercial rights with only the usual MIT obligation to include the license and copyright notice.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE)."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs."
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    }
  ],
  "1-4 (Paper)": "The project is accompanied by an official technical report. A direct hyperlink is supplied: ‚Äú<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>,‚Äù pointing to a PDF in the same GitHub repository. The BibTeX header reinforces its formal status: ‚Äú@misc{deepseekai2025deepseekr1incentivizingreasoningcapability, title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, author={DeepSeek-AI}.‚Äù Hence, there is a citable manuscript entitled ‚ÄúDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,‚Äù authored by the DeepSeek-AI team and hosted alongside the code/weights. This document constitutes the primary reference describing model architecture, training procedure, and results.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},"
    }
  ]
}