{
  "1-1 (Weights)": "The provided material explicitly states that the core DeepSeek-R1 weights have been released to the public. In the sentence ‚ÄúTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen,‚Äù the authors confirm that the complete base model as well as derivative dense checkpoints are freely downloadable. A second line presents a table row ‚Äî ‚Äú| DeepSeek-R1 | 671B | 37B | 128K | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1) |‚Äù ‚Äî which shows that the checkpoint is hosted on HuggingFace and gives core statistics (671 B total parameters, 37 B trainable, 128 K sequence length). Accessibility is therefore self-serve through the linked HF hub. Finally, the quote ‚ÄúThis code repository and the model weights are licensed under the [MIT License]‚Äù confirms that no gated request, email, or special agreement is required; anyone who accepts the MIT terms can obtain and redistribute the weights. In short, the weights for DeepSeek-R1 and its immediate family are fully published under MIT, placed on HuggingFace for direct download, and accompanied by similarly open sibling variants (DeepSeek-R1-Zero and six dense distilled versions).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen."
    },
    {
      "source": "[readme]",
      "quote": "| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |"
    },
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE)."
    }
  ],
  "1-2 (Code)": "The corpus indicates that the training-related code is available in the same repository that holds the weights. The statement ‚ÄúThis code repository and the model weights are licensed under the MIT License‚Äù shows that the repository is public and governed by an open license. The authors go beyond simple inference scripts, noting ‚ÄúWe introduce our pipeline to develop DeepSeek-R1,‚Äù which implies inclusion of development / training pipeline components (data preparation, configuration, or scheduling logic) rather than merely runtime utilities. A follow-up sentence ‚Äî ‚ÄúPlease visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally‚Äù ‚Äî hints that certain deployment helpers might live in a companion repository (DeepSeek-V3), but the reference to ‚Äúour pipeline‚Äù in the current repo underscores that at least part of the end-to-end training workflow is documented or scripted here. Therefore, the available code base openly covers both the core DeepSeek-R1 training pipeline and any associated utilities, all released under MIT.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the MIT License."
    },
    {
      "source": "[readme]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1."
    },
    {
      "source": "[readme]",
      "quote": "Please visit DeepSeek-V3 repo for more information about running DeepSeek-R1 locally."
    }
  ],
  "1-3 (License)": "Every licensing excerpt centers on the permissive MIT grant. The headline sentence ‚Äî ‚ÄúThis code repository and the model weights are licensed under the [MIT License]‚Äù ‚Äî identifies the license type and scope (code + weights). The clause ‚ÄúDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs‚Äù explicitly asserts that (a) commercial exploitation and (b) derivative model creation are authorized. The longer legal extract ‚Äî ‚ÄúPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software‚Ä¶‚Äù ‚Äî enumerates the traditional MIT rights: unrestricted use, modification, redistribution, sublicense, and sale. No special carve-outs, non-commercial clauses, or evaluation-only constraints appear. Consequently, users are granted full (a) use, (b) modification, (c) redistribution, and (d) commercial rights with only the usual MIT obligation to include the license and copyright notice.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE)."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs."
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    }
  ],
  "1-4 (Paper)": "The project is accompanied by an official technical report. A direct hyperlink is supplied: ‚Äú<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>,‚Äù pointing to a PDF in the same GitHub repository. The BibTeX header reinforces its formal status: ‚Äú@misc{deepseekai2025deepseekr1incentivizingreasoningcapability, title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, author={DeepSeek-AI}.‚Äù Hence, there is a citable manuscript entitled ‚ÄúDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,‚Äù authored by the DeepSeek-AI team and hosted alongside the code/weights. This document constitutes the primary reference describing model architecture, training procedure, and results.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},"
    }
  ],
  "1-5 (Architecture)": "The available details explicitly state that ‚ÄúDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.‚Äù  This indicates that the architectural blueprint of DeepSeek-R1 is not bespoke but rather an adoption (or at least a very close derivative) of the DeepSeek-V3-Base design.  The documentation further directs readers to external material‚Äî‚ÄúFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.‚Äù  Consequently, the core architectural parameters (layer counts, hidden sizes, attention heads, etc.) are expected to mirror those published for DeepSeek-V3-Base, and any fine-tuning or variant-specific adjustments for DeepSeek-R1 would be incremental extensions of that same layout.  Apart from this inheritance statement and the reference link, no additional architectural hyperparameters (e.g., depth, width, positional-encoding scheme, normalization strategy) are explicitly provided in the quotes.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    },
    {
      "source": "[readme]",
      "quote": "For more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The material explicitly states that DeepSeek makes an \"OpenAI-Compatible\" interface available: \"We also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com.\" This indicates that the DeepSeek ecosystem (and, by inclusion, models such as DeepSeek-R1) can be accessed over the network through an HTTP-style programmatic endpoint that imitates the request / response format popularised by the OpenAI API. The sentence also reveals that the entry point is public and documented at the URL https://platform.deepseek.com/, implying hosted inference rather than a mere local library. No other access modes, rate limits, or authentication schemes are mentioned in the provided excerpts.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "The only direct pre-training detail provided is the statement: \"DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\" This establishes that both DeepSeek-R1 and its variant DeepSeek-R1-Zero inherit their initial weights or architectural foundation from a predecessor model named DeepSeek-V3-Base. The quote, however, does not supply further hyper-parameters, dataset composition, token counts, or training hardware; it simply identifies the initialization lineage and thus positions DeepSeek-R1 as a continued-pre-training (or further-pre-training) effort on top of the V3-Base checkpoint.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    }
  ],
  "3-2 (Fine-tuning)": "Two sentences outline the fine-tuning activities connected to DeepSeek-R1:\n1) \"Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community.\" This conveys that inference outputs produced by DeepSeek-R1 were collected as \"reasoning data,\" and those generated samples then served as supervision to further fine-tune multiple existing dense (i.e., standard, non-Mixture-of-Experts) language models.\n2) \"DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\" Here, the Distill-branded variants are specifically described as open-source model checkpoints that underwent an additional fine-tuning pass driven by data generated from DeepSeek-R1 itself.\nTogether these lines describe a knowledge-distillation or self-instruct style pipeline: DeepSeek-R1 first produces reasoning demonstrations; those demonstrations then become training data for other dense or open-source backbones, yielding \"DeepSeek-R1-Distill\" derivatives. No numeric hyper-parameters or dataset sizes are revealed in the quoted material.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    }
  ],
  "3-3 (Reinforcement Learning)": "Three supplied sentences map out the reinforcement-learning (RL) strategy for the DeepSeek-R1 family:\n‚Ä¢ \"We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step.\" This indicates that, unlike conventional RLHF pipelines that start with a supervised phase, the DeepSeek approach begins RL from the base model checkpoint.\n‚Ä¢ \"The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\" Although the first quote downplayed SFT, this elaboration clarifies the overall 4-stage schedule: two SFT passes (seeding both reasoning and non-reasoning abilities) followed by two RL passes, each with distinct objectives‚Äîone to enhance reasoning quality and one to enforce human preference alignment.\n‚Ä¢ \"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\" This emphasises the existence of a special variant, DeepSeek-R1-Zero, that pursued a ‚Äòpure RL‚Äô route (skipping SFT entirely) and achieved noteworthy reasoning skill.\nIn sum, the reinforcement-learning methodology for the DeepSeek-R1 line explores both hybrid (SFT + RL) and RL-only workflows, employs a multi-stage RL segment focused on reasoning improvement and alignment, and culminates in the release of DeepSeek-R1-Zero as proof of the effectiveness of large-scale RL without mandatory supervised warm-up.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step."
    },
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    }
  ],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The fine-tuning procedure for DeepSeek-R1 is organised in multiple explicit stages. The authors state that \"the pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\"  Within this scheme, the two supervised-fine-tuning (SFT) stages constitute the core of the fine-tuning data process, providing the initial examples that endow the model with both general and reasoning-specific skills.  In a parallel branch, the team builds derivative \"DeepSeek-R1-Distill\" variants: \"DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\"  Consequently, the fine-tuning data for these distillation runs comes from a mixture of (i) parameters inherited from an open-source base model and (ii) training samples that the primary DeepSeek-R1 model has generated itself.  Together, the two SFT stages and the distillation scheme outline the composition and origin of the fine-tuning datasets used in the development of the DeepSeek-R1 series.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "\"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\"  This sentence indicates that the RL dataset for DeepSeek-R1-Zero is sufficiently extensive to train the model from scratch‚Äîno SFT warm-up is needed‚Äîyet it still delivers strong reasoning ability.  The statement \"We introduce our pipeline to develop DeepSeek-R1\" places this RL-centric training approach inside the broader, officially described development pipeline.  Together, these quotes clarify that a large-scale RL corpus forms a major component of the DeepSeek-R1 training workflow, and that at least one model variant (DeepSeek-R1-Zero) relies exclusively on this RL data to achieve high reasoning performance.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    },
    {
      "source": "[readme]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}