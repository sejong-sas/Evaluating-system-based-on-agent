{
  "model_id": "microsoft/phi-4",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/pdf/2412.08905",
      "full_text": "Phi-4 Technical Report\nMarah Abdin\nJyoti Aneja\nHarkirat Behl\nS´ebastien Bubeck\nRonen Eldan\nSuriya Gunasekar\nMichael Harrison\nRussell J. Hewett\nMojan Javaheripi\nPiero Kauffmann\nJames R. Lee\nYin Tat Lee\nYuanzhi Li\nWeishung Liu\nCaio C. T. Mendes\nAnh Nguyen\nEric Price\nGustavo de Rosa\nOlli Saarikivi\nAdil Salim\nShital Shah\nXin Wang\nRachel Ward\nYue Wu\nDingli Yu\nCyril Zhang\nYi Zhang\nMicrosoft Research\nAbstract\nWe present phi-4, a 14-billion parameter language model developed with a training recipe that\nis centrally focused on data quality. Unlike most language models, where pre-training is based pri-\nmarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic\ndata throughout the training process. While previous models in the Phi family largely distill the\ncapabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model\non STEM-focused QA capabilities, giving evidence that our data-generation and post-training tech-\nniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size – especially on reasoning-focused benchmarks – due to improved data,\ntraining curriculum, and innovations in the post-training scheme.\n1\nIntroduction\nRecent advancements in Large Language Models (LLMs) have shown that significant improvements in\ndata quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling\ncompute with model and dataset size. Building on the success of the Phi family [GZA+23, LBE+23,\nJBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance\nof small language models by introducing innovative synthetic data generation methods for reasoning-\nfocused tasks, by optimizing the training curriculum and data mixture, and by introducing new tech-\nniques in post-training.\nSynthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse\narray of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal.\nThese methods enable the construction of datasets that induce stronger reasoning and problem-solving\nabilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic\ndata in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and\na novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs.\nThe development of phi-4 is guided by three core pillars:\n1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are de-\nsigned to prioritize reasoning and problem-solving, carefully generated to ensure diversity and\n1\narXiv:2412.08905v1  [cs.CL]  12 Dec 2024\n\nSmall models\nLarge models\nphi-4\n14b\nphi-3\n14b\nQwen 2.5\n14b instruct\nGPT\n4o-mini\nLlama-3.3\n70b instruct\nQwen 2.5\n72b instruct\nGPT\n4o\nsimple-evals\nMMLU\n84.8\n77.9\n79.9\n81.8\n86.3\n85.3\n88.1\nGPQA\n56.1\n31.2\n42.9\n40.9\n49.1\n49.0\n50.6\nMATH\n80.4\n44.6\n75.6\n73.0\n66.31\n80.0\n74.6\nHumanEval\n82.6\n67.8\n72.1\n86.2\n78.91\n80.4\n90.6\nMGSM\n80.6\n53.5\n79.6\n86.5\n89.1\n87.3\n90.4\nSimpleQA\n3.0\n7.6\n5.4\n9.9\n20.9\n10.2\n39.4\nDROP\n75.5\n68.3\n85.5\n79.3\n90.2\n76.7\n80.9\nMMLUPro\n70.4\n51.3\n63.2\n63.4\n64.4\n69.6\n73.0\nHumanEval+\n82.8\n69.2\n79.1\n82.0\n77.9\n78.4\n88.0\nArenaHard\n75.4\n45.8\n70.2\n76.2\n65.5\n78.4\n75.6\nLiveBench\n47.6\n28.1\n46.6\n48.1\n57.6\n55.3\n57.6\nIFEval\n63.0\n57.9\n78.7\n80.0\n89.3\n85.0\n84.8\nPhiBench\n(internal)\n56.2\n43.9\n49.8\n58.7\n57.1\n64.6\n72.4\nTable 1: Performance of phi-4\non a set of standard benchmarks. The first set of benchmarks uses OpenAI’s\nsimple-evals framework [Ope24b], specifying the prompts/extraction/temperature=0.5. We compare to small\nmodels of similar inference cost, as well as to larger models.\nrelevance. We change our training curriculum and create new pretraining and midtraining data\nmixtures to increase the allocation of synthetic tokens, compared to older generations of phi.\n2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter\norganic2 data sources, including web content, licensed books, and code repositories to extract seeds\nfor the synthetic data pipeline that encourage high-depth reasoning and prioritize educational\nvalue (to the model). These seeds form the foundation of the synthetic generation pipeline. To\ncomplement these synthetic datasets, we also filter the web for high-quality data (in terms of\nknowledge and reasoning) to use directly in pretraining.\n3. Post-Training: We further advance the post-training recipe in phi-4 by creating new refined\nversions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on\npivotal token search.\nWith these innovations, the performance of phi-4 on reasoning-related tasks is comparable to or\nsurpasses much larger models. For example, its performance on many widely used reasoning-related\nbenchmarks meets or exceeds that of Llama-3.1-405B.\nIn Table 1 we compare the performance of\nour model on academic benchmarks to several contemporary foundation models.\nWe find that phi-\n4 significantly exceeds its teacher GPT-4o on the GPQA (graduate-level STEM Q&A) and MATH\n(math competition) benchmarks.\n1These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement\nthat Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but\nMeta reports 77 for MATH and 88 for HumanEval on Llama-3.3.\n2We use organic to refer to human-generated or otherwise non-synthetic data.\n2\n\nLlama-3.3\n70B Instruct\nClaude 3.5\nSonnet\nQwen 2.5\n14b-Instruct\nGPT\n4o\nGPT\n4o-mini\nQwen 2.5\n72b-Instruct\nGemini\nFlash 1.5\nGemini\nPro 1.5\nphi-4\n60\n70\n80\n90\nAverage score\n(temperature = 0.5)\n66.4\n74.8\n77.4\n77.9\n78.2\n78.7\n81.6\n89.8\n91.8\nAverage performance on November 2024 AMC 10/12 tests\nLarge model\nSmall model\nFigure 1: Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is\nthe average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. We chose\nt = 0.5 to follow simple-evals [Ope24b]. Error bars are 2σ of the estimate. On competition math, phi-4 scores\nwell above its weight-class even compared to non–open-weight models.\n1.1\nAddressing Overfitting and Data Contamination\nDecontamination:\nOne pitfall of foundation models is overfitting to benchmarks, such as through\nthe leakage of benchmark test sets via the web corpus. We improved the data decontamination process\nfor phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results. More\ndetails of the decontamination method are given in Appendix B.\nAMC Benchmark:\nThe surest way to guard against overfitting to the test set is to test on fresh data.\nWe tested our model on the November 2024 AMC-10 and AMC-12 math competitions [Com24], which\noccurred after all our training data was collected, and we only measured our performance after choosing\nall the hyperparameters in training our final model. These contests are the entry points to the Math\nOlympiad track in the United States and over 150,000 students take the tests each year.\nIn Figure 1 we plot the average score over the four versions of the test, all of which have a maximum\nscore of 150. phi-4 outperforms not only similar-size or open-weight models but also much larger frontier\nmodels. Such strong performance on a fresh test set suggests that phi-4’s top-tier performance on the\nMATH benchmark is not due to overfitting or contamination. We provide further details in Appendix C.\nRelying on Contamination-Proof Benchmarks:\nWe give significant weight to benchmarks which\nwere designed in such a way that the questions are original and do not appear on the web, such as\nGPQA [RHS+23]. While optimizing our model, we relied on an internal benchmark composed primarily\nof original prompts written by the team (see Section 5 for further details).\nLong Chain-of-Thought Models:\nA style of LLM that scales inference-time compute by generating\nlong chains of thought has emerged over the past few months, as pioneered by OpenAI O1 [Ope24a] and\nfollowed by DeepSeek-R1-Lite-Preview [Dee24] and Qwen/QwQ-32B-Preview [Tea24].\nThese models\nperform well on reasoning benchmarks, where QwQ, the only such model with open weights, averages\n124.5 points in the AMC-10/12 setting of Figure 1. However, QwQ also uses 4X more tokens on this\ntask than phi-4 and has more than twice as many parameters. Thus, the inference cost of QwQ is an\n3\n\norder of magnitude higher than phi-4. Consequently, these models are not in the same class as phi-4 with\nrespect to cost or latency.\n2\nApproach to Data\nThe pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of tech-\nniques. In addition, we employ several methods for filtering organic data sources that are used both as\ncomplementary datasets in the pretraining and as seeds for generating synthetic data.\n2.1\nPurpose of Synthetic Data\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi\nseries of models has consistently emphasized the importance of synthetic data. Rather than serving as\na cheap substitute for organic data, synthetic data has several direct advantages over organic data.\nStructured and Gradual Learning.\nIn organic datasets, the relationship between tokens is often\ncomplex and indirect. Many reasoning steps may be required to connect the current token to the next,\nmaking it challenging for the model to learn effectively from next-token prediction. By contrast, each\ntoken generated by a language model is by definition predicted by the preceding tokens, making it easier\nfor a model to follow the resulting reasoning patterns. In this way, synthetic data may act as a form of\n“spoonfeeding,” presenting challenges in a digestible and progression-oriented manner.\nA simple example to illustrate this is that a human-written solution to a math problem might start\nwith the final answer. This answer is much too hard to output immediately, for either a human or an\nLLM—the human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce\nit linearly. Synthetic solutions to math problems will not have such roadblocks.\nAlignment with Inference Contexts.\nSynthetic data is typically closer to the format of outputs\nwe expect our models to generate. Training on such data helps align the model’s pretraining experience\nwith the scenarios it encounters during inference. This alignment ensures that the context seen during\ngeneration remains in-distribution with respect to the data the model was pretrained on.\nFor example, web forums are very different in style from LLM interactions. If a fact only appears\nin web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces.\nRewriting facts from the web forum into the language style of an LLM makes the facts more accessible\nduring the LLM chat context of inference.\nPrinciples.\nOur approach to generating synthetic data for phi-4 is guided by the following principles:\n1. Diversity: The data should comprehensively cover subtopics and skills within each domain. This\nrequires curating diverse seeds from organic sources.\n2. Nuance and Complexity: Effective training requires nuanced, non-trivial examples that reflect\nthe complexity and the richness of the domain. Data must go beyond basics to include edge cases\nand advanced examples.\n3. Accuracy: Code should execute correctly, proofs should be valid, and explanations should adhere\nto established knowledge, etc.\n4\n\n4. Chain-of-Thought: Data should encourage systematic reasoning, teaching the model various\napproaches to the problems in a step-by-step manner. This fosters coherent outputs for complex\ntasks.\n2.2\nSynthetic Data for Pretraining and Midtraining\nWe created 50 broad types of synthetic datasets, each one relying on a different set of seeds and dif-\nferent multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction,\naccumulating to a total of about 400B unweighted tokens. In Appendix D, we give a few examples\nof transcripts taken from our synthetic generations. Here, we highlight novel methodologies used in\ngenerating synthetic datasets for phi-4:\n• Seed Curation: The synthetic dataset generation begins with high-quality seeds sourced from\nmultiple domains.\nThese curated seeds provide the foundation for synthetic data generation,\nenabling the creation of exercises, discussions, and reasoning tasks tailored to the model’s training\nobjectives.\n1. Web and Code-based Seeds: Excerpts and snippets are extracted from web pages, books,\nand code repositories with a focus on content that demonstrates high complexity, reasoning\ndepth, and educational value. To ensure quality, we employ a two-stage filtering process:\nfirst, identifying pages with strong educational potential, and second, segmenting the selected\npages into passages, scoring each for its factual and reasoning content.\n2. Question Datasets: A large set of questions was collected from websites, forums, and Q&A\nplatforms. These questions were then filtered using a plurality-based technique to balance\ndifficulty.\nSpecifically, we generated multiple independent answers for each question and\napplied majority voting to assess the consistency of responses. We discarded questions where\nall answers agreed (indicating the question was too easy) or where answers were entirely\ninconsistent (indicating the question was too difficult or ambiguous). This filtering process\nproduces a dataset of questions that challenge the model’s reasoning and problem-solving\nabilities while remaining approachable.\nThe plurality answers were used in place of the\nground truth in our rejection-sampling based generations.\n3. Creating Question-Answer pairs from Diverse Sources: Another technique we use\nfor seed curation involves leveraging language models to extract question-answer pairs from\norganic sources such as books, scientific papers, and code. This approach does not rely on\nmerely identifying explicit Q&A pairs within the text. Instead, it involves a pipeline designed\nto detect deduction chains or logical progressions in the text. The language model identifies\nkey steps in reasoning or problem-solving processes and reformulates them into questions\nand corresponding answers. Our experiments show that, if done correctly, training on the\nresulting content can be far more effective (in terms of improvement on academic and internal\nbenchmarks) than training on the original content.\n• Rewrite and Augment: Seeds are transformed into synthetic data through multi-step prompting\nworkflows. This includes rewriting most of the useful content in given passages into exercises,\ndiscussions, or structured reasoning tasks.\n• Self-revision: The initial responses are then iteratively refined through a feedback loop where\na model critiques and subsequently improves its own outputs, guided by the rubrics focused on\nreasoning and factual accuracy.\n5\n\n• Instruction Reversal for Code and Other Tasks: To enhance the model’s ability to generate\noutputs from instructions, we used an instruction reversal technique. For example, we take existing\ncode snippets from the code data corpus and use it to generate corresponding instructions that\ninclude the problem description or task prompt. The resulting synthetic data pairs were structured\nwith the instruction appearing before the code. Only data with high fidelity between the original\nand regenerated code are retained, ensuring alignment between the instructions and the outputs.\nThis method can be generalized to other targeted use cases.\n• Validation of Code and Other Scientific Data: When appropriate, we incorporate tests for\nvalidating our reasoning-heavy synthetic datasets. The synthetic code data is validated through ex-\necution loops and tests. For scientific datasets, the questions are extracted from scientific materials\nusing a method designed to ensure high relevance, groundedness, and difficulty balance.\n2.3\nCuration and Filtering of Web and Q&A Data\nQ&A datasets.\nWe collected tens-of-millions high-quality organic problems and solutions by review-\ning public websites, relying on existing datasets, and acquiring external datasets. Our experience from\nprevious models showed that question-answer data contributed significantly to various capabilities, such\nas mathematical reasoning and academic performance. Our ablation studies showed that organic ques-\ntions are substantially more effective than synthetic questions. We used several ways to synthetically\naugment the dataset of organic questions to obtain a larger dataset. While these rewritten questions\nimproved the model’s capabilities, the gains were not as pronounced. A significant portion of the col-\nlected questions lacked accurate solutions. To address this, we replaced the answers with synthetically\ngenerated ones and used majority-voting to increase accuracy. All collected questions and solutions\nunderwent a thorough decontamination process to ensure there is no overlap with test sets3.\nTargeting High-quality Web Data.\nWe collected a wide variety of high-quality organic data sources\nfor phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,\nand programming tutorials). In addition to directly training on this text, we used various web sources\nas seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to\nbe absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations\nfor derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web\ndata. We discuss the main techniques and considerations below:\n• Targeted Acquisitions: We included major repositories of reasoning-dense documents that are\npublicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g.,\nlicensed books) aiming for a level of comprehensiveness, recency, and cleanliness above the typical\nstandard of externally available corpora.\n• Filtering Web Dumps: To capture the long tail of information-rich web sources (e.g., forums,\nblogs, course material, domain-specific wikis), we took the approach of selecting a small fraction\nof highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on\n∼106 LLM-generated annotations. This approach tends to over-index on STEM-related keywords,\nso we created a specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history,\ntravel, culture, and entertainment). These topic classifications were also obtained by distilling an\n3This step is crucial to the reliability of some of the academic benchmarks: for instance, some test benchmark variants\ncan be found on platforms like Hugging Face. Moreover, benchmarks such as MMLU are frequently compiled from web-\nsourced questions.\n6\n\nLLM annotator. Finally, we removed corrupted text and binary files by detecting outliers according\nto n-gram statistics and compression ratios.\n• Multilingual Data: We incorporated multilingual datasets to ensure that our model could han-\ndle a wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi\nand Japanese. This involved sourcing and processing high-quality multilingual documents from\nCommonCrawl and Wikipedia. Our multilingual processing pipeline consists of a language identifi-\ncation model, based on fastText used to categorize documents into 176 languages, then uses the\nsame classifiers for filtering web dumps to filter for quality. Note that the classifiers were trained\non multilingual LLM-generated annotations.\n• Custom Extraction and Cleaning Pipelines: To ensure sufficient cleanliness and uniformity\nbetween heterogeneous organic data sources, we needed a collection of customized heuristics and\nparsers. For each targeted data source, we built custom pipelines to ingest a variety of file formats\n(e.g., multi-file TeX source, ePub and other XML-like formats, Microsoft Word documents, and\nPDFs).\nFor general web data, we built a custom HTML-to-text extractor, taking significant\ncare to preserve fragile content that is frequently corrupted by na¨ıve parsers (e.g., TeX/MathML\nequations, code blocks, tables, and forum thread structure). This extractor prunes and normalizes\nthe DOM tree, using a variety of signals (e.g., HTML tag names, CSS classes, content length, and\ntree depth) to distinguish elements such as boilerplate, advertisements, equations, and syntax-\nhighlighter artifacts.\n2.4\nPost-Training datasets\nOur post-training data is composed of:\n• Supervised Fine-Tuning (SFT) Datasets: Using carefully curated user prompts taken from\na mixture of publicly available datasets and synthetically generated data, we generate multiple\nmodel responses and select the best using an LLM-based evaluation process.\n• Direct Preference Optimization (DPO): We generate DPO pairs based on rejection sampling\nand LLM evaluation, a part of which is based on our approach to creating pivotal token-based\npairs, explained in Section 4.3 below.\n3\nPretraining details\nThe phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and\na default context length of 4096. This is later extended to a 16K context length during midtraining. The\narchitecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better\nmultilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use\nfull attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium.\nThe model was pretrained for approximately 10T tokens using linear warm-up and decay schedules\nwith peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. The\ntraining hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted\nby stress testing the learning rate warm-up stage for stability.\nPretraining is followed by a shorter\nmidtraining stage to increase the original context length of 4k to 16k.\nSince pre-trained models are not good at instruction following, it is not very informative to use\n0-shot evaluations that require the answer to be in a specific format, for example simple-evals. We\n7\n\nMMLU\nMMLU pro\nGSM8k\nHuman-Eval\nARCC\nMBPP\nMATH\nTQA\nphi-4 (4k)\n+3.0\n+10.3\n+2.2\n+7.8\n+1.1\n+6.8\n+8.9\n-0.7\nphi-4 (16k)\n+2.7\n+8.9\n+1.2\n+9.0\n+0.9\n+9.6\n+8.4\n-1.5\nTable 2: Pretraining benchmarks for phi-4 compared to its predecessor, phi-3-medium after pretraining.\ntherefore use an internal implementation of benchmarks for pretraining which uses a mixture of log-\nlikelihood and/or few-shot prompts for various tasks. Specifically, we used log-likelihood evaluations\nfor MMLU (5-shot), MMLU-pro, and ARCC (1-shot). We used 1, 3, 4, and 8 few-shot examples for\nTriviaQA (TQA), MBPP, MATH, and GSM8k to help the model adhere to the answer format for easier\nextraction of the solution. We use this evaluation method throughout Section 3. Table 2 summarizes\nthe performance boost of pretrained phi-4 compared with its predecessor phi-3-medium.\n3.1\nData Composition in Pretraining\nThe phi-3 model family were trained using a two-phase strategy. Most of the training tokens were used\nin phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with a\ndata mixture consisting primarily of synthetic tokens and a much smaller allocation for ultra-filtered\nand reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed a\nmarginal drop in the benefit from using non-synthetic tokens for the phi-3 family of model sizes. We\nnote two key observations.\n• Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs\nover our synthetic data led to better performance with respect to adding fresh web tokens.\n• Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and\ndemonstrated increased hallucinations.\nFigure 2 demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In\nthis example, we conduct two training runs per model scale, using the same number of training tokens\non top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed\n(a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and\n12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen,\nperforming more iterations on the synthetic data is more beneficial than supplying more web tokens.\nInspired by this scaling behavior of our synthetic data, we trained a 13B parameter model solely on\nsynthetic4 data, for ablation purposes only – the model sees over 20 repetitions of each data source. For\nthe sake of ablations, we partitioned our synthetic data into web rewrites, which includes more direct\nrewrites of our filtered web content relative to all other types of synthetic data. Table 3 compares the\nprevious phi-3-medium model with the new model trained entirely on the synthetic data. Throughout\ntraining, all benchmarks consistently improved, despite the increase in epochs, and the majority of\nthe benchmarks showed improvements over phi-3. However, knowledge-related benchmarks, like 1-shot\ntriviaqa (TQA), show a large gap where synthetic models are subpar. These observations led us to\nrethink the role of web data in our data mixture.\n4This is an updated mixture of synthetic data that contains new sources compared to phi-3.\n8\n\nFigure 2: 5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are\ntrained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens.\nWe see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch\nmodels perform better than those that have seen more unique web tokens.\nMMLU\nMMLU pro\nGSM8k\nHuman-Eval\nARCC\nMBPP\nMATH\nTQA\nSynthetic\n+0.8\n+4.0\n+2.2\n+12.1\n0.0\n+5.0\n+4.9\n-14.8\nSynthetic + Web Rewrites\n+0.3\n+4.1\n+1.8\n+13.3\n+3.0\n+7.6\n+8.1\n-7.7\nTable 3: Benchmark performance of 13B models (used for ablations only) trained on data mixtures containing\nno web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic\ndata and web rewrites. All numbers are reported relative to the performance of phi-3-medium, which has seen a\ncombination of web and synthetic data.\n3.2\nData Mixture\nTo design our pretraining data mixture for a given training token budget, we search over different\nallocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites5, 3) filtered web\n(divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g.,\nacademic data, books, and forums), and 5) code data.\nWe conducted ablations using a shorter token horizon of 1T tokens to derive the data mixture.\nThese ablations rely on our established result on the high-rank correlation of short training with longer\ntraining, up to the over-fitting saturation threshold of data sources. In addition we observe a high rank\ncorrelation between the performance of the 7B and 14B models on different data mixtures, given a large\nenough distance between the data mixtures. This allowed us to conduct the experiments at 7B scale and\ntransfer the findings to phi-4. Among the numerous ablations, we highlight a few that show best insights\non our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions\nand code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters.\nTable 4 summarizes the results for the hand-picked ablations, as compared with the data mixture\nthat was used for the final training run. A uniform allocation of tokens among the three categories is\nsuboptimal due to the higher quality of synthetic data and the only benchmark that shows a clear benefit\nfrom web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally\nbetter than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy\nfiltered web data sources to improve knowledge benchmarks (see Section 3.1) to balance all model\n5Web rewrites is a sub-category of synthetic data that is substantially large and contains direct rewrites of web content.\n9\n\nMMLU\nMATH\nGSM8k\nHuman-Eval\nARCC\nMBPP\nTQA\nMMLU pro\nAverage\nUniform\n-3.3\n-5.4\n-5.8\n-1.2\n+0.6\n-2.0\n+3.3\n-3.6\n-2.2\nS\n+3.3\n+4.0\n+2.1\n-6.1\n+1.9\n+0.4\n-3.0\n+3.7\n+0.8\nS + WR\n+0.6\n+1.2\n+1.5\n-1.2\n+1.6\n+1.6\n-3.7\n+1.2\n+0.4\nS + W\n-0.6\n-0.7\n-0.7\n-4.3\n+0.3\n-2.0\n+6.9\n+0.9\n0.0\nTable 4: Ablations on the allocation of 75% of training tokens to synthetic (S), filtered web (W), and web rewrite\n(WR) categories, while other data sources are held constant in the remaining 25% token budget. All benchmark\nnumbers are measured relative to the final data mixture used for training phi-4.\ncapabilities. We also note that we observed the gap between the chosen data mixture and the synthetic\nheavy runs largely closes as the model goes through the post-training stage. An end-to-end optimization\nof pretraining data mixture that also takes into account the effects of post-training is an interesting\nfuture area of investigation.\nData\nSource\nFraction\nof Training\nUnique\nToken Count\nNumber of\nEpochs\nWeb\n15%\n1.3T\n1.2\nWeb rewrites\n15%\n290B\n5.2\nSynthetic\n40%\n290B\n13.8\nCode data\n20%\n820B\n2.4\nAcquired sources\n10%\n580B\n1.7\nTable 5: Data mixture for pretraining.\nThe final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites\ndata sources, divided equally between them. The remaining tokens are largely sourced from synthetic\ndata which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data\n(mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books.\nIn terms of total number of unique tokens in each data mixture cluster, filtered web data is the largest\ncluster with ∼1.3T tokens. Code and targeted acquisitions are the second and third largest clusters with\n∼820B and ∼580B tokens, respectively. Finally, web rewrites and synthetic data have similar token\ncount of ∼290B tokens. The total number of epochs on each data source is determined using the ratio\nof allocated tokens in the mixture and the number of unique tokens in that source.\n3.3\nMidtraining Details\nphi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K.\nWe conduct several ablations to study the role of data on long-context performance. Specifically, we try\ndata sources that are inherently long context, and compare them with artificially created long context\ndata where samples are padded together to fill the sequence. We observe the former to perform better\nin longer context tasks.\nInspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and\ncode data) to separate samples above 8K context. We then up-weight the data subsets that are 16K or\nhigher in length. We also create new synthetic datasets that satisfy the > 4K sequence requirement. The\nfinal data mixture includes 30% of the newly curated longer context data and a 70% portion of recall\n10\n\nModel\nMax Length\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm\nphi-4\n8K\n100.0\n58.1\n68.0\n65.3\n26.7\n38.3\nQwen-2.5-14B\n8K\n100.0\n62.2\n67.8\n58.2\n24.7\n37.2\nLlama-3.3-70B\n8K\n92.0\n65.3\n69.4\n64.4\n30.0\n37.8\nGPT-4o-mini\n8K\n99.2\n65.8\n74.4\n69.4\n31.3\n38.5\nGPT-4o\n8K\n100.0\n66.9\n83.0\n75.1\n37.3\n43.0\nphi-4\n16K\n99.0\n57.1\n77.0\n54.4\n36.0\n40.5\nQwen-2.5-14B\n16K\n100.0\n59.1\n67.6\n50.3\n29.7\n42.3\nLlama-3.3-70B\n16K\n92.0\n62.2\n70.0\n63.3\n36.7\n41.9\nGPT-4o-mini\n16K\n100.0\n63.6\n78.4\n63.9\n36.0\n45.2\nGPT-4o\n16K\n100.0\n66.7\n85.6\n73.8\n43.7\n46.3\nTable 6: Evaluation results on the long-context benchmark HELMET [YGH+24].\ntokens from the pretraining stage. To accommodate longer context, we increase the base frequency of\nrope position encoding to 250K following [AI23b]. We drop the maximum learning rate by a factor of\n10 compared to the pretraining stage and train for a total of 250B tokens.\nTo effectively evaluate the long-context capability of our model, it is essential to have a comprehensive\nevaluation framework with practical scenarios. While synthetic benchmarks like needle-in-a-haystack and\nRULER are preferred for their simplicity and control, our emphasis is on a diverse range of tasks that\nreflect real-world applications, such as reasoning across entire documents. We report the performance\nof phi-4 and other models on the tasks we selected from the HELMET [YGH+24] evaluation suite in\nTable 6 and outline our evaluation methods below. Note that results are average across 5 runs for each\ncategories.\n• Recall: The task involves retrieving the corresponding value from a randomly-generated long JSON\nfile given a specific key (Metric: SubEM)\n• RAG: Answer questions based on many retrieved and shuffled Wikipedia documents. The datasets\nused for this task are NaturalQuestions, HotpotQA, and PopQA. Final results are average of all\ndatasets (Metric: SubEM)\n• Re-rank: The task is to re-rank the top-10 documents given a query and many retrieved and\nshuffled documents. This uses the MSMARCO dataset (Metric: nDCG@10)\n• ICL: The task involves many-shot in-context learning with datasets such as TREC coarse, TREC\nfine, Banking77, NLU and CLINC150. Final results are average of all datasets (Metric: F1)\n• QA: Answer questions given a lengthy document. The dataset associated with this task is Narra-\ntiveQAv2 (Metric: GPT-4o scoring)\n• Summ: The task involves summarizing a lengthy legal document, and the dataset used is Multi-\nLexSum (Metric: GPT-4o scoring)\n11\n\nDataset Name\nSample Count\nunknown + safety data\n3,000\ngeneric multiple-choice Q&A\n132,859\nmath data\n76,552\npython data\n16,080\ncpp, go, java, js, rust data\n21,806\nTable 7: Data Mixture for Pivotal Token DPO\nDataset Name\nSample Count\nunknown + safety data\n43,842\nany vs any overall\n266,000\nany vs any accuracy\n532,000\nTable 8: Data Mixture for Judge Guided DPO\n4\nPost-Training\nPost-training is aimed at transforming the pretrained language model into an AI assistant that users\ncan safely interact with.\nWe align the pretrained model with one round of SFT 4.1, one round of\nDPO [RSM+23] on data from our pivotal token search method (see Section 4.3), and one round of DPO\non full length preference pairs. The model is chat finetuned using the standard chatml format, example\nusage template for two rounds of a conversation is as follows:\n<|im start|>system<|im sep|>system message<|im end|>\n<|im start|>user<|im sep|>prompt1<|im end|><|im start|>assistant<|im sep|>response1<|im end|>\n<|im start|>user<|im sep|>prompt2<|im end|><|im start|>assistant<|im sep|>\n4.1\nSupervised Fine-Tuning\nIn this phase, we fine-tune the pretrained model with a learning rate of 10−6 on a variety of data generated\nfrom high-quality data across diverse domains, including math, coding, reasoning, conversation, model\nidentity, and safety. We also added multilingual data for 40 languages. We use around 8B tokens of\ndata in this phase, all formatted in the chatml format.\n4.2\nDirect Preference Optimization\nWe use DPO [RSM+23] to align the model with human preferences, and also to steer the model away\nfrom unwanted behavior through pairs of desired and undesired outputs. DPO data covers chat format\ndata, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning,\nrobustness, and safety. We do two rounds of DPO on the SFT model. We introduce a technique, Pivotal\nToken Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture\nfor first round are provided in Table 7.\nFor the second round, which we call judge-guided DPO, we gather approximately 850k pairs of\ndesired and undesired outputs. The prompts are sourced from various publicly available instruction\ntuning datasets and also include prompts related to safety and Responsible AI (RAI). Next, for each of\nthese prompts, we generate responses from GPT-4o, GPT-4t and our model. From these responses, we\ncreate various combinations of DPO pairs and use GPT-4o as a judge to label positive or negative for\na given pair. For a given pair of responses, each assistant response is given a score based on accuracy,\nstyle, and detail. We label the response with higher accuracy or overall (average of accuracy, style, and\ndetail) score as the positive response. We provide the prompt we used in Appendix A. The data mixture\nfor this round is provided in Table 8. Both stages also include a small amount of data for safety and\nmitigating hallucinations.\n12\n\nUser:\nSuppose that the roots of x3 + 3x2 + 4x −11 = 0 are a, b, and c, and that the roots\nof x3 + rx2 + sx + t = 0 are a + b, b + c, and c + a.\nFind t.\nAI: To solve the problem, we need to deal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal with the roots of the two cubic equations\ngiven.\nFirst, consider the cubic polynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial \\( xˆ3 + 3xˆ2 + 4x - 11 = 0 \\),\nwith roots \\( a \\), \\( b \\), and \\( c \\).\nBy Vieta’s formulas, we have:\n\\[ a +\nb + c = -3, \\] \\[ ab + bc + ca = 4, \\] \\[ abc = 11.\n\\] Now, consider the other\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\npolynomial \\( xˆ3 + rxˆ2 + sx + t = 0 \\) with roots \\( a+b \\), \\( b+c \\), and \\(\nc+a \\).\nWe need to find the value of \\( t \\), which corresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds to the\nnegative\n0.42 →0.93\nof\nthe product of the roots according\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording to Vieta’s formulas::::\n\\[\n(a\n0.95 →0.71 +b)(b+c)(c+a).\n\\] First\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst, note\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote the important\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant identity:\n\\[ (a+b)(b+c)(c+a) = (a+b+c)(ab+bc+ca) -\nabc.\n\\] Plug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlugging in known\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown values from our\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour Vieta’s results for\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor \\( a \\), \\( b \\), and\n\\( c \\), we have:\n\\[ a ++++ b + c = -3, \\] \\[ ab + bc + ca = 4, \\] \\[ abc = 11.\n\\]\nSubstitute these into the identity:\n\\[ (a+b)(b+c)(c+a) = (-3)(4) - 11 = -12 - 11 =\n-23.\n\\] Thus, \\( t = -(a+b)(b+c)(c+a) = 23 \\).\nTherefore, the value of \\( t \\) is\n\\(\\boxed\\23\\\\).\n0\n1\np(success)\nFigure 3: Illustration of pivotal tokens for GPT-4o at temperature 1 on a problem from the MATH bench-\nmark [HBK+21], where the initial success probability is 0.31. Each token is colorized by the probability of success\nfor an independent completion (N = 529) continued from after the token, with red for p(success) = 0 and blue\nfor p(success) = 1. The line plot shows the same probabilities. The tokens that changes p(success) by ≥0.2 are\nshown boxed , with subscripts showing the change in probability. Tokens with probability ≤0.1 are underlined to\nillustrate that pivotal tokens are distinct from low-probability tokens. The token probabilities of\nnegative and\n(a were 0.31 and 0.12, respectively. The greedy tokens for the same prefixes are\nproduct with 0.66 probability\nand t with 0.88 probability.\nprocedure PivotalTokenSearch(Q,Tfull,pgap)\nprocedure Subdivide(Tprefix,T)\nif ∣T∣≤1 or ∣p(success ∣Tprefix) −p(success ∣Tprefix + T)∣< pgap then\n▷Base cases.\nreturn [T]\nTleft,Tright ←Split(T)\n▷We split at the cumulative midpoint of token log probabilities.\nreturn Subdivide(Tprefix,Tleft) ∪Subdivide(Tprefix + Tleft,Tright)\nTprefix ←ϵ\nfor all T ∈Subdivide(ϵ,Tfull) do\nif ∣T∣= 1 and ∣p(success ∣Tprefix) −p(success ∣Tprefix + T)∣≥pgap then\nyield (Q,Tprefix,T)\n▷Output pivotal tokens T and context for postprocessing.\nTprefix ←Tprefix + T\nFigure 4: Pseudocode for Pivotal Token Search (PTS). Note that estimating p(success ∣...) involves sampling the\nlanguage model and invoking the oracle. In an efficient implementation p(success ∣...) should be memoized.\n13\n\n4.3\nPivotal Token Search\nConsider a generative model producing a token-by-token response to a given prompt. For each token\nproduced, which corresponds to a prefix of the model response, one can consider the conditional proba-\nbility of the model’s answer being correct given that prefix, as well as the increment in this probability\nwith respect to that token (in other words, the difference in the probability of being correct before and\nafter producing that token). It is often the case that the overall correctness is highly dependent on\na successful generation of a small number of key tokens. For example, we can see in Figure 3 where\nthe model outputs a math solution and a “fortunate” sampling of a crucial token\nnegative shifts the\nsolution from possible failure to likely success, while sampling of the token (a subsequently risks failure\nagain. We refer to these tokens as pivotal tokens as they have an outsized effect on the course of the\nsolution.\nNow, consider how the solution from Figure 3 would be used in DPO as a full-length accepted\nresponse. As the figure shows, there are many tokens with probabilities much lower than the 0.31 of\nnegative , which would contribute to noise in the gradients diluting the signal from the pivotal token.\nEven worse, the token (a that contributed to the lack of robustness would receive a strong positive\nlearning signal thanks to its low probability of 0.12.\nMoreover, intuition suggests that when two texts substantially deviate from each other, comparison\nof their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it\nmakes more sense that the signal should come from the first tokens after the two texts starts diverging\nfrom each other.\nTo alleviate these effects, we employ a method we call Pivotal Token Search (PTS) for generating\npreference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the\npreference optimization takes effect with respect to a single token.\nPTS identifies points of a completion token sequence Tfull = t1,t2,... for some user query Q where\nthe next token ti has a significant impact on the probability of success p(success ∣t1,...,ti). PTS es-\ntimates these probabilities by sampling completions starting from Q + t1,...,ti, which are checked for\ncorrectness with an oracle6 for Q. Figure 4 shows a basic instantiation of the algorithm. The proce-\ndure Subdivide recursively splits the sequence into segments ti,...,tj until the change in probability\n∣p(success ∣t1,...,ti−1) −p(success ∣t1,...,tj)∣for each segment is below a threshold pgap or the segment\nis just a single token. Tokens with a sharp change in success probability are kept as pivotal. We turn\npivotal tokens into preference data by taking Q + t1,...,ti−1 as the query, and single tokens tacc and trej\nthat increase/decrease p(success ∣t1,...,ti−1,tacc/rej) as the accepted and rejected completions, respec-\ntively.7 The binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but\nit only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the\ncourse of the solution.\nWe used PTS to generate preference data for tasks where ground-truth is readily available, such as\nmathematics, various forms of question answering and coding. To improve sample efficiency, we filter\nthe target questions to only include those with 0.2 ≤p(success) ≤0.8, as pivotal tokens are rare for tasks\nthat are very easy or hard.\nSee Figure 5 for examples of preference data we generated using PTS. The math question answering\nexample shows how pivotal tokens often are not actual mistakes, but choices that drive the model down\na less advantageous path. Here, multiplying both sides by the denominators separately is equally valid\nto directly cross-multiplying, but for the model doing the latter here is more robust. By generating\nDPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially\n6For coding, a comprehensive test suite can be used. For math problems, answers can be compared to the ground truth.\n7We find drawing tacc and trej from rollouts PTS already used to estimate p(success ∣t1, . . . , ti−1) to be effective.\n14\n\nUser:\nFind the product of all real values of r for which\n1\n2x\n=\nr−x\n7\nhas exactly one real\nsolution.\nAI: To solve the problem, we need to determine the values of r such that the equation\n1\n2x = r −x\n7\nhas exactly one real solution for x.\nFirst, clear the fractions by\nGood:\ncross-multiplying\nBad:\nmultiplying both sides by\nUser:\nConsider a 1 keV helium atom impinging at a one-degree incidence angle on a LiF\ncrystal surface.\nAdopt the axial channeling approximation to model the interaction\nbetween the helium atom and the crystal as a 2D potential energy landscape.\nApply\nNewton’s equations to compute the helium atom’s trajectory and determine the subsequent\nmomentum transfers from binary collisions with surface atoms.\nSum the total virtual\nenergy transferred across all collisions and compute the probability, Pe, that the\nhelium atom remains elastically scattered assuming a vibrational frequency for surface\natoms of ω = 1 × 1014Hz.\nWhat is Pe?\n1.\nPe = exp(−Eloss\n2̵hω ), 2.\nPe = exp( Eloss\n̵hω ), 3.\nPe = exp(−Eloss\n̵hω ), or 4.\nPe = 1 −exp(−Eloss\n̵hω )\nAI: To determine the probability Pe that the helium atom remains elastically scattered,\nwe need to calculate the total virtual energy transferred, Eloss, during the helium\natom’s interaction with the LiF crystal surface.\nGiven\nGood:\nthe vibrational frequency\nBad:\nthat the potential interactions\nUser:\nRead the following function signature and docstring, and fully implement the\nfunction described.\nYour response should only contain the code for this function.\ndef reorder_sequence(n: int, sequence: list[int]) -> list[int]:\n\"\"\"\nGiven a sequence of integer numbers that Polycarp wrote down on the board, this\nfunction rearranges (reorders) elements of this sequence to match the order\nPolycarp could have possibly written them. Each next number should be exactly two\ntimes the previous number or exactly one third of the previous number.\n:param n: The number of elements in the sequence (2 <= n <= 100).\n:param sequence: A list of integer numbers in arbitrary order (1 <= elements <= 3 *\n10ˆ18).\n:return: A reordered list of integer numbers representing a valid sequence Polycarp\ncould have written.\n\"\"\"\nAI: ...\nfrom collections import defaultdict\n# Create adjacency list where each number can point to its possible predecessor and\nsuccessors\ngraph = defaultdict(list)\nfor num in sequence:\nfor other in sequence:\nif num != other:\nif num == other * 2 or (\nGood:\nother % 3\nBad:\nnum * 2\nFigure 5: Preference data generated by Pivotal Token Search in answering math and physics questions, and\nimplementing a function in Python. The tokens that form the actual pair for DPO are underlined.\n15\n\nBase\nSFT\nDPO Stage 1\nFinal\n0\n20\n40\n60\n80\n100\nPercentage (%)\n6.8%\n3.2%\n90.0%\n3.7%\n57.5%\n38.7%\n2.9%\n79.8%\n17.4%\n3.0%\n81.1%\n15.8%\nSimpleQA performance over the course of post-training\nCorrect\nNot Attempted\nIncorrect\nFigure 6: The post-training process described in Appendix A.1 decreases hallucinations. One measure is that\nthe problems in SimpleQA—which the model very rarely gets correct—are increasingly not attempted during the\ncourse of post-training. We believe the final result is better behavior, even though the simple-evals score for\nSimpleQA (the F1 score) actually gives our base model a higher score than our final model.\nstronger.\nRelated Work:\nIn [LLX+24] a contrastive estimation approach involving a model trained on incorrect\ntrajectories is used to score which tokens likely contributed to failure, which is further employed to\nweigh rejected responses in DPO. In comparison, our PTS avoids complications from learned proxies by\ndirectly estimating p(success). They also report difficulties applying their method to accepted responses\nin DPO, while our method generates both positive and negative preference data directly targeting pivotal\ntokens. Automated process supervision methods [WLS+24, LLL+24] have applied search and rollouts to\ngenerate data for training process reward models. PTS can be seen as an automated process supervision\nmethod that generates token-level preference data suitable for DPO.\n4.4\nHallucination mitigation\nWe generate SFT data and DPO pairs to mitigate hallucination. If the model does not know the answer,\nwe would rather it refuse to answer than to make up a hallucination. We present the details of this\nprocess, including prompts to create the data, in Appendix A.1. This greatly decreases hallucinations\nin SimpleQA (see Figure 6).\n4.5\nPost-Training Ablation\nIn Table 1 we show how our benchmark scores evolve during post-training. We also evaluate dropping\npivotal token DPO and only performing the second stage of DPO. In general, we find that pivotal token\nDPO is most useful on reasoning-heavy tasks (GPQA, MATH) while judge-guided DPO is particularly\nuseful for the benchmark that itself involves a GPT-4 judge: ArenaHard. We also find the two approaches\nto be complementary.\n5\nBenchmarking Considerations\nWhile academic benchmarks are a widely used to measure the progress in LLM advancement, they\nsuffer from several limitations that can fail to reveal a model’s true capabilities and weaknesses. These\nlimitations include:\n• Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora,\ncreating a risk of data contamination. Although we took extensive measures to deduplicate and\n16\n\nSFT\nDPO\nstage 1\nDPO\nstage 2 only\nphi-4\n(stage 1 + 2)\nsimple-evals\nMMLU\n82.8\n84.8\n84.2\n84.8\nGPQA\n47.3\n53.6\n52.4\n56.1\nMATH\n77.1\n80.5\n77.6\n80.4\nHumanEval\n79.5\n81.6\n81.5\n82.6\nMGSM\n80.8\n80.8\n81.5\n80.6\nSimpleQA\n3.7\n2.9\n2.9\n3.0\nDROP\n82.8\n86.1\n71.8\n75.5\nMMLUPro\n61.9\n70.0\n67.2\n70.4\nHumanEval+\n77.9\n81.9\n81.4\n82.8\nArenaHard\n56.7\n66.5\n69.8\n75.4\nIFEval\n66.2\n63.0\n63.0\n63.0\nPhiBench\n(internal)\n48.2\n54.5\n53.0\n56.2\nTable 9: Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is\nmore standard judge-guided DPO. Each also has 1-5% hallucination and safety data mixed in.\ndecontaminate our training data, including standard n-gram deduplication and decontamination,\nthese methods are not effective against all scenarios, including rephrasing, which leaves some\nuncertainty about the true extent of generalization.\n• Limited Skill Scope: Most benchmarks evaluate models on narrowly defined skills, such as\nsolving specific style of math problems at certain grade level or implementing isolated Python\nfunctions. This narrow scope can fail to capture a model’s broader capabilities and weaknesses.\n• Bias in Generation-Based Benchmarks: Some benchmarks use LLM-as-judge for evaluat-\ning generated outputs. These judgments sometimes may prioritize style, fluency, or surface-level\nqualities over accuracy and validity of the reasoning chain, leading to potential biases in scoring.\n• Limitations of Multiple-Choice Tasks: Benchmarks that rely on multiple-choice questions\noften test a model’s ability to make clever guesses that can be achieved by pattern matching\nrather than effectively utilizing the underlying concepts through reasoning.\nTo address these issues, we maintain an internal benchmark called PhiBench, which is tailored to\nevaluate the diverse skills and reasoning abilities that we found critical to phi-4’s development. This\nbenchmark was designed with the following goals:\n1. Originality: All questions in the benchmark were composed by our team making sure that they\nwere not present in our pretraining data. Our goal for the internal benchmark is to reveal model’s\ngeneralization ability in various domains.\n2. Skill Diversity: Our benchmark includes a wide range of tasks to assess multiple dimensions\nof model performance. For instance, in coding, it goes beyond isolated function implementation\nto include debugging, extending incomplete code, and explaining code snippets.\nSimilarly, in\n17\n\nmathematics, it incorporates tasks like identifying the errors in proofs or generating related prob-\nlems, rather than simply solving equations. This ensures that the benchmark captures a broader\nspectrum of skills and reasoning processes.\n3. Rigorous Scoring for Generation Tasks: For tasks requiring judgment of model-generated\noutputs, we addressed the common pitfalls of LLM-based scoring by carefully curating detailed\njudge instructions (or “judge notes”). These rubrics specify exactly how to evaluate responses,\nfocusing on achieving accuracy, logical structure, and adherence to task requirements, while min-\nimizing tendencies towards stylistic biases. We observed significantly improved consistency and\nreduction of adverse impact due to subjective preferences in the scoring outcomes.\nPhiBench played a central role in optimizing phi-4. We used it to guide decisions about dataset\nmixtures and hyperparameter choices for more effective post-training techniques. PhiBench was also\nused to perform high-signal studies that identify weaknesses in the model and provide feedback for new\nincoming data sources.\n6\nPerformance on Key Benchmarks\nOur benchmark results were presented in Table 1, along with comparisons to other models. We first re-\nport the values from OpenAI’s simple-evals benchmark, which is a framework (including prompts, tem-\nperature, and extraction) for evaluating MMLU [HBB+20], GPQA diamond [RHS+23], MATH [HBK+21],\nHumanEval [CTJ+21], MGSM [SSF+22], and the SimpleQA [WKC+24] F1-score.\nWe also consider\nMMLU-pro [WMZ+24], HumanEval+ [LXWZ23], ArenaHard [CZS+24], and IFEval [ZLM+23], for which\nwe use an internal framework and prompting and extraction. Finally, we use PhiBench, our internal\ncollection of evaluations (see Section 5).\nphi-4 outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12\nbenchmarks. While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers\nfor SimpleQA, DROP, and IFEval, we consider phi-4’s behavior on SimpleQA to actually be better\nthan Qwen’s. In fact, our base model gets a higher benchmark score than Qwen-2.5-14B-Instruct on\nSimpleQA, and we intentionally modified the model’s behavior in post-training to optimize for a better\nuser experience rather than a higher benchmark score. See Figure 6 and Appendix A.1 for details.\nOur model excels at STEM Q&A tasks. For example, on GPQA (graduate-level STEM questions)\nand MATH (math competitions), it even outscores its teacher model, GPT-4o. It also scores higher at\ncoding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark\nagainst, including much larger Llama models.\nphi-4’s weakest benchmark scores are on SimpleQA, DROP, and IFEval. We believe for the first\ntwo that the number reported by simple-evals is reductive and does not accurately reflect model\nperformance on the benchmark problems. However, IFEval reveals a real weakness of our model – it\nhas trouble strictly following instructions. While strict instruction following was not an emphasis of our\nsynthetic data generations for this model, we are confident that phi-4’s instruction-following performance\ncould be significantly improved with targeted synthetic data.\n7\nSafety\nWe developed phi-4 in accordance with Microsoft’s Responsible AI principles. Our overall approach to\nRAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations\nacross dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets\n18\n\nphi-3\n(3B-4K)\nphi-3\n(7B-8K)\nphi-3\n(14B-4K)\nMistral\n(7B-v0.1)\nMistral\n(7B-v0.2)\nLlama-3\n(8B)\nGemma\n(7B)\nphi-4\nGrounding\n4.469\n4.701\n4.787\n4.065\n4.692\n4.672\n4.32\n4.619\n3P Content Harms\n(DR1)\nBooks, News, Recipes, Songs\n0.26\n0.253\n0.251\n0.562\n0.399\n0.373\n0.383\n0.121\nHarmful Content\nContinuation (DR3)\nHate/Fairness, Self-Harm, Sexual, Violence\n0.007\n0.003\n0.01\n0.026\n0.018\n0.013\n0.013\n0.036\nHarmful Content\nSummarization (DR3)\nHate/Fairness, Self-Harm, Sexual, Violence\n0.105\n0.11\n0.112\n0.223\n0.16\n0.082\n0.103\n0.102\nJailbreak(DR1)\nSee text for covered topics\n0.117\n0.107\n0.111\n0.156\n0.153\n0.13\n0.114\n0.073\nTable 10: Performance comparison across models. Lower scores are better, except for “Grounding,” where a higher\nscore is better. phi-4 values are bold for readability.\n[BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets to\naddress the RAI harm categories in safety post-training.\n7.1\nRAI Benchmarks\nTable 10 shows the results of in-house RAI benchmarks [MHJ+23] for phi-4 compared to the phi-3 models\n[AAA+24], Mistral-7b-v0.1 [JSM+23], Mistral-7b-v0.2, Gemma 7b [TMH+24], and Llama-3-instruct-\n8b [AI23b].\nThis benchmark utilized GPT-4o to simulate multi-turn conversations in five different\ncategories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5\n(fully grounded), and measures if the information in a response is based on a given prompt. In other\ncategories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm)\nto 7 (severe harm) and the defect rates (DR-x) were computed as the percentage of samples with the\nseverity score being greater than or equal to x. The Jailbreak (DR1) benchmark consists of simulated\nconversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular\nconspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people.\nFor more details on the RAI prompts and evaluation framework, see [HPBP+24].\n7.2\nRed Teaming\nIn addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an inde-\npendent group tasked with identifying safety and security vulnerabilities in Microsoft’s GenAI products.\nAIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both\naverage and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior\nof phi-4 was similar to that of the phi-3 family, but identified several risky behaviors that were addressed\nby further rounds of safety post-training. In addition, the adversarial user scenario tested a wide range\nof techniques aimed at intentionally subverting the model’s safety training including jailbreaks, prompt\nencodings, and multi-turn attacks. phi-4 showed strong defenses against these techniques. AIRT also\ngenerated adversarial suffixes using the GCG algorithm [ZWC+23] on phi-3-medium, but found that\nthese suffixes did not transfer to phi-4. Further red teaming is required to identify possible risks across\na broader range of scenarios and harm categories.\n19\n\n8\nWeaknesses\nWhile phi-4 achieves similar level of language understanding and reasoning ability as much larger models,\nit is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual\nknowledge. For example, if X is a plausible human name, the model sometimes responds to prompts of the\nform “Who is X?” with a hallucinated biography of the person X. This limitation would be improved by\naugmenting the model with a search engine, but factual hallucinations cannot be eliminated completely.\nWhile phi-4 demonstrates relatively strong performance in answering questions and performing rea-\nsoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving\nspecific formatting requirements. For instance, when tasked with generating outputs in strict tabular\nformats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model\nmay produce outputs that deviate from the specified guidelines. This limitation arises in part from the\nmodel’s training focus, which prioritized synthetic datasets tailored toward Q&A and reasoning tasks\nover instruction-following scenarios.\nEven on reasoning tasks, phi-4 can make mistakes.\nFor example, when asked “which number is\nsmaller, 9.9 or 9.11?”, the model can conclude incorrectly that “9.9 is smaller than 9.11”.\nMoreover, as our data contains a lot of chain-of-thought examples, phi-4 sometimes gives long elab-\norate answers even for simple problems—this might make user interactions tedious. We also note that\nwhile phi-4 can function as a chat bot, it has been fine-tuned to maximize performance on single-turn\nqueries.\nDespite diligent RAI efforts, we acknowledge challenges around reproduction or amplification of\nbiases, inappropriate content generation, and safety issues. The use of carefully curated training data, as\nwell as targeted post-training, and improvements from red-teaming insights, have resulted in mitigating\nthese issues across all dimensions, but have not resolved the issues completely.\nAcknowledgments\nWe thank Janardhan Kulkarni and Sivakanth Gopi from Microsoft Research for the initial discussion\naround Pivotal Token Search.\nWe thank the AI Red Team (AIRT) at Microsoft, especially Blake\nBullwinkel, Bolor-Erdene Jagdagdorj, Daniel Jones, Shiven Chawla, Tori Westerhoff, and Ram Shankar\nSiva Kumar, and Olga Dutova-Fairfax from the Deployment Safety Board and the Office of Responsible\nAI at Microsoft for collaborating with us on evaluating and improving our model on vulnerabilities in\nsafety and security, which helped us adhere to the Microsoft’s RAI standards. Finally, we are grateful to\nEce Kamar, Doug Burger and Peter Lee from Microsoft Research for the support provided to the team\nduring the work on the model.\nReferences\n[AAA+24] Marah Abdin, Sam Jacobs Ade, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3\ntechnical report: A highly capable language model locally on your phone. arXiv preprint\narXiv:2404.14219, 2024.\n[AI23a] Meta AI. Introducing Meta Llama 3: The most capable openly available llm to date, 2023.\n[AI23b] Meta AI. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2023.\n20\n\n[BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Ka-\ndavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-\nDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,\nNeel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,\nChris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with\nreinforcement learning from human feedback, 2022.\n[BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R¨ottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned Llamas: Lessons from improving the safety of large\nlanguage models that follow instructions, 2024.\n[Com24] American Mathematics Competitions. American mathematics competitions problems and\nsolutions: Amc 10/12. https://www.maa.org/math-competitions, 2024. Accessed:\n2024-12-08.\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-\nating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[CZS+24] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,\nDacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica.\nChatbot arena: An open platform for evaluating llms by human preference, 2024.\n[Dee24] DeepSeek.\nDeepseek r1 lite preview.\nhttps://api-docs.deepseek.com/news/\nnews1120, 2024. Accessed: 2024-12-08.\n[GZA+23] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Gustavo de Rosa Piero Kauffmann, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, S´ebastien Bubeck, Ronen Eldan,\nAdam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv\npreprint arXiv:2306.11644, 2023.\n[HBB+20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n[HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math\ndataset. NeurIPS, 2021.\n[HPBP+24] Emman Haider,\nDaniel Perez-Becker,\nThomas Portet,\nPiyush Madan,\nAmit Garg,\nAtabak Ashfaq, David Majercak, Wen Wen, Dongwoo Kim, Ziyi Yang, et al.\nPhi-3\nsafety post-training: Aligning language models with a “break-fix”cycle.\narXiv preprint\narXiv:2407.13833, 2024.\n[JBA+23] Mojan\nJavaheripi,\nS´ebastien\nBubeck,\nMarah\nAbdin,\nJyoti\nAneja,\nCaio\nC´esar\nTeodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya\nGunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli\nSaarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Tau-\nmann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2:\nThe surprising power of small language models. Microsoft Research Blog, 2023.\n21\n\n[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension, 2017.\n[JLD+23] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang\nSun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of\nllm via a human-preference dataset, 2023.\n[JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7b, 2023.\n[LBE+23] Yuanzhi Li, S´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and\nYin Tat Lee.\nTextbooks are all you need II: phi-1.5 technical report.\narXiv preprint\narXiv:2309.05463, 2023.\n[LLL+24] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei\nShu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning\nin language models by automated process supervision, 2024.\n[LLX+24] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu\nYang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances\nllm’s reasoning capability, 2024.\n[LXWZ23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated\nby chatgpt really correct? rigorous evaluation of large language models for code generation.\narXiv preprint arXiv:2305.01210, 2023.\n[MHJ+23] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng,\nDan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun,\nEslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen. A framework\nfor automated measurement of responsible AI harms in generative AI applications, 2023.\n[Ope24a] OpenAI.\nLearning to reason with language models.\nhttps://openai.com/index/\nlearning-to-reason-with-llms/, 2024. Accessed: 2024-12-08.\n[Ope24b] OpenAI. Simple evals. https://github.com/openai/simple-evals, 2024.\n[RHS+23] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\nJulien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof\nq&a benchmark, 2023.\n[RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon,\nand Chelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[SSF+22] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.\nLanguage models are multilingual chain-of-thought reasoners, 2022.\n[Tea24] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024.\n22\n\n[TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology, 2024.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn Advances in Neural\nInformation Processing Systems, volume 30, 2017.\n[WFM+24] Yue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen McAleer, Yonatan Bisk,\nRuslan Salakhutdinov, Yuanzhi Li, and Tom Mitchell. Agentkit: Flow engineering with\ngraphs, not coding. In COLM, 2024.\n[WKC+24] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia\nGlaese, John Schulman, and William Fedus. Measuring short-form factuality in large lan-\nguage models, 2024.\n[WLS+24] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu,\nand Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human\nannotations, 2024.\n[WMZ+24] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex\nZhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and chal-\nlenging multi-task language understanding benchmark, 2024.\n[YGH+24] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe\nWasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effec-\ntively and thoroughly, 2024.\n[ZLM+23] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan,\nDenny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv\npreprint arXiv:2311.07911, 2023.\n[ZWC+23] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson.\nUniversal and transferable adversarial attacks on aligned language models, 2023.\nA\nPost-Training Dataset Details\nA.1\nRefusal to Hallucinate\nWe created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without\nany mitigation, phi-4 would almost never admit to ignorance. For example, in response to too-difficult\nquestions like “Who is the 297th highest ranked tennis player?” the model would essentially act as an\nimprov-style “Yes, and. . . ” engine, inventing a superficially plausible answer.\nOur goal in pretraining was to pack as much information into the model as possible, that is, to teach\nmore to the model rather than to teach it its own limitations. Then in post-training, we can identify\nthe level of problem that is too difficult for the model, and teach it to generate refusals rather than\nhallucinations on those problems.\nWe started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran\nphi-4 multiple times to estimate its chance of accurately solving it. We also used GPT-4o to generate\n23\n\n(1) a correct answer, (2) a refusal to answer, (3) a bogus variant of the question that is impossible to\nsolve, and (4) a refusal to answer the bogus question.\nFor SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually\ncorrect, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus\nquestions. For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes\nanswered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly. The DPO data used\nthe first 5 tokens of the response. Example synthetic generation prompts can be found below.\nTo evaluate our progress, we can use SimpleQA [WKC+24], which is a dataset mostly comprised\nof obscure facts from Wikipedia (e.g., “How many more votes did Freeman Freeman-Thomas win than\nGeorge Sandys in the 1906 Bodmin by-election?”). Small models like phi-4 or GPT-4o-mini can only\ncorrectly answer 5-10% of them. Our performance can be found in Figure 6.\nNote that SimpleQA is included in Table 1 as part of simple-evals, and our model does not have a\ngood score. This is because simple-evals uses the F1 score, which is not a good measure of quality at\nthis accuracy scale. For example, suppose we start with a model that always guesses, but almost always\nwrongly, 6% correct and 94% incorrect. Some of the 6% correct answers will be from lucky guesses, so\npost-training to limit hallucination will have fewer correct answers, and for example, the result might be\n(3% correct, 3% incorrect, 94% refusal). In this case, a model will score worse by the F1 metric compared\nto original (5.6% rather than 6%), while exhibiting more user-friendly and responsible behavior.\nA.1.1\nSynthetic Generation Prompts\nHere, we share the main synthetic generation prompts, provided to GPT-4o, to generate post-training\ndata to decrease hallucinations.\nGeneration of bogus questions\nConsider the following trivia question:\n# Question\n{{ question }}\n# Instructions\nYour job is to turn this problem into a nonsensical one, for which the\nanswer is invalid or unlikely to be known by anyone.\nFor example, you\nmight change the name from a well-known figure to a random name, or\nchange the date from a well-known event to a random date, or the place\nto a different one.\nFor example, you might change \"When did Amelia\nEarhart cross the Atlantic Ocean?\" to \"When did Edgar Greenwood cross\nthe Atlantic Ocean?\" or \"How many times did Amelia Earhart cross the\nEnglish Channel?\".\n↪\n↪\n↪\n↪\n↪\n↪\n↪\nYour goal is that the new question is *plausibly real*, but impossible to\nanswer.\nYou should not make the question obviously fake, silly, or\nfictional; for example, all country names should be real countries,\nand no names should be obvious homages to the original question.\nIt\nshould sound like a serious trivia question.\n↪\n↪\n↪\n↪\n24\n\nYou may start with a very brief discussion, then end with two markdown\nsections:\n↪\n- The section '# Response' that contains the question.\n- The section '# Quality' that rates the generated question in quality\nfrom 1-5, with 5 being the highest quality.\n↪\nA high quality question is (1) different from the given question and\n(2) plausible\n↪\nGenerations of refusals\nConsider the following question:\n# Question\n{{ question }}\n# Instructions\nYou might well know the answer, but imagine that you were an LLM that did\nnot know the correct answer. Write a plausible response to this\nquestion that the LLM might give if it did not know the answer and\nwould like to say so rather than guess incorrectly.\n↪\n↪\n↪\nThis LLM cannot look things up.\nIt could suggest relevant information\nthat it knows; it can also just say that it does not know the answer,\nif it is unlikely to know anything useful.\n↪\n↪\nYou may start with a very brief discussion, then end with a markdown\nsection '# Response' that contains the response.\n↪\nAnswer checking\nI am grading solutions to a trivia question.\nYour job is to determine\nwhether the given submission matches the answer key.\n↪\n## Original question\n{{ question }}\n## Submission\n{{ response }}\n## Answer key\n{{ answer.value }}\n{% if answer.alternates %}\n25\n\n### Alternative Answer Forms\n{% for alt in answer.alternates %}\n{{ alt }}\n{% endfor %}\n{% endif %}\n## Instructions\nYou job is **NOT to solve the question**.\nYour job is to determine\nwhether the given submission should be graded as correct *without\nneeding a protest*.\nIt does not matter if you disagree with the\nofficial answer; you should only consider whether the submission is\nequivalent to the answer key.\n(There is a separate process for\nprotests.)\n↪\n↪\n↪\n↪\n↪\nGive a freeform analysis comparing the submission to the answer key.\nYou\nshould then output a JSON dictionary in the following form:\n↪\n```json\n{\n\"matches_key\": [Answer 'True', 'False', or 'Uncertain'],\n}\n```\nA.2\nJudge-guided DPO\nFor the second round of DPO, we generate responses from GPT-4o, GPT-4t and our model. To label\nresponses as positive or negative, we use GPT-4o as a judge and use the following prompt.\nYour task is to judge which of the following reply given by an AI\nassistant is better.\n↪\n# Conversation\n{{ chat }}\n# Replies\n{{ replies }}\n# Guideline\nProduce your output in the following JSON format (without comments and\nwith correct escape characters):\n↪\n```json\n{\n\"faults\": {\n26\n\n\"Assistant 1\": \"(string) List all the problems with the assistant 1\nreply. For each problem try to determine whether this is due to\nlack of comprehension of the relevant material, a logical\nerror, a factual error, a stylistic issue. If the answer is\nperfect, write none. If the question did not ask for a specific\nlevel of detail in the explanation, do not penalize the answer\nfor being too detailed or too concise.\",\n↪\n↪\n↪\n↪\n↪\n↪\n\"Assistant 2\": ...\n...\n},\n\"faults_discussion\": \"(string) Discuss the general strengths and\nweaknesses of each assistant. What are the main differences\nbetween the answers in terms of style, accuracy and level of\ndetail?\",\n↪\n↪\n↪\n\"accuracy\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\naccuracy?,\n↪\n...\n},\n\"style\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\nstyle?,\n↪\n...\n},\n\"detail\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\nlevel of detail?,\n↪\n...\n}\n}\n```\nB\nData Processing\nB.1\nDecontamination\nWe decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi,\nMedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k,\nHumanEval, arena hard, ARC-Challenge, and MMLU benchmarks. We apply a hybrid n-gram algorithm\nfor decontamination which uses 13-gram and 7-gram features for removing matches to the test set, which\nis described in more detail in 1. We create a set of common 13-grams in the Wiki and train set and try\nto not remove them since these are some common phrases which are ubiquitous. Some examples include\n’a i only b ii only c iii only d ii and iii’, ’a true true b false false c true false d false true’, ’logically\nequivalent b contradictory c neither logically equivalent nor contradictory but consistent d’, ’a (ii) and\n(iv) only b (i) and (iii) only c (i) (ii)’, ’b e b a b e c c b d c e d’.\n27\n\nAlgorithm 1 Decontamination Algorithm\nRequire:\nInput training text train\nBenchmark texts tests\nAllowed 13-grams allowed 13gram\nThresholds: info 7gram threshold, contaminated 7gram threshold\nEnsure:\nA result object with contamination details\n1: procedure CheckContamination(train)\n2:\nStep 1: Extract 13-grams and Check Contamination\n3:\nfeatures 13 ←ExtractNGrams(train,13)\n4:\nfor all feature ∈features 13 do\n5:\nif feature ∈BenchmarkFeatures13 and feature ∉allowed 13gram then\n6:\nreturn Contaminated (13-gram match)\n7:\nStep 2: Extract 7-grams and Compute Overlaps\n8:\nfeatures 7 ←ExtractNGrams(train,7)\n9:\noverlap counts ←CountOverlaps(features 7,BenchmarkFeatures7)\n10:\nStep 3: Compute Overlap Ratio for Tests\n11:\nmax ratio ←0, max test ←None\n12:\nfor all test ∈BenchmarkTests do\n13:\nratio ←\noverlap counts[test]\nmin(len(features 7),len(BenchmarkFeatures7[test]))\n14:\nif ratio > max ratio then\n15:\nmax ratio ←ratio, max test ←test\n16:\nStep 4: Determine Contamination\n17:\nif max ratio > info 7gram threshold then\n18:\nif max ratio ≥contaminated 7gram threshold then\n19:\nreturn Contaminated (7-gram match)\n20:\nelse\n21:\nreturn Partial Contamination (7-gram info match)\n22:\nreturn Clean (No significant overlap)\n28\n\ncontaminated:\nTrue\nTrain:\nThere are some oarsmen in a boat.\nThe average weight is increased by 1.8 kg\nwhen one of the crew, who weighs 53 kg, is replaced by a new man who weighs 71 kg.\nHow\nmany oarsmen are there in the boat?\nTrain Dataset:\norca-math-word-problems-200k\n13gram test:\nThe average weight of 10 oarsmen in a boat is increased by 1.8 kg when\none of the crew, who weighs 53 kg is replaced by a new man.\nFind the weight of the new\nman.A. 71 B.62 C.43 D.67 E.40\nTest Dataset:\nAGIEval\n13gram segment:\none of the crew who weighs 53 kg is replaced by a new\n13gram contaminated:\nTrue\n7gram test:\nThe average weight of 10 oarsmen in a boat is increased by 1.8 kg when one\nof the crew, who weighs 53 kg is replaced by a new man.\nFind the weight of the new\nman.A. 71 B. 62 C. 43 D. 67 E. 40\n7gram overlaps:\n[’1 8 kg when one of the’, ’of the crew who weighs 53 kg’, ’the\ncrew who weighs 53 kg is’, ’increased by 1 8 kg when one’, ’crew who weighs 53 kg is\nreplaced’, ’weighs 53 kg is replaced by a’, ’8 kg when one of the crew’, ’is increased\nby 1 8 kg when’, ’kg when one of the crew who’, ’53 kg is replaced by a new’, ’when one\nof the crew who weighs’, ’by 1 8 kg when one of’, ’kg is replaced by a new man’, ’one\nof the crew who weighs 53’, ’who weighs 53 kg is replaced by’]\n7gram ratio:\n0.39473684210526316\nC\nAMC Evaluation Details\nIn this section, we fully describe our inference and grading schemes used for to obtain the November\n2024 AMC scores displayed in Figure 1. The 78 questions in these contests (4 sets of 25 questions, with\noverlaps between the 10A/12A and 10B/12B exams) were made available on or after November 6, 2024.\nAll external models we tested were published before this date, as were the datasets for all stages of phi-\n4’s training. Thus, these contests are our best attempt at conducting a completely contamination-proof\nevaluation of mathematical reasoning capabilities. We only benchmarked on this dataset after choosing\nthe hyperparameters used in post-training our final candidate models, making this dataset completely\nindependent of our final model. 8\nWe obtained the questions from the Art of Problem Solving Wiki9, and formatted them with the\nfollowing template:\nThe following question is from a 25-question, multiple choice test. Each\nquestion is followed by answers marked A, B, C, D, and E.\nOnly one of\nthese is correct.\n↪\n↪\nSCORING: You will receive 6 points for each correct answer, 1.5 points for\neach problem left unanswered, and 0 points for each incorrect answer.\n↪\nSolve the question step by step, then answer \\boxed{A}, \\boxed{B},\n\\boxed{C}, \\boxed{D}, \\boxed{E}, or \\boxed{blank}.\n↪\n# Question\n8For full disclosure, we evaluated our final three candidate models on this dataset and all three average scores exceeded\n89. We settled on our final model based on other factors, before measuring its score but after seeing the scores for the other\ntwo candidates.\n9https://artofproblemsolving.com/wiki/index.php/2024_AMC_10A (10B, 12A, 12B)\n29\n\n{{question}}\n(A) {{option_a}}\n(B) {{option_b}}\n(C) {{option_c}}\n(D) {{option_d}}\n(E) {{option_e}}\nWith each question formatted this way, we obtained 10 independent generations at temperature 0.5\nfrom each model we tested. We then followed the grading scheme described in the above prompt. We\nfound that every model we tested (including our own) frequently failed to follow the “box your final\nanswer” instruction, particularly after a long chain of thought. To stabilize the evaluations, we decided\nto count otherwise correct solutions (e.g. boxing the correct numerical expression) as correct. To do\nthis, we prompted GPT-4o to extract a final answer (A/B/C/D/E or none) from each model’s solution,\nwith temperature 1.\nD\nSynthetic generation examples\nD.1\nGeneration examples\nWe review a few examples of what our synthetic datasets look like, to give a general flavor of some of\nour techniques.\nD.1.1\nExtracting and Tagging Excerpts from Content\nTo construct a dataset focused on reasoning and complexity, we extract excerpts from sources such as\nweb pages, books, and scientific articles. Each excerpt is annotated with metadata such as complexity\nlevel, factual obscurity, and the presence of reasoning chains. These annotations help identify high-value\ncontent for synthetic data generation. Below, we provide an example of metadata we generated for\nscientific content:\nStart Line: 8, End Line: 11\nInformation Type: Empirical Result Discussion\nBrief Description: Discussion of the role of SNPs in AD development and severity.\nComplexity Level: B\nFactual Obscurity: C\nChain of Reasoning: True\nReasoning Steps:\n1. Assumption: Specific SNPs influence immune response.\nConclusion: These SNPs may affect AD severity by modulating IgE levels.\nDescription: The study links genetic variations to clinical phenotypes.\n2. Assumption: SNPs associated with AD are replicated across cohorts.\nConclusion: Replication supports their role as potential genetic markers.\nDescription: Statistical analysis confirms the robustness of the findings.\nDependencies: Previous GWAS findings, statistical methods (lines 10-15)\nThe raw content will later be filtered based on the metadata, and both will be used as seed for a variety\nof synthetic generations pipelines.\n30\n\nD.1.2\nSelf-revision\nAnother component of our synthetic data pipeline is the iterative self-revision process. This process\ninvolves multiple stages where the model generates, critiques, and revises its output to meet specific\ncriteria. Below, we give an example of this workflow, focusing on the creation of a reading comprehension\nexercise based on a scientific excerpt:\n**Excerpt:**\n\"Future studies should replicate our findings for OXTR and BDNF, but also\ninclude additional stress-related candidate genes. [...] could reverse\naberrant DNA methylation-could become an important goal in the\ndevelopment of new treatment approaches.\"\n↪\n↪\n↪\n**Revision 0:**\n*Plan:* To create an advanced reading comprehension exercise:\n- Incorporate technical terms from genetics and epigenetics.\n- Add nested counterfactuals and conditional reasoning.\n- Require logical reasoning and background knowledge.\n*Exercise:* Researchers focused on DNA methylation following psychosocial\nstress. What conclusion can be drawn from studies examining both\naversive and positive experiences, diversified cohorts, and refined\nsampling intervals?\n↪\n↪\n↪\n1. A monotonic relationship between stress and methylation is confirmed.\n2. Differential methylation patterns reveal bidirectional sensitivity\n(correct).\n↪\n3. Early-life adversities remain the primary influence.\n4. Positive experiences have negligible effects.\n**Requirements (Flavored):**\n- Exercises must require both understanding the text and external advanced\nknowledge.\n↪\n- Incorrect choices should be plausible enough to mislead without full\ncomprehension.\n↪\n- The content should challenge advanced undergraduates in genetics or\nepigenetics.\n↪\n**Critique (Agent):**\n- **External Knowledge:** Does not explicitly require concepts beyond the\nexcerpt. *Score: 1*\n↪\n- **Plausibility of Choices:** Incorrect options are plausible but not\nmisleading enough. *Score: 2*\n↪\n- **Suggestions:** Introduce external concepts like epigenetic drift or\nthe diathesis-stress model, and refine incorrect choices to address\ncommon misconceptions.\n↪\n↪\n**Revision 1:**\n31\n\n*Plan:* Add references to the hypothalamic-pituitary-adrenal (HPA) axis\nand cortisol's role in stress responses, integrating advanced\nneuroendocrinology knowledge.\n↪\n↪\n*Exercise:* Considering DNA methylation and the HPA axis's role, how could\ncortisol influence classical and non-classical epigenetic changes?\n↪\n1. Cortisol is irrelevant to the modifiers discussed.\n2. Cortisol effects are linear and align with classical models.\n3. The dynamic epigenetic role of cortisol enriches research paradigms\n(correct).\n↪\n4. Cortisol’s role is limited to downregulation of methylation.\n**Critique (Agent):**\n- **Challenge Level:** Still insufficiently difficult for advanced\nundergraduates. *Score: 1*\n↪\n- **Suggestions:** Add nuanced alternatives based on theories like\neustress vs. distress or glucocorticoid response elements.\n↪\n**Revision 2:**\n*Plan:* Refine incorrect options and add concepts like glucocorticoid\nresponse elements to deepen the challenge. Reframe exercise to compare\nclassical and non-classical pathways in epigenetics.\n↪\n↪\n---\nD.1.3\nCreating Conversations from Snippets\nAnother component of our synthetic data pipeline involves transforming factual snippets into multi-turn,\nnuanced conversations. This process is designed to produce dialogues that are contextually accurate and\nintellectually engaging, encouraging critical thinking and deeper reasoning. The process of generating\nconversations includes several iterative steps:\n1. Initial Turn: Based on the snippet, the model generates a foundational response that presents\ncore information in a clear and direct manner. This step may also include an optional curation of\na participant’s profile that guides the tone and style of dialogue throughout the generation.\n2. Subsequent Turns: The dialogue grows through a dynamic flow of agents generating alternating\nturns, directing the conversation, summarizing earlier context and injecting new scenarios and\ntopics consistent with earlier messages.\n3. Self-Revision: After each turn, the model evaluates its responses, brainstorms the best next\ncourse of action given the current context, to maximize the complexity and nuance of the discussion.\nThis includes suggestions for exploring alternative perspectives or creating connections to broader\ncontexts.\nD.2\nFill-in-the-middle exercises\nMany data generation techniques require pairs of contexts and ground-truth answers for seeding and\nrely on rejection sampling during data generation. One approach we developed creates fill-in-the-middle\n32\n\nexercises from free-form text that does not naturally translate into such pairs. This involves removing\na meaningful portion of the text (the “middle”), framing the remaining text as the context, and using\nthe removed content as the ground-truth answer. The key challenge lies in identifying portions of the\ntext where reasoning can reconstruct the removed content, ensuring a “correct” answer exists.\nFor\ninstance, we generate such pairs from public code snippets, turning them into exercises and producing\ntheir solutions (augmented with chain-of-thought tokens) via rejection sampling. We give an example of\nsuch a transcript.\n**Fill-in-the-Middle Exercise**\n**Context (Excerpt from Code Snippet):**\nfor p in [path + [n] for n in graph[node]]:\nsmalls = collections.defaultdict(lambda: 0)\nfor n in path:\nif all_c['start'] > 1 or all_c['end'] > 1:\ncontinue\nfor c in smalls.values():\nif c <= 1:\ncontinue\n<<THIS PART DELETED>>\nif not bad:\npaths.append(p)\n**Solution:**\n**Background:**\nThis snippet processes paths in a graph, tracking visits to nodes,\nparticularly \"small\" nodes identified by lowercase strings. Key\nconditions:\n↪\n↪\n- Start and end nodes cannot be revisited.\n- \"Small\" nodes may have specific limits on revisits.\n**Reasoning:**\nThe missing part handles violations of small node visit limits using two\nvariables:\n↪\n- `cused` indicates if one small node has already been revisited.\n- `bad` marks the path as invalid if a second such revisit occurs.\nThe loop iterating over `smalls.values()` likely checks whether a small\nnode is revisited for the first time (allowing it by setting `cused`\nto `True`) or violates the limit (setting `bad` to `True` and breaking\nthe loop).\n↪\n↪\n↪\n**Final Guess:**\nThe missing part is likely:\nif c > 1:\nif not cused:\ncused = True\n33\n\nelse:\nbad = True\nbreak\n**Comparison to Correct Answer:**\n- The solution captures the general logic but omits the specific condition\n`elif c == 2 and not cused`, making it broader.\n↪\n- **Correctness Discussion:** The logic is close but lacks precision, as\nthe revisits must explicitly check for exactly two visits.\n↪\n**Grading and Feedback:**\n- **Grade:** 3 (partial correctness).\n- **Hint:** Consider revisiting the specific condition for revisits. Focus\non the exact count of visits that should trigger allowances.\n↪\nD.3\nAgents\nAgent tasks that involve interactions with longer horizons may present unique challenges such as plan-\nning, reflection, and error correction. Our training data consists of trajectories collected from Agen-\ntKit [WFM+24] with detailed chain-of-thought. Specifically, we generate high-quality trajectories by\nrunning AgentKit on a wide range of environments, and rewrite the guided raw reasoning outputs from\nAgentKit into a variety of self-contained statements or thoughts that aim to capture the gist of Agen-\ntKit reasoning. Training with AgentKit data improves planning, reasoning, tool use, math, and error\ncorrection on our internal benchmark.\nIn agent tasks, phi-4 follows a characteristic chain-of-thought with explicit planning and reflections.\nWe show an example of phi-4 interacting with the tower of Hanoi below:\nAnalyzing the current situation and reflecting on past actions is crucial\nfor determining the next best step. Here's a breakdown of the thought\nprocess:\n↪\n↪\n### Analysis of Current and Past Environment\n1. **Current Configuration:**\n- Rod A: |bottom, [0], top|\n- Rod B: |bottom, [1], top|\n- Rod C: |bottom, [2], top|\n2. **Past Performance:**\n- There have been repeated unsuccessful attempts to move disks,\nparticularly from rod B to C and A to C. These failures were due to\nrule violations, such as attempting to place a larger disk on a\nsmaller one.\n↪\n↪\n↪\n3. **Environmental Patterns and Rules:**\n- Only one disk can be moved at a time.\n- A larger disk cannot be placed on a smaller disk.\n- The goal is to move all disks from rod A to rod C.\n34\n\n### Reflection on Past Performance\n- **Mistakes Identified:**\n- Repeated attempts to make illegal moves indicate a misunderstanding or\noversight of the rules.\n↪\n- The plan was not effectively followed, as seen by the repeated failed\nattempts to move disks from rod B to C and A to C.\n↪\n- **Correct Moves:**\n- Successfully moved disk 0 from C to A, which was a necessary step to\nfree rod C for subsequent moves.\n↪\n### Relevant Considerations\n- **Current Objective:**\n- The immediate goal is to move disk 1 from rod B to rod C. This move is\nlegal and aligns with the rules since disk 1 is smaller than disk 2\non rod C.\n↪\n↪\n- **Strategic Alignment:**\n- Moving disk 1 from B to C is a strategic step that aligns with the\noverall plan to eventually stack all disks on rod C in the correct\norder.\n↪\n↪\n### Additional Thoughts\n- Ensuring compliance with the rules is paramount to avoid repeated\nfailures.\n↪\n- Each move should be evaluated for legality before execution to maintain\nprogress towards the goal.\n↪\n### Updated Plan\n1. **Move Disk 1 from B to C:**\n- Completion Quantification: Disk 1 is on rod C.\n- Certainty: Known. This move is legal and necessary to progress\ntowards the goal configuration.\n↪\n2. **Move Disk 0 from A to C:**\n- Completion Quantification: Disk 0 is on rod C, on top of disk 1.\n- Certainty: Known. This move is legal once disk 1 is on rod C.\n3. **Move Disk 2 from C to B:**\n- Completion Quantification: Disk 2 is on rod B.\n- Certainty: Known. This move is necessary to free rod C for the final\nsequence.\n↪\n35\n\n### Chosen Action\nBased on the analysis and the updated plan, the next logical and legal\naction is to move disk 1 from rod B to rod C. This move adheres to the\nrules and progresses towards the goal configuration.\n↪\n↪\n<updated plan>\n1. **Move Disk 1 from B to C**:\n- Completion Quantification: Disk 1 is on rod C.\n- Certainty: Known. This move is legal and necessary to progress\ntowards the goal configuration.\n↪\n2. **Move Disk 0 from A to C**:\n- Completion Quantification: Disk 0 is on rod C, on top of disk 1.\n- Certainty: Known. This move is legal once disk 1 is on rod C.\n3. **Move Disk 2 from C to B**:\n- Completion Quantification: Disk 2 is on rod B.\n- Certainty: Known. This move is necessary to free rod C for the final\nsequence.\n↪\n</updated plan>\n<action>Move the top disk of rod B to the top of rod C</action>\n36\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2412.08905",
      "full_text": "Phi-4 Technical Report\nMarah Abdin\nJyoti Aneja\nHarkirat Behl\nS´ebastien Bubeck\nRonen Eldan\nSuriya Gunasekar\nMichael Harrison\nRussell J. Hewett\nMojan Javaheripi\nPiero Kauffmann\nJames R. Lee\nYin Tat Lee\nYuanzhi Li\nWeishung Liu\nCaio C. T. Mendes\nAnh Nguyen\nEric Price\nGustavo de Rosa\nOlli Saarikivi\nAdil Salim\nShital Shah\nXin Wang\nRachel Ward\nYue Wu\nDingli Yu\nCyril Zhang\nYi Zhang\nMicrosoft Research\nAbstract\nWe present phi-4, a 14-billion parameter language model developed with a training recipe that\nis centrally focused on data quality. Unlike most language models, where pre-training is based pri-\nmarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic\ndata throughout the training process. While previous models in the Phi family largely distill the\ncapabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model\non STEM-focused QA capabilities, giving evidence that our data-generation and post-training tech-\nniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size – especially on reasoning-focused benchmarks – due to improved data,\ntraining curriculum, and innovations in the post-training scheme.\n1\nIntroduction\nRecent advancements in Large Language Models (LLMs) have shown that significant improvements in\ndata quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling\ncompute with model and dataset size. Building on the success of the Phi family [GZA+23, LBE+23,\nJBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance\nof small language models by introducing innovative synthetic data generation methods for reasoning-\nfocused tasks, by optimizing the training curriculum and data mixture, and by introducing new tech-\nniques in post-training.\nSynthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse\narray of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal.\nThese methods enable the construction of datasets that induce stronger reasoning and problem-solving\nabilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic\ndata in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and\na novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs.\nThe development of phi-4 is guided by three core pillars:\n1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are de-\nsigned to prioritize reasoning and problem-solving, carefully generated to ensure diversity and\n1\narXiv:2412.08905v1  [cs.CL]  12 Dec 2024\n\nSmall models\nLarge models\nphi-4\n14b\nphi-3\n14b\nQwen 2.5\n14b instruct\nGPT\n4o-mini\nLlama-3.3\n70b instruct\nQwen 2.5\n72b instruct\nGPT\n4o\nsimple-evals\nMMLU\n84.8\n77.9\n79.9\n81.8\n86.3\n85.3\n88.1\nGPQA\n56.1\n31.2\n42.9\n40.9\n49.1\n49.0\n50.6\nMATH\n80.4\n44.6\n75.6\n73.0\n66.31\n80.0\n74.6\nHumanEval\n82.6\n67.8\n72.1\n86.2\n78.91\n80.4\n90.6\nMGSM\n80.6\n53.5\n79.6\n86.5\n89.1\n87.3\n90.4\nSimpleQA\n3.0\n7.6\n5.4\n9.9\n20.9\n10.2\n39.4\nDROP\n75.5\n68.3\n85.5\n79.3\n90.2\n76.7\n80.9\nMMLUPro\n70.4\n51.3\n63.2\n63.4\n64.4\n69.6\n73.0\nHumanEval+\n82.8\n69.2\n79.1\n82.0\n77.9\n78.4\n88.0\nArenaHard\n75.4\n45.8\n70.2\n76.2\n65.5\n78.4\n75.6\nLiveBench\n47.6\n28.1\n46.6\n48.1\n57.6\n55.3\n57.6\nIFEval\n63.0\n57.9\n78.7\n80.0\n89.3\n85.0\n84.8\nPhiBench\n(internal)\n56.2\n43.9\n49.8\n58.7\n57.1\n64.6\n72.4\nTable 1: Performance of phi-4\non a set of standard benchmarks. The first set of benchmarks uses OpenAI’s\nsimple-evals framework [Ope24b], specifying the prompts/extraction/temperature=0.5. We compare to small\nmodels of similar inference cost, as well as to larger models.\nrelevance. We change our training curriculum and create new pretraining and midtraining data\nmixtures to increase the allocation of synthetic tokens, compared to older generations of phi.\n2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter\norganic2 data sources, including web content, licensed books, and code repositories to extract seeds\nfor the synthetic data pipeline that encourage high-depth reasoning and prioritize educational\nvalue (to the model). These seeds form the foundation of the synthetic generation pipeline. To\ncomplement these synthetic datasets, we also filter the web for high-quality data (in terms of\nknowledge and reasoning) to use directly in pretraining.\n3. Post-Training: We further advance the post-training recipe in phi-4 by creating new refined\nversions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on\npivotal token search.\nWith these innovations, the performance of phi-4 on reasoning-related tasks is comparable to or\nsurpasses much larger models. For example, its performance on many widely used reasoning-related\nbenchmarks meets or exceeds that of Llama-3.1-405B.\nIn Table 1 we compare the performance of\nour model on academic benchmarks to several contemporary foundation models.\nWe find that phi-\n4 significantly exceeds its teacher GPT-4o on the GPQA (graduate-level STEM Q&A) and MATH\n(math competition) benchmarks.\n1These scores are lower than those reported by Meta, perhaps because simple-evals has a strict formatting requirement\nthat Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but\nMeta reports 77 for MATH and 88 for HumanEval on Llama-3.3.\n2We use organic to refer to human-generated or otherwise non-synthetic data.\n2\n\nLlama-3.3\n70B Instruct\nClaude 3.5\nSonnet\nQwen 2.5\n14b-Instruct\nGPT\n4o\nGPT\n4o-mini\nQwen 2.5\n72b-Instruct\nGemini\nFlash 1.5\nGemini\nPro 1.5\nphi-4\n60\n70\n80\n90\nAverage score\n(temperature = 0.5)\n66.4\n74.8\n77.4\n77.9\n78.2\n78.7\n81.6\n89.8\n91.8\nAverage performance on November 2024 AMC 10/12 tests\nLarge model\nSmall model\nFigure 1: Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is\nthe average score (with maximum score 150) over the four tests on 100 runs with temperature t = 0.5. We chose\nt = 0.5 to follow simple-evals [Ope24b]. Error bars are 2σ of the estimate. On competition math, phi-4 scores\nwell above its weight-class even compared to non–open-weight models.\n1.1\nAddressing Overfitting and Data Contamination\nDecontamination:\nOne pitfall of foundation models is overfitting to benchmarks, such as through\nthe leakage of benchmark test sets via the web corpus. We improved the data decontamination process\nfor phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results. More\ndetails of the decontamination method are given in Appendix B.\nAMC Benchmark:\nThe surest way to guard against overfitting to the test set is to test on fresh data.\nWe tested our model on the November 2024 AMC-10 and AMC-12 math competitions [Com24], which\noccurred after all our training data was collected, and we only measured our performance after choosing\nall the hyperparameters in training our final model. These contests are the entry points to the Math\nOlympiad track in the United States and over 150,000 students take the tests each year.\nIn Figure 1 we plot the average score over the four versions of the test, all of which have a maximum\nscore of 150. phi-4 outperforms not only similar-size or open-weight models but also much larger frontier\nmodels. Such strong performance on a fresh test set suggests that phi-4’s top-tier performance on the\nMATH benchmark is not due to overfitting or contamination. We provide further details in Appendix C.\nRelying on Contamination-Proof Benchmarks:\nWe give significant weight to benchmarks which\nwere designed in such a way that the questions are original and do not appear on the web, such as\nGPQA [RHS+23]. While optimizing our model, we relied on an internal benchmark composed primarily\nof original prompts written by the team (see Section 5 for further details).\nLong Chain-of-Thought Models:\nA style of LLM that scales inference-time compute by generating\nlong chains of thought has emerged over the past few months, as pioneered by OpenAI O1 [Ope24a] and\nfollowed by DeepSeek-R1-Lite-Preview [Dee24] and Qwen/QwQ-32B-Preview [Tea24].\nThese models\nperform well on reasoning benchmarks, where QwQ, the only such model with open weights, averages\n124.5 points in the AMC-10/12 setting of Figure 1. However, QwQ also uses 4X more tokens on this\ntask than phi-4 and has more than twice as many parameters. Thus, the inference cost of QwQ is an\n3\n\norder of magnitude higher than phi-4. Consequently, these models are not in the same class as phi-4 with\nrespect to cost or latency.\n2\nApproach to Data\nThe pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of tech-\nniques. In addition, we employ several methods for filtering organic data sources that are used both as\ncomplementary datasets in the pretraining and as seeds for generating synthetic data.\n2.1\nPurpose of Synthetic Data\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi\nseries of models has consistently emphasized the importance of synthetic data. Rather than serving as\na cheap substitute for organic data, synthetic data has several direct advantages over organic data.\nStructured and Gradual Learning.\nIn organic datasets, the relationship between tokens is often\ncomplex and indirect. Many reasoning steps may be required to connect the current token to the next,\nmaking it challenging for the model to learn effectively from next-token prediction. By contrast, each\ntoken generated by a language model is by definition predicted by the preceding tokens, making it easier\nfor a model to follow the resulting reasoning patterns. In this way, synthetic data may act as a form of\n“spoonfeeding,” presenting challenges in a digestible and progression-oriented manner.\nA simple example to illustrate this is that a human-written solution to a math problem might start\nwith the final answer. This answer is much too hard to output immediately, for either a human or an\nLLM—the human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce\nit linearly. Synthetic solutions to math problems will not have such roadblocks.\nAlignment with Inference Contexts.\nSynthetic data is typically closer to the format of outputs\nwe expect our models to generate. Training on such data helps align the model’s pretraining experience\nwith the scenarios it encounters during inference. This alignment ensures that the context seen during\ngeneration remains in-distribution with respect to the data the model was pretrained on.\nFor example, web forums are very different in style from LLM interactions. If a fact only appears\nin web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces.\nRewriting facts from the web forum into the language style of an LLM makes the facts more accessible\nduring the LLM chat context of inference.\nPrinciples.\nOur approach to generating synthetic data for phi-4 is guided by the following principles:\n1. Diversity: The data should comprehensively cover subtopics and skills within each domain. This\nrequires curating diverse seeds from organic sources.\n2. Nuance and Complexity: Effective training requires nuanced, non-trivial examples that reflect\nthe complexity and the richness of the domain. Data must go beyond basics to include edge cases\nand advanced examples.\n3. Accuracy: Code should execute correctly, proofs should be valid, and explanations should adhere\nto established knowledge, etc.\n4\n\n4. Chain-of-Thought: Data should encourage systematic reasoning, teaching the model various\napproaches to the problems in a step-by-step manner. This fosters coherent outputs for complex\ntasks.\n2.2\nSynthetic Data for Pretraining and Midtraining\nWe created 50 broad types of synthetic datasets, each one relying on a different set of seeds and dif-\nferent multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction,\naccumulating to a total of about 400B unweighted tokens. In Appendix D, we give a few examples\nof transcripts taken from our synthetic generations. Here, we highlight novel methodologies used in\ngenerating synthetic datasets for phi-4:\n• Seed Curation: The synthetic dataset generation begins with high-quality seeds sourced from\nmultiple domains.\nThese curated seeds provide the foundation for synthetic data generation,\nenabling the creation of exercises, discussions, and reasoning tasks tailored to the model’s training\nobjectives.\n1. Web and Code-based Seeds: Excerpts and snippets are extracted from web pages, books,\nand code repositories with a focus on content that demonstrates high complexity, reasoning\ndepth, and educational value. To ensure quality, we employ a two-stage filtering process:\nfirst, identifying pages with strong educational potential, and second, segmenting the selected\npages into passages, scoring each for its factual and reasoning content.\n2. Question Datasets: A large set of questions was collected from websites, forums, and Q&A\nplatforms. These questions were then filtered using a plurality-based technique to balance\ndifficulty.\nSpecifically, we generated multiple independent answers for each question and\napplied majority voting to assess the consistency of responses. We discarded questions where\nall answers agreed (indicating the question was too easy) or where answers were entirely\ninconsistent (indicating the question was too difficult or ambiguous). This filtering process\nproduces a dataset of questions that challenge the model’s reasoning and problem-solving\nabilities while remaining approachable.\nThe plurality answers were used in place of the\nground truth in our rejection-sampling based generations.\n3. Creating Question-Answer pairs from Diverse Sources: Another technique we use\nfor seed curation involves leveraging language models to extract question-answer pairs from\norganic sources such as books, scientific papers, and code. This approach does not rely on\nmerely identifying explicit Q&A pairs within the text. Instead, it involves a pipeline designed\nto detect deduction chains or logical progressions in the text. The language model identifies\nkey steps in reasoning or problem-solving processes and reformulates them into questions\nand corresponding answers. Our experiments show that, if done correctly, training on the\nresulting content can be far more effective (in terms of improvement on academic and internal\nbenchmarks) than training on the original content.\n• Rewrite and Augment: Seeds are transformed into synthetic data through multi-step prompting\nworkflows. This includes rewriting most of the useful content in given passages into exercises,\ndiscussions, or structured reasoning tasks.\n• Self-revision: The initial responses are then iteratively refined through a feedback loop where\na model critiques and subsequently improves its own outputs, guided by the rubrics focused on\nreasoning and factual accuracy.\n5\n\n• Instruction Reversal for Code and Other Tasks: To enhance the model’s ability to generate\noutputs from instructions, we used an instruction reversal technique. For example, we take existing\ncode snippets from the code data corpus and use it to generate corresponding instructions that\ninclude the problem description or task prompt. The resulting synthetic data pairs were structured\nwith the instruction appearing before the code. Only data with high fidelity between the original\nand regenerated code are retained, ensuring alignment between the instructions and the outputs.\nThis method can be generalized to other targeted use cases.\n• Validation of Code and Other Scientific Data: When appropriate, we incorporate tests for\nvalidating our reasoning-heavy synthetic datasets. The synthetic code data is validated through ex-\necution loops and tests. For scientific datasets, the questions are extracted from scientific materials\nusing a method designed to ensure high relevance, groundedness, and difficulty balance.\n2.3\nCuration and Filtering of Web and Q&A Data\nQ&A datasets.\nWe collected tens-of-millions high-quality organic problems and solutions by review-\ning public websites, relying on existing datasets, and acquiring external datasets. Our experience from\nprevious models showed that question-answer data contributed significantly to various capabilities, such\nas mathematical reasoning and academic performance. Our ablation studies showed that organic ques-\ntions are substantially more effective than synthetic questions. We used several ways to synthetically\naugment the dataset of organic questions to obtain a larger dataset. While these rewritten questions\nimproved the model’s capabilities, the gains were not as pronounced. A significant portion of the col-\nlected questions lacked accurate solutions. To address this, we replaced the answers with synthetically\ngenerated ones and used majority-voting to increase accuracy. All collected questions and solutions\nunderwent a thorough decontamination process to ensure there is no overlap with test sets3.\nTargeting High-quality Web Data.\nWe collected a wide variety of high-quality organic data sources\nfor phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,\nand programming tutorials). In addition to directly training on this text, we used various web sources\nas seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to\nbe absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations\nfor derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web\ndata. We discuss the main techniques and considerations below:\n• Targeted Acquisitions: We included major repositories of reasoning-dense documents that are\npublicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g.,\nlicensed books) aiming for a level of comprehensiveness, recency, and cleanliness above the typical\nstandard of externally available corpora.\n• Filtering Web Dumps: To capture the long tail of information-rich web sources (e.g., forums,\nblogs, course material, domain-specific wikis), we took the approach of selecting a small fraction\nof highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on\n∼106 LLM-generated annotations. This approach tends to over-index on STEM-related keywords,\nso we created a specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history,\ntravel, culture, and entertainment). These topic classifications were also obtained by distilling an\n3This step is crucial to the reliability of some of the academic benchmarks: for instance, some test benchmark variants\ncan be found on platforms like Hugging Face. Moreover, benchmarks such as MMLU are frequently compiled from web-\nsourced questions.\n6\n\nLLM annotator. Finally, we removed corrupted text and binary files by detecting outliers according\nto n-gram statistics and compression ratios.\n• Multilingual Data: We incorporated multilingual datasets to ensure that our model could han-\ndle a wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi\nand Japanese. This involved sourcing and processing high-quality multilingual documents from\nCommonCrawl and Wikipedia. Our multilingual processing pipeline consists of a language identifi-\ncation model, based on fastText used to categorize documents into 176 languages, then uses the\nsame classifiers for filtering web dumps to filter for quality. Note that the classifiers were trained\non multilingual LLM-generated annotations.\n• Custom Extraction and Cleaning Pipelines: To ensure sufficient cleanliness and uniformity\nbetween heterogeneous organic data sources, we needed a collection of customized heuristics and\nparsers. For each targeted data source, we built custom pipelines to ingest a variety of file formats\n(e.g., multi-file TeX source, ePub and other XML-like formats, Microsoft Word documents, and\nPDFs).\nFor general web data, we built a custom HTML-to-text extractor, taking significant\ncare to preserve fragile content that is frequently corrupted by na¨ıve parsers (e.g., TeX/MathML\nequations, code blocks, tables, and forum thread structure). This extractor prunes and normalizes\nthe DOM tree, using a variety of signals (e.g., HTML tag names, CSS classes, content length, and\ntree depth) to distinguish elements such as boilerplate, advertisements, equations, and syntax-\nhighlighter artifacts.\n2.4\nPost-Training datasets\nOur post-training data is composed of:\n• Supervised Fine-Tuning (SFT) Datasets: Using carefully curated user prompts taken from\na mixture of publicly available datasets and synthetically generated data, we generate multiple\nmodel responses and select the best using an LLM-based evaluation process.\n• Direct Preference Optimization (DPO): We generate DPO pairs based on rejection sampling\nand LLM evaluation, a part of which is based on our approach to creating pivotal token-based\npairs, explained in Section 4.3 below.\n3\nPretraining details\nThe phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and\na default context length of 4096. This is later extended to a 16K context length during midtraining. The\narchitecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better\nmultilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use\nfull attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium.\nThe model was pretrained for approximately 10T tokens using linear warm-up and decay schedules\nwith peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. The\ntraining hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted\nby stress testing the learning rate warm-up stage for stability.\nPretraining is followed by a shorter\nmidtraining stage to increase the original context length of 4k to 16k.\nSince pre-trained models are not good at instruction following, it is not very informative to use\n0-shot evaluations that require the answer to be in a specific format, for example simple-evals. We\n7\n\nMMLU\nMMLU pro\nGSM8k\nHuman-Eval\nARCC\nMBPP\nMATH\nTQA\nphi-4 (4k)\n+3.0\n+10.3\n+2.2\n+7.8\n+1.1\n+6.8\n+8.9\n-0.7\nphi-4 (16k)\n+2.7\n+8.9\n+1.2\n+9.0\n+0.9\n+9.6\n+8.4\n-1.5\nTable 2: Pretraining benchmarks for phi-4 compared to its predecessor, phi-3-medium after pretraining.\ntherefore use an internal implementation of benchmarks for pretraining which uses a mixture of log-\nlikelihood and/or few-shot prompts for various tasks. Specifically, we used log-likelihood evaluations\nfor MMLU (5-shot), MMLU-pro, and ARCC (1-shot). We used 1, 3, 4, and 8 few-shot examples for\nTriviaQA (TQA), MBPP, MATH, and GSM8k to help the model adhere to the answer format for easier\nextraction of the solution. We use this evaluation method throughout Section 3. Table 2 summarizes\nthe performance boost of pretrained phi-4 compared with its predecessor phi-3-medium.\n3.1\nData Composition in Pretraining\nThe phi-3 model family were trained using a two-phase strategy. Most of the training tokens were used\nin phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with a\ndata mixture consisting primarily of synthetic tokens and a much smaller allocation for ultra-filtered\nand reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed a\nmarginal drop in the benefit from using non-synthetic tokens for the phi-3 family of model sizes. We\nnote two key observations.\n• Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs\nover our synthetic data led to better performance with respect to adding fresh web tokens.\n• Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and\ndemonstrated increased hallucinations.\nFigure 2 demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In\nthis example, we conduct two training runs per model scale, using the same number of training tokens\non top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed\n(a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and\n12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen,\nperforming more iterations on the synthetic data is more beneficial than supplying more web tokens.\nInspired by this scaling behavior of our synthetic data, we trained a 13B parameter model solely on\nsynthetic4 data, for ablation purposes only – the model sees over 20 repetitions of each data source. For\nthe sake of ablations, we partitioned our synthetic data into web rewrites, which includes more direct\nrewrites of our filtered web content relative to all other types of synthetic data. Table 3 compares the\nprevious phi-3-medium model with the new model trained entirely on the synthetic data. Throughout\ntraining, all benchmarks consistently improved, despite the increase in epochs, and the majority of\nthe benchmarks showed improvements over phi-3. However, knowledge-related benchmarks, like 1-shot\ntriviaqa (TQA), show a large gap where synthetic models are subpar. These observations led us to\nrethink the role of web data in our data mixture.\n4This is an updated mixture of synthetic data that contains new sources compared to phi-3.\n8\n\nFigure 2: 5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are\ntrained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens.\nWe see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch\nmodels perform better than those that have seen more unique web tokens.\nMMLU\nMMLU pro\nGSM8k\nHuman-Eval\nARCC\nMBPP\nMATH\nTQA\nSynthetic\n+0.8\n+4.0\n+2.2\n+12.1\n0.0\n+5.0\n+4.9\n-14.8\nSynthetic + Web Rewrites\n+0.3\n+4.1\n+1.8\n+13.3\n+3.0\n+7.6\n+8.1\n-7.7\nTable 3: Benchmark performance of 13B models (used for ablations only) trained on data mixtures containing\nno web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic\ndata and web rewrites. All numbers are reported relative to the performance of phi-3-medium, which has seen a\ncombination of web and synthetic data.\n3.2\nData Mixture\nTo design our pretraining data mixture for a given training token budget, we search over different\nallocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites5, 3) filtered web\n(divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g.,\nacademic data, books, and forums), and 5) code data.\nWe conducted ablations using a shorter token horizon of 1T tokens to derive the data mixture.\nThese ablations rely on our established result on the high-rank correlation of short training with longer\ntraining, up to the over-fitting saturation threshold of data sources. In addition we observe a high rank\ncorrelation between the performance of the 7B and 14B models on different data mixtures, given a large\nenough distance between the data mixtures. This allowed us to conduct the experiments at 7B scale and\ntransfer the findings to phi-4. Among the numerous ablations, we highlight a few that show best insights\non our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions\nand code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters.\nTable 4 summarizes the results for the hand-picked ablations, as compared with the data mixture\nthat was used for the final training run. A uniform allocation of tokens among the three categories is\nsuboptimal due to the higher quality of synthetic data and the only benchmark that shows a clear benefit\nfrom web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally\nbetter than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy\nfiltered web data sources to improve knowledge benchmarks (see Section 3.1) to balance all model\n5Web rewrites is a sub-category of synthetic data that is substantially large and contains direct rewrites of web content.\n9\n\nMMLU\nMATH\nGSM8k\nHuman-Eval\nARCC\nMBPP\nTQA\nMMLU pro\nAverage\nUniform\n-3.3\n-5.4\n-5.8\n-1.2\n+0.6\n-2.0\n+3.3\n-3.6\n-2.2\nS\n+3.3\n+4.0\n+2.1\n-6.1\n+1.9\n+0.4\n-3.0\n+3.7\n+0.8\nS + WR\n+0.6\n+1.2\n+1.5\n-1.2\n+1.6\n+1.6\n-3.7\n+1.2\n+0.4\nS + W\n-0.6\n-0.7\n-0.7\n-4.3\n+0.3\n-2.0\n+6.9\n+0.9\n0.0\nTable 4: Ablations on the allocation of 75% of training tokens to synthetic (S), filtered web (W), and web rewrite\n(WR) categories, while other data sources are held constant in the remaining 25% token budget. All benchmark\nnumbers are measured relative to the final data mixture used for training phi-4.\ncapabilities. We also note that we observed the gap between the chosen data mixture and the synthetic\nheavy runs largely closes as the model goes through the post-training stage. An end-to-end optimization\nof pretraining data mixture that also takes into account the effects of post-training is an interesting\nfuture area of investigation.\nData\nSource\nFraction\nof Training\nUnique\nToken Count\nNumber of\nEpochs\nWeb\n15%\n1.3T\n1.2\nWeb rewrites\n15%\n290B\n5.2\nSynthetic\n40%\n290B\n13.8\nCode data\n20%\n820B\n2.4\nAcquired sources\n10%\n580B\n1.7\nTable 5: Data mixture for pretraining.\nThe final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites\ndata sources, divided equally between them. The remaining tokens are largely sourced from synthetic\ndata which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data\n(mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books.\nIn terms of total number of unique tokens in each data mixture cluster, filtered web data is the largest\ncluster with ∼1.3T tokens. Code and targeted acquisitions are the second and third largest clusters with\n∼820B and ∼580B tokens, respectively. Finally, web rewrites and synthetic data have similar token\ncount of ∼290B tokens. The total number of epochs on each data source is determined using the ratio\nof allocated tokens in the mixture and the number of unique tokens in that source.\n3.3\nMidtraining Details\nphi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K.\nWe conduct several ablations to study the role of data on long-context performance. Specifically, we try\ndata sources that are inherently long context, and compare them with artificially created long context\ndata where samples are padded together to fill the sequence. We observe the former to perform better\nin longer context tasks.\nInspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and\ncode data) to separate samples above 8K context. We then up-weight the data subsets that are 16K or\nhigher in length. We also create new synthetic datasets that satisfy the > 4K sequence requirement. The\nfinal data mixture includes 30% of the newly curated longer context data and a 70% portion of recall\n10\n\nModel\nMax Length\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm\nphi-4\n8K\n100.0\n58.1\n68.0\n65.3\n26.7\n38.3\nQwen-2.5-14B\n8K\n100.0\n62.2\n67.8\n58.2\n24.7\n37.2\nLlama-3.3-70B\n8K\n92.0\n65.3\n69.4\n64.4\n30.0\n37.8\nGPT-4o-mini\n8K\n99.2\n65.8\n74.4\n69.4\n31.3\n38.5\nGPT-4o\n8K\n100.0\n66.9\n83.0\n75.1\n37.3\n43.0\nphi-4\n16K\n99.0\n57.1\n77.0\n54.4\n36.0\n40.5\nQwen-2.5-14B\n16K\n100.0\n59.1\n67.6\n50.3\n29.7\n42.3\nLlama-3.3-70B\n16K\n92.0\n62.2\n70.0\n63.3\n36.7\n41.9\nGPT-4o-mini\n16K\n100.0\n63.6\n78.4\n63.9\n36.0\n45.2\nGPT-4o\n16K\n100.0\n66.7\n85.6\n73.8\n43.7\n46.3\nTable 6: Evaluation results on the long-context benchmark HELMET [YGH+24].\ntokens from the pretraining stage. To accommodate longer context, we increase the base frequency of\nrope position encoding to 250K following [AI23b]. We drop the maximum learning rate by a factor of\n10 compared to the pretraining stage and train for a total of 250B tokens.\nTo effectively evaluate the long-context capability of our model, it is essential to have a comprehensive\nevaluation framework with practical scenarios. While synthetic benchmarks like needle-in-a-haystack and\nRULER are preferred for their simplicity and control, our emphasis is on a diverse range of tasks that\nreflect real-world applications, such as reasoning across entire documents. We report the performance\nof phi-4 and other models on the tasks we selected from the HELMET [YGH+24] evaluation suite in\nTable 6 and outline our evaluation methods below. Note that results are average across 5 runs for each\ncategories.\n• Recall: The task involves retrieving the corresponding value from a randomly-generated long JSON\nfile given a specific key (Metric: SubEM)\n• RAG: Answer questions based on many retrieved and shuffled Wikipedia documents. The datasets\nused for this task are NaturalQuestions, HotpotQA, and PopQA. Final results are average of all\ndatasets (Metric: SubEM)\n• Re-rank: The task is to re-rank the top-10 documents given a query and many retrieved and\nshuffled documents. This uses the MSMARCO dataset (Metric: nDCG@10)\n• ICL: The task involves many-shot in-context learning with datasets such as TREC coarse, TREC\nfine, Banking77, NLU and CLINC150. Final results are average of all datasets (Metric: F1)\n• QA: Answer questions given a lengthy document. The dataset associated with this task is Narra-\ntiveQAv2 (Metric: GPT-4o scoring)\n• Summ: The task involves summarizing a lengthy legal document, and the dataset used is Multi-\nLexSum (Metric: GPT-4o scoring)\n11\n\nDataset Name\nSample Count\nunknown + safety data\n3,000\ngeneric multiple-choice Q&A\n132,859\nmath data\n76,552\npython data\n16,080\ncpp, go, java, js, rust data\n21,806\nTable 7: Data Mixture for Pivotal Token DPO\nDataset Name\nSample Count\nunknown + safety data\n43,842\nany vs any overall\n266,000\nany vs any accuracy\n532,000\nTable 8: Data Mixture for Judge Guided DPO\n4\nPost-Training\nPost-training is aimed at transforming the pretrained language model into an AI assistant that users\ncan safely interact with.\nWe align the pretrained model with one round of SFT 4.1, one round of\nDPO [RSM+23] on data from our pivotal token search method (see Section 4.3), and one round of DPO\non full length preference pairs. The model is chat finetuned using the standard chatml format, example\nusage template for two rounds of a conversation is as follows:\n<|im start|>system<|im sep|>system message<|im end|>\n<|im start|>user<|im sep|>prompt1<|im end|><|im start|>assistant<|im sep|>response1<|im end|>\n<|im start|>user<|im sep|>prompt2<|im end|><|im start|>assistant<|im sep|>\n4.1\nSupervised Fine-Tuning\nIn this phase, we fine-tune the pretrained model with a learning rate of 10−6 on a variety of data generated\nfrom high-quality data across diverse domains, including math, coding, reasoning, conversation, model\nidentity, and safety. We also added multilingual data for 40 languages. We use around 8B tokens of\ndata in this phase, all formatted in the chatml format.\n4.2\nDirect Preference Optimization\nWe use DPO [RSM+23] to align the model with human preferences, and also to steer the model away\nfrom unwanted behavior through pairs of desired and undesired outputs. DPO data covers chat format\ndata, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning,\nrobustness, and safety. We do two rounds of DPO on the SFT model. We introduce a technique, Pivotal\nToken Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture\nfor first round are provided in Table 7.\nFor the second round, which we call judge-guided DPO, we gather approximately 850k pairs of\ndesired and undesired outputs. The prompts are sourced from various publicly available instruction\ntuning datasets and also include prompts related to safety and Responsible AI (RAI). Next, for each of\nthese prompts, we generate responses from GPT-4o, GPT-4t and our model. From these responses, we\ncreate various combinations of DPO pairs and use GPT-4o as a judge to label positive or negative for\na given pair. For a given pair of responses, each assistant response is given a score based on accuracy,\nstyle, and detail. We label the response with higher accuracy or overall (average of accuracy, style, and\ndetail) score as the positive response. We provide the prompt we used in Appendix A. The data mixture\nfor this round is provided in Table 8. Both stages also include a small amount of data for safety and\nmitigating hallucinations.\n12\n\nUser:\nSuppose that the roots of x3 + 3x2 + 4x −11 = 0 are a, b, and c, and that the roots\nof x3 + rx2 + sx + t = 0 are a + b, b + c, and c + a.\nFind t.\nAI: To solve the problem, we need to deal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal\ndeal with the roots of the two cubic equations\ngiven.\nFirst, consider the cubic polynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial\npolynomial \\( xˆ3 + 3xˆ2 + 4x - 11 = 0 \\),\nwith roots \\( a \\), \\( b \\), and \\( c \\).\nBy Vieta’s formulas, we have:\n\\[ a +\nb + c = -3, \\] \\[ ab + bc + ca = 4, \\] \\[ abc = 11.\n\\] Now, consider the other\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\nother\npolynomial \\( xˆ3 + rxˆ2 + sx + t = 0 \\) with roots \\( a+b \\), \\( b+c \\), and \\(\nc+a \\).\nWe need to find the value of \\( t \\), which corresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds\ncorresponds to the\nnegative\n0.42 →0.93\nof\nthe product of the roots according\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording\naccording to Vieta’s formulas::::\n\\[\n(a\n0.95 →0.71 +b)(b+c)(c+a).\n\\] First\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst\nFirst, note\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote\nnote the important\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant identity:\n\\[ (a+b)(b+c)(c+a) = (a+b+c)(ab+bc+ca) -\nabc.\n\\] Plug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlug\nPlugging in known\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown\nknown values from our\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour\nour Vieta’s results for\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor \\( a \\), \\( b \\), and\n\\( c \\), we have:\n\\[ a ++++ b + c = -3, \\] \\[ ab + bc + ca = 4, \\] \\[ abc = 11.\n\\]\nSubstitute these into the identity:\n\\[ (a+b)(b+c)(c+a) = (-3)(4) - 11 = -12 - 11 =\n-23.\n\\] Thus, \\( t = -(a+b)(b+c)(c+a) = 23 \\).\nTherefore, the value of \\( t \\) is\n\\(\\boxed\\23\\\\).\n0\n1\np(success)\nFigure 3: Illustration of pivotal tokens for GPT-4o at temperature 1 on a problem from the MATH bench-\nmark [HBK+21], where the initial success probability is 0.31. Each token is colorized by the probability of success\nfor an independent completion (N = 529) continued from after the token, with red for p(success) = 0 and blue\nfor p(success) = 1. The line plot shows the same probabilities. The tokens that changes p(success) by ≥0.2 are\nshown boxed , with subscripts showing the change in probability. Tokens with probability ≤0.1 are underlined to\nillustrate that pivotal tokens are distinct from low-probability tokens. The token probabilities of\nnegative and\n(a were 0.31 and 0.12, respectively. The greedy tokens for the same prefixes are\nproduct with 0.66 probability\nand t with 0.88 probability.\nprocedure PivotalTokenSearch(Q,Tfull,pgap)\nprocedure Subdivide(Tprefix,T)\nif ∣T∣≤1 or ∣p(success ∣Tprefix) −p(success ∣Tprefix + T)∣< pgap then\n▷Base cases.\nreturn [T]\nTleft,Tright ←Split(T)\n▷We split at the cumulative midpoint of token log probabilities.\nreturn Subdivide(Tprefix,Tleft) ∪Subdivide(Tprefix + Tleft,Tright)\nTprefix ←ϵ\nfor all T ∈Subdivide(ϵ,Tfull) do\nif ∣T∣= 1 and ∣p(success ∣Tprefix) −p(success ∣Tprefix + T)∣≥pgap then\nyield (Q,Tprefix,T)\n▷Output pivotal tokens T and context for postprocessing.\nTprefix ←Tprefix + T\nFigure 4: Pseudocode for Pivotal Token Search (PTS). Note that estimating p(success ∣...) involves sampling the\nlanguage model and invoking the oracle. In an efficient implementation p(success ∣...) should be memoized.\n13\n\n4.3\nPivotal Token Search\nConsider a generative model producing a token-by-token response to a given prompt. For each token\nproduced, which corresponds to a prefix of the model response, one can consider the conditional proba-\nbility of the model’s answer being correct given that prefix, as well as the increment in this probability\nwith respect to that token (in other words, the difference in the probability of being correct before and\nafter producing that token). It is often the case that the overall correctness is highly dependent on\na successful generation of a small number of key tokens. For example, we can see in Figure 3 where\nthe model outputs a math solution and a “fortunate” sampling of a crucial token\nnegative shifts the\nsolution from possible failure to likely success, while sampling of the token (a subsequently risks failure\nagain. We refer to these tokens as pivotal tokens as they have an outsized effect on the course of the\nsolution.\nNow, consider how the solution from Figure 3 would be used in DPO as a full-length accepted\nresponse. As the figure shows, there are many tokens with probabilities much lower than the 0.31 of\nnegative , which would contribute to noise in the gradients diluting the signal from the pivotal token.\nEven worse, the token (a that contributed to the lack of robustness would receive a strong positive\nlearning signal thanks to its low probability of 0.12.\nMoreover, intuition suggests that when two texts substantially deviate from each other, comparison\nof their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it\nmakes more sense that the signal should come from the first tokens after the two texts starts diverging\nfrom each other.\nTo alleviate these effects, we employ a method we call Pivotal Token Search (PTS) for generating\npreference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the\npreference optimization takes effect with respect to a single token.\nPTS identifies points of a completion token sequence Tfull = t1,t2,... for some user query Q where\nthe next token ti has a significant impact on the probability of success p(success ∣t1,...,ti). PTS es-\ntimates these probabilities by sampling completions starting from Q + t1,...,ti, which are checked for\ncorrectness with an oracle6 for Q. Figure 4 shows a basic instantiation of the algorithm. The proce-\ndure Subdivide recursively splits the sequence into segments ti,...,tj until the change in probability\n∣p(success ∣t1,...,ti−1) −p(success ∣t1,...,tj)∣for each segment is below a threshold pgap or the segment\nis just a single token. Tokens with a sharp change in success probability are kept as pivotal. We turn\npivotal tokens into preference data by taking Q + t1,...,ti−1 as the query, and single tokens tacc and trej\nthat increase/decrease p(success ∣t1,...,ti−1,tacc/rej) as the accepted and rejected completions, respec-\ntively.7 The binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but\nit only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the\ncourse of the solution.\nWe used PTS to generate preference data for tasks where ground-truth is readily available, such as\nmathematics, various forms of question answering and coding. To improve sample efficiency, we filter\nthe target questions to only include those with 0.2 ≤p(success) ≤0.8, as pivotal tokens are rare for tasks\nthat are very easy or hard.\nSee Figure 5 for examples of preference data we generated using PTS. The math question answering\nexample shows how pivotal tokens often are not actual mistakes, but choices that drive the model down\na less advantageous path. Here, multiplying both sides by the denominators separately is equally valid\nto directly cross-multiplying, but for the model doing the latter here is more robust. By generating\nDPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially\n6For coding, a comprehensive test suite can be used. For math problems, answers can be compared to the ground truth.\n7We find drawing tacc and trej from rollouts PTS already used to estimate p(success ∣t1, . . . , ti−1) to be effective.\n14\n\nUser:\nFind the product of all real values of r for which\n1\n2x\n=\nr−x\n7\nhas exactly one real\nsolution.\nAI: To solve the problem, we need to determine the values of r such that the equation\n1\n2x = r −x\n7\nhas exactly one real solution for x.\nFirst, clear the fractions by\nGood:\ncross-multiplying\nBad:\nmultiplying both sides by\nUser:\nConsider a 1 keV helium atom impinging at a one-degree incidence angle on a LiF\ncrystal surface.\nAdopt the axial channeling approximation to model the interaction\nbetween the helium atom and the crystal as a 2D potential energy landscape.\nApply\nNewton’s equations to compute the helium atom’s trajectory and determine the subsequent\nmomentum transfers from binary collisions with surface atoms.\nSum the total virtual\nenergy transferred across all collisions and compute the probability, Pe, that the\nhelium atom remains elastically scattered assuming a vibrational frequency for surface\natoms of ω = 1 × 1014Hz.\nWhat is Pe?\n1.\nPe = exp(−Eloss\n2̵hω ), 2.\nPe = exp( Eloss\n̵hω ), 3.\nPe = exp(−Eloss\n̵hω ), or 4.\nPe = 1 −exp(−Eloss\n̵hω )\nAI: To determine the probability Pe that the helium atom remains elastically scattered,\nwe need to calculate the total virtual energy transferred, Eloss, during the helium\natom’s interaction with the LiF crystal surface.\nGiven\nGood:\nthe vibrational frequency\nBad:\nthat the potential interactions\nUser:\nRead the following function signature and docstring, and fully implement the\nfunction described.\nYour response should only contain the code for this function.\ndef reorder_sequence(n: int, sequence: list[int]) -> list[int]:\n\"\"\"\nGiven a sequence of integer numbers that Polycarp wrote down on the board, this\nfunction rearranges (reorders) elements of this sequence to match the order\nPolycarp could have possibly written them. Each next number should be exactly two\ntimes the previous number or exactly one third of the previous number.\n:param n: The number of elements in the sequence (2 <= n <= 100).\n:param sequence: A list of integer numbers in arbitrary order (1 <= elements <= 3 *\n10ˆ18).\n:return: A reordered list of integer numbers representing a valid sequence Polycarp\ncould have written.\n\"\"\"\nAI: ...\nfrom collections import defaultdict\n# Create adjacency list where each number can point to its possible predecessor and\nsuccessors\ngraph = defaultdict(list)\nfor num in sequence:\nfor other in sequence:\nif num != other:\nif num == other * 2 or (\nGood:\nother % 3\nBad:\nnum * 2\nFigure 5: Preference data generated by Pivotal Token Search in answering math and physics questions, and\nimplementing a function in Python. The tokens that form the actual pair for DPO are underlined.\n15\n\nBase\nSFT\nDPO Stage 1\nFinal\n0\n20\n40\n60\n80\n100\nPercentage (%)\n6.8%\n3.2%\n90.0%\n3.7%\n57.5%\n38.7%\n2.9%\n79.8%\n17.4%\n3.0%\n81.1%\n15.8%\nSimpleQA performance over the course of post-training\nCorrect\nNot Attempted\nIncorrect\nFigure 6: The post-training process described in Appendix A.1 decreases hallucinations. One measure is that\nthe problems in SimpleQA—which the model very rarely gets correct—are increasingly not attempted during the\ncourse of post-training. We believe the final result is better behavior, even though the simple-evals score for\nSimpleQA (the F1 score) actually gives our base model a higher score than our final model.\nstronger.\nRelated Work:\nIn [LLX+24] a contrastive estimation approach involving a model trained on incorrect\ntrajectories is used to score which tokens likely contributed to failure, which is further employed to\nweigh rejected responses in DPO. In comparison, our PTS avoids complications from learned proxies by\ndirectly estimating p(success). They also report difficulties applying their method to accepted responses\nin DPO, while our method generates both positive and negative preference data directly targeting pivotal\ntokens. Automated process supervision methods [WLS+24, LLL+24] have applied search and rollouts to\ngenerate data for training process reward models. PTS can be seen as an automated process supervision\nmethod that generates token-level preference data suitable for DPO.\n4.4\nHallucination mitigation\nWe generate SFT data and DPO pairs to mitigate hallucination. If the model does not know the answer,\nwe would rather it refuse to answer than to make up a hallucination. We present the details of this\nprocess, including prompts to create the data, in Appendix A.1. This greatly decreases hallucinations\nin SimpleQA (see Figure 6).\n4.5\nPost-Training Ablation\nIn Table 1 we show how our benchmark scores evolve during post-training. We also evaluate dropping\npivotal token DPO and only performing the second stage of DPO. In general, we find that pivotal token\nDPO is most useful on reasoning-heavy tasks (GPQA, MATH) while judge-guided DPO is particularly\nuseful for the benchmark that itself involves a GPT-4 judge: ArenaHard. We also find the two approaches\nto be complementary.\n5\nBenchmarking Considerations\nWhile academic benchmarks are a widely used to measure the progress in LLM advancement, they\nsuffer from several limitations that can fail to reveal a model’s true capabilities and weaknesses. These\nlimitations include:\n• Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora,\ncreating a risk of data contamination. Although we took extensive measures to deduplicate and\n16\n\nSFT\nDPO\nstage 1\nDPO\nstage 2 only\nphi-4\n(stage 1 + 2)\nsimple-evals\nMMLU\n82.8\n84.8\n84.2\n84.8\nGPQA\n47.3\n53.6\n52.4\n56.1\nMATH\n77.1\n80.5\n77.6\n80.4\nHumanEval\n79.5\n81.6\n81.5\n82.6\nMGSM\n80.8\n80.8\n81.5\n80.6\nSimpleQA\n3.7\n2.9\n2.9\n3.0\nDROP\n82.8\n86.1\n71.8\n75.5\nMMLUPro\n61.9\n70.0\n67.2\n70.4\nHumanEval+\n77.9\n81.9\n81.4\n82.8\nArenaHard\n56.7\n66.5\n69.8\n75.4\nIFEval\n66.2\n63.0\n63.0\n63.0\nPhiBench\n(internal)\n48.2\n54.5\n53.0\n56.2\nTable 9: Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is\nmore standard judge-guided DPO. Each also has 1-5% hallucination and safety data mixed in.\ndecontaminate our training data, including standard n-gram deduplication and decontamination,\nthese methods are not effective against all scenarios, including rephrasing, which leaves some\nuncertainty about the true extent of generalization.\n• Limited Skill Scope: Most benchmarks evaluate models on narrowly defined skills, such as\nsolving specific style of math problems at certain grade level or implementing isolated Python\nfunctions. This narrow scope can fail to capture a model’s broader capabilities and weaknesses.\n• Bias in Generation-Based Benchmarks: Some benchmarks use LLM-as-judge for evaluat-\ning generated outputs. These judgments sometimes may prioritize style, fluency, or surface-level\nqualities over accuracy and validity of the reasoning chain, leading to potential biases in scoring.\n• Limitations of Multiple-Choice Tasks: Benchmarks that rely on multiple-choice questions\noften test a model’s ability to make clever guesses that can be achieved by pattern matching\nrather than effectively utilizing the underlying concepts through reasoning.\nTo address these issues, we maintain an internal benchmark called PhiBench, which is tailored to\nevaluate the diverse skills and reasoning abilities that we found critical to phi-4’s development. This\nbenchmark was designed with the following goals:\n1. Originality: All questions in the benchmark were composed by our team making sure that they\nwere not present in our pretraining data. Our goal for the internal benchmark is to reveal model’s\ngeneralization ability in various domains.\n2. Skill Diversity: Our benchmark includes a wide range of tasks to assess multiple dimensions\nof model performance. For instance, in coding, it goes beyond isolated function implementation\nto include debugging, extending incomplete code, and explaining code snippets.\nSimilarly, in\n17\n\nmathematics, it incorporates tasks like identifying the errors in proofs or generating related prob-\nlems, rather than simply solving equations. This ensures that the benchmark captures a broader\nspectrum of skills and reasoning processes.\n3. Rigorous Scoring for Generation Tasks: For tasks requiring judgment of model-generated\noutputs, we addressed the common pitfalls of LLM-based scoring by carefully curating detailed\njudge instructions (or “judge notes”). These rubrics specify exactly how to evaluate responses,\nfocusing on achieving accuracy, logical structure, and adherence to task requirements, while min-\nimizing tendencies towards stylistic biases. We observed significantly improved consistency and\nreduction of adverse impact due to subjective preferences in the scoring outcomes.\nPhiBench played a central role in optimizing phi-4. We used it to guide decisions about dataset\nmixtures and hyperparameter choices for more effective post-training techniques. PhiBench was also\nused to perform high-signal studies that identify weaknesses in the model and provide feedback for new\nincoming data sources.\n6\nPerformance on Key Benchmarks\nOur benchmark results were presented in Table 1, along with comparisons to other models. We first re-\nport the values from OpenAI’s simple-evals benchmark, which is a framework (including prompts, tem-\nperature, and extraction) for evaluating MMLU [HBB+20], GPQA diamond [RHS+23], MATH [HBK+21],\nHumanEval [CTJ+21], MGSM [SSF+22], and the SimpleQA [WKC+24] F1-score.\nWe also consider\nMMLU-pro [WMZ+24], HumanEval+ [LXWZ23], ArenaHard [CZS+24], and IFEval [ZLM+23], for which\nwe use an internal framework and prompting and extraction. Finally, we use PhiBench, our internal\ncollection of evaluations (see Section 5).\nphi-4 outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12\nbenchmarks. While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers\nfor SimpleQA, DROP, and IFEval, we consider phi-4’s behavior on SimpleQA to actually be better\nthan Qwen’s. In fact, our base model gets a higher benchmark score than Qwen-2.5-14B-Instruct on\nSimpleQA, and we intentionally modified the model’s behavior in post-training to optimize for a better\nuser experience rather than a higher benchmark score. See Figure 6 and Appendix A.1 for details.\nOur model excels at STEM Q&A tasks. For example, on GPQA (graduate-level STEM questions)\nand MATH (math competitions), it even outscores its teacher model, GPT-4o. It also scores higher at\ncoding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark\nagainst, including much larger Llama models.\nphi-4’s weakest benchmark scores are on SimpleQA, DROP, and IFEval. We believe for the first\ntwo that the number reported by simple-evals is reductive and does not accurately reflect model\nperformance on the benchmark problems. However, IFEval reveals a real weakness of our model – it\nhas trouble strictly following instructions. While strict instruction following was not an emphasis of our\nsynthetic data generations for this model, we are confident that phi-4’s instruction-following performance\ncould be significantly improved with targeted synthetic data.\n7\nSafety\nWe developed phi-4 in accordance with Microsoft’s Responsible AI principles. Our overall approach to\nRAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations\nacross dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets\n18\n\nphi-3\n(3B-4K)\nphi-3\n(7B-8K)\nphi-3\n(14B-4K)\nMistral\n(7B-v0.1)\nMistral\n(7B-v0.2)\nLlama-3\n(8B)\nGemma\n(7B)\nphi-4\nGrounding\n4.469\n4.701\n4.787\n4.065\n4.692\n4.672\n4.32\n4.619\n3P Content Harms\n(DR1)\nBooks, News, Recipes, Songs\n0.26\n0.253\n0.251\n0.562\n0.399\n0.373\n0.383\n0.121\nHarmful Content\nContinuation (DR3)\nHate/Fairness, Self-Harm, Sexual, Violence\n0.007\n0.003\n0.01\n0.026\n0.018\n0.013\n0.013\n0.036\nHarmful Content\nSummarization (DR3)\nHate/Fairness, Self-Harm, Sexual, Violence\n0.105\n0.11\n0.112\n0.223\n0.16\n0.082\n0.103\n0.102\nJailbreak(DR1)\nSee text for covered topics\n0.117\n0.107\n0.111\n0.156\n0.153\n0.13\n0.114\n0.073\nTable 10: Performance comparison across models. Lower scores are better, except for “Grounding,” where a higher\nscore is better. phi-4 values are bold for readability.\n[BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets to\naddress the RAI harm categories in safety post-training.\n7.1\nRAI Benchmarks\nTable 10 shows the results of in-house RAI benchmarks [MHJ+23] for phi-4 compared to the phi-3 models\n[AAA+24], Mistral-7b-v0.1 [JSM+23], Mistral-7b-v0.2, Gemma 7b [TMH+24], and Llama-3-instruct-\n8b [AI23b].\nThis benchmark utilized GPT-4o to simulate multi-turn conversations in five different\ncategories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5\n(fully grounded), and measures if the information in a response is based on a given prompt. In other\ncategories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm)\nto 7 (severe harm) and the defect rates (DR-x) were computed as the percentage of samples with the\nseverity score being greater than or equal to x. The Jailbreak (DR1) benchmark consists of simulated\nconversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular\nconspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people.\nFor more details on the RAI prompts and evaluation framework, see [HPBP+24].\n7.2\nRed Teaming\nIn addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an inde-\npendent group tasked with identifying safety and security vulnerabilities in Microsoft’s GenAI products.\nAIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both\naverage and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior\nof phi-4 was similar to that of the phi-3 family, but identified several risky behaviors that were addressed\nby further rounds of safety post-training. In addition, the adversarial user scenario tested a wide range\nof techniques aimed at intentionally subverting the model’s safety training including jailbreaks, prompt\nencodings, and multi-turn attacks. phi-4 showed strong defenses against these techniques. AIRT also\ngenerated adversarial suffixes using the GCG algorithm [ZWC+23] on phi-3-medium, but found that\nthese suffixes did not transfer to phi-4. Further red teaming is required to identify possible risks across\na broader range of scenarios and harm categories.\n19\n\n8\nWeaknesses\nWhile phi-4 achieves similar level of language understanding and reasoning ability as much larger models,\nit is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual\nknowledge. For example, if X is a plausible human name, the model sometimes responds to prompts of the\nform “Who is X?” with a hallucinated biography of the person X. This limitation would be improved by\naugmenting the model with a search engine, but factual hallucinations cannot be eliminated completely.\nWhile phi-4 demonstrates relatively strong performance in answering questions and performing rea-\nsoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving\nspecific formatting requirements. For instance, when tasked with generating outputs in strict tabular\nformats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model\nmay produce outputs that deviate from the specified guidelines. This limitation arises in part from the\nmodel’s training focus, which prioritized synthetic datasets tailored toward Q&A and reasoning tasks\nover instruction-following scenarios.\nEven on reasoning tasks, phi-4 can make mistakes.\nFor example, when asked “which number is\nsmaller, 9.9 or 9.11?”, the model can conclude incorrectly that “9.9 is smaller than 9.11”.\nMoreover, as our data contains a lot of chain-of-thought examples, phi-4 sometimes gives long elab-\norate answers even for simple problems—this might make user interactions tedious. We also note that\nwhile phi-4 can function as a chat bot, it has been fine-tuned to maximize performance on single-turn\nqueries.\nDespite diligent RAI efforts, we acknowledge challenges around reproduction or amplification of\nbiases, inappropriate content generation, and safety issues. The use of carefully curated training data, as\nwell as targeted post-training, and improvements from red-teaming insights, have resulted in mitigating\nthese issues across all dimensions, but have not resolved the issues completely.\nAcknowledgments\nWe thank Janardhan Kulkarni and Sivakanth Gopi from Microsoft Research for the initial discussion\naround Pivotal Token Search.\nWe thank the AI Red Team (AIRT) at Microsoft, especially Blake\nBullwinkel, Bolor-Erdene Jagdagdorj, Daniel Jones, Shiven Chawla, Tori Westerhoff, and Ram Shankar\nSiva Kumar, and Olga Dutova-Fairfax from the Deployment Safety Board and the Office of Responsible\nAI at Microsoft for collaborating with us on evaluating and improving our model on vulnerabilities in\nsafety and security, which helped us adhere to the Microsoft’s RAI standards. Finally, we are grateful to\nEce Kamar, Doug Burger and Peter Lee from Microsoft Research for the support provided to the team\nduring the work on the model.\nReferences\n[AAA+24] Marah Abdin, Sam Jacobs Ade, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3\ntechnical report: A highly capable language model locally on your phone. arXiv preprint\narXiv:2404.14219, 2024.\n[AI23a] Meta AI. Introducing Meta Llama 3: The most capable openly available llm to date, 2023.\n[AI23b] Meta AI. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2023.\n20\n\n[BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Ka-\ndavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-\nDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,\nNeel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,\nChris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with\nreinforcement learning from human feedback, 2022.\n[BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R¨ottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned Llamas: Lessons from improving the safety of large\nlanguage models that follow instructions, 2024.\n[Com24] American Mathematics Competitions. American mathematics competitions problems and\nsolutions: Amc 10/12. https://www.maa.org/math-competitions, 2024. Accessed:\n2024-12-08.\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-\nating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[CZS+24] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,\nDacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica.\nChatbot arena: An open platform for evaluating llms by human preference, 2024.\n[Dee24] DeepSeek.\nDeepseek r1 lite preview.\nhttps://api-docs.deepseek.com/news/\nnews1120, 2024. Accessed: 2024-12-08.\n[GZA+23] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Gustavo de Rosa Piero Kauffmann, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, S´ebastien Bubeck, Ronen Eldan,\nAdam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv\npreprint arXiv:2306.11644, 2023.\n[HBB+20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n[HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math\ndataset. NeurIPS, 2021.\n[HPBP+24] Emman Haider,\nDaniel Perez-Becker,\nThomas Portet,\nPiyush Madan,\nAmit Garg,\nAtabak Ashfaq, David Majercak, Wen Wen, Dongwoo Kim, Ziyi Yang, et al.\nPhi-3\nsafety post-training: Aligning language models with a “break-fix”cycle.\narXiv preprint\narXiv:2407.13833, 2024.\n[JBA+23] Mojan\nJavaheripi,\nS´ebastien\nBubeck,\nMarah\nAbdin,\nJyoti\nAneja,\nCaio\nC´esar\nTeodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya\nGunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli\nSaarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Tau-\nmann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2:\nThe surprising power of small language models. Microsoft Research Blog, 2023.\n21\n\n[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension, 2017.\n[JLD+23] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang\nSun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of\nllm via a human-preference dataset, 2023.\n[JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7b, 2023.\n[LBE+23] Yuanzhi Li, S´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and\nYin Tat Lee.\nTextbooks are all you need II: phi-1.5 technical report.\narXiv preprint\narXiv:2309.05463, 2023.\n[LLL+24] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei\nShu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning\nin language models by automated process supervision, 2024.\n[LLX+24] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu\nYang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances\nllm’s reasoning capability, 2024.\n[LXWZ23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated\nby chatgpt really correct? rigorous evaluation of large language models for code generation.\narXiv preprint arXiv:2305.01210, 2023.\n[MHJ+23] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng,\nDan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun,\nEslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen. A framework\nfor automated measurement of responsible AI harms in generative AI applications, 2023.\n[Ope24a] OpenAI.\nLearning to reason with language models.\nhttps://openai.com/index/\nlearning-to-reason-with-llms/, 2024. Accessed: 2024-12-08.\n[Ope24b] OpenAI. Simple evals. https://github.com/openai/simple-evals, 2024.\n[RHS+23] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\nJulien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof\nq&a benchmark, 2023.\n[RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon,\nand Chelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[SSF+22] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.\nLanguage models are multilingual chain-of-thought reasoners, 2022.\n[Tea24] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024.\n22\n\n[TMH+24] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology, 2024.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn Advances in Neural\nInformation Processing Systems, volume 30, 2017.\n[WFM+24] Yue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen McAleer, Yonatan Bisk,\nRuslan Salakhutdinov, Yuanzhi Li, and Tom Mitchell. Agentkit: Flow engineering with\ngraphs, not coding. In COLM, 2024.\n[WKC+24] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia\nGlaese, John Schulman, and William Fedus. Measuring short-form factuality in large lan-\nguage models, 2024.\n[WLS+24] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu,\nand Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human\nannotations, 2024.\n[WMZ+24] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex\nZhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and chal-\nlenging multi-task language understanding benchmark, 2024.\n[YGH+24] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe\nWasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effec-\ntively and thoroughly, 2024.\n[ZLM+23] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan,\nDenny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv\npreprint arXiv:2311.07911, 2023.\n[ZWC+23] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson.\nUniversal and transferable adversarial attacks on aligned language models, 2023.\nA\nPost-Training Dataset Details\nA.1\nRefusal to Hallucinate\nWe created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without\nany mitigation, phi-4 would almost never admit to ignorance. For example, in response to too-difficult\nquestions like “Who is the 297th highest ranked tennis player?” the model would essentially act as an\nimprov-style “Yes, and. . . ” engine, inventing a superficially plausible answer.\nOur goal in pretraining was to pack as much information into the model as possible, that is, to teach\nmore to the model rather than to teach it its own limitations. Then in post-training, we can identify\nthe level of problem that is too difficult for the model, and teach it to generate refusals rather than\nhallucinations on those problems.\nWe started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran\nphi-4 multiple times to estimate its chance of accurately solving it. We also used GPT-4o to generate\n23\n\n(1) a correct answer, (2) a refusal to answer, (3) a bogus variant of the question that is impossible to\nsolve, and (4) a refusal to answer the bogus question.\nFor SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually\ncorrect, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus\nquestions. For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes\nanswered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly. The DPO data used\nthe first 5 tokens of the response. Example synthetic generation prompts can be found below.\nTo evaluate our progress, we can use SimpleQA [WKC+24], which is a dataset mostly comprised\nof obscure facts from Wikipedia (e.g., “How many more votes did Freeman Freeman-Thomas win than\nGeorge Sandys in the 1906 Bodmin by-election?”). Small models like phi-4 or GPT-4o-mini can only\ncorrectly answer 5-10% of them. Our performance can be found in Figure 6.\nNote that SimpleQA is included in Table 1 as part of simple-evals, and our model does not have a\ngood score. This is because simple-evals uses the F1 score, which is not a good measure of quality at\nthis accuracy scale. For example, suppose we start with a model that always guesses, but almost always\nwrongly, 6% correct and 94% incorrect. Some of the 6% correct answers will be from lucky guesses, so\npost-training to limit hallucination will have fewer correct answers, and for example, the result might be\n(3% correct, 3% incorrect, 94% refusal). In this case, a model will score worse by the F1 metric compared\nto original (5.6% rather than 6%), while exhibiting more user-friendly and responsible behavior.\nA.1.1\nSynthetic Generation Prompts\nHere, we share the main synthetic generation prompts, provided to GPT-4o, to generate post-training\ndata to decrease hallucinations.\nGeneration of bogus questions\nConsider the following trivia question:\n# Question\n{{ question }}\n# Instructions\nYour job is to turn this problem into a nonsensical one, for which the\nanswer is invalid or unlikely to be known by anyone.\nFor example, you\nmight change the name from a well-known figure to a random name, or\nchange the date from a well-known event to a random date, or the place\nto a different one.\nFor example, you might change \"When did Amelia\nEarhart cross the Atlantic Ocean?\" to \"When did Edgar Greenwood cross\nthe Atlantic Ocean?\" or \"How many times did Amelia Earhart cross the\nEnglish Channel?\".\n↪\n↪\n↪\n↪\n↪\n↪\n↪\nYour goal is that the new question is *plausibly real*, but impossible to\nanswer.\nYou should not make the question obviously fake, silly, or\nfictional; for example, all country names should be real countries,\nand no names should be obvious homages to the original question.\nIt\nshould sound like a serious trivia question.\n↪\n↪\n↪\n↪\n24\n\nYou may start with a very brief discussion, then end with two markdown\nsections:\n↪\n- The section '# Response' that contains the question.\n- The section '# Quality' that rates the generated question in quality\nfrom 1-5, with 5 being the highest quality.\n↪\nA high quality question is (1) different from the given question and\n(2) plausible\n↪\nGenerations of refusals\nConsider the following question:\n# Question\n{{ question }}\n# Instructions\nYou might well know the answer, but imagine that you were an LLM that did\nnot know the correct answer. Write a plausible response to this\nquestion that the LLM might give if it did not know the answer and\nwould like to say so rather than guess incorrectly.\n↪\n↪\n↪\nThis LLM cannot look things up.\nIt could suggest relevant information\nthat it knows; it can also just say that it does not know the answer,\nif it is unlikely to know anything useful.\n↪\n↪\nYou may start with a very brief discussion, then end with a markdown\nsection '# Response' that contains the response.\n↪\nAnswer checking\nI am grading solutions to a trivia question.\nYour job is to determine\nwhether the given submission matches the answer key.\n↪\n## Original question\n{{ question }}\n## Submission\n{{ response }}\n## Answer key\n{{ answer.value }}\n{% if answer.alternates %}\n25\n\n### Alternative Answer Forms\n{% for alt in answer.alternates %}\n{{ alt }}\n{% endfor %}\n{% endif %}\n## Instructions\nYou job is **NOT to solve the question**.\nYour job is to determine\nwhether the given submission should be graded as correct *without\nneeding a protest*.\nIt does not matter if you disagree with the\nofficial answer; you should only consider whether the submission is\nequivalent to the answer key.\n(There is a separate process for\nprotests.)\n↪\n↪\n↪\n↪\n↪\nGive a freeform analysis comparing the submission to the answer key.\nYou\nshould then output a JSON dictionary in the following form:\n↪\n```json\n{\n\"matches_key\": [Answer 'True', 'False', or 'Uncertain'],\n}\n```\nA.2\nJudge-guided DPO\nFor the second round of DPO, we generate responses from GPT-4o, GPT-4t and our model. To label\nresponses as positive or negative, we use GPT-4o as a judge and use the following prompt.\nYour task is to judge which of the following reply given by an AI\nassistant is better.\n↪\n# Conversation\n{{ chat }}\n# Replies\n{{ replies }}\n# Guideline\nProduce your output in the following JSON format (without comments and\nwith correct escape characters):\n↪\n```json\n{\n\"faults\": {\n26\n\n\"Assistant 1\": \"(string) List all the problems with the assistant 1\nreply. For each problem try to determine whether this is due to\nlack of comprehension of the relevant material, a logical\nerror, a factual error, a stylistic issue. If the answer is\nperfect, write none. If the question did not ask for a specific\nlevel of detail in the explanation, do not penalize the answer\nfor being too detailed or too concise.\",\n↪\n↪\n↪\n↪\n↪\n↪\n\"Assistant 2\": ...\n...\n},\n\"faults_discussion\": \"(string) Discuss the general strengths and\nweaknesses of each assistant. What are the main differences\nbetween the answers in terms of style, accuracy and level of\ndetail?\",\n↪\n↪\n↪\n\"accuracy\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\naccuracy?,\n↪\n...\n},\n\"style\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\nstyle?,\n↪\n...\n},\n\"detail\": {\n\"Assistant 1\": (1-5) how would you rate assistant 1 in terms of\nlevel of detail?,\n↪\n...\n}\n}\n```\nB\nData Processing\nB.1\nDecontamination\nWe decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi,\nMedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k,\nHumanEval, arena hard, ARC-Challenge, and MMLU benchmarks. We apply a hybrid n-gram algorithm\nfor decontamination which uses 13-gram and 7-gram features for removing matches to the test set, which\nis described in more detail in 1. We create a set of common 13-grams in the Wiki and train set and try\nto not remove them since these are some common phrases which are ubiquitous. Some examples include\n’a i only b ii only c iii only d ii and iii’, ’a true true b false false c true false d false true’, ’logically\nequivalent b contradictory c neither logically equivalent nor contradictory but consistent d’, ’a (ii) and\n(iv) only b (i) and (iii) only c (i) (ii)’, ’b e b a b e c c b d c e d’.\n27\n\nAlgorithm 1 Decontamination Algorithm\nRequire:\nInput training text train\nBenchmark texts tests\nAllowed 13-grams allowed 13gram\nThresholds: info 7gram threshold, contaminated 7gram threshold\nEnsure:\nA result object with contamination details\n1: procedure CheckContamination(train)\n2:\nStep 1: Extract 13-grams and Check Contamination\n3:\nfeatures 13 ←ExtractNGrams(train,13)\n4:\nfor all feature ∈features 13 do\n5:\nif feature ∈BenchmarkFeatures13 and feature ∉allowed 13gram then\n6:\nreturn Contaminated (13-gram match)\n7:\nStep 2: Extract 7-grams and Compute Overlaps\n8:\nfeatures 7 ←ExtractNGrams(train,7)\n9:\noverlap counts ←CountOverlaps(features 7,BenchmarkFeatures7)\n10:\nStep 3: Compute Overlap Ratio for Tests\n11:\nmax ratio ←0, max test ←None\n12:\nfor all test ∈BenchmarkTests do\n13:\nratio ←\noverlap counts[test]\nmin(len(features 7),len(BenchmarkFeatures7[test]))\n14:\nif ratio > max ratio then\n15:\nmax ratio ←ratio, max test ←test\n16:\nStep 4: Determine Contamination\n17:\nif max ratio > info 7gram threshold then\n18:\nif max ratio ≥contaminated 7gram threshold then\n19:\nreturn Contaminated (7-gram match)\n20:\nelse\n21:\nreturn Partial Contamination (7-gram info match)\n22:\nreturn Clean (No significant overlap)\n28\n\ncontaminated:\nTrue\nTrain:\nThere are some oarsmen in a boat.\nThe average weight is increased by 1.8 kg\nwhen one of the crew, who weighs 53 kg, is replaced by a new man who weighs 71 kg.\nHow\nmany oarsmen are there in the boat?\nTrain Dataset:\norca-math-word-problems-200k\n13gram test:\nThe average weight of 10 oarsmen in a boat is increased by 1.8 kg when\none of the crew, who weighs 53 kg is replaced by a new man.\nFind the weight of the new\nman.A. 71 B.62 C.43 D.67 E.40\nTest Dataset:\nAGIEval\n13gram segment:\none of the crew who weighs 53 kg is replaced by a new\n13gram contaminated:\nTrue\n7gram test:\nThe average weight of 10 oarsmen in a boat is increased by 1.8 kg when one\nof the crew, who weighs 53 kg is replaced by a new man.\nFind the weight of the new\nman.A. 71 B. 62 C. 43 D. 67 E. 40\n7gram overlaps:\n[’1 8 kg when one of the’, ’of the crew who weighs 53 kg’, ’the\ncrew who weighs 53 kg is’, ’increased by 1 8 kg when one’, ’crew who weighs 53 kg is\nreplaced’, ’weighs 53 kg is replaced by a’, ’8 kg when one of the crew’, ’is increased\nby 1 8 kg when’, ’kg when one of the crew who’, ’53 kg is replaced by a new’, ’when one\nof the crew who weighs’, ’by 1 8 kg when one of’, ’kg is replaced by a new man’, ’one\nof the crew who weighs 53’, ’who weighs 53 kg is replaced by’]\n7gram ratio:\n0.39473684210526316\nC\nAMC Evaluation Details\nIn this section, we fully describe our inference and grading schemes used for to obtain the November\n2024 AMC scores displayed in Figure 1. The 78 questions in these contests (4 sets of 25 questions, with\noverlaps between the 10A/12A and 10B/12B exams) were made available on or after November 6, 2024.\nAll external models we tested were published before this date, as were the datasets for all stages of phi-\n4’s training. Thus, these contests are our best attempt at conducting a completely contamination-proof\nevaluation of mathematical reasoning capabilities. We only benchmarked on this dataset after choosing\nthe hyperparameters used in post-training our final candidate models, making this dataset completely\nindependent of our final model. 8\nWe obtained the questions from the Art of Problem Solving Wiki9, and formatted them with the\nfollowing template:\nThe following question is from a 25-question, multiple choice test. Each\nquestion is followed by answers marked A, B, C, D, and E.\nOnly one of\nthese is correct.\n↪\n↪\nSCORING: You will receive 6 points for each correct answer, 1.5 points for\neach problem left unanswered, and 0 points for each incorrect answer.\n↪\nSolve the question step by step, then answer \\boxed{A}, \\boxed{B},\n\\boxed{C}, \\boxed{D}, \\boxed{E}, or \\boxed{blank}.\n↪\n# Question\n8For full disclosure, we evaluated our final three candidate models on this dataset and all three average scores exceeded\n89. We settled on our final model based on other factors, before measuring its score but after seeing the scores for the other\ntwo candidates.\n9https://artofproblemsolving.com/wiki/index.php/2024_AMC_10A (10B, 12A, 12B)\n29\n\n{{question}}\n(A) {{option_a}}\n(B) {{option_b}}\n(C) {{option_c}}\n(D) {{option_d}}\n(E) {{option_e}}\nWith each question formatted this way, we obtained 10 independent generations at temperature 0.5\nfrom each model we tested. We then followed the grading scheme described in the above prompt. We\nfound that every model we tested (including our own) frequently failed to follow the “box your final\nanswer” instruction, particularly after a long chain of thought. To stabilize the evaluations, we decided\nto count otherwise correct solutions (e.g. boxing the correct numerical expression) as correct. To do\nthis, we prompted GPT-4o to extract a final answer (A/B/C/D/E or none) from each model’s solution,\nwith temperature 1.\nD\nSynthetic generation examples\nD.1\nGeneration examples\nWe review a few examples of what our synthetic datasets look like, to give a general flavor of some of\nour techniques.\nD.1.1\nExtracting and Tagging Excerpts from Content\nTo construct a dataset focused on reasoning and complexity, we extract excerpts from sources such as\nweb pages, books, and scientific articles. Each excerpt is annotated with metadata such as complexity\nlevel, factual obscurity, and the presence of reasoning chains. These annotations help identify high-value\ncontent for synthetic data generation. Below, we provide an example of metadata we generated for\nscientific content:\nStart Line: 8, End Line: 11\nInformation Type: Empirical Result Discussion\nBrief Description: Discussion of the role of SNPs in AD development and severity.\nComplexity Level: B\nFactual Obscurity: C\nChain of Reasoning: True\nReasoning Steps:\n1. Assumption: Specific SNPs influence immune response.\nConclusion: These SNPs may affect AD severity by modulating IgE levels.\nDescription: The study links genetic variations to clinical phenotypes.\n2. Assumption: SNPs associated with AD are replicated across cohorts.\nConclusion: Replication supports their role as potential genetic markers.\nDescription: Statistical analysis confirms the robustness of the findings.\nDependencies: Previous GWAS findings, statistical methods (lines 10-15)\nThe raw content will later be filtered based on the metadata, and both will be used as seed for a variety\nof synthetic generations pipelines.\n30\n\nD.1.2\nSelf-revision\nAnother component of our synthetic data pipeline is the iterative self-revision process. This process\ninvolves multiple stages where the model generates, critiques, and revises its output to meet specific\ncriteria. Below, we give an example of this workflow, focusing on the creation of a reading comprehension\nexercise based on a scientific excerpt:\n**Excerpt:**\n\"Future studies should replicate our findings for OXTR and BDNF, but also\ninclude additional stress-related candidate genes. [...] could reverse\naberrant DNA methylation-could become an important goal in the\ndevelopment of new treatment approaches.\"\n↪\n↪\n↪\n**Revision 0:**\n*Plan:* To create an advanced reading comprehension exercise:\n- Incorporate technical terms from genetics and epigenetics.\n- Add nested counterfactuals and conditional reasoning.\n- Require logical reasoning and background knowledge.\n*Exercise:* Researchers focused on DNA methylation following psychosocial\nstress. What conclusion can be drawn from studies examining both\naversive and positive experiences, diversified cohorts, and refined\nsampling intervals?\n↪\n↪\n↪\n1. A monotonic relationship between stress and methylation is confirmed.\n2. Differential methylation patterns reveal bidirectional sensitivity\n(correct).\n↪\n3. Early-life adversities remain the primary influence.\n4. Positive experiences have negligible effects.\n**Requirements (Flavored):**\n- Exercises must require both understanding the text and external advanced\nknowledge.\n↪\n- Incorrect choices should be plausible enough to mislead without full\ncomprehension.\n↪\n- The content should challenge advanced undergraduates in genetics or\nepigenetics.\n↪\n**Critique (Agent):**\n- **External Knowledge:** Does not explicitly require concepts beyond the\nexcerpt. *Score: 1*\n↪\n- **Plausibility of Choices:** Incorrect options are plausible but not\nmisleading enough. *Score: 2*\n↪\n- **Suggestions:** Introduce external concepts like epigenetic drift or\nthe diathesis-stress model, and refine incorrect choices to address\ncommon misconceptions.\n↪\n↪\n**Revision 1:**\n31\n\n*Plan:* Add references to the hypothalamic-pituitary-adrenal (HPA) axis\nand cortisol's role in stress responses, integrating advanced\nneuroendocrinology knowledge.\n↪\n↪\n*Exercise:* Considering DNA methylation and the HPA axis's role, how could\ncortisol influence classical and non-classical epigenetic changes?\n↪\n1. Cortisol is irrelevant to the modifiers discussed.\n2. Cortisol effects are linear and align with classical models.\n3. The dynamic epigenetic role of cortisol enriches research paradigms\n(correct).\n↪\n4. Cortisol’s role is limited to downregulation of methylation.\n**Critique (Agent):**\n- **Challenge Level:** Still insufficiently difficult for advanced\nundergraduates. *Score: 1*\n↪\n- **Suggestions:** Add nuanced alternatives based on theories like\neustress vs. distress or glucocorticoid response elements.\n↪\n**Revision 2:**\n*Plan:* Refine incorrect options and add concepts like glucocorticoid\nresponse elements to deepen the challenge. Reframe exercise to compare\nclassical and non-classical pathways in epigenetics.\n↪\n↪\n---\nD.1.3\nCreating Conversations from Snippets\nAnother component of our synthetic data pipeline involves transforming factual snippets into multi-turn,\nnuanced conversations. This process is designed to produce dialogues that are contextually accurate and\nintellectually engaging, encouraging critical thinking and deeper reasoning. The process of generating\nconversations includes several iterative steps:\n1. Initial Turn: Based on the snippet, the model generates a foundational response that presents\ncore information in a clear and direct manner. This step may also include an optional curation of\na participant’s profile that guides the tone and style of dialogue throughout the generation.\n2. Subsequent Turns: The dialogue grows through a dynamic flow of agents generating alternating\nturns, directing the conversation, summarizing earlier context and injecting new scenarios and\ntopics consistent with earlier messages.\n3. Self-Revision: After each turn, the model evaluates its responses, brainstorms the best next\ncourse of action given the current context, to maximize the complexity and nuance of the discussion.\nThis includes suggestions for exploring alternative perspectives or creating connections to broader\ncontexts.\nD.2\nFill-in-the-middle exercises\nMany data generation techniques require pairs of contexts and ground-truth answers for seeding and\nrely on rejection sampling during data generation. One approach we developed creates fill-in-the-middle\n32\n\nexercises from free-form text that does not naturally translate into such pairs. This involves removing\na meaningful portion of the text (the “middle”), framing the remaining text as the context, and using\nthe removed content as the ground-truth answer. The key challenge lies in identifying portions of the\ntext where reasoning can reconstruct the removed content, ensuring a “correct” answer exists.\nFor\ninstance, we generate such pairs from public code snippets, turning them into exercises and producing\ntheir solutions (augmented with chain-of-thought tokens) via rejection sampling. We give an example of\nsuch a transcript.\n**Fill-in-the-Middle Exercise**\n**Context (Excerpt from Code Snippet):**\nfor p in [path + [n] for n in graph[node]]:\nsmalls = collections.defaultdict(lambda: 0)\nfor n in path:\nif all_c['start'] > 1 or all_c['end'] > 1:\ncontinue\nfor c in smalls.values():\nif c <= 1:\ncontinue\n<<THIS PART DELETED>>\nif not bad:\npaths.append(p)\n**Solution:**\n**Background:**\nThis snippet processes paths in a graph, tracking visits to nodes,\nparticularly \"small\" nodes identified by lowercase strings. Key\nconditions:\n↪\n↪\n- Start and end nodes cannot be revisited.\n- \"Small\" nodes may have specific limits on revisits.\n**Reasoning:**\nThe missing part handles violations of small node visit limits using two\nvariables:\n↪\n- `cused` indicates if one small node has already been revisited.\n- `bad` marks the path as invalid if a second such revisit occurs.\nThe loop iterating over `smalls.values()` likely checks whether a small\nnode is revisited for the first time (allowing it by setting `cused`\nto `True`) or violates the limit (setting `bad` to `True` and breaking\nthe loop).\n↪\n↪\n↪\n**Final Guess:**\nThe missing part is likely:\nif c > 1:\nif not cused:\ncused = True\n33\n\nelse:\nbad = True\nbreak\n**Comparison to Correct Answer:**\n- The solution captures the general logic but omits the specific condition\n`elif c == 2 and not cused`, making it broader.\n↪\n- **Correctness Discussion:** The logic is close but lacks precision, as\nthe revisits must explicitly check for exactly two visits.\n↪\n**Grading and Feedback:**\n- **Grade:** 3 (partial correctness).\n- **Hint:** Consider revisiting the specific condition for revisits. Focus\non the exact count of visits that should trigger allowances.\n↪\nD.3\nAgents\nAgent tasks that involve interactions with longer horizons may present unique challenges such as plan-\nning, reflection, and error correction. Our training data consists of trajectories collected from Agen-\ntKit [WFM+24] with detailed chain-of-thought. Specifically, we generate high-quality trajectories by\nrunning AgentKit on a wide range of environments, and rewrite the guided raw reasoning outputs from\nAgentKit into a variety of self-contained statements or thoughts that aim to capture the gist of Agen-\ntKit reasoning. Training with AgentKit data improves planning, reasoning, tool use, math, and error\ncorrection on our internal benchmark.\nIn agent tasks, phi-4 follows a characteristic chain-of-thought with explicit planning and reflections.\nWe show an example of phi-4 interacting with the tower of Hanoi below:\nAnalyzing the current situation and reflecting on past actions is crucial\nfor determining the next best step. Here's a breakdown of the thought\nprocess:\n↪\n↪\n### Analysis of Current and Past Environment\n1. **Current Configuration:**\n- Rod A: |bottom, [0], top|\n- Rod B: |bottom, [1], top|\n- Rod C: |bottom, [2], top|\n2. **Past Performance:**\n- There have been repeated unsuccessful attempts to move disks,\nparticularly from rod B to C and A to C. These failures were due to\nrule violations, such as attempting to place a larger disk on a\nsmaller one.\n↪\n↪\n↪\n3. **Environmental Patterns and Rules:**\n- Only one disk can be moved at a time.\n- A larger disk cannot be placed on a smaller disk.\n- The goal is to move all disks from rod A to rod C.\n34\n\n### Reflection on Past Performance\n- **Mistakes Identified:**\n- Repeated attempts to make illegal moves indicate a misunderstanding or\noversight of the rules.\n↪\n- The plan was not effectively followed, as seen by the repeated failed\nattempts to move disks from rod B to C and A to C.\n↪\n- **Correct Moves:**\n- Successfully moved disk 0 from C to A, which was a necessary step to\nfree rod C for subsequent moves.\n↪\n### Relevant Considerations\n- **Current Objective:**\n- The immediate goal is to move disk 1 from rod B to rod C. This move is\nlegal and aligns with the rules since disk 1 is smaller than disk 2\non rod C.\n↪\n↪\n- **Strategic Alignment:**\n- Moving disk 1 from B to C is a strategic step that aligns with the\noverall plan to eventually stack all disks on rod C in the correct\norder.\n↪\n↪\n### Additional Thoughts\n- Ensuring compliance with the rules is paramount to avoid repeated\nfailures.\n↪\n- Each move should be evaluated for legality before execution to maintain\nprogress towards the goal.\n↪\n### Updated Plan\n1. **Move Disk 1 from B to C:**\n- Completion Quantification: Disk 1 is on rod C.\n- Certainty: Known. This move is legal and necessary to progress\ntowards the goal configuration.\n↪\n2. **Move Disk 0 from A to C:**\n- Completion Quantification: Disk 0 is on rod C, on top of disk 1.\n- Certainty: Known. This move is legal once disk 1 is on rod C.\n3. **Move Disk 2 from C to B:**\n- Completion Quantification: Disk 2 is on rod B.\n- Certainty: Known. This move is necessary to free rod C for the final\nsequence.\n↪\n35\n\n### Chosen Action\nBased on the analysis and the updated plan, the next logical and legal\naction is to move disk 1 from rod B to rod C. This move adheres to the\nrules and progresses towards the goal configuration.\n↪\n↪\n<updated plan>\n1. **Move Disk 1 from B to C**:\n- Completion Quantification: Disk 1 is on rod C.\n- Certainty: Known. This move is legal and necessary to progress\ntowards the goal configuration.\n↪\n2. **Move Disk 0 from A to C**:\n- Completion Quantification: Disk 0 is on rod C, on top of disk 1.\n- Certainty: Known. This move is legal once disk 1 is on rod C.\n3. **Move Disk 2 from C to B**:\n- Completion Quantification: Disk 2 is on rod B.\n- Certainty: Known. This move is necessary to free rod C for the final\nsequence.\n↪\n</updated plan>\n<action>Move the top disk of rod B to the top of rod C</action>\n36\n",
      "fetch_method": "direct-pdf"
    }
  ]
}