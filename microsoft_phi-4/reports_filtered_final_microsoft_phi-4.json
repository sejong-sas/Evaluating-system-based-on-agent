{
  "1-1 (Weights)": "The quotes repeatedly describe Phi-4-Multimodal as an “open-sourced” model, calling it both “the first open-sourced model with speech summarization capability” and “the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size.”  They also clarify release status across the Phi family: “reasoning-enhanced Phi-4-Mini is a separate model and currently in a preview stage and will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal.”  Together, these lines indicate that (1) weights for Phi-4-Multimodal are openly released, (2) the weights enable speech-summarization and other multimodal functionality, and (3) the reasoning variant of Phi-4-Mini is not yet released at the same time, so its weights remain unavailable according to the statements provided.",
  "1-2 (Code)": "No sentence in the supplied quotes explicitly mentions the release, location, or availability of any Phi-4 training code, data-preparation scripts, or configuration files. Therefore, no information about public or private TRAINING code can be extracted from the given material.",
  "1-3 (License)": "The only licensing detail present is a single link: “https://www.bigcode-project.org/docs/pages/model-license/”.  The quotes provide no further text describing the specific license name, version, or any clauses on use, modification, redistribution, or commercial terms.",
  "1-4 (Paper)": "Multiple technical reports are explicitly cited for the Phi-4 model family:\n• “Phi-4 Technical Report” – repeatedly referenced with the statement, “We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality.”  Architectural facts included are “The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096,” and that “phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K.”  The report is hosted on arXiv as “arXiv:2412.08905, 2024,” with the author list headed by Marah Abdin and colleagues.\n• “Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs” – introduced alongside the sentence, “We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models.”\n• “Phi-4-Multimodal” is further detailed: “In this report, we introduce Phi-4-Multimodal, a unified multimodal SLM that supports multiple inference modes combining various modalities (e.g., text-only, text + image, speech/audio, speech + image) within a single model checkpoint.”\n• “Phi-4-reasoning Technical Report” – summarized in two separate lines: “We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks,” and “We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning.”  The report states it reuses “the benchmarks from the Phi-4 report [2].”\nThese quotes confirm at least four publicly described documents: the main Phi-4 technical report (arXiv link provided), the Phi-4-Mini report, the Phi-4-Multimodal report, and the Phi-4-reasoning report, each outlining different members of the Phi-4 family and their respective architectures, training approaches, or application domains.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is the first open-sourced model with speech summarization capability."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Please note that reasoning-enhanced Phi-4-Mini is a separate model and currently in a preview stage and will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the first open-sourced model with speech summarization capability."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Phi-4 Technical Report"
    },
    {
      "source": "[pdf_text]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[title]",
      "quote": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this report, we introduce Phi-4-Multimodal, a unified multimodal SLM that supports multi- ple inference modes combining various modalities (e.g., text-only, text + image, speech/audio, speech + image) within a single model checkpoint."
    },
    {
      "source": "[pdf_text]",
      "quote": "Marah Abdin, Jyoti Aneja, Harkirat Behl, S´ebastien Bubeck, Ronen Eldan, Suriya Gu-nasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning Technical Report"
    },
    {
      "source": "[pdf_text]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[pdf_text]",
      "quote": "[2] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Phi-4 Technical Report"
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs"
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models."
    },
    {
      "source": "[sections/References]",
      "quote": "[AAB+24] Marah Abdin, Jyoti Aneja, Harkirat Behl, S´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning Technical Report\n...\nWe introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "First, we use the benchmarks from the Phi-4 report [2]."
    },
    {
      "source": "[pdf_text]",
      "quote": "[2] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905."
    }
  ],
  "1-5 (Architecture)": "The available statements describe several closely related variants that all carry the “phi-4” name.  The core phi-4 base model is said to be “based on a decoder-only transformer architecture … with 14 B parameters and a default context length of 4096.”  Its architecture “closely follows phi-3-medium,” but with two main differences: it switches from the 2 K-token sliding-window attention used in phi-3-medium to “full attention over the 4 K context length,” and it later goes through “a mid-training stage where the context length is increased from the original 4 K to 16 K.”  One quote repeats that the same 14 B-parameter decoder-only design is later “extended to a 16 K context length during mid-training.”\n\nSeveral specialized descendants retain the same backbone while changing auxiliary mechanisms or operating ranges:\n•  Phi-4-reasoning keeps “the same architecture … with two key modifications,” the most important being an “Increased Token Length” in which “the RoPE base frequency was doubled, and the model was trained for a maximum length of 32 K tokens.”  A further derivative, Phi-4-reasoning-plus, was “trained with 32 K maximum length but has been tested … up to 64 K tokens,” and inference experiments set generation limits as high as 65 536 tokens without changing RoPE parameters.\n•  Phi-4-Multimodal “keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules” and uses “a novel ‘mixture of LoRAs’” to add modality-specific abilities.\n\nSmaller releases adopt the same design principles at reduced scale: “Phi-4-Mini is a 3.8-billion-parameter language model” that “consists of 32 Transformer layers with hidden state size of 3 072 and tied input / output embedding.”  Each block “includes an attention mechanism based on Group Query Attention (GQA), which optimizes key and value memory usage for long-context generation.”  All Phi-4-Mini models are “based on decoder-only Transformer and support 128 K context length based on LongRoPE.”  The authors note that, despite the small 3.8 B size, Phi-4-Mini “significantly” outperforms multiple 7 B–8 B open-source systems on math and coding tasks.\n\nAcross all lines the common themes are: (1) decoder-only transformer backbone, (2) emphasis on extending context length—from 4 K (base), to 16 K (mid-training), 32 K (reasoning), and 128 K (Mini with LongRoPE), (3) parameter counts of 14 B for the flagship and 3.8 B for the Mini, and (4) auxiliary innovations such as GQA for memory savings and mixture-of-LoRAs for multimodal ability.",
  "1-6 (Tokenizer)": "Every passage mentioning the tokenizer ties it explicitly to the phi-4 family.  For the 14 B base model, the architecture “now use[s] the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100 352 (including unused tokens).”  The smaller Phi-4-Mini line “features an expanded vocabulary size of 200 K tokens” and, in two equivalent phrasings, “All Phi-4-Mini models use the tokenizer o200k_base_tiktoken … with a vocabulary size of 200 064 intended to support multilingual and multimodal input and output more efficiently.”  Thus, phi-4 employs tiktoken throughout, but the exact vocabulary differs: ~100 K for the original 14 B model and ~200 K for the Mini variants to strengthen multilingual coverage.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "The only concrete software-stack information concerns the supervised-fine-tuning and reinforcement-learning phases for Phi-4-reasoning.  During SFT, the 14 B-parameter phi-4 backbone is trained “over roughly 16 K steps, with a global batch size of 32 and a context length of 32 K tokens.”  Optimization uses “AdamW with a learning rate of 10⁻⁵, linear warm up over 450 steps, and a weight decay of 10⁻⁴.”  After SFT, the team applies outcome-based reinforcement learning: they “utilized the Group Relative Policy Optimization (GRPO) algorithm … incorporating modifications tailored specifically to [the phi-4] setup.”  No other implementation libraries or framework versions are named in the provided text.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[pdf_text]",
      "quote": "While phi-4 achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual knowledge."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer and support 128K context length based on LongRoPE."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5. Each Transformer block includes an attention mechanism based on Group Query Attention (GQA), which optimizes key and value memory (KV cache) usage for long-context generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal 5.6B"
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications."
    },
    {
      "source": "[pdf_text]",
      "quote": "The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE [51] base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k_base_tiktoken with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer and support 128K context length based on LongRoPE."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini consists of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite having only 3.8B parameters, Phi-4-Mini reasoning-enhanced model outperforms DeepSeek-R1-Distill-Llama-8B [GYZ+25], Bespoke-Stratos-7B [Lab25], OpenThinker-7B [Tea25a], and achieves performance comparable to DeepSeek-R1-Distill-Qwen-7B as shown in the Table 9."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "• Increased Token Length: The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE [51] base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Phi-4-reasoning and Phi-4-reasoning-plus evaluations on AIME, HMMT, GPQA, and Codeforces we use 65,536 as the maximum number of tokens for generation without changing any RoPE parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "†For Phi-4 we use temp=0.8 for the reasoning benchmarks, and 0.0 for the general-purpose benchmarks."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens)"
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k_base_tiktoken with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model... Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. We use AdamW with a learning rate of 10−5, linear warm up over 450 steps, and a weight decay of 10−4."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. We use AdamW with a learning rate of 10−5, linear warm up over 450 steps, and a weight decay of 10−4."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. Specifically, we utilized the Group Relative Policy Optimization (GRPO) algorithm [48, 21], incorporating modifications tailored specifically to our setup."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "phi-4 is a decoder-only Transformer family whose training recipe revolves around data quality and large-scale synthetic corpora.  For the 14-billion-parameter base model, roughly 10 trillion tokens were consumed with linear warm-up, decay scheduling, a peak learning rate of 0.0003, constant weight-decay of 0.1, and a global batch-size of 5 760.  The default context window starts at 4 096 tokens, then a dedicated mid-training stage extends it to 16 K; later experiments probe generation up to 65 536 tokens without modifying RoPE parameters.  The final mixture assigns 30 % of tokens to web + web-rewrite data (split evenly) and 40 % to diverse synthetic data created by multi-agent prompting, self-revision workflows and instruction-reversal.  This synthetic portion is credited with the model’s tendency to avoid admitting ignorance and with its strong math, coding and reasoning skills.\n\nA smaller variant, Phi-4-Mini (3.8 B parameters), follows the same philosophy: it is trained on a 5-trillion-token corpus of high-quality web and reasoning-rich synthetic text.  It uses the o200k_base tiktoken-2 tokenizer (vocab = 200 064), supports 128 K context via LongRoPE, and comprises 32 layers with hidden size = 3 072, tied input/output embeddings, and Group Query Attention for KV-cache efficiency.  Compared with Phi-3.5-Mini, pre-training data were filtered more aggressively, the share of reasoning data was increased, and the overall corpus size and quality were raised.  Across the family, synthetic data curation “at the heart of training all Phi models” is explicitly intended to prioritize complex problem-solving.",
  "3-2 (Fine-tuning)": "phi-4 undergoes an extensive \"post-training\" (i.e., fine-tuning) phase that begins with supervised fine-tuning (SFT) and can extend to Direct Preference Optimization (DPO) while deliberately freezing or lightly adapting the base weights depending on the variant.\n\nSupervised fine-tuning / SFT:\n• A refined SFT data set is built from three patterns: (question, correct answer) when the base phi-4 is usually correct; (question, refusal) when it is usually wrong; and (bogus question, refusal) for nonsensical inputs.  This recipe directly addresses hallucinations.\n• Phi-4-reasoning is produced by SFT over 14 B-parameter phi-4 for ≈16 K steps, global batch-size 32, at 32 K-token context.  Two placeholder tokens are repurposed as <think> … </think> markers so the model can emit explicit reasoning chains.  Experiments used both phi-4 and a mid-trained \"phi-4-base\" checkpoint as starting points, with the full phi-4 giving slightly safer and better-aligned results.\n• Fine-tunes intentionally optimize single-turn QA behaviour even when that sacrifices some leaderboard scores (e.g., SimpleQA, DROP, IFEval).\n\nMultimodal and LoRA-based fine-tuning:\n• Phi-4-Multimodal keeps the language model completely frozen and adds a “mixture of LoRAs”.  Two LoRA modules are trained alongside modality-specific encoders/projectors to unlock vision-language and speech-language tasks.\n• Speech/audio adaptation follows a two-stage scheme: after language pre-training the audio encoder is frozen, while the audio projector and LoRA are updated for another 50 K steps with learning-rate 1 e-4.  Conversational SQQA data is weighted heavily in this post-training stage.\n\nReasoning-focused fine-tuning for smaller models:\n• The continued training of Phi-4-Mini proceeds through a three-stage pipeline whose final \"Roll-Out DPO\" stage (see RL summary) begins only after SFT.  Fine-tuned Phi-4-Mini models are subsequently evaluated for reasoning gains.\n\nTogether these procedures demonstrate that phi-4 variants rely on carefully curated SFT data, strategic refusal tokens, and parameter-efficient LoRA insertions to adapt the frozen backbone to instruction-following, multimodal and reasoning-heavy use-cases.",
  "3-3 (Reinforcement Learning)": "phi-4’s reinforcement-learning stack revolves around preference-based methods applied after a strong SFT checkpoint.\n\nDirect Preference Optimization (DPO):\n• Synthetic data continues to matter: rejection sampling plus a new \"pivotal token search\" (PTS) method generate preference pairs.  For each question where the base phi-4 is intermittently right, (correct > refusal) examples are logged; when it is sometimes wrong, (refusal > wrong) is captured.  Only the first five response tokens are kept in DPO pairs.\n• In Phi-4-Mini’s three-stage training, the final \"Roll-Out DPO\" step labels filtered wrong outputs as dis-preferred and their corrected versions as preferred, yielding ~300 K preference samples.\n\nOutcome-based RL / GRPO:\n• Starting from the SFT model Phi-4-reasoning, a brief round of Group Relative Policy Optimization (GRPO) — just 90 updates — lifts AIME accuracy by >10 %.  A full plot tracks behaviour across the first 125 GRPO updates.\n• Phi-4-reasoning-plus is produced by this RL stage; it incorporates longer reasoning traces and is trained on ~6 K high-quality, verifiable math problems.  The model was tuned with 32 K maximum length and empirically performs well up to 64 K tokens.\n\nOverall, reinforcement learning in the phi-4 family mixes DPO (driven by synthetic or roll-out preferences) and outcome-based GRPO to sharpen answer preference, enhance safety, and extend chain-of-thought quality without extensive additional compute.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Without any mitigation, phi-4 would almost never admit to ignorance. Our goal in pretraining was to pack as much information into the model as possible, that is, to teach more to the model rather than to teach it its own limitations."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[pdf_text]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer [VSP+17] and support 128K context length based on LongRoPE [DZZ+24a]."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5. Each Transformer block includes an attention mechanism based on Group Query Attention (GQA) [ALTdJ+23], which optimizes key and value memory (KV cache) usage for long-context generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: ... 4. Better data mixture: With the better classifiers, we re-tuned the data mixture with ablation experiments. Especially we increased the ratio for the reasoning data. With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "2. Strong Math and Reasoning capabilities: Phi-4-Mini excels on math and reasoning related benchmarks thanks to the reasoning-rich synthetic data it’s trained on."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[pdf_text]",
      "quote": "High quality data curation that integrates creatively designed synthetic generations and filtered organic data has been at the heart of training all Phi models [20, 28, 1, 2]. The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "The base model (Phi-4) originally supported a maximum token length of 16K."
    },
    {
      "source": "[pdf_text]",
      "quote": "*For Phi-4-reasoning and Phi-4-reasoning-plus evaluations on AIME, HMMT, GPQA, and Codeforces we use 65,536 as the maximum number of tokens for generation without changing any RoPE parameters. We note that neither model has properly trained on this length."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[pdf_text]",
      "quote": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without any mitigation, phi-4 would almost never admit to ignorance."
    },
    {
      "source": "[pdf_text]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen."
    },
    {
      "source": "[pdf_text]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data. Once the language model training is complete, we freeze the language model and implement our “Mixture of LoRAs” technique to proceed with the multimodal training stage. Specifically, we train two additional LoRA modules alongside modality-specific encoders and projectors to enable vision-related tasks (e.g., vision-language and vision-speech) and speech/audio-related tasks (e.g., speech-language)."
    },
    {
      "source": "[pdf_text]",
      "quote": "With the Phi-4-Mini language model, we conduct a two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training. In speech/audio post-training, the audio encoder is frozen. We update the audio projector and LoRAA with a learning rate of 1e-4 for another 50k steps."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules."
    },
    {
      "source": "[pdf_text]",
      "quote": "The results on SQQA show that Phi-4-Multimodal is more good at conversational chat rather than general knowledge and reasoning chat (less gap to closed-source models on MT-bench than that on MMMLU). The reason might be that we weighed more conversational SQQA data in the speech/audio post-training stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    },
    {
      "source": "[pdf_text]",
      "quote": "Base Model for Reasoning. We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training). Both variations performed similarly on reasoning benchmarks, while Phi-4 performed slightly better in terms of safety and alignment, as measured by the automated measurement of Responsible AI metrics for LLMs framework [37]."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[pdf_text]",
      "quote": "While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers for SimpleQA, DROP, and IFEval, we consider phi-4’s behavior on SimpleQA to actually be better than Qwen’s. In fact, our base model gets a higher benchmark score than Qwen-2.5-14B-Instruct on SimpleQA, and we intentionally modified the model’s behavior in post-training to optimize for a better user experience rather than a higher benchmark score."
    },
    {
      "source": "[pdf_text]",
      "quote": "while phi-4 can function as a chat bot, it has been fine-tuned to maximize performance on single-turn queries."
    },
    {
      "source": "[sections/Post-Training_Dataset_Details]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data. Once the language model training is complete, we freeze the language model and implement our “Mixture of LoRAs” technique to proceed with the multimodal training stage."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. For supervised fine-tuning (SFT), we utilized a combination of a text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. It exhibits competitive performance on both Golden3 and AMI test sets, compared with Gemini-2.0-Flash and GPT-4o. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We evaluate the reasoning performance of a reasoning-enhanced model that we have trained over Phi-4-Mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Starting from a strong SFT model, i.e., Phi-4-reasoning, additional GRPO training for only 90 steps boosts AIME performance by more than 10% (Figure 7a)."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[pdf_text]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "+ Roll-Out DPO (final reasoning-enhanced Phi-4-Mini)"
    },
    {
      "source": "[pdf_text]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. We specifically utilized the Group Relative Policy Optimization (GRPO) algorithm [48, 21], incorporating modifications tailored specifically to our setup."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially favorable to."
    },
    {
      "source": "[pdf_text]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially stronger."
    },
    {
      "source": "[sections/Post-Training_Dataset_Details]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly. The DPO data used the first 5 tokens of the response."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. In the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. We specifically utilized the Group Relative Policy Optimization (GRPO) algorithm [48, 21], incorporating modifications tailored specifically to our setup."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Figure 7: Behaviour of Phi-4-reasoning-plus during the first 125 GRPO updates."
    }
  ],
  "4-1 (Pre-training Data)": "Across every description, the authors stress that phi-4’s foundation is an unusually heavy reliance on synthetic tokens. Multiple sentences say explicitly that “synthetic data constitutes the bulk of the training data for phi-4” and that, unlike many LLMs, phi-4 “strategically incorporates synthetic data throughout the training process.”  The team reports creating “50 broad types of synthetic datasets … accumulating to a total of about 400 B unweighted tokens.”  These sets are produced with diverse mechanisms such as multi-agent prompting, self-revision workflows, instruction reversal and other multi-stage prompting pipelines.\n\nThe final mixture fed to the base model is spelled out numerically: 30 % web + web-rewrite, 40 % synthetic, 20 % code (a mix of raw and synthetic), and 10 % “targeted acquired sources like academic data and books.”  Separate sentences repeat the 30 % web split and emphasise that the remainder is “largely sourced from synthetic data.”\n\nCore training details also appear in the quotations.  The released phi-4 base is a 14-billion-parameter decoder-only Transformer with a 4 096-token context window.  It was “pre-trained for approximately 10 T tokens” under a linear warm-up / decay schedule, peak learning-rate = 3 × 10⁻⁴, weight-decay = 0.1 and global batch size = 5 760.\n\nVariants that still contain the target string are likewise documented.  “Phi-4-Mini” (3.8 B params) is pre-trained on “high-quality web and synthetic data,” while “Phi-4-Multimodal” is trained on a “rich … dataset encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic OCR data and synthesized chart-comprehension data,” totaling “0.5 T tokens” across vision and text.  Continued pre-training of Phi-4-Mini for reasoning adds “approximately 60 B reasoning CoT tokens generated by frontier reasoning LLMs.”  Several quotes say the whole phi family’s data curation philosophy is “to prioritize reasoning and complex problem solving” and that the curriculum was adjusted “to increase the allocation of synthetic tokens compared to older generations of phi.”\n\nFinally, the authors compare to earlier work: they state that an improved filtering pipeline and the inclusion of “Phi-4 synthetic data” let them build “the 5 trillion pre-training data corpus … larger and higher quality compared to Phi-3.5-Mini.”  Overall, the picture that emerges from the allowed sentences is a ~10-trillion-token pre-training run whose largest single component is purpose-built synthetic text specifically engineered to teach deep reasoning.",
  "4-2 (Fine-tuning Data)": "The post-training (supervised) phase for microsoft/phi-4 is described as a two-pronged SFT + DPO effort that deliberately manipulates answer / refusal behaviour.  One quote states: “For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions.”  For preference-learning, “For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly.”  The authors further say they “created new refined versions of SFT datasets” and introduced “a new technique to create DPO pairs based on pivotal-token search.”  A separate sentence notes that these steps were taken expressly “to mitigate hallucinations in simple settings,” because “without any mitigation, phi-4 would almost never admit to ignorance.”\n\nConcrete scale numbers are supplied for the specialised reasoning variant: “Phi-4-reasoning is obtained by supervised finetuning … Our SFT data comprises over 1.4 M prompt-response pairs, totaling 8.3 B unique tokens” focused on math, coding and alignment / safety.  The prompts (called Seeds) are drawn from public websites, existing datasets, licensed collections and “synthetically generated problems.”  A companion sentence emphasises that the prompts are “filtered to cover a range of difficulty levels and to lie at the boundary of the base model capabilities.”\n\nOther model flavours that still include the target token also disclose their post-training sources.  “Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function-calling and summarization data” plus “a substantial amount of instruction-following data.”  For modalities beyond text, “Phi-4-Multimodal” uses “a combination of a text SFT dataset, publicly available multimodal instruction-tuning datasets, and large-scale in-house multimodal instruction-tuning datasets.”  The same model reports that speech summarization examples make up only “1 % of the data in speech post-training,” while conversational speech Q&A was weighted more heavily.\n\nFinally, Microsoft’s internal red-team fed back into the process: “An independent red team at Microsoft iteratively examined Phi-4-Mini … Based on their feedback, we curated additional datasets tailored to address their insights.”  Altogether, the allowed sentences outline a fine-tuning stack that (i) generates targeted correct / refusal pairs, (ii) augments them with preference data for DPO, (iii) scales to millions of reasoning demonstrations, and (iv) extends to domain- or modality-specific SFT sets when training the Mini, Multimodal and Reasoning variants.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement-style preference optimization for phi-4 appears in two distinct contexts: generic DPO and outcome-based RL specialised for math.\n\nPreference optimisation (DPO):  Several sentences explain that after SFT, the authors create pairwise preference data.  In the generic recipe the data rule is restated twice: “(correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly.”  For Phi-4-Mini’s continued reasoning training, the process is trilateral: stage 1 extra CoT pre-training, stage 2 SFT, and stage 3 “Roll-Out DPO: … we label filtered incorrect outputs as ‘dis-preferred’ and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training.”  All those sentences explicitly contain the phi-4 token, satisfying the strict filter.\n\nOutcome-based RL for specialised math:  The “Phi-4-reasoning-plus” model is “further trained with Reinforcement Learning on a small set of ∼6 K high-quality math-focused problems with verifiable solutions.”  Another line says they “applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities” following earlier GRPO-style recipes, using a seed pool of “72 401 mathematical problems … from which we subsample 64 problem seeds per RL iteration.”\n\nSafety / RAI preference data:  A quote that explicitly mentions target strings states: “Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets … and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training.”\n\nIn short, the reinforcement phase for microsoft/phi-4 family models combines (i) DPO datasets built from the model’s own corrected / refused answers at a scale of 300 K pairs and (ii) a maths-focused RL loop that iterates over a ~72 K problem seed pool but actually trains on ~6 K curated instances, all performed after the SFT stage.",
  "4-4 (Data Filtering)": "Every permitted sentence stresses that high-quality filtering and decontamination are central to the phi-4 pipeline.  At the highest level, one pillar of development is explicitly named: “Curation and Filtering of High-Quality Organic Data.”  The stated objective is to “extract seeds for the synthetic data pipeline that encourage high-depth reasoning” from web content, licensed books and code.  A two-stage web-filtering procedure is described in a quote that explicitly references the target model: first identify pages with “strong educational potential,” then segment pages, “scoring each for its factual and reasoning content.”\n\nQuality classifiers and language-wide filtering:  An allowed comparison sentence notes that “better data filtering” was achieved “by using an enhanced quality classifier, … trained on a larger curated dataset consisting of cleaner positive and negative samples,” improving coverage “across multiple languages with various aspects (e.g., toxic, obscure, scientific, etc.).”  Another approved line states that rejection sampling is applied after a 60 B-token reasoning CoT pre-training stage for Phi-4-Mini “to filter out incorrect outputs.”\n\nBenchmark decontamination:  Several sentences containing the phi-4 token refer to an explicit decontamination pass.  The authors say they “improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results,” and that later variants “pass the full training data through the same rigorous decontamination process used [for] Phi-4 … decontaminating against popular reasoning as well as general-purpose benchmarks,” followed by a long benchmark list in the quote (AIME-2024, MATH, GPQA, GSM8k, etc.).  A separate sentence lists another set of benchmarks (ARC-Easy, MBPP, CommonsenseQA, WinoGrande, etc.) against which the data is also decontaminated.\n\nCurriculum and difficulty filtering:  Two sentences directly mentioning phi-4 explain that, because the base model is already strong, seed questions are “specifically target[ed] … at the edge of Phi-4’s current abilities,” and that when ground-truth is missing the team “estimate[s] seed difficulty based on the agreement rate of weaker models’ (e.g., Phi-4 or GPT-4o) generations with the proxy ground truth.”  This documents an explicit heuristic difficulty filter.\n\nSafety and content filters at inference:  One allowed sentence notes that “Like every other model, both Phi-4-Mini and Phi-4-Multimodal can sometimes output undesirable content. Mitigation strategies include (but are not limited to) system prompts, content filters, etc.”  While this refers to deployment-time filtering, it still satisfies the target-token rule and shows the end-to-end pipeline mindset.\n\nCollectively, the permitted sentences give a layered picture: (1) automated quality-classification and two-stage web passage scoring, (2) large-scale benchmark decontamination tailored to phi-4, (3) rejection-sampling on generated synthetic reasoning data, (4) seed selection heuristics based on model ability, and (5) runtime content filters to catch residual unsafe outputs.  Although no numeric thresholds (e.g., Jaccard = 0.95) appear in the allowed quotes, the authors repeatedly emphasise that these multi-stage filters were strengthened for the phi-4 generation to raise data cleanliness and evaluation integrity.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[pdf_text]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. The pre-training process involves a total of 0.5T tokens, combining both visual and textual elements."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[pdf_text]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[pdf_text]",
      "quote": "High quality data curation that integrates creatively designed synthetic generations and filtered organic data has been at the heart of training all Phi models [20, 28, 1, 2]. The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data (mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of phi."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and relevance."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens."
    },
    {
      "source": "[sections/A.1]",
      "quote": "Without any mitigation, phi-4 would almost never admit to ignorance. Our goal in pretraining was to pack as much information into the model as possible, that is, to teach more to the model rather than to teach it its own limitations."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall. 2. Better math and coding data: For the math and coding data, we have augmented our original data with a specific instruction-based math and coding data set. 3. Better synthetic data: we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination. 4. Better data mixture: With the better classifiers, we re-tuned the data mixture with ablation experiments. Especially we increased the ratio for the reasoning data."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects ... With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "High quality data curation that integrates creatively designed synthetic generations and filtered organic data has been at the heart of training all Phi models [20, 28, 1, 2]. The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without any mitigation, phi-4 would almost never admit to ignorance."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. For supervised fine-tuning (SFT), we utilized a combination of a text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed."
    },
    {
      "source": "[pdf_text]",
      "quote": "An independent red team at Microsoft iteratively examined Phi-4-Mini to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "Seeds are a set of prompts or problems which are used in both supervised fine tuning for Phi-4-reasoning and reinforcement learning for Phi-4-reasoning-plus. We begin by collecting a diverse and comprehensive dataset of questions from various web-based sources, existing datasets, and licensed collections, and are further augmented with synthetically generated problems."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/A.1]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique … Our training process comprises multiple stages, including language training (encompassing both pre-training and post-training) and then expansion of the language backbone to vision and speech/audio modalities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. It exhibits competitive performance on both Golden3 and AMI test sets, compared with Gemini-2.0-Flash and GPT-4o. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data."
    },
    {
      "source": "[pdf_text]",
      "quote": "The results on SQQA show that Phi-4-Multimodal is more good at conversational chat rather than general knowledge and reasoning chat (less gap to closed-source models on MT-bench than that on MMMLU). The reason might be that we weighed more conversational SQQA data in the speech/audio post-training stage."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. The prompts are specifically filtered to cover a range of difficulty levels and to lie at the boundary of the base model capabilities."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Seeds are a set of prompts or problems which are used in both supervised fine tuning for Phi-4-reasoning and reinforcement learning for Phi-4-reasoning-plus. Our prompts are sourced from publicly available websites, existing datasets, and licensed collections, and are further augmented with synthetically generated problems."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/A.1]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. … 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model. The seed dataset for GRPO consisted of 72,401 mathematical problems (prompts without solutions), from which we subsample 64 problem seeds per RL iteration."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[pdf_text]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[pdf_text]",
      "quote": "Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities, and we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[pdf_text]",
      "quote": "Like every other model, both Phi-4-Mini and Phi-4-Multimodal can sometimes output undesirable content. Mitigation strategies include (but are not limited to) system prompts, content filters, etc."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report. The full list of benchmarks decontaminated against is: AIME-2024, MATH, GPQA, LiveCodeBench, Codeforces, OmniMATH, SWE-Bench Verified, SimpleQA, DROP, AGIEval, ARC-Challenge, ARC-Easy, CommonsenseQA, GPQA, GSM8k, HellaSwag, HumanEval, MBPP, OpenBookQA, PIQA, WinoGrande, ArenaHard, MT-Bench, PhiBench."
    },
    {
      "source": "[pdf_text]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Recognizing that verifiable ground-truth solutions or objective notions of difficulty may not be available across all domains, we implement heuristic measures of “difficulty”. In cases where verifiable ground-truth solutions are unavailable, we use plurality responses from a strong reference model as a proxy for ground truth and then estimate seed difficulty based on the agreement rate of weaker model’s (e.g., Phi-4 or GPT-4o) generations with the (proxy) ground-truth solution."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). In addition to directly training on this text, we used various web sources as seeds for specialized synthetic data generation pipelines."
    },
    {
      "source": "[pdf_text]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: To ensure quality, we employ a two-stage filtering process: first, identifying pages with strong educational potential, and second, segmenting the selected pages into passages, scoring each for its factual and reasoning content."
    },
    {
      "source": "[sections/B.1]",
      "quote": "We decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi, MedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k, HumanEval, arena hard, ARC-Challenge, and MMLU benchmarks."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "3. Better synthetic data: we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}