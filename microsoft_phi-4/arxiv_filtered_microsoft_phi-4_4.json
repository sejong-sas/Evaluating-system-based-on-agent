{
  "4-1 (Pre-training Data)": "Phi-4 places heavy emphasis on synthetic corpora. Synthetic data constitutes the bulk of the training tokens and is produced with multi-agent prompting, self-revision workflows, instruction reversal and 50 distinct dataset-generation pipelines, together yielding roughly 400 B unweighted tokens. Nevertheless, the final mixture still reserves 30 % of tokens for organic Web material, split equally between raw‐web and web-rewrite sources. The base phi-4 model is a decoder-only Transformer that was pretrained for ~10 T tokens with linear warm-up/decay, peak learning-rate 3 × 10⁻⁴, constant weight-decay 0.1 and a global batch size of 5 760. Because the corpus contains many chain-of-thought examples, phi-4 often produces lengthy, elaborate answers.\n\nLanguage and domain coverage. Phi-4 primarily supports English; performance declines both for other languages and for non-standard English varieties. For code, most exposure is to Python with common packages, so users should verify calls written in other languages.\n\nModel variants that share this pre-training recipe:\n• Phi-4-Mini (3.8 B parameters) is trained on high-quality web plus reasoning-rich synthetic text, achieving strong math and coding results. Its training specifically increases the amount of coding data, function-calling examples and instruction-following demonstrations.\n• Building on Phi-4-Mini, an extra 60 B chain-of-thought tokens generated by frontier reasoning LLMs are added; rejection sampling filters out incorrect generations before they enter the mix.\n• Phi-4-Multimodal extends the mixture to interleaved image-text documents, image-grounding data, OCR-extracted PDFs, realistic pictures, charts and synthetic vision-speech pairs; large-scale ASR data aligns an audio encoder with Phi-4-Mini in the semantic space.\n\nAcross all versions, the guiding idea is to use “large innovative synthetic datasets” that prioritise deep reasoning and complex problem solving while complementing them with a carefully filtered slice of high-quality organic data.",
  "4-2 (Fine-tuning Data)": "Supervised fine-tuning (SFT) and preference tuning enlarge phi-4’s skills after pre-training. The team “advances the post-training recipe” by (a) creating refined SFT datasets and (b) introducing a Direct Preference Optimisation (DPO) pipeline based on a new Pivotal-Token-Search (PTS) technique.\n\nConstruction of instruction data and DPO pairs. Seed trivia questions (e.g. from TriviaQA) are run through phi-4 multiple times. For SFT, the pairs are: (question, correct answer) when phi-4 is usually right, (question, refusal) when it is usually wrong and (bogus question, refusal) for nonsensical queries. For DPO, comparisons are labelled (correct > refusal) if the model sometimes succeeds, or (refusal > wrong) when it sometimes fails.\n\nModel-specific SFT details.\n• Phi-4-Mini receives a “significantly larger and more diverse” set of function-calling, summarization and synthetic instruction-following data than its Phi-3.5 predecessor; its reasoning curriculum includes a second-stage fine-tune on ~200 K carefully chosen chain-of-thought samples.\n• Phi-4-Multimodal unlocks instruction following for audio by post-training on ~100 M weighted-up curated speech & audio SFT examples. Vision-speech fine-tuning reuses vision-language SFT data, converting user queries to audio with an in-house TTS engine. Multimodal SFT combines public and large in-house multimodal instruction-tuning datasets with a text SFT set.\n• Safety alignment. Phi-4-Mini and Phi-4-Multimodal use Helpfulness and Harmlessness preference datasets (plus in-house collections) in accordance with Microsoft Responsible AI principles.\n• Phi-4-reasoning is built by SFT of phi-4 on a “teachable” prompt set with teacher answers from o3-mini. Its corpus holds >1.4 M prompt-response pairs (8.3 B tokens) over math, coding, logical puzzles and safety; the final model trains for 16 B tokens on a weighted mixture. Responses are stored as structured “thinking”+“answer” blocks.\n\nTaken together, the fine-tuning pipeline blends high-quality, reasoning-centric, domain-diverse and safety-oriented datasets with PTS-generated DPO pairs to steer phi-4 toward more helpful, harmless and capable behaviour.",
  "4-3 (Reinforcement Learning Data)": "After SFT, phi-4 variants receive preference- and reward-based optimisation.\n\nDirect Preference Optimisation (DPO). Using the Pivotal-Token-Search method, phi-4 produces comparison data where (correct > refusal) is chosen when the base model sometimes solves a question and (refusal > wrong) when it sometimes fails. In Phi-4-Mini’s three-stage reasoning curriculum, the final “Roll-Out DPO” step labels filtered incorrect outputs as dis-preferred and their corrected versions as preferred, creating a 300 K example dataset for training.\n\nOutcome-based Reinforcement Learning (RL). A derivative called Phi-4-reasoning-plus is subjected to a short RL phase that focuses exclusively on mathematical reasoning and encourages the production of longer reasoning traces. The GRPO seed pool contains 72 401 math problems (no coding tasks); each RL iteration samples 64 seeds. The actual reward-learning set comprises ~6 K high-quality math problems with verifiable solutions. Generating extra DPO data targeting questions at the “edge” of phi-4’s competence further sharpens performance.\n\nThrough these preference and reward datasets, the already-aligned SFT model is upgraded into higher-performing versions that reason more deeply and accurately.",
  "4-4 (Data Filtering)": "Phi-4 relies on rigorous filtering and decontamination to build a clean, reasoning-focused corpus.\n\nHigh-quality organic data selection. One of the project’s pillars is the “Curation and Filtering of High-Quality Organic Data”. For phi-4 the team crawls web pages, licensed books and code repositories but keeps only a small fraction of the highest-quality documents, prioritising reasoning-dense sources such as academic papers, educational forums and programming tutorials. Selection uses lightweight non-LLM classifiers trained on ≈10⁶ LLM-generated annotations.\n\nSynthetic-data filtering. When extending Phi-4-Mini with 60 B chain-of-thought tokens, rejection sampling removes incorrect generations before they enter the mix. For vision-speech data, audio is transcribed with an in-house ASR system and only examples with acceptably low word-error-rate are retained.\n\nBenchmark decontamination. The full training corpus passes through the same rigorous decontamination pipeline used for phi-4: n-gram deduplication plus removal of overlaps with a long list of evaluation benchmarks (AIME-2024, MATH, GSM8K, ARC, HumanEval, MT-Bench, PhiBench, etc.). An internal benchmark, PhiBench, monitors remaining contamination and skill coverage. The team reports that even with these defences, re-phrased overlaps cannot be entirely ruled out.\n\nSeed-level filtering for learning stages. Because phi-4 already answers many questions correctly, seeds for further learning are chosen to sit at the edge of its current abilities. SFT experiments for Phi-4-reasoning show that “careful selection and filtering of seeds” is crucial for success.\n\nProcess improvements and scope. Compared with earlier Phi models, the decontamination procedure for phi-4 is explicitly strengthened to avoid unfair evaluation advantages, and the same processing is applied whenever Phi-4 synthetic data are reused in downstream training.\n\nIn summary, phi-4’s pipeline combines classifier-based quality filtering, rejection-sampling, WER thresholds, n-gram deduplication, benchmark exclusion lists and targeted seed selection to produce clean, high-value training data.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [...] The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/Weaknesses]",
      "quote": "Moreover, as our data contains a lot of chain-of-thought examples, phi-4 sometimes gives long elaborate answers even for simple problems—this might make user interactions tedious."
    },
    {
      "source": "[abstract]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects ... With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Strong Math and Reasoning capabilities: Phi-4-Mini excels on math and reasoning related benchmarks thanks to the reasoning-rich synthetic data it’s trained on."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[sections/Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "first, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "In the pre-training stage, we use large-scale automatic speech recognition (ASR) data to align the audio encoder and Phi-4-Mini in the semantic space."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/3.2 Vision-language training data]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "Building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4 model primarily supports English text, with performance declining for other languages and less represented English varieties compared to standard American English."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For coding, Phi-4 is mainly trained on Python using common packages, and users should manually verify API uses if scripts involve other languages or packages."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/3.1.2 Post-training data]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data, and additionally we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/3.4.2 Post-training Data]",
      "quote": "After the pre-training stage, the model can only perform the ASR task. To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training."
    },
    {
      "source": "[abstract]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/Training data]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "This stage established the final architecture and training pipeline for Phi-4-reasoning, integrating lessons from both mixture design and teacher quality into a scalable, reasoning-optimized system. On the training side, we conducted SFT over a combined data mixture spanning multiple domains—including math, code, logical puzzles, and safety & responsible AI—using weights derived from the exploration experiments (see Section 3.1). The final model was trained for 16B tokens using this mixture."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[appendix/Post-Training Dataset Details]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it. For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions. For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "With the Phi-4-Mini language model, we conduct a two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100 M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. In the second stage, the model is fine-tuned on a smaller but carefully curated dataset of around 200 K high-quality CoT samples, chosen to cover diverse domains and varying difficulty levels."
    },
    {
      "source": "[sections/3.1.2 Post-training data]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/3.2 Vision-language training data]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. For supervised fine-tuning (SFT), we utilized a combination of a text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed."
    },
    {
      "source": "[sections/3.3 Vision-speech training data]",
      "quote": "For vision-speech data, Phi-4-Multimodal model is trained on a diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. Specifically, we reuse a subset of vision-language SFT data and run in-house text-to-speech (TTS) engine to convert the user queries from texts to audios."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. The datasets used in supervised fine-tuning include topics in STEM (science, technology, engineering, and mathematics), coding, and safety-focused tasks."
    },
    {
      "source": "[pdf_text]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds. We generate both reasoning traces and final responses and combine them into a structured format consisting of “thinking” and “answer” blocks."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[abstract]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    },
    {
      "source": "[sections/Training data]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning. The seed dataset for GRPO consisted of 72,401 mathematical problems (prompts without solutions), from which we subsample 64 problem seeds per RL iteration. The seed set was curated from the larger training corpus described in Section 2. We would like to highlight that the seed data contained no coding exercises, as perhaps evident by the LiveCodeBench scores of our model."
    },
    {
      "source": "[pdf_text]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially ..."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[pdf_text]",
      "quote": "The development of phi-4 is guided by three core pillars: … 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Targeting High-quality Web Data. We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials)."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. In addition, we employ several methods for filtering organic data sources that are used both as complementary datasets in the pretraining and as seeds for generating synthetic data."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "We incorporate Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[sections/Seeds database]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[sections/Training data]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining … 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). To capture the long tail of information-rich web sources (e.g., forums, blogs, course material, domain-specific wikis), we took the approach of selecting a small fraction of highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on ∼10^6 LLM-generated annotations."
    },
    {
      "source": "[sections/Benchmarking Considerations]",
      "quote": "To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4’s development. Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora, creating a risk of data contamination. Although we took extensive measures to deduplicate and decontaminate our training data, including standard n-gram deduplication and decontamination, these methods are not effective against all scenarios, including rephrasing, which leaves some uncertainty about the true extent of generalization."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "first, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/3.3 Vision-speech training data]",
      "quote": "For vision-speech data, Phi-4-Multimodal model is trained on a diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. We also measure the quality of the synthetic speech by transcribing the audio with in-house ASR model and calculating the word error rate (WER) between original text and transcription. Our final vision-speech data is generated with the WER-based filtering to ensure the quality."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report. The full list of benchmarks decontaminated against is: AIME-2024, MATH, GPQA, LiveCodeBench, Codeforces, OmniMATH, SWE-Bench Verified, SimpleQA, DROP, AGIEval, ARC-Challenge, ARC-Easy, CommonsenseQA, GPQA, GSM8k, HellaSwag, HumanEval, MBPP, OpenBookQA, PIQA, WinoGrande, ArenaHard, MT-Bench, PhiBench."
    },
    {
      "source": "[pdf_text]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds. ... We find in our SFT experiments that even in this simple generation setting, careful selection and filtering of seeds to be crucial for the model’s success."
    }
  ]
}