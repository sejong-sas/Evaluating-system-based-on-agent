{
  "1-1 (Weights)": "The quotes explicitly referencing microsoft/phi-4 show that the weight files are publicly downloadable on Hugging Face. A sample usage snippet (“pipeline = transformers.pipeline( … model=\"microsoft/phi-4\" … )”) demonstrates that anyone can pull the checkpoint directly with the Transformers API. The repository exposes six sharded weight files named “model-00001-of-00006.safetensors” through “model-00006-of-00006.safetensors,” confirming that the full set of parameters is available for download without restriction mentioned in the text.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"microsoft/phi-4\",\n    model_kwargs={\"torch_dtype\": \"auto\"},\n    device_map=\"auto\",\n)"
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00006.safetensors\nmodel-00002-of-00006.safetensors\nmodel-00003-of-00006.safetensors\nmodel-00004-of-00006.safetensors\nmodel-00005-of-00006.safetensors\nmodel-00006-of-00006.safetensors"
    }
  ],
  "1-2 (Code)": "No quoted sentences mention any public release of training, fine-tuning, RL, or inference code for phi-4, so no information is available.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Every quoted license sentence names MIT as the governing terms for phi-4: “license: mit,” “**License** | MIT,” and the full MIT notice beginning “Permission is hereby granted, free of charge, to any person obtaining a copy… .” A LICENSE file is present in the repository with a direct link (“license_link: https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE”). The MIT License allows “use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies” of the software, subject to the standard MIT conditions.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: mit"
    },
    {
      "source": "[readme]",
      "quote": "**License**             | MIT"
    },
    {
      "source": "[license_file]",
      "quote": "MIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:"
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/phi-4/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- phi\n- nlp\n- math\n- code\n- chat\n-"
    },
    {
      "source": "[readme]",
      "quote": "|\n| **Release date** | December 12, 2024 |\n| **License** | MIT |\n\n## Intended Use \n\n| |"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE"
    }
  ],
  "1-4 (Paper)": "The only referenced publication is the “[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905),” indicating that an official technical report exists on arXiv specifically for phi-4.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)"
    }
  ],
  "1-5 (Architecture)": "The quoted material presents a concise yet data-rich snapshot of the model’s internals. It identifies the system as a “dense decoder-only Transformer model” with a total scale of “14B parameters.” Configuration lines then spell out the principal hyper-parameters: a hidden (embedding) size of 5 120 (\"hidden_size\": 5120), forty transformer blocks (\"num_hidden_layers\": 40), and forty attention heads per block (\"num_attention_heads\": 40). The feed-forward sub-layers expand to 17 920 neurons (\"intermediate_size\": 17920), while positional encoding is provisioned up to 16 384 tokens (\"max_position_embeddings\": 16384). Taken together, these figures detail depth, width, attention parallelism, and maximum context window purely from the supplied quotes.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Architecture**        | 14B parameters, dense decoder-only Transformer model"
    },
    {
      "source": "[config]",
      "quote": "\"hidden_size\": 5120,"
    },
    {
      "source": "[config]",
      "quote": "\"num_hidden_layers\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"intermediate_size\": 17920,"
    },
    {
      "source": "[config]",
      "quote": "\"max_position_embeddings\": 16384,"
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer information is given in explicit key–value pairs. Special token identifiers are fixed as \"bos_token_id\": 100 257, \"eos_token_id\": 100 265, and \"pad_token_id\": 100 349. The full lexical inventory spans 100 352 entries (\"vocab_size\": 100352). Accompanying file names—tokenizer.json, tokenizer_config.json, and vocab.json—signal that the tokenizer implementation and its configuration are available as standard JSON assets, enabling deterministic reconstruction or deployment using only these referenced files.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[config]",
      "quote": "\"bos_token_id\": 100257,"
    },
    {
      "source": "[config]",
      "quote": "\"eos_token_id\": 100265,"
    },
    {
      "source": "[config]",
      "quote": "\"pad_token_id\": 100349,"
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 100352"
    },
    {
      "source": "[files]",
      "quote": "tokenizer.json"
    },
    {
      "source": "[files]",
      "quote": "tokenizer_config.json"
    },
    {
      "source": "[files]",
      "quote": "vocab.json"
    }
  ],
  "2-1 (Hardware)": "The hardware footprint is summarized in a single but highly specific line: “**GPUs | 1920 H100-80G**.” This states that training leveraged 1 920 NVIDIA H100 GPUs, each equipped with 80 GB of memory, providing a large-scale compute cluster for model training workloads described elsewhere.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "**GPUs**                | 1920 H100-80G"
    }
  ],
  "2-2 (Software)": "The software stack is represented by one direct configuration entry: \"transformers_version\": \"4.47.0\". This pinpoints the exact release of the Hugging Face Transformers library used during the training process, allowing precise replication of the code environment referenced by the quotes.",
  "2-2 (Software)__evidence": [
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.47.0\""
    }
  ],
  "2-3 (API)": "The only explicit information about an end-user interface appears in a short code fragment that demonstrates how to invoke text generation for the microsoft/phi-4 model through the Hugging Face Transformers library. The example constructs a transformers.pipeline with the task string set to \"text-generation\" and the model argument set to the exact identifier \"microsoft/phi-4\"; it also passes model_kwargs={\"torch_dtype\": \"auto\"} so that the framework will automatically pick an appropriate PyTorch data type, and it sets device_map=\"auto\" so that hardware placement is handled for the user. From this snippet we can conclude that phi-4 is already published on (or is compatible with) the Hugging Face Hub, that it can be loaded with default text-generation heads, and that standard convenience features—automatic dtype selection and automatic multi-GPU or CPU/GPU placement—work out of the box. No additional public documentation, endpoint URL, authentication scheme, or rate-limit policy is given in the quotes, so the only concrete, quoted evidence of an API is this transformers pipeline usage example.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"microsoft/phi-4\",\n    model_kwargs={\"torch_dtype\": \"auto\"},\n    device_map=\"auto\",\n)"
    }
  ],
  "3-1 (Pre-training)": "The pre-training description of phi-4 portrays it as “a state-of-the-art open model” that was trained on a deliberately curated mixture of data sources. According to the quote, those sources include (1) synthetic datasets, (2) data extracted from filtered public-domain websites, and (3) acquired academic books and Q&A corpora. The stated objective of blending these ingredients was to ensure that even a relatively small model could learn from material that is simultaneously high in quality and rich in advanced-reasoning signals. No token counts, model-size details, or curriculum schedule are provided in the supplied quotation, but the passage underlines that the designers emphasized data quality over raw volume and targeted reasoning-oriented examples during the initial pre-training stage.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning."
    }
  ],
  "3-2 (Fine-tuning)": "For post-pre-training enhancement, the quotation indicates that phi-4 “underwent a rigorous enhancement and alignment process” that explicitly combined supervised fine-tuning with direct preference optimization (DPO). The supervised component implies that the model was further trained on instruction–response pairs in order to tighten its adherence to desired prompts, while the DPO step directly optimizes the model toward human-preferred outputs without the need for separate reward models. The stated goals of this fine-tuning phase were twofold: (1) to achieve precise instruction compliance and (2) to instill robust safety safeguards. No dataset names, epoch counts, or hyperparameter specifics are given in the available text, but the quote makes clear that fine-tuning was a deliberate, multi-stage pipeline aimed at both capability and alignment.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures"
    }
  ],
  "3-3 (Reinforcement Learning)": "The reinforcement-learning-related information is embedded in the same sentence that describes the broader alignment process: phi-4 incorporated direct preference optimization in addition to supervised fine-tuning. DPO is a form of policy-gradient-style optimization that sidesteps separately trained reward models and directly adjusts the model’s parameters to prefer preferred outputs over less-preferred ones, thereby acting as a lightweight alternative to traditional RLHF or PPO. The explicit mention of DPO reveals that the developers used at least one reinforcement-learning-based method to tune phi-4 toward human preferences. While the quote does not enumerate hyperparameters such as learning rate, KL penalties, or step counts, it does highlight the purpose of the RL step: reinforcing precise instruction adherence and safety. No other RL variants (e.g., PPO, RLHF with reward models) are referenced in the supplied material.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures"
    }
  ],
  "4-1 (Pre-training Data)": "The available statements explain that \"`phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.\"  In addition, it is noted that the project’s \"training data is an extension of the data used for Phi-3 and includes a wide variety of sources.\"  Taken together, these sentences indicate that Phi-4’s pre-training corpus inherits the breadth of the earlier Phi-3 collection while further expanding it.  Concretely, the mix covers (1) synthetic material generated for training purposes, (2) content drawn from public-domain web pages that have already passed a filtering step, and (3) licensed or otherwise acquired academic books and question-and-answer resources.  No other source types, quantities or licenses are listed beyond this blend, but the phrasing “wide variety of sources” underscores that the final corpus is intentionally diverse and larger than its Phi-3 predecessor.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets."
    },
    {
      "source": "[readme]",
      "quote": "Our training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:"
    }
  ],
  "4-2 (Fine-tuning Data)": "For supervised or instruction-style post-training, the documentation states that \"`phi-4` has adopted a robust safety post-training approach.\"  Specifically, \"the overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories.\"  From these remarks we know that the safety-oriented fine-tuning stage draws on open, publicly available corpora whose primary themes are helpful behavior, avoidance of harmful content, and coverage of numerous predefined safety categories.  The data are applied first in SFT (to teach desired behavior directly) and then reused within repeated DPO cycles that leverage preference feedback to refine the model’s alignment.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` has adopted a robust safety post-training approach. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The same passage that describes supervised fine-tuning also describes the reinforcement learning component used for alignment: \"`phi-4` has adopted a robust safety post-training approach\" where \"the overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization).\"  In this context, DPO acts as the reinforcement-learning-from-preferences phase.  The data that feed this iterative DPO loop are \"publicly available datasets focusing on helpfulness and harmlessness\" together with \"various questions and answers targeted to multiple safety categories.\"  Thus, Phi-4’s RL (preference optimization) corpus is identical in source to its supervised fine-tuning set—public, safety-oriented, helpful/harmless Q&A material—but is re-used under a preference-optimization paradigm to further align the model’s responses.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` has adopted a robust safety post-training approach. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories."
    }
  ],
  "4-4 (Data Filtering)": "The only explicit filtering information provided states that \"`phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.\"  This sentence makes clear that at least the web-sourced portion of the training corpus underwent a filtering process before inclusion.  While no numeric thresholds, classifier names, or rejection ratios are disclosed, the wording confirms that public-domain websites were actively screened so that only pre-vetted content entered the mixture alongside synthetic data and licensed textual resources.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}