{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "phi-4 pre-training is organized around a data-quality–first recipe that places synthetic, reasoning-rich corpora at its core. The released description states that synthetic data constitutes “the bulk of the training data for phi-4,” generated with multi-agent prompting, self-revision workflows, instruction reversal and other techniques; only 30 % of the final mixture is real web or “web-rewrites,” split 15 % / 15 %. The flagship model is a 14-billion-parameter decoder-only Transformer (VSP+17) with a default 4 096-token context; a dedicated mid-training stage later stretches the context to 16 K. Training runs for ≈10 trillion tokens with linear warm-up and decay schedules, peak learning-rate 3 × 10⁻⁴, constant weight-decay 0.1 and a global batch size of 5 760.  \n\nA family of smaller siblings follows identical data principles. “Phi-4-Mini” (3.8 B parameters, 32 layers, 3 072-dim hidden size, tied input/output embeddings) is trained on a five-trillion-token corpus that is larger and higher quality than that used for Phi-3.5-Mini. Its mixture intentionally over-represents curated code (chiefly Python) and high-quality, reasoning-focused text; an additional 60 billion chain-of-thought tokens are injected in the first of three continuation stages, filtered with rejection sampling to discard incorrect reasoning.  \n\nPre-training is modality-aware when needed. In a speech pipeline the team “align[s] the audio encoder and Phi-4-Mini in semantic space” using large-scale ASR data before any instruction tuning, and a multimodal variant ingests interleaved image–text documents, grounding pairs, OCR extractions and synthetic chart data while only computing next-token loss on the text portion.  \n\nThroughout the project the developers emphasise three pillars: (1) high-quality synthetic pre-training and mid-training data prioritising reasoning and complex problem-solving, (2) scalable model variants (Phi-4, Phi-4-base mid-training checkpoints, Phi-4-Mini) that share the same decoder-only architecture, and (3) systematic improvements in filtering via enhanced quality classifiers covering toxicity, obscenity, scientific jargon, etc. These choices yield base models that outperform earlier Phi-3.5 iterations, remain strongest in English, and specialise in Python-centric coding while inviting manual review for other languages or packages.",
  "3-2 (Fine-tuning)": "Fine-tuning for the phi-4 line is framed as “post-training” and combines supervised fine-tuning (SFT), direct-preference optimisation (DPO) data generation and, for some variants, parameter-efficient LoRA inserts.  \n\nSupervised fine-tuning.  • The flagship reasoning model “Phi-4-reasoning” is obtained by SFT of the 14-B-parameter base on 1.4 million prompt–response pairs (≈8.3 B unique tokens) covering math, coding and safety domains. Standard instruction SFT recipes proved inadequate, so a reasoning-specific curriculum was built: prompts are “teachable” (appropriate complexity/diversity) and answers are synthetically generated with o3-mini. Tokens unused by the base vocabulary are reused as <think> and </think> delimiters so that the model explicitly marks internal reasoning spans.  • For another project path, the “continued training of Phi-4-Mini” follows three stages: (1) 60 B CoT pre-training tokens, (2) fine-tuning on ~200 K high-quality CoT samples chosen for domain breadth and difficulty, and (3) DPO (see below).  • Throughout, all SFT data are rigorously de-contaminated using the same procedures developed for base phi-4.  \n\nData selection strategy. SFT datasets are refined by an iterative red-team loop: a Microsoft red-team probes Phi-4-Mini, surfaces deficiencies, and the authors curate additional data to patch those areas. When assembling instruction sets, three canonical pairings are used: (question, correct answer) if the base phi-4 is usually correct; (question, refusal) if the base is usually wrong; and (bogus question, refusal) to teach safe refusal.  \n\nParameter-efficient routes. The multimodal branch keeps the language backbone frozen and attaches modality-specific “mixtures of LoRAs,” thereby avoiding any degradation on pure-text tasks. A parallel speech path first performs speech/audio pre-training and then a “speech post-training” SFT stage with ~100 M weighted-up speech examples so that Phi-4-Multimodal can follow spoken instructions.  \n\nTechnique experimentation. Two bases—Phi-4 and an earlier “Phi-4-base” checkpoint—were benchmarked for reasoning-centric SFT, performing similarly, with the fully post-trained phi-4 slightly safer and better aligned according to Responsible-AI metrics.  \n\nLimitations acknowledged: the leading reasoning checkpoints are capped at 32 K context during SFT (even if later RL pushes them), and the supervised data is skewed toward STEM, code and safety, leaving other domains less covered.",
  "3-3 (Reinforcement Learning)": "phi-4 reinforcement learning focuses on outcome-based optimisation and an extensive Direct Preference Optimisation (DPO) pipeline.  \n\nDPO data generation. During post-training the team devises a “pivotal token search” (PTS) method to craft preference pairs that expose places where the model must choose between partially correct, incorrect or refusal answers. Rules include (correct > refusal) when base phi-4 sometimes answers correctly and (refusal > wrong) when it sometimes answers incorrectly. In Phi-4-Mini’s three-stage reasoning training, Stage 3 rolls out DPO: ≈300 K preference samples are built by labelling filtered incorrect outputs as “dis-preferred” and corrected counterparts as “preferred.” The authors believe PTS-targeted DPO helps phi-4 operate better in the modes where it is strongest.  \n\nOutcome-based RL on reasoning. “Phi-4-reasoning-plus” extends the SFT model with a short, high-precision reinforcement phase on ~6 000 math problems with verifiable solutions. Training uses Grouped Reinforcement Policy Optimisation (GRPO) with the following hyper-parameters: 32 × Nvidia H100 GPUs, global batch 64, group size G = 8, Adam learning-rate 5 × 10⁻⁸, cosine warm-up for the first 10 steps, KL regularisation β = 0.001 and entropy coefficient γ = 0.001. Sequences up to 32 K tokens are used for training, yet the resulting model is reported to perform well on some 64 K-token benchmarks. Merely 90 GRPO steps after a strong SFT starting point raise AIME scores by >10 %.  \n\nRejection sampling and synthetic filtering. Across the entire phi-4 post-training stack, synthetic data is repeatedly filtered by rejection sampling before being fed into RL or DPO stages, ensuring that the preference or reward signal emphasises correctness and safe behaviour.  \n\nKnown gaps. The RL corpus is presently limited to math; other domains rely solely on SFT, and the context window of 32 K tokens remains the practical limit for RL-trained checkpoints.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[abstract]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data. Notably, we include curated, high-quality code datasets to enhance performance on coding tasks."
    },
    {
      "source": "[sections/Model architecture]",
      "quote": "Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5."
    },
    {
      "source": "[sections/Data and training details]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/Data and training details]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[sections/Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[pdf_text]",
      "quote": "Base Model for Reasoning. We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training). Both variations performed similarly on reasoning benchmarks, while Phi-4 performed slightly better in terms of safety and alignment, as measured by the automated measurement of Responsible AI metrics for LLMs framework [37]."
    },
    {
      "source": "[abstract]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and relevance."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "In the pre-training stage, we use large-scale automatic speech recognition (ASR) data to align the audio encoder and Phi-4-Mini in the semantic space."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. During this phase, the model’s primary focus is on predicting the next token, concentrating solely on text tokens and disregarding any loss associated with image tokens."
    },
    {
      "source": "[sections/2504.21318 Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For example, the Phi-4 model primarily supports English text, with performance declining for other languages and less represented English varieties compared to standard American English."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For coding, Phi-4 is mainly trained on Python using common packages, and users should manually verify API uses if scripts involve other languages or packages."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen."
    },
    {
      "source": "[sections/Multimodal model architecture]",
      "quote": "We adopt the mixture of LoRAs design for our Phi-4-Multimodal architecture to support variant multi-modality use cases."
    },
    {
      "source": "[sections/Speech and Audio Training]",
      "quote": "With the Phi-4-Mini language model, we conduct a two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules. This approach ensures that language performance remains unchanged for pure text inputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "An independent red team at Microsoft iteratively examined Phi-4-Mini to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset."
    },
    {
      "source": "[abstract]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/Phi-4-reasoning]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "Base Model for Reasoning. We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training). Both variations performed similarly on reasoning benchmarks, while Phi-4 performed slightly better in terms of safety and alignment, as measured by the automated measurement of Responsible AI metrics for LLMs framework [37]."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "After the pre-training stage, the model can only perform the ASR task. To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 2) In the second stage, the model is fine-tuned on a smaller but carefully curated dataset of around 200K high-quality CoT samples, chosen to cover diverse domains and varying difficulty levels."
    },
    {
      "source": "[sections/2504.21318 Data Methodology]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Early experiments made it clear that SFT recipes used for instruction finetuning of Phi-4 do not transfer directly to reasoning-focused training."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training)."
    },
    {
      "source": "[sections/2.2 Training data]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4-reasoning model, while powerful, also has notable limitations, particularly with its context length of 32k tokens. Furthermore, the supervised fine-tuning (SFT) training data is limited to STEM, code, and safety, while the reinforcement learning (RL) data is limited to math."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[abstract]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]."
    },
    {
      "source": "[sections/4.2 Training Details and Experimental Observations]",
      "quote": "Hyper-parameters for the RL training are: a global batch size of 64 across 32 Nvidia H100 GPUs, Adam optimizer learning rate 5 × 10−8 with cosine warm-up in the first 10 steps, GRPO group size of G = 8, KL regularization of β = 0.001 and entropy coefficient of γ = 0.001. The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially effective."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[pdf_text]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially stronger."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[sections/2504.21318 Introduction]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus: A bit of RL on top of Phi-4-reasoning]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus: A bit of RL on top of Phi-4-reasoning]",
      "quote": "The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[sections/4.2 Training Details and Experimental Observations]",
      "quote": "Starting from a strong SFT model, i.e., Phi-4-reasoning, additional GRPO training for only 90 steps boosts AIME performance by more than 10% (Figure 7a)."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4-reasoning model, while powerful, also has notable limitations, particularly with its context length of 32k tokens. Furthermore, the supervised fine-tuning (SFT) training data is limited to STEM, code, and safety, while the reinforcement learning (RL) data is limited to math."
    }
  ]
}