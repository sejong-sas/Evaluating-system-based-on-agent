{
  "4-1 (Pre-training Data)": "Across every description, the authors stress that phi-4’s foundation is an unusually heavy reliance on synthetic tokens. Multiple sentences say explicitly that “synthetic data constitutes the bulk of the training data for phi-4” and that, unlike many LLMs, phi-4 “strategically incorporates synthetic data throughout the training process.”  The team reports creating “50 broad types of synthetic datasets … accumulating to a total of about 400 B unweighted tokens.”  These sets are produced with diverse mechanisms such as multi-agent prompting, self-revision workflows, instruction reversal and other multi-stage prompting pipelines.\n\nThe final mixture fed to the base model is spelled out numerically: 30 % web + web-rewrite, 40 % synthetic, 20 % code (a mix of raw and synthetic), and 10 % “targeted acquired sources like academic data and books.”  Separate sentences repeat the 30 % web split and emphasise that the remainder is “largely sourced from synthetic data.”\n\nCore training details also appear in the quotations.  The released phi-4 base is a 14-billion-parameter decoder-only Transformer with a 4 096-token context window.  It was “pre-trained for approximately 10 T tokens” under a linear warm-up / decay schedule, peak learning-rate = 3 × 10⁻⁴, weight-decay = 0.1 and global batch size = 5 760.\n\nVariants that still contain the target string are likewise documented.  “Phi-4-Mini” (3.8 B params) is pre-trained on “high-quality web and synthetic data,” while “Phi-4-Multimodal” is trained on a “rich … dataset encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic OCR data and synthesized chart-comprehension data,” totaling “0.5 T tokens” across vision and text.  Continued pre-training of Phi-4-Mini for reasoning adds “approximately 60 B reasoning CoT tokens generated by frontier reasoning LLMs.”  Several quotes say the whole phi family’s data curation philosophy is “to prioritize reasoning and complex problem solving” and that the curriculum was adjusted “to increase the allocation of synthetic tokens compared to older generations of phi.”\n\nFinally, the authors compare to earlier work: they state that an improved filtering pipeline and the inclusion of “Phi-4 synthetic data” let them build “the 5 trillion pre-training data corpus … larger and higher quality compared to Phi-3.5-Mini.”  Overall, the picture that emerges from the allowed sentences is a ~10-trillion-token pre-training run whose largest single component is purpose-built synthetic text specifically engineered to teach deep reasoning.",
  "4-2 (Fine-tuning Data)": "The post-training (supervised) phase for microsoft/phi-4 is described as a two-pronged SFT + DPO effort that deliberately manipulates answer / refusal behaviour.  One quote states: “For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions.”  For preference-learning, “For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly.”  The authors further say they “created new refined versions of SFT datasets” and introduced “a new technique to create DPO pairs based on pivotal-token search.”  A separate sentence notes that these steps were taken expressly “to mitigate hallucinations in simple settings,” because “without any mitigation, phi-4 would almost never admit to ignorance.”\n\nConcrete scale numbers are supplied for the specialised reasoning variant: “Phi-4-reasoning is obtained by supervised finetuning … Our SFT data comprises over 1.4 M prompt-response pairs, totaling 8.3 B unique tokens” focused on math, coding and alignment / safety.  The prompts (called Seeds) are drawn from public websites, existing datasets, licensed collections and “synthetically generated problems.”  A companion sentence emphasises that the prompts are “filtered to cover a range of difficulty levels and to lie at the boundary of the base model capabilities.”\n\nOther model flavours that still include the target token also disclose their post-training sources.  “Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function-calling and summarization data” plus “a substantial amount of instruction-following data.”  For modalities beyond text, “Phi-4-Multimodal” uses “a combination of a text SFT dataset, publicly available multimodal instruction-tuning datasets, and large-scale in-house multimodal instruction-tuning datasets.”  The same model reports that speech summarization examples make up only “1 % of the data in speech post-training,” while conversational speech Q&A was weighted more heavily.\n\nFinally, Microsoft’s internal red-team fed back into the process: “An independent red team at Microsoft iteratively examined Phi-4-Mini … Based on their feedback, we curated additional datasets tailored to address their insights.”  Altogether, the allowed sentences outline a fine-tuning stack that (i) generates targeted correct / refusal pairs, (ii) augments them with preference data for DPO, (iii) scales to millions of reasoning demonstrations, and (iv) extends to domain- or modality-specific SFT sets when training the Mini, Multimodal and Reasoning variants.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement-style preference optimization for phi-4 appears in two distinct contexts: generic DPO and outcome-based RL specialised for math.\n\nPreference optimisation (DPO):  Several sentences explain that after SFT, the authors create pairwise preference data.  In the generic recipe the data rule is restated twice: “(correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly.”  For Phi-4-Mini’s continued reasoning training, the process is trilateral: stage 1 extra CoT pre-training, stage 2 SFT, and stage 3 “Roll-Out DPO: … we label filtered incorrect outputs as ‘dis-preferred’ and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training.”  All those sentences explicitly contain the phi-4 token, satisfying the strict filter.\n\nOutcome-based RL for specialised math:  The “Phi-4-reasoning-plus” model is “further trained with Reinforcement Learning on a small set of ∼6 K high-quality math-focused problems with verifiable solutions.”  Another line says they “applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities” following earlier GRPO-style recipes, using a seed pool of “72 401 mathematical problems … from which we subsample 64 problem seeds per RL iteration.”\n\nSafety / RAI preference data:  A quote that explicitly mentions target strings states: “Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets … and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training.”\n\nIn short, the reinforcement phase for microsoft/phi-4 family models combines (i) DPO datasets built from the model’s own corrected / refused answers at a scale of 300 K pairs and (ii) a maths-focused RL loop that iterates over a ~72 K problem seed pool but actually trains on ~6 K curated instances, all performed after the SFT stage.",
  "4-4 (Data Filtering)": "Every permitted sentence stresses that high-quality filtering and decontamination are central to the phi-4 pipeline.  At the highest level, one pillar of development is explicitly named: “Curation and Filtering of High-Quality Organic Data.”  The stated objective is to “extract seeds for the synthetic data pipeline that encourage high-depth reasoning” from web content, licensed books and code.  A two-stage web-filtering procedure is described in a quote that explicitly references the target model: first identify pages with “strong educational potential,” then segment pages, “scoring each for its factual and reasoning content.”\n\nQuality classifiers and language-wide filtering:  An allowed comparison sentence notes that “better data filtering” was achieved “by using an enhanced quality classifier, … trained on a larger curated dataset consisting of cleaner positive and negative samples,” improving coverage “across multiple languages with various aspects (e.g., toxic, obscure, scientific, etc.).”  Another approved line states that rejection sampling is applied after a 60 B-token reasoning CoT pre-training stage for Phi-4-Mini “to filter out incorrect outputs.”\n\nBenchmark decontamination:  Several sentences containing the phi-4 token refer to an explicit decontamination pass.  The authors say they “improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results,” and that later variants “pass the full training data through the same rigorous decontamination process used [for] Phi-4 … decontaminating against popular reasoning as well as general-purpose benchmarks,” followed by a long benchmark list in the quote (AIME-2024, MATH, GPQA, GSM8k, etc.).  A separate sentence lists another set of benchmarks (ARC-Easy, MBPP, CommonsenseQA, WinoGrande, etc.) against which the data is also decontaminated.\n\nCurriculum and difficulty filtering:  Two sentences directly mentioning phi-4 explain that, because the base model is already strong, seed questions are “specifically target[ed] … at the edge of Phi-4’s current abilities,” and that when ground-truth is missing the team “estimate[s] seed difficulty based on the agreement rate of weaker models’ (e.g., Phi-4 or GPT-4o) generations with the proxy ground truth.”  This documents an explicit heuristic difficulty filter.\n\nSafety and content filters at inference:  One allowed sentence notes that “Like every other model, both Phi-4-Mini and Phi-4-Multimodal can sometimes output undesirable content. Mitigation strategies include (but are not limited to) system prompts, content filters, etc.”  While this refers to deployment-time filtering, it still satisfies the target-token rule and shows the end-to-end pipeline mindset.\n\nCollectively, the permitted sentences give a layered picture: (1) automated quality-classification and two-stage web passage scoring, (2) large-scale benchmark decontamination tailored to phi-4, (3) rejection-sampling on generated synthetic reasoning data, (4) seed selection heuristics based on model ability, and (5) runtime content filters to catch residual unsafe outputs.  Although no numeric thresholds (e.g., Jaccard = 0.95) appear in the allowed quotes, the authors repeatedly emphasise that these multi-stage filters were strengthened for the phi-4 generation to raise data cleanliness and evaluation integrity.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[pdf_text]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. The pre-training process involves a total of 0.5T tokens, combining both visual and textual elements."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[pdf_text]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[pdf_text]",
      "quote": "High quality data curation that integrates creatively designed synthetic generations and filtered organic data has been at the heart of training all Phi models [20, 28, 1, 2]. The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data (mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of phi."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and relevance."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens."
    },
    {
      "source": "[sections/A.1]",
      "quote": "Without any mitigation, phi-4 would almost never admit to ignorance. Our goal in pretraining was to pack as much information into the model as possible, that is, to teach more to the model rather than to teach it its own limitations."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall. 2. Better math and coding data: For the math and coding data, we have augmented our original data with a specific instruction-based math and coding data set. 3. Better synthetic data: we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination. 4. Better data mixture: With the better classifiers, we re-tuned the data mixture with ablation experiments. Especially we increased the ratio for the reasoning data."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects ... With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "High quality data curation that integrates creatively designed synthetic generations and filtered organic data has been at the heart of training all Phi models [20, 28, 1, 2]. The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without any mitigation, phi-4 would almost never admit to ignorance."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. For supervised fine-tuning (SFT), we utilized a combination of a text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed."
    },
    {
      "source": "[pdf_text]",
      "quote": "An independent red team at Microsoft iteratively examined Phi-4-Mini to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "Seeds are a set of prompts or problems which are used in both supervised fine tuning for Phi-4-reasoning and reinforcement learning for Phi-4-reasoning-plus. We begin by collecting a diverse and comprehensive dataset of questions from various web-based sources, existing datasets, and licensed collections, and are further augmented with synthetically generated problems."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/A.1]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique … Our training process comprises multiple stages, including language training (encompassing both pre-training and post-training) and then expansion of the language backbone to vision and speech/audio modalities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. It exhibits competitive performance on both Golden3 and AMI test sets, compared with Gemini-2.0-Flash and GPT-4o. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data."
    },
    {
      "source": "[pdf_text]",
      "quote": "The results on SQQA show that Phi-4-Multimodal is more good at conversational chat rather than general knowledge and reasoning chat (less gap to closed-source models on MT-bench than that on MMMLU). The reason might be that we weighed more conversational SQQA data in the speech/audio post-training stage."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. The prompts are specifically filtered to cover a range of difficulty levels and to lie at the boundary of the base model capabilities."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Seeds are a set of prompts or problems which are used in both supervised fine tuning for Phi-4-reasoning and reinforcement learning for Phi-4-reasoning-plus. Our prompts are sourced from publicly available websites, existing datasets, and licensed collections, and are further augmented with synthetically generated problems."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Post-Training Dataset Details]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/A.1]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. … 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model. The seed dataset for GRPO consisted of 72,401 mathematical problems (prompts without solutions), from which we subsample 64 problem seeds per RL iteration."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[pdf_text]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[pdf_text]",
      "quote": "Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities, and we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[pdf_text]",
      "quote": "Like every other model, both Phi-4-Mini and Phi-4-Multimodal can sometimes output undesirable content. Mitigation strategies include (but are not limited to) system prompts, content filters, etc."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report. The full list of benchmarks decontaminated against is: AIME-2024, MATH, GPQA, LiveCodeBench, Codeforces, OmniMATH, SWE-Bench Verified, SimpleQA, DROP, AGIEval, ARC-Challenge, ARC-Easy, CommonsenseQA, GPQA, GSM8k, HellaSwag, HumanEval, MBPP, OpenBookQA, PIQA, WinoGrande, ArenaHard, MT-Bench, PhiBench."
    },
    {
      "source": "[pdf_text]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Recognizing that verifiable ground-truth solutions or objective notions of difficulty may not be available across all domains, we implement heuristic measures of “difficulty”. In cases where verifiable ground-truth solutions are unavailable, we use plurality responses from a strong reference model as a proxy for ground truth and then estimate seed difficulty based on the agreement rate of weaker model’s (e.g., Phi-4 or GPT-4o) generations with the (proxy) ground-truth solution."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). In addition to directly training on this text, we used various web sources as seeds for specialized synthetic data generation pipelines."
    },
    {
      "source": "[pdf_text]",
      "quote": "Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: To ensure quality, we employ a two-stage filtering process: first, identifying pages with strong educational potential, and second, segmenting the selected pages into passages, scoring each for its factual and reasoning content."
    },
    {
      "source": "[sections/B.1]",
      "quote": "We decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi, MedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k, HumanEval, arena hard, ARC-Challenge, and MMLU benchmarks."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "3. Better synthetic data: we incorporated Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    }
  ]
}