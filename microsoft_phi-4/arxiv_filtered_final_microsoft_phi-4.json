{
  "1-1 (Weights)": "The available quotes state that Phi-4-Multimodal is explicitly described as \"the first open-sourced model with speech summarization capability\" and as \"the smallest open-sourced multi-modal LLM\"—wording that indicates its checkpoint is publicly released. The material also confirms that an \"officially released checkpoint\" exists for Phi-4-Mini, whereas the \"reasoning-enhanced Phi-4-Mini\" is \"a separate model … in a preview stage\" that \"will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal.\" Finally, the data used for an experimental reasoning model \"has not been applied to the officially released checkpoint Phi-4-Mini,\" underscoring that the released Phi-4-Mini weights do not incorporate that additional dataset.",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "Multiple official technical reports and papers are cited for the Phi-4 family. The \"Phi-4 Technical Report\" repeatedly states, \"We present phi-4, a 14-billion parameter language model\" trained with a curriculum focused on high-quality data. Complementing this, another sentence notes that Phi-4 \"further advances performance … by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training.\" \n\nFollow-on work introduces specialized variants: \"Phi-4-Mini and Phi-4-Multimodal\" are described as \"compact yet highly capable\" models, with the \"Phi-4-Mini Technical Report\" subtitled \"Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs.\"  A separate \"Phi-4-reasoning Technical Report\" presents \"Phi-4-reasoning, a 14-billion parameter reasoning model … supervised fine-tuned on Phi-4 [2]\" and also details \"Phi-4-reasoning-plus\" obtained through an additional reinforcement-learning stage.  The authors note that they \"use the benchmarks from the Phi-4 report [2]\" to evaluate these derivatives.  Collectively, the quoted material emphasizes that the Phi-4 series leverages carefully curated or synthesized data—\"the Phi family of models … have shown that carefully curated and synthesized data enables Small Language Models (SLMs) to achieve highly competitive performance\"—and positions Phi-4 and its descendants as state-of-the-art 14-billion-parameter small language models aimed at strong reasoning and multimodal capabilities.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "1 Please note that reasoning-enhanced Phi-4-Mini is a separate model and currently in a preview stage and will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the first open-sourced model with speech summarization capability."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "This data has been utilized exclusively for the experimental reasoning model and has not been applied to the officially released checkpoint Phi-4-Mini."
    },
    {
      "source": "[sections/Speech and Audio Benchmarks]",
      "quote": "Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The Phi family of models [AJA+24, AAB+24] have shown that carefully curated and synthesized data enables Small Language Models (SLMs) to achieve highly competitive performance despite having a significantly smaller number of parameters."
    },
    {
      "source": "[title]",
      "quote": "Phi-4-reasoning Technical Report"
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[sections/General-purpose Benchmarks]",
      "quote": "First, we use the benchmarks from the Phi-4 report [2]."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Phi-4 Technical Report ... We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[title]",
      "quote": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs"
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "First, we use the benchmarks from the Phi-4 report [2]."
    }
  ],
  "1-5 (Architecture)": "The phi-4 family is built around a decoder-only Transformer design with 14 billion parameters and an initial context window of 4 096 tokens, which is later stretched to 16 K tokens during a dedicated mid-training phase.  Phi-4-Mini and Phi-4-Multimodal inherit the same language-model backbone: Phi-4-Mini packs 3.8 billion parameters into 32 Transformer layers with a 3 072-dimensional hidden size.  It ties input and output embeddings to cut memory use, and adopts 24 query heads alongside 8 key/value heads so the KV cache shrinks to roughly one-third of the standard footprint—yet the wider vocabulary coverage surpasses Phi-3.5.  For multimodal usage, Phi-4-Multimodal preserves the entire language model in a frozen state and adds only lightweight LoRA adapters, so pure-text performance stays intact.  A reasoning-oriented branch, Phi-4-reasoning, keeps the same core architecture but introduces two structural changes: (1) two placeholder tokens are reassigned as <think> … </think> delimiters to bracket explicit reasoning chains, and (2) the RoPE base frequency is doubled so the maximum sequence length can be trained out to 32 K tokens (double the 16 K limit of the base Phi-4).  Collectively, these variants demonstrate that the underlying Phi-4 architecture can scale from a memory-lean 3.8 B-parameter Mini model up through full 14 B-parameter versions with extended context and specialized reasoning hooks.",
  "1-6 (Tokenizer)": "Across the suite, phi-4 models migrate to the tiktoken family of tokenizers.  The base phi-4 swaps the phi-3-medium tokenizer for tiktoken, padded to a 100 352-token vocabulary and paired with full attention over the 4 K context (abandoning the earlier 2 K sliding window).  All Phi-4-Mini checkpoints move further to an o200k-base tiktoken-2 tokenizer whose 200 064-token vocabulary is designed for stronger multilingual and multimodal coverage; compared with Phi-3.5-Mini the vocabulary expansion reaches roughly 200 K tokens.  Within Phi-4-reasoning, two tokens already present in the tokenizer are repurposed as the special <think> and </think> markers that wrap internal reasoning blocks.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "Phi-4-reasoning is trained via supervised fine-tuning on the 14 B-parameter Phi-4 base.  The run spans about 16 000 steps with a global batch size of 32 examples and a 32 K-token sequence length.  Optimization uses AdamW at a 1×10⁻⁵ learning rate, features linear warm-up over the first 450 steps, and applies a weight decay of 1×10⁻⁴.  The supervised stage relies on a carefully curated set of “teachable” prompts and reasoning demonstrations, while an additional, shorter outcome-based reinforcement-learning pass (yielding Phi-4-reasoning-plus) further boosts quality by encouraging longer reasoning traces.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[sections/Model architecture]",
      "quote": "Phi-4-Mini and Phi-4-Multimodal share the same language model backbone. Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite having only 3.8B parameters, Phi-4-Mini reasoning-enhanced model outperforms DeepSeek-R1-Distill-Llama-8B [GYZ+25], Bespoke-Stratos-7B [Lab25], OpenThinker-7B [Tea25a], and achieves performance comparable to DeepSeek-R1-Distill-Qwen-7B as shown in the Table 9."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Increased Token Length: The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini consists of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5. Specifically, the model employs 24 query heads and 8 key/value heads, reducing KV cache consumption to one-third of its standard size."
    },
    {
      "source": "[sections/Multimodal Benchmarks]",
      "quote": "Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules. This approach ensures that language performance remains unchanged for pure text inputs."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    },
    {
      "source": "[pdf_text]",
      "quote": "The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE [51] base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/Model architecture]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. We use AdamW with a learning rate of 10−5, linear warm up over 450 steps, and a weight decay of 10−4."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "phi-4 pre-training is organized around a data-quality–first recipe that places synthetic, reasoning-rich corpora at its core. The released description states that synthetic data constitutes “the bulk of the training data for phi-4,” generated with multi-agent prompting, self-revision workflows, instruction reversal and other techniques; only 30 % of the final mixture is real web or “web-rewrites,” split 15 % / 15 %. The flagship model is a 14-billion-parameter decoder-only Transformer (VSP+17) with a default 4 096-token context; a dedicated mid-training stage later stretches the context to 16 K. Training runs for ≈10 trillion tokens with linear warm-up and decay schedules, peak learning-rate 3 × 10⁻⁴, constant weight-decay 0.1 and a global batch size of 5 760.  \n\nA family of smaller siblings follows identical data principles. “Phi-4-Mini” (3.8 B parameters, 32 layers, 3 072-dim hidden size, tied input/output embeddings) is trained on a five-trillion-token corpus that is larger and higher quality than that used for Phi-3.5-Mini. Its mixture intentionally over-represents curated code (chiefly Python) and high-quality, reasoning-focused text; an additional 60 billion chain-of-thought tokens are injected in the first of three continuation stages, filtered with rejection sampling to discard incorrect reasoning.  \n\nPre-training is modality-aware when needed. In a speech pipeline the team “align[s] the audio encoder and Phi-4-Mini in semantic space” using large-scale ASR data before any instruction tuning, and a multimodal variant ingests interleaved image–text documents, grounding pairs, OCR extractions and synthetic chart data while only computing next-token loss on the text portion.  \n\nThroughout the project the developers emphasise three pillars: (1) high-quality synthetic pre-training and mid-training data prioritising reasoning and complex problem-solving, (2) scalable model variants (Phi-4, Phi-4-base mid-training checkpoints, Phi-4-Mini) that share the same decoder-only architecture, and (3) systematic improvements in filtering via enhanced quality classifiers covering toxicity, obscenity, scientific jargon, etc. These choices yield base models that outperform earlier Phi-3.5 iterations, remain strongest in English, and specialise in Python-centric coding while inviting manual review for other languages or packages.",
  "3-2 (Fine-tuning)": "Fine-tuning for the phi-4 line is framed as “post-training” and combines supervised fine-tuning (SFT), direct-preference optimisation (DPO) data generation and, for some variants, parameter-efficient LoRA inserts.  \n\nSupervised fine-tuning.  • The flagship reasoning model “Phi-4-reasoning” is obtained by SFT of the 14-B-parameter base on 1.4 million prompt–response pairs (≈8.3 B unique tokens) covering math, coding and safety domains. Standard instruction SFT recipes proved inadequate, so a reasoning-specific curriculum was built: prompts are “teachable” (appropriate complexity/diversity) and answers are synthetically generated with o3-mini. Tokens unused by the base vocabulary are reused as <think> and </think> delimiters so that the model explicitly marks internal reasoning spans.  • For another project path, the “continued training of Phi-4-Mini” follows three stages: (1) 60 B CoT pre-training tokens, (2) fine-tuning on ~200 K high-quality CoT samples chosen for domain breadth and difficulty, and (3) DPO (see below).  • Throughout, all SFT data are rigorously de-contaminated using the same procedures developed for base phi-4.  \n\nData selection strategy. SFT datasets are refined by an iterative red-team loop: a Microsoft red-team probes Phi-4-Mini, surfaces deficiencies, and the authors curate additional data to patch those areas. When assembling instruction sets, three canonical pairings are used: (question, correct answer) if the base phi-4 is usually correct; (question, refusal) if the base is usually wrong; and (bogus question, refusal) to teach safe refusal.  \n\nParameter-efficient routes. The multimodal branch keeps the language backbone frozen and attaches modality-specific “mixtures of LoRAs,” thereby avoiding any degradation on pure-text tasks. A parallel speech path first performs speech/audio pre-training and then a “speech post-training” SFT stage with ~100 M weighted-up speech examples so that Phi-4-Multimodal can follow spoken instructions.  \n\nTechnique experimentation. Two bases—Phi-4 and an earlier “Phi-4-base” checkpoint—were benchmarked for reasoning-centric SFT, performing similarly, with the fully post-trained phi-4 slightly safer and better aligned according to Responsible-AI metrics.  \n\nLimitations acknowledged: the leading reasoning checkpoints are capped at 32 K context during SFT (even if later RL pushes them), and the supervised data is skewed toward STEM, code and safety, leaving other domains less covered.",
  "3-3 (Reinforcement Learning)": "phi-4 reinforcement learning focuses on outcome-based optimisation and an extensive Direct Preference Optimisation (DPO) pipeline.  \n\nDPO data generation. During post-training the team devises a “pivotal token search” (PTS) method to craft preference pairs that expose places where the model must choose between partially correct, incorrect or refusal answers. Rules include (correct > refusal) when base phi-4 sometimes answers correctly and (refusal > wrong) when it sometimes answers incorrectly. In Phi-4-Mini’s three-stage reasoning training, Stage 3 rolls out DPO: ≈300 K preference samples are built by labelling filtered incorrect outputs as “dis-preferred” and corrected counterparts as “preferred.” The authors believe PTS-targeted DPO helps phi-4 operate better in the modes where it is strongest.  \n\nOutcome-based RL on reasoning. “Phi-4-reasoning-plus” extends the SFT model with a short, high-precision reinforcement phase on ~6 000 math problems with verifiable solutions. Training uses Grouped Reinforcement Policy Optimisation (GRPO) with the following hyper-parameters: 32 × Nvidia H100 GPUs, global batch 64, group size G = 8, Adam learning-rate 5 × 10⁻⁸, cosine warm-up for the first 10 steps, KL regularisation β = 0.001 and entropy coefficient γ = 0.001. Sequences up to 32 K tokens are used for training, yet the resulting model is reported to perform well on some 64 K-token benchmarks. Merely 90 GRPO steps after a strong SFT starting point raise AIME scores by >10 %.  \n\nRejection sampling and synthetic filtering. Across the entire phi-4 post-training stack, synthetic data is repeatedly filtered by rejection sampling before being fed into RL or DPO stages, ensuring that the preference or reward signal emphasises correctness and safe behaviour.  \n\nKnown gaps. The RL corpus is presently limited to math; other domains rely solely on SFT, and the context window of 32 K tokens remains the practical limit for RL-trained checkpoints.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[abstract]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data. Notably, we include curated, high-quality code datasets to enhance performance on coding tasks."
    },
    {
      "source": "[sections/Model architecture]",
      "quote": "Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5."
    },
    {
      "source": "[sections/Data and training details]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/Data and training details]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[sections/Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[pdf_text]",
      "quote": "Base Model for Reasoning. We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training). Both variations performed similarly on reasoning benchmarks, while Phi-4 performed slightly better in terms of safety and alignment, as measured by the automated measurement of Responsible AI metrics for LLMs framework [37]."
    },
    {
      "source": "[abstract]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and relevance."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "In the pre-training stage, we use large-scale automatic speech recognition (ASR) data to align the audio encoder and Phi-4-Mini in the semantic space."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 1) First, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. During this phase, the model’s primary focus is on predicting the next token, concentrating solely on text tokens and disregarding any loss associated with image tokens."
    },
    {
      "source": "[sections/2504.21318 Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For example, the Phi-4 model primarily supports English text, with performance declining for other languages and less represented English varieties compared to standard American English."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For coding, Phi-4 is mainly trained on Python using common packages, and users should manually verify API uses if scripts involve other languages or packages."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen."
    },
    {
      "source": "[sections/Multimodal model architecture]",
      "quote": "We adopt the mixture of LoRAs design for our Phi-4-Multimodal architecture to support variant multi-modality use cases."
    },
    {
      "source": "[sections/Speech and Audio Training]",
      "quote": "With the Phi-4-Mini language model, we conduct a two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Unlike other open-source vision-language models that fully fine-tune their base language models (often resulting in performance degradation on pure language benchmarks), Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules. This approach ensures that language performance remains unchanged for pure text inputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "An independent red team at Microsoft iteratively examined Phi-4-Mini to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset."
    },
    {
      "source": "[abstract]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/Phi-4-reasoning]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "Base Model for Reasoning. We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training). Both variations performed similarly on reasoning benchmarks, while Phi-4 performed slightly better in terms of safety and alignment, as measured by the automated measurement of Responsible AI metrics for LLMs framework [37]."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "After the pre-training stage, the model can only perform the ASR task. To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 2) In the second stage, the model is fine-tuned on a smaller but carefully curated dataset of around 200K high-quality CoT samples, chosen to cover diverse domains and varying difficulty levels."
    },
    {
      "source": "[sections/2504.21318 Data Methodology]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications. • Reasoning tokens: Two placeholder tokens from the base model were repurposed as <think> and </think> tokens to mark the beginning and end of a reasoning (“thinking”) block, respectively."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "Early experiments made it clear that SFT recipes used for instruction finetuning of Phi-4 do not transfer directly to reasoning-focused training."
    },
    {
      "source": "[sections/3 Phi-4-reasoning: Supervised Finetuning of Phi-4]",
      "quote": "We experimented with two base models for reasoning-focused SFT: Phi-4 and Phi-4-base (mid-trained checkpoint before vanilla post-training)."
    },
    {
      "source": "[sections/2.2 Training data]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4-reasoning model, while powerful, also has notable limitations, particularly with its context length of 32k tokens. Furthermore, the supervised fine-tuning (SFT) training data is limited to STEM, code, and safety, while the reinforcement learning (RL) data is limited to math."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[abstract]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]."
    },
    {
      "source": "[sections/4.2 Training Details and Experimental Observations]",
      "quote": "Hyper-parameters for the RL training are: a global batch size of 64 across 32 Nvidia H100 GPUs, Adam optimizer learning rate 5 × 10−8 with cosine warm-up in the first 10 steps, GRPO group size of G = 8, KL regularization of β = 0.001 and entropy coefficient of γ = 0.001. The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[abstract]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially effective."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[pdf_text]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially stronger."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[pdf_text]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[sections/2504.21318 Introduction]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus: A bit of RL on top of Phi-4-reasoning]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]."
    },
    {
      "source": "[sections/4 Phi-4-reasoning-plus: A bit of RL on top of Phi-4-reasoning]",
      "quote": "The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[sections/4.2 Training Details and Experimental Observations]",
      "quote": "Starting from a strong SFT model, i.e., Phi-4-reasoning, additional GRPO training for only 90 steps boosts AIME performance by more than 10% (Figure 7a)."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4-reasoning model, while powerful, also has notable limitations, particularly with its context length of 32k tokens. Furthermore, the supervised fine-tuning (SFT) training data is limited to STEM, code, and safety, while the reinforcement learning (RL) data is limited to math."
    }
  ],
  "4-1 (Pre-training Data)": "Phi-4 places heavy emphasis on synthetic corpora. Synthetic data constitutes the bulk of the training tokens and is produced with multi-agent prompting, self-revision workflows, instruction reversal and 50 distinct dataset-generation pipelines, together yielding roughly 400 B unweighted tokens. Nevertheless, the final mixture still reserves 30 % of tokens for organic Web material, split equally between raw‐web and web-rewrite sources. The base phi-4 model is a decoder-only Transformer that was pretrained for ~10 T tokens with linear warm-up/decay, peak learning-rate 3 × 10⁻⁴, constant weight-decay 0.1 and a global batch size of 5 760. Because the corpus contains many chain-of-thought examples, phi-4 often produces lengthy, elaborate answers.\n\nLanguage and domain coverage. Phi-4 primarily supports English; performance declines both for other languages and for non-standard English varieties. For code, most exposure is to Python with common packages, so users should verify calls written in other languages.\n\nModel variants that share this pre-training recipe:\n• Phi-4-Mini (3.8 B parameters) is trained on high-quality web plus reasoning-rich synthetic text, achieving strong math and coding results. Its training specifically increases the amount of coding data, function-calling examples and instruction-following demonstrations.\n• Building on Phi-4-Mini, an extra 60 B chain-of-thought tokens generated by frontier reasoning LLMs are added; rejection sampling filters out incorrect generations before they enter the mix.\n• Phi-4-Multimodal extends the mixture to interleaved image-text documents, image-grounding data, OCR-extracted PDFs, realistic pictures, charts and synthetic vision-speech pairs; large-scale ASR data aligns an audio encoder with Phi-4-Mini in the semantic space.\n\nAcross all versions, the guiding idea is to use “large innovative synthetic datasets” that prioritise deep reasoning and complex problem solving while complementing them with a carefully filtered slice of high-quality organic data.",
  "4-2 (Fine-tuning Data)": "Supervised fine-tuning (SFT) and preference tuning enlarge phi-4’s skills after pre-training. The team “advances the post-training recipe” by (a) creating refined SFT datasets and (b) introducing a Direct Preference Optimisation (DPO) pipeline based on a new Pivotal-Token-Search (PTS) technique.\n\nConstruction of instruction data and DPO pairs. Seed trivia questions (e.g. from TriviaQA) are run through phi-4 multiple times. For SFT, the pairs are: (question, correct answer) when phi-4 is usually right, (question, refusal) when it is usually wrong and (bogus question, refusal) for nonsensical queries. For DPO, comparisons are labelled (correct > refusal) if the model sometimes succeeds, or (refusal > wrong) when it sometimes fails.\n\nModel-specific SFT details.\n• Phi-4-Mini receives a “significantly larger and more diverse” set of function-calling, summarization and synthetic instruction-following data than its Phi-3.5 predecessor; its reasoning curriculum includes a second-stage fine-tune on ~200 K carefully chosen chain-of-thought samples.\n• Phi-4-Multimodal unlocks instruction following for audio by post-training on ~100 M weighted-up curated speech & audio SFT examples. Vision-speech fine-tuning reuses vision-language SFT data, converting user queries to audio with an in-house TTS engine. Multimodal SFT combines public and large in-house multimodal instruction-tuning datasets with a text SFT set.\n• Safety alignment. Phi-4-Mini and Phi-4-Multimodal use Helpfulness and Harmlessness preference datasets (plus in-house collections) in accordance with Microsoft Responsible AI principles.\n• Phi-4-reasoning is built by SFT of phi-4 on a “teachable” prompt set with teacher answers from o3-mini. Its corpus holds >1.4 M prompt-response pairs (8.3 B tokens) over math, coding, logical puzzles and safety; the final model trains for 16 B tokens on a weighted mixture. Responses are stored as structured “thinking”+“answer” blocks.\n\nTaken together, the fine-tuning pipeline blends high-quality, reasoning-centric, domain-diverse and safety-oriented datasets with PTS-generated DPO pairs to steer phi-4 toward more helpful, harmless and capable behaviour.",
  "4-3 (Reinforcement Learning Data)": "After SFT, phi-4 variants receive preference- and reward-based optimisation.\n\nDirect Preference Optimisation (DPO). Using the Pivotal-Token-Search method, phi-4 produces comparison data where (correct > refusal) is chosen when the base model sometimes solves a question and (refusal > wrong) when it sometimes fails. In Phi-4-Mini’s three-stage reasoning curriculum, the final “Roll-Out DPO” step labels filtered incorrect outputs as dis-preferred and their corrected versions as preferred, creating a 300 K example dataset for training.\n\nOutcome-based Reinforcement Learning (RL). A derivative called Phi-4-reasoning-plus is subjected to a short RL phase that focuses exclusively on mathematical reasoning and encourages the production of longer reasoning traces. The GRPO seed pool contains 72 401 math problems (no coding tasks); each RL iteration samples 64 seeds. The actual reward-learning set comprises ~6 K high-quality math problems with verifiable solutions. Generating extra DPO data targeting questions at the “edge” of phi-4’s competence further sharpens performance.\n\nThrough these preference and reward datasets, the already-aligned SFT model is upgraded into higher-performing versions that reason more deeply and accurately.",
  "4-4 (Data Filtering)": "Phi-4 relies on rigorous filtering and decontamination to build a clean, reasoning-focused corpus.\n\nHigh-quality organic data selection. One of the project’s pillars is the “Curation and Filtering of High-Quality Organic Data”. For phi-4 the team crawls web pages, licensed books and code repositories but keeps only a small fraction of the highest-quality documents, prioritising reasoning-dense sources such as academic papers, educational forums and programming tutorials. Selection uses lightweight non-LLM classifiers trained on ≈10⁶ LLM-generated annotations.\n\nSynthetic-data filtering. When extending Phi-4-Mini with 60 B chain-of-thought tokens, rejection sampling removes incorrect generations before they enter the mix. For vision-speech data, audio is transcribed with an in-house ASR system and only examples with acceptably low word-error-rate are retained.\n\nBenchmark decontamination. The full training corpus passes through the same rigorous decontamination pipeline used for phi-4: n-gram deduplication plus removal of overlaps with a long list of evaluation benchmarks (AIME-2024, MATH, GSM8K, ARC, HumanEval, MT-Bench, PhiBench, etc.). An internal benchmark, PhiBench, monitors remaining contamination and skill coverage. The team reports that even with these defences, re-phrased overlaps cannot be entirely ruled out.\n\nSeed-level filtering for learning stages. Because phi-4 already answers many questions correctly, seeds for further learning are chosen to sit at the edge of its current abilities. SFT experiments for Phi-4-reasoning show that “careful selection and filtering of seeds” is crucial for success.\n\nProcess improvements and scope. Compared with earlier Phi models, the decontamination procedure for phi-4 is explicitly strengthened to avoid unfair evaluation advantages, and the same processing is applied whenever Phi-4 synthetic data are reused in downstream training.\n\nIn summary, phi-4’s pipeline combines classifier-based quality filtering, rejection-sampling, WER thresholds, n-gram deduplication, benchmark exclusion lists and targeted seed selection to produce clean, high-value training data.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [...] The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[sections/Weaknesses]",
      "quote": "Moreover, as our data contains a lot of chain-of-thought examples, phi-4 sometimes gives long elaborate answers even for simple problems—this might make user interactions tedious."
    },
    {
      "source": "[abstract]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects ... With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "For the language model, we train Phi-4-Mini using high-quality, reasoning-rich text data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Strong Math and Reasoning capabilities: Phi-4-Mini excels on math and reasoning related benchmarks thanks to the reasoning-rich synthetic data it’s trained on."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Phi-4-Mini training, we have put special emphasis on the coding capability. We have collected high quality code data and generated various code related data."
    },
    {
      "source": "[sections/Data Methodology]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "first, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "In the pre-training stage, we use large-scale automatic speech recognition (ASR) data to align the audio encoder and Phi-4-Mini in the semantic space."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "With these techniques, we built the 5 trillion pre-training data corpus, which is larger and in higher quality compared to the Phi-3.5-Mini."
    },
    {
      "source": "[sections/3.2 Vision-language training data]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "Building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "The Phi-4 base model was pretrained using large innovative synthetic datasets specifically curated to prioritize reasoning and complex problem-solving."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "The Phi-4 model primarily supports English text, with performance declining for other languages and less represented English varieties compared to standard American English."
    },
    {
      "source": "[sections/Limitations]",
      "quote": "For coding, Phi-4 is mainly trained on Python using common packages, and users should manually verify API uses if scripts involve other languages or packages."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    },
    {
      "source": "[sections/3.1.2 Post-training data]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data, and additionally we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/3.4.2 Post-training Data]",
      "quote": "After the pre-training stage, the model can only perform the ASR task. To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal instead can naturally encode long-form audio in one-shot and perform speech understanding. Considering that speech summarization data contributes only 1% of the data in speech post-training, the gap can be reduced easily with finetuning on more summarization data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini and Phi-4-Multimodal were developed in accordance with Microsoft’s responsible AI principles. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training."
    },
    {
      "source": "[abstract]",
      "quote": "Trained via supervised fine-tuning of Phi-4 on carefully curated set of “teachable” prompts–selected for the right level of complexity and diversity–and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute."
    },
    {
      "source": "[sections/Training data]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. Our SFT data comprises over 1.4 million prompt-response pairs, totaling 8.3 billion unique tokens of reasoning domains such as math and coding, and alignment data for safety and Responsible AI."
    },
    {
      "source": "[pdf_text]",
      "quote": "This stage established the final architecture and training pipeline for Phi-4-reasoning, integrating lessons from both mixture design and teacher quality into a scalable, reasoning-optimized system. On the training side, we conducted SFT over a combined data mixture spanning multiple domains—including math, code, logical puzzles, and safety & responsible AI—using weights derived from the exploration experiments (see Section 3.1). The final model was trained for 16B tokens using this mixture."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[appendix/Post-Training Dataset Details]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it. For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions. For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "With the Phi-4-Mini language model, we conduct a two-stage paradigm for speech and audio training, also known as speech/audio pre-training and post-training."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "To unlock the instruction following capability of Phi-4-Multimodal for variety of speech and audio tasks, we continue to train the model with about 100 M curated speech and audio SFT samples (after weighted up) as the speech post-training stage."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. In the second stage, the model is fine-tuned on a smaller but carefully curated dataset of around 200 K high-quality CoT samples, chosen to cover diverse domains and varying difficulty levels."
    },
    {
      "source": "[sections/3.1.2 Post-training data]",
      "quote": "Compared to Phi-3.5-Mini, Phi-4-Mini includes a significantly larger and more diverse set of function calling and summarization data. Additionally, we synthesize a substantial amount of instruction-following data to enhance the model’s instruction-following capabilities."
    },
    {
      "source": "[sections/3.2 Vision-language training data]",
      "quote": "The Phi-4-Multimodal model’s pre-training phase involves a rich and varied dataset, encompassing interleaved image-text documents, image-text pairs, image grounding data, synthetic datasets from OCR of PDFs and realistic images, and synthesized datasets for chart comprehension. For supervised fine-tuning (SFT), we utilized a combination of a text SFT dataset, publicly available multimodal instruction tuning datasets, and large-scale in-house multimodal instruction tuning datasets that we developed."
    },
    {
      "source": "[sections/3.3 Vision-speech training data]",
      "quote": "For vision-speech data, Phi-4-Multimodal model is trained on a diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. Specifically, we reuse a subset of vision-language SFT data and run in-house text-to-speech (TTS) engine to convert the user queries from texts to audios."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is trained on high-quality datasets with over 1.4M prompts and high-quality answers containing long reasoning traces generated using o3-mini. The datasets used in supervised fine-tuning include topics in STEM (science, technology, engineering, and mathematics), coding, and safety-focused tasks."
    },
    {
      "source": "[pdf_text]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds. We generate both reasoning traces and final responses and combine them into a structured format consisting of “thinking” and “answer” blocks."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/A.1 Refusal to Hallucinate]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[abstract]",
      "quote": "We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces."
    },
    {
      "source": "[sections/Training data]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning. The seed dataset for GRPO consisted of 72,401 mathematical problems (prompts without solutions), from which we subsample 64 problem seeds per RL iteration. The seed set was curated from the larger training corpus described in Section 2. We would like to highlight that the seed data contained no coding exercises, as perhaps evident by the LiveCodeBench scores of our model."
    },
    {
      "source": "[pdf_text]",
      "quote": "By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially ..."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300 K preference samples for DPO training."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "The continued training of Phi-4-Mini for reasoning proceeds in three distinct stages. 3) Roll-Out DPO: finally, in the third stage, we label filtered incorrect outputs as “dis-preferred” and their corrected counterparts as ‘preferred’, compiling a new dataset of 300K preference samples for DPO training."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning-plus is further trained with Reinforcement Learning on a small set of ∼6K high-quality math-focused problems with verifiable solutions."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. The RL training focused exclusively on mathematical reasoning."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[pdf_text]",
      "quote": "The development of phi-4 is guided by three core pillars: … 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Targeting High-quality Web Data. We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials)."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. In addition, we employ several methods for filtering organic data sources that are used both as complementary datasets in the pretraining and as seeds for generating synthetic data."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "Compared with Phi-3.5-Mini, we improved the quality of the pre-training data from several key aspects: 1. Better data filtering: By using an enhanced quality classifier, which is trained on a larger curated dataset consisting of cleaner positive and negative samples, we end up with better filtering quality across multiple languages with various aspects (e.g. toxic, obscure, scientific, etc.), leading to a more comprehensive and controllable filtering strategy overall."
    },
    {
      "source": "[sections/3.1.1 Pre-training data]",
      "quote": "We incorporate Phi-4 synthetic data [AAB+24] into this model training with the same processing and decontamination."
    },
    {
      "source": "[sections/Seeds database]",
      "quote": "Given the strong baseline reasoning capabilities of Phi-4, many of the initial seed questions are already handled competently by the base model. To make further learning impactful, we specifically target seeds situated at the edge of Phi-4’s current abilities."
    },
    {
      "source": "[sections/Training data]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining … 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We collected a wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). To capture the long tail of information-rich web sources (e.g., forums, blogs, course material, domain-specific wikis), we took the approach of selecting a small fraction of highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on ∼10^6 LLM-generated annotations."
    },
    {
      "source": "[sections/Benchmarking Considerations]",
      "quote": "To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4’s development. Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora, creating a risk of data contamination. Although we took extensive measures to deduplicate and decontaminate our training data, including standard n-gram deduplication and decontamination, these methods are not effective against all scenarios, including rephrasing, which leaves some uncertainty about the true extent of generalization."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "first, building on Phi-4-Mini, the model is pre-trained on approximately 60 billion reasoning CoT tokens generated by frontier reasoning LLMs, after which rejection sampling is employed to filter out incorrect outputs."
    },
    {
      "source": "[sections/3.3 Vision-speech training data]",
      "quote": "For vision-speech data, Phi-4-Multimodal model is trained on a diverse set of synthetic vision-speech data, covering single-frame and multi-frame scenarios. We also measure the quality of the synthetic speech by transcribing the audio with in-house ASR model and calculating the word error rate (WER) between original text and transcription. Our final vision-speech data is generated with the WER-based filtering to ensure the quality."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pass the full training data through the same rigorous decontamination process used Phi-4 [2] for decontaminating against popular reasoning as well as general-purpose benchmarks including many not discussed in this report. The full list of benchmarks decontaminated against is: AIME-2024, MATH, GPQA, LiveCodeBench, Codeforces, OmniMATH, SWE-Bench Verified, SimpleQA, DROP, AGIEval, ARC-Challenge, ARC-Easy, CommonsenseQA, GPQA, GSM8k, HellaSwag, HumanEval, MBPP, OpenBookQA, PIQA, WinoGrande, ArenaHard, MT-Bench, PhiBench."
    },
    {
      "source": "[pdf_text]",
      "quote": "The supervised fine tuning for Phi-4-reasoning uses synthetically generated responses for our curated set of seeds. ... We find in our SFT experiments that even in this simple generation setting, careful selection and filtering of seeds to be crucial for the model’s success."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}