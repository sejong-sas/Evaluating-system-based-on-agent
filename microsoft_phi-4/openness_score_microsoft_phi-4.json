{
  "model": "microsoft/phi-4",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Multiple quotes specify the model is distributed under the MIT licence and a LICENSE file is present, which grants all four freedoms (use, modification, redistribution, commercial use)."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored technical report titled “Phi-4 Technical Report” is linked on arXiv (https://arxiv.org/pdf/2412.08905)."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Training hardware is quoted as “1920 H100-80G” GPUs—both the type and the exact quantity are disclosed."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "The only software detail is \"transformers_version: 4.47.0\", which is merely an export/inference library; no training-stack components (Deepspeed, Megatron, optimizers, etc.) are quoted."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://learn.microsoft.com/en-us/microsoft-edge/web-platform/prompt-api, https://learn.microsoft.com/en-us/microsoft-edge/web-platform/writing-assistance-apis, https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/. Official API documentation available; includes key issuance and authorization details."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Many hyper-parameters are given (10 T tokens, LR = 3e-4, weight-decay = 0.1, batch = 5 760, curriculum with mid-training 4 K→16 K), but the full reproducible pipeline (code, optimizer details, exact data order) is not provided."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The paper explains the SFT recipe and precise labelling rules and mentions iterative DPO, but omits full hyper-parameters and scripts; thus only partial disclosure."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "DPO and rejection-sampling are described with pair-construction rules (correct > refusal, etc.), but not enough details (e.g., loss weights, epochs) to be fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources and mixture proportions (40 % synthetic, 30 % web, 20 % code, 10 % books/Q&A) plus 50 synthetic dataset types are listed, but the raw datasets themselves and full access paths are not released."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Rules for constructing SFT and DPO datasets are given, but the datasets’ full contents, sizes, or download links are not disclosed."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions public safety datasets and AgentKit trajectories, but no full dumps or exact sizes/URLs, so only partial disclosure."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "The report details a decontamination pipeline: (i) hybrid n-gram algorithm using 13-gram and 7-gram features, (ii) explicit target benchmark list (ARC-Easy, MBPP, GSM8K, etc.) against which matches are removed, and (iii) statement that the process is an improved multi-stage flow over previous Phi models—meeting the rubric’s requirements for a reconstructable pipeline."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Multiple quotes specify the model is distributed under the MIT licence and a LICENSE file is present, which grants all four freedoms (use, modification, redistribution, commercial use)."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored technical report titled “Phi-4 Technical Report” is linked on arXiv (https://arxiv.org/pdf/2412.08905)."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Training hardware is quoted as “1920 H100-80G” GPUs—both the type and the exact quantity are disclosed."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "The only software detail is \"transformers_version: 4.47.0\", which is merely an export/inference library; no training-stack components (Deepspeed, Megatron, optimizers, etc.) are quoted."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://learn.microsoft.com/en-us/microsoft-edge/web-platform/prompt-api, https://learn.microsoft.com/en-us/microsoft-edge/web-platform/writing-assistance-apis, https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/. Official API documentation available; includes key issuance and authorization details."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Many hyper-parameters are given (10 T tokens, LR = 3e-4, weight-decay = 0.1, batch = 5 760, curriculum with mid-training 4 K→16 K), but the full reproducible pipeline (code, optimizer details, exact data order) is not provided."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The paper explains the SFT recipe and precise labelling rules and mentions iterative DPO, but omits full hyper-parameters and scripts; thus only partial disclosure."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "DPO and rejection-sampling are described with pair-construction rules (correct > refusal, etc.), but not enough details (e.g., loss weights, epochs) to be fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources and mixture proportions (40 % synthetic, 30 % web, 20 % code, 10 % books/Q&A) plus 50 synthetic dataset types are listed, but the raw datasets themselves and full access paths are not released."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Rules for constructing SFT and DPO datasets are given, but the datasets’ full contents, sizes, or download links are not disclosed."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions public safety datasets and AgentKit trajectories, but no full dumps or exact sizes/URLs, so only partial disclosure."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "The report details a decontamination pipeline: (i) hybrid n-gram algorithm using 13-gram and 7-gram features, (ii) explicit target benchmark list (ARC-Easy, MBPP, GSM8K, etc.) against which matches are removed, and (iii) statement that the process is an improved multi-stage flow over previous Phi models—meeting the rubric’s requirements for a reconstructable pipeline."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}