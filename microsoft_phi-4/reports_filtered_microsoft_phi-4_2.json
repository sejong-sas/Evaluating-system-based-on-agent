{
  "1-5 (Architecture)": "The available statements describe several closely related variants that all carry the “phi-4” name.  The core phi-4 base model is said to be “based on a decoder-only transformer architecture … with 14 B parameters and a default context length of 4096.”  Its architecture “closely follows phi-3-medium,” but with two main differences: it switches from the 2 K-token sliding-window attention used in phi-3-medium to “full attention over the 4 K context length,” and it later goes through “a mid-training stage where the context length is increased from the original 4 K to 16 K.”  One quote repeats that the same 14 B-parameter decoder-only design is later “extended to a 16 K context length during mid-training.”\n\nSeveral specialized descendants retain the same backbone while changing auxiliary mechanisms or operating ranges:\n•  Phi-4-reasoning keeps “the same architecture … with two key modifications,” the most important being an “Increased Token Length” in which “the RoPE base frequency was doubled, and the model was trained for a maximum length of 32 K tokens.”  A further derivative, Phi-4-reasoning-plus, was “trained with 32 K maximum length but has been tested … up to 64 K tokens,” and inference experiments set generation limits as high as 65 536 tokens without changing RoPE parameters.\n•  Phi-4-Multimodal “keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules” and uses “a novel ‘mixture of LoRAs’” to add modality-specific abilities.\n\nSmaller releases adopt the same design principles at reduced scale: “Phi-4-Mini is a 3.8-billion-parameter language model” that “consists of 32 Transformer layers with hidden state size of 3 072 and tied input / output embedding.”  Each block “includes an attention mechanism based on Group Query Attention (GQA), which optimizes key and value memory usage for long-context generation.”  All Phi-4-Mini models are “based on decoder-only Transformer and support 128 K context length based on LongRoPE.”  The authors note that, despite the small 3.8 B size, Phi-4-Mini “significantly” outperforms multiple 7 B–8 B open-source systems on math and coding tasks.\n\nAcross all lines the common themes are: (1) decoder-only transformer backbone, (2) emphasis on extending context length—from 4 K (base), to 16 K (mid-training), 32 K (reasoning), and 128 K (Mini with LongRoPE), (3) parameter counts of 14 B for the flagship and 3.8 B for the Mini, and (4) auxiliary innovations such as GQA for memory savings and mixture-of-LoRAs for multimodal ability.",
  "1-6 (Tokenizer)": "Every passage mentioning the tokenizer ties it explicitly to the phi-4 family.  For the 14 B base model, the architecture “now use[s] the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100 352 (including unused tokens).”  The smaller Phi-4-Mini line “features an expanded vocabulary size of 200 K tokens” and, in two equivalent phrasings, “All Phi-4-Mini models use the tokenizer o200k_base_tiktoken … with a vocabulary size of 200 064 intended to support multilingual and multimodal input and output more efficiently.”  Thus, phi-4 employs tiktoken throughout, but the exact vocabulary differs: ~100 K for the original 14 B model and ~200 K for the Mini variants to strengthen multilingual coverage.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "The only concrete software-stack information concerns the supervised-fine-tuning and reinforcement-learning phases for Phi-4-reasoning.  During SFT, the 14 B-parameter phi-4 backbone is trained “over roughly 16 K steps, with a global batch size of 32 and a context length of 32 K tokens.”  Optimization uses “AdamW with a learning rate of 10⁻⁵, linear warm up over 450 steps, and a weight decay of 10⁻⁴.”  After SFT, the team applies outcome-based reinforcement learning: they “utilized the Group Relative Policy Optimization (GRPO) algorithm … incorporating modifications tailored specifically to [the phi-4] setup.”  No other implementation libraries or framework versions are named in the provided text.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[pdf_text]",
      "quote": "While phi-4 achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual knowledge."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer and support 128K context length based on LongRoPE."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Mini consist of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5. Each Transformer block includes an attention mechanism based on Group Query Attention (GQA), which optimizes key and value memory (KV cache) usage for long-context generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal employs a novel “mixture of LoRAs” technique, enabling multimodal capabilities by integrating modality-specific LoRAs while keeping the base language model entirely frozen."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal 5.6B"
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal keeps the language model entirely frozen by only incorporating additional fine-tunable LoRA modules."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications."
    },
    {
      "source": "[pdf_text]",
      "quote": "The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE [51] base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Phi-4-reasoning-plus was trained with 32k maximum length but has been tested to perform well on select benchmarks for up to 64k tokens."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k_base_tiktoken with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently. All models are based on decoder-only Transformer and support 128K context length based on LongRoPE."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "Phi-4-Mini consists of 32 Transformer layers with hidden state size of 3,072 and tied input / output embedding which reduces the memory consumption significantly while providing much wider coverage of vocabularies compared Phi-3.5."
    },
    {
      "source": "[pdf_text]",
      "quote": "Despite having only 3.8B parameters, Phi-4-Mini reasoning-enhanced model outperforms DeepSeek-R1-Distill-Llama-8B [GYZ+25], Bespoke-Stratos-7B [Lab25], OpenThinker-7B [Tea25a], and achieves performance comparable to DeepSeek-R1-Distill-Qwen-7B as shown in the Table 9."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. The architecture of Phi-4-reasoning is the same as Phi-4 model, with two key modifications."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "• Increased Token Length: The base model (Phi-4) originally supported a maximum token length of 16K. To accommodate additional reasoning tokens, the RoPE [51] base frequency was doubled, and the model was trained for a maximum length of 32K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Phi-4-reasoning and Phi-4-reasoning-plus evaluations on AIME, HMMT, GPQA, and Codeforces we use 65,536 as the maximum number of tokens for generation without changing any RoPE parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "†For Phi-4 we use temp=0.8 for the reasoning benchmarks, and 0.0 for the general-purpose benchmarks."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens)"
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation."
    },
    {
      "source": "[pdf_text]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k base tiktoken 2 with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[pdf_text]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/2503.01743]",
      "quote": "All Phi-4-Mini models use the tokenizer o200k_base_tiktoken with a vocabulary size of 200,064 intended to support multilingual and multimodal input and output more efficiently."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model... Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. We use AdamW with a learning rate of 10−5, linear warm up over 450 steps, and a weight decay of 10−4."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning. Training is run over roughly 16K steps, with a global batch size of 32 and a context length of 32K tokens. We use AdamW with a learning rate of 10−5, linear warm up over 450 steps, and a weight decay of 10−4."
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "Following the supervised fine-tuning (SFT) stage described previously, we applied outcome-based reinforcement learning (RL) to further enhance the reasoning capabilities of the Phi-4-reasoning model following a similar recipe to [48, 21, 36]. Specifically, we utilized the Group Relative Policy Optimization (GRPO) algorithm [48, 21], incorporating modifications tailored specifically to our setup."
    }
  ]
}