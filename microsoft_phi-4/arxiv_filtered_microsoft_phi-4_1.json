{
  "1-1 (Weights)": "The available quotes state that Phi-4-Multimodal is explicitly described as \"the first open-sourced model with speech summarization capability\" and as \"the smallest open-sourced multi-modal LLM\"—wording that indicates its checkpoint is publicly released. The material also confirms that an \"officially released checkpoint\" exists for Phi-4-Mini, whereas the \"reasoning-enhanced Phi-4-Mini\" is \"a separate model … in a preview stage\" that \"will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal.\" Finally, the data used for an experimental reasoning model \"has not been applied to the officially released checkpoint Phi-4-Mini,\" underscoring that the released Phi-4-Mini weights do not incorporate that additional dataset.",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "Multiple official technical reports and papers are cited for the Phi-4 family. The \"Phi-4 Technical Report\" repeatedly states, \"We present phi-4, a 14-billion parameter language model\" trained with a curriculum focused on high-quality data. Complementing this, another sentence notes that Phi-4 \"further advances performance … by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training.\" \n\nFollow-on work introduces specialized variants: \"Phi-4-Mini and Phi-4-Multimodal\" are described as \"compact yet highly capable\" models, with the \"Phi-4-Mini Technical Report\" subtitled \"Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs.\"  A separate \"Phi-4-reasoning Technical Report\" presents \"Phi-4-reasoning, a 14-billion parameter reasoning model … supervised fine-tuned on Phi-4 [2]\" and also details \"Phi-4-reasoning-plus\" obtained through an additional reinforcement-learning stage.  The authors note that they \"use the benchmarks from the Phi-4 report [2]\" to evaluate these derivatives.  Collectively, the quoted material emphasizes that the Phi-4 series leverages carefully curated or synthesized data—\"the Phi family of models … have shown that carefully curated and synthesized data enables Small Language Models (SLMs) to achieve highly competitive performance\"—and positions Phi-4 and its descendants as state-of-the-art 14-billion-parameter small language models aimed at strong reasoning and multimodal capabilities.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "1 Please note that reasoning-enhanced Phi-4-Mini is a separate model and currently in a preview stage and will not be released concurrently with Phi-4-Mini and Phi-4-Multimodal."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the first open-sourced model with speech summarization capability."
    },
    {
      "source": "[pdf_text]",
      "quote": "• Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    },
    {
      "source": "[sections/Reasoning Training]",
      "quote": "This data has been utilized exclusively for the experimental reasoning model and has not been applied to the officially released checkpoint Phi-4-Mini."
    },
    {
      "source": "[sections/Speech and Audio Benchmarks]",
      "quote": "Phi-4-Multimodal is the smallest open-sourced multi-modal LLM that behaves better than the open-sourced Qwen2-audio [CXY+24] with ∼2x in size."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The Phi family of models [AJA+24, AAB+24] have shown that carefully curated and synthesized data enables Small Language Models (SLMs) to achieve highly competitive performance despite having a significantly smaller number of parameters."
    },
    {
      "source": "[title]",
      "quote": "Phi-4-reasoning Technical Report"
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[sections/General-purpose Benchmarks]",
      "quote": "First, we use the benchmarks from the Phi-4 report [2]."
    },
    {
      "source": "[sections/2412.08905]",
      "quote": "Phi-4 Technical Report ... We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[title]",
      "quote": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs"
    },
    {
      "source": "[sections/2504.21318]",
      "quote": "We present Phi-4-reasoning, a 14-billion parameter model supervised fine-tuned on Phi-4 [2], and Phi-4-reasoning-plus obtained by a further round of reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Phi-4-reasoning is obtained by supervised finetuning (SFT) of the 14-billion parameter Phi-4 model [2], prior to any reinforcement learning."
    },
    {
      "source": "[pdf_text]",
      "quote": "First, we use the benchmarks from the Phi-4 report [2]."
    }
  ]
}